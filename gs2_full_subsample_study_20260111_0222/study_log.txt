[2026-01-11 02:22:55] === Starting Complete Generalization Study ===
[2026-01-11 02:22:55] Training group sizes: 64
[2026-01-11 02:22:55] Training subsample sizes: 64
[2026-01-11 02:22:55] Test groups: 606
[2026-01-11 02:22:55] Test subsample: 2424
[2026-01-11 02:22:55] Number of trials per size: 1
[2026-01-11 02:22:55] Output directory: gs2_full_subsample_study_20260111_0222
[2026-01-11 02:22:55] Total training runs planned: 2
[2026-01-11 02:22:55] Validating environment...
[2026-01-11 02:22:55] Environment validation passed
[2026-01-11 02:22:55] Configuration saved to: gs2_full_subsample_study_20260111_0222/study_config.txt
[2026-01-11 02:22:55] Skipping dataset preparation (--skip-data-prep)
[2026-01-11 02:22:55] === STEP 2: Model Training ===
[2026-01-11 02:22:55] Training models sequentially with 1 trials per training size
[2026-01-11 02:22:55] Starting training with train_groups=64, train_subsample=64 (1 trials)
[2026-01-11 02:22:55] Training models for train_subsample=64, train_groups=64 (Trial 1/1)
[2026-01-11 02:22:55] EXECUTING: PtychoPINN training (subsample=64, groups=64, trial=1)
[2026-01-11 02:22:55] COMMAND: python scripts/training/train.py --config 'configs/comparison_config_gs2_quick.yaml' \
            --train_data_file 'prepare_1e4_photons_5k/dataset/train.npz' \
            --test_data_file 'prepare_1e4_photons_5k/dataset/test.npz' \
            --n_groups 64 --n_subsample 64 \
            --neighbor_count 7 \
            --output_dir 'gs2_full_subsample_study_20260111_0222/train_subsample_64_groups_64/trial_1/pinn_run'
2026-01-11 02:22:55.595297: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1768126975.606679  898802 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1768126975.610593  898802 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1768126975.620735  898802 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1768126975.620747  898802 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1768126975.620749  898802 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1768126975.620750  898802 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2026-01-11 02:22:55.623530: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2026-01-11 02:22:58,410 - INFO - Configuration setup complete
2026-01-11 02:22:58,410 - INFO - Final configuration: TrainingConfig(model=ModelConfig(N=64, gridsize=1, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', object_big=True, probe_big=True, probe_mask=False, pad_object=True, probe_scale=4.0, gaussian_smoothing_sigma=0.0), train_data_file=PosixPath('prepare_1e4_photons_5k/dataset/train.npz'), test_data_file=PosixPath('prepare_1e4_photons_5k/dataset/test.npz'), batch_size=16, nepochs=50, mae_weight=0.0, nll_weight=1.0, realspace_mae_weight=0.0, realspace_weight=0.0, nphotons=1000000000.0, n_groups=64, n_images=512, n_subsample=64, subsample_seed=None, neighbor_count=7, enable_oversampling=False, neighbor_pool_size=None, positions_provided=True, probe_trainable=False, intensity_scale_trainable=True, output_dir=PosixPath('gs2_full_subsample_study_20260111_0222/train_subsample_64_groups_64/trial_1/pinn_run'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
2026-01-11 02:22:58,410 - INFO - Independent sampling control: subsampling 64 images, using 64 groups for training
2026-01-11 02:22:58,410 - INFO - Starting training with n_subsample=64, n_groups=64, stitching=disabled
2026-01-11 02:22:58,410 - INFO - Loading data from prepare_1e4_photons_5k/dataset/train.npz with n_images=64, n_subsample=64
2026-01-11 02:22:58,760 - INFO - Independent sampling: subsampling 64 images from 2576 total
2026-01-11 02:22:58,760 - INFO - Randomly subsampled 64 images
diff3d shape: (64, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (64,)
objectGuess shape: (232, 232)
xcoords shape: (64,)
ycoords shape: (64,)
xcoords_start shape: (64,)
ycoords_start shape: (64,)
2026-01-11 02:22:59,075 - INFO - Overriding nphotons from config (1.0e+09) with value from dataset metadata: 1.0e+04
2026-01-11 02:22:59,075 - INFO - Loading data from prepare_1e4_photons_5k/dataset/test.npz with n_images=None, n_subsample=None
2026-01-11 02:22:59,402 - INFO - Using full dataset of 2424 images
diff3d shape: (2424, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (2424,)
objectGuess shape: (232, 232)
xcoords shape: (2424,)
ycoords shape: (2424,)
xcoords_start shape: (2424,)
ycoords_start shape: (2424,)
2026-01-11 02:22:59,418 - INFO - Loaded test data from prepare_1e4_photons_5k/dataset/test.npz
2026-01-11 02:22:59,418 - INFO - Backend dispatcher: params.cfg synchronized with TrainingConfig (backend=tensorflow)
2026-01-11 02:22:59,418 - INFO - Backend dispatcher: routing to TensorFlow workflow (ptycho.workflows.components)
2026-01-11 02:22:59,419 - INFO - [OVERSAMPLING DEBUG] generate_grouped_data called with: nsamples=64, n_points=64, C=1, K=7
2026-01-11 02:22:59,419 - INFO - [OVERSAMPLING DEBUG] Parameters: gridsize=1, N=64, sequential_sampling=False
2026-01-11 02:22:59,419 - INFO - [OVERSAMPLING DEBUG] Oversampling flags: enable_oversampling=False, neighbor_pool_size=None, effective_K=7
2026-01-11 02:22:59,419 - INFO - [OVERSAMPLING DEBUG] Oversampling check: nsamples > n_points = 64 > 64 = False
2026-01-11 02:22:59,419 - INFO - [OVERSAMPLING DEBUG] Oversampling check: C > 1 = 1 > 1 = False
2026-01-11 02:22:59,419 - INFO - [OVERSAMPLING DEBUG] needs_oversampling = False
DEBUG: nsamples: 64, gridsize: 1 (using efficient random sample-then-group strategy)
2026-01-11 02:22:59,419 - INFO - Using efficient random sampling strategy for gridsize=1
2026-01-11 02:22:59,419 - INFO - [OVERSAMPLING DEBUG] Taking efficient branch: standard sample-then-group
2026-01-11 02:22:59,419 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently called with: nsamples=64, K=7, C=1
2026-01-11 02:22:59,419 - INFO - Generating 64 groups efficiently from 64 points (K=7, C=1)
2026-01-11 02:22:59,419 - INFO - [OVERSAMPLING DEBUG] Standard case: using 64 groups from 64 points
2026-01-11 02:22:59,419 - INFO - [OVERSAMPLING DEBUG] Using all 64 points as seeds (no sampling needed)
2026-01-11 02:22:59,419 - INFO - Using all 64 points as seeds
2026-01-11 02:22:59,419 - INFO - Using seed indices directly for C=1 (gridsize=1) - no neighbor search needed
2026-01-11 02:22:59,419 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently completed: generated 64 groups
2026-01-11 02:22:59,419 - INFO - Successfully generated 64 groups with shape (64, 1)
2026-01-11 02:22:59,419 - INFO - [OVERSAMPLING DEBUG] Generated 64 groups in total
2026-01-11 02:22:59,419 - INFO - Generated 64 groups efficiently
I0000 00:00:1768126979.545767  898802 gpu_process_state.cc:208] Using CUDA malloc Async allocator for GPU: 0
I0000 00:00:1768126979.547169  898802 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22259 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:04:00.0, compute capability: 8.6
INFO: Using pre-computed 'Y' array from the input file.
neighbor-sampled diffraction shape (64, 64, 64, 1)
loader: using provided ground truth patches.
INFO: None
<PtychoDataContainer X=(64, 64, 64, 1) mean=0.407 Y_I=(64, 64, 64, 1) mean=0.564 Y_phi=(64, 64, 64, 1) mean=-0.045 coords_nominal=(64, 1, 2, 1) mean=0.000 coords_true=(64, 1, 2, 1) mean=0.000 probe=(64, 64) mean_amplitude=0.006 norm_Y_I=<scalar> nn_indices=(64, 1) global_offsets=(64, 1, 2, 1) local_offsets=(64, 1, 2, 1)>
2026-01-11 02:22:59,858 - INFO - [OVERSAMPLING DEBUG] generate_grouped_data called with: nsamples=64, n_points=2424, C=1, K=7
2026-01-11 02:22:59,858 - INFO - [OVERSAMPLING DEBUG] Parameters: gridsize=1, N=64, sequential_sampling=False
2026-01-11 02:22:59,858 - INFO - [OVERSAMPLING DEBUG] Oversampling flags: enable_oversampling=False, neighbor_pool_size=None, effective_K=7
2026-01-11 02:22:59,858 - INFO - [OVERSAMPLING DEBUG] Oversampling check: nsamples > n_points = 64 > 2424 = False
2026-01-11 02:22:59,858 - INFO - [OVERSAMPLING DEBUG] Oversampling check: C > 1 = 1 > 1 = False
2026-01-11 02:22:59,858 - INFO - [OVERSAMPLING DEBUG] needs_oversampling = False
DEBUG: nsamples: 64, gridsize: 1 (using efficient random sample-then-group strategy)
2026-01-11 02:22:59,858 - INFO - Using efficient random sampling strategy for gridsize=1
2026-01-11 02:22:59,858 - INFO - [OVERSAMPLING DEBUG] Taking efficient branch: standard sample-then-group
2026-01-11 02:22:59,859 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently called with: nsamples=64, K=7, C=1
2026-01-11 02:22:59,859 - INFO - Generating 64 groups efficiently from 2424 points (K=7, C=1)
2026-01-11 02:22:59,859 - INFO - [OVERSAMPLING DEBUG] Standard case: using 64 groups from 2424 points
2026-01-11 02:22:59,859 - INFO - [OVERSAMPLING DEBUG] Randomly sampled 64 seed points
2026-01-11 02:22:59,859 - INFO - Sampled 64 seed points from 2424 total points
2026-01-11 02:22:59,859 - INFO - Using seed indices directly for C=1 (gridsize=1) - no neighbor search needed
2026-01-11 02:22:59,859 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently completed: generated 64 groups
2026-01-11 02:22:59,859 - INFO - Successfully generated 64 groups with shape (64, 1)
2026-01-11 02:22:59,859 - INFO - [OVERSAMPLING DEBUG] Generated 64 groups in total
2026-01-11 02:22:59,859 - INFO - Generated 64 groups efficiently
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1768126981.261629  898802 service.cc:152] XLA service 0x3eb31fe0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1768126981.261649  898802 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6
2026-01-11 02:23:01.277914: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
I0000 00:00:1768126981.294396  898802 cuda_dnn.cc:529] Loaded cuDNN version 91002
I0000 00:00:1768126981.490675  898802 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
INFO: Using pre-computed 'Y' array from the input file.
neighbor-sampled diffraction shape (64, 64, 64, 1)
loader: using provided ground truth patches.
INFO: None
<PtychoDataContainer X=(64, 64, 64, 1) mean=0.407 Y_I=(64, 64, 64, 1) mean=0.563 Y_phi=(64, 64, 64, 1) mean=-0.032 coords_nominal=(64, 1, 2, 1) mean=0.000 coords_true=(64, 1, 2, 1) mean=0.000 probe=(64, 64) mean_amplitude=0.006 norm_Y_I=<scalar> nn_indices=(64, 1) global_offsets=(64, 1, 2, 1) local_offsets=(64, 1, 2, 1)>
DEBUG: Setting probe to tf.Tensor(
[[[ 0.13510734-0.08366051j]
  [-0.08103182-0.0294797j ]
  [ 0.0637618 +0.02550233j]
  ...
  [-0.05742923+0.10465012j]
  [ 0.061039  +0.02731375j]
  [-0.07001488-0.0336627j ]]

 [[-0.08103182-0.0294797j ]
  [ 0.11364837-0.04388378j]
  [-0.08643168+0.03117809j]
  ...
  [ 0.02872955+0.03712314j]
  [-0.08476108+0.03176096j]
  [ 0.10533146-0.04452544j]]

 [[ 0.0637618 +0.02550233j]
  [-0.08643168+0.03117809j]
  [ 0.06238977-0.04183936j]
  ...
  [-0.01890127-0.04941669j]
  [ 0.06107059-0.04259919j]
  [-0.07948506+0.03188787j]]

 ...

 [[-0.05742923+0.10465012j]
  [ 0.02872955+0.03712314j]
  [-0.01890127-0.04941669j]
  ...
  [-0.00577373-0.08714116j]
  [-0.01781569-0.0517069j ]
  [ 0.02285527+0.04448525j]]

 [[ 0.061039  +0.02731375j]
  [-0.08476108+0.03176096j]
  [ 0.06107059-0.04259919j]
  ...
  [-0.01781569-0.0517069j ]
  [ 0.05980607-0.0434007j ]
  [-0.07803912+0.03257083j]]

 [[-0.07001488-0.0336627j ]
  [ 0.10533146-0.04452544j]
  [-0.07948506+0.03188787j]
  ...
  [ 0.02285527+0.04448525j]
  [-0.07803912+0.03257083j]
  [ 0.09786177-0.04529788j]]], shape=(64, 64, 1), dtype=complex64) in params
DEBUG: Setting intensity_scale to 3.125 in params
DEBUG: Setting probe to tf.Tensor(
[[[ 0.13510734-0.08366051j]
  [-0.08103182-0.0294797j ]
  [ 0.0637618 +0.02550233j]
  ...
  [-0.05742923+0.10465012j]
  [ 0.061039  +0.02731375j]
  [-0.07001488-0.0336627j ]]

 [[-0.08103182-0.0294797j ]
  [ 0.11364837-0.04388378j]
  [-0.08643168+0.03117809j]
  ...
  [ 0.02872955+0.03712314j]
  [-0.08476108+0.03176096j]
  [ 0.10533146-0.04452544j]]

 [[ 0.0637618 +0.02550233j]
  [-0.08643168+0.03117809j]
  [ 0.06238977-0.04183936j]
  ...
  [-0.01890127-0.04941669j]
  [ 0.06107059-0.04259919j]
  [-0.07948506+0.03188787j]]

 ...

 [[-0.05742923+0.10465012j]
  [ 0.02872955+0.03712314j]
  [-0.01890127-0.04941669j]
  ...
  [-0.00577373-0.08714116j]
  [-0.01781569-0.0517069j ]
  [ 0.02285527+0.04448525j]]

 [[ 0.061039  +0.02731375j]
  [-0.08476108+0.03176096j]
  [ 0.06107059-0.04259919j]
  ...
  [-0.01781569-0.0517069j ]
  [ 0.05980607-0.0434007j ]
  [-0.07803912+0.03257083j]]

 [[-0.07001488-0.0336627j ]
  [ 0.10533146-0.04452544j]
  [-0.07948506+0.03188787j]
  ...
  [ 0.02285527+0.04448525j]
  [-0.07803912+0.03257083j]
  [ 0.09786177-0.04529788j]]], shape=(64, 64, 1), dtype=complex64) in params
Current Parameters:
--------------------
N: 64
amp_activation: sigmoid
backend: tensorflow
batch_size: 16
bigN: 64
big_gridsize: 10
data_source: generic
debug: True
default_probe_scale: 0.7
enable_oversampling: False
gaussian_smoothing_sigma: 0.0
gridsize: 1
h5_path: wts.h5
intensity_scale: 3.125
intensity_scale.trainable: True
label: 
mae_weight: 0.0
max_position_jitter: 10
model_type: pinn
n_filters_scale: 2
n_groups: 64
n_images: 512
n_subsample: 64
neighbor_count: 7
nepochs: 50
nimgs_test: 3
nimgs_train: 9
nll_weight: 1.0
nphotons: 10000.0
npseed: 42
object.big: True
offset: 4
outer_offset_test: None
outer_offset_train: None
output_prefix: gs2_full_subsample_study_20260111_0222/train_subsample_64_groups_64/trial_1/pinn_run
pad_object: True
positions.provided: True
probe:
  shape: (64, 64, 1)
  mean: -0.000-0.000j
  std: 0.627
  min: -2.156-0.387j
  max: 2.182-0.308j
probe.big: True
probe.mask: False
probe.trainable: False
probe_scale: 4.0
realspace_mae_weight: 0.0
realspace_weight: 0.0
sequential_sampling: False
set_phi: False
sim_jitter_scale: 0.0
size: 392
test_data_file_path: prepare_1e4_photons_5k/dataset/test.npz
torch_loss_mode: poisson
train_data_file_path: prepare_1e4_photons_5k/dataset/train.npz
tv_weight: 0.0
use_xla_translate: True
DEBUG _flat_to_channel: gridsize from global params: 1
DEBUG _flat_to_channel: N from global params: 64
DEBUG _flat_to_channel: input shape=(64, 64, 64, 1), reshaping to (-1, 1, 64, 64)
Epoch 1/50
DEBUG _flat_to_channel: gridsize from parameter: 1
DEBUG _flat_to_channel: N from parameter: 74
DEBUG _flat_to_channel: input shape=(None, 74, 74, 1), reshaping to (-1, 1, 74, 74)
DEBUG _flat_to_channel: gridsize from parameter: 1
DEBUG _flat_to_channel: N from parameter: 74
DEBUG _flat_to_channel: input shape=(None, 74, 74, 1), reshaping to (-1, 1, 74, 74)
DEBUG _flat_to_channel: gridsize from parameter: 1
DEBUG _flat_to_channel: N from parameter: 74
DEBUG _flat_to_channel: input shape=(None, 74, 74, 1), reshaping to (-1, 1, 74, 74)
DEBUG _flat_to_channel: gridsize from parameter: 1
DEBUG _flat_to_channel: N from parameter: 74
DEBUG _flat_to_channel: input shape=(None, 74, 74, 1), reshaping to (-1, 1, 74, 74)
input shape (None, 64, 64, 1)
DEBUG _flat_to_channel: gridsize from parameter: 1
DEBUG _flat_to_channel: N from parameter: 64
DEBUG _flat_to_channel: input shape=(None, 64, 64, 1), reshaping to (-1, 1, 64, 64)
2026-01-11 02:23:02,269 - WARNING - You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.
DEBUG _flat_to_channel: gridsize from parameter: 1
DEBUG _flat_to_channel: N from parameter: 74
DEBUG _flat_to_channel: input shape=(None, 74, 74, 1), reshaping to (-1, 1, 74, 74)
DEBUG _flat_to_channel: gridsize from parameter: 1
DEBUG _flat_to_channel: N from parameter: 74
DEBUG _flat_to_channel: input shape=(None, 74, 74, 1), reshaping to (-1, 1, 74, 74)
DEBUG _flat_to_channel: gridsize from parameter: 1
DEBUG _flat_to_channel: N from parameter: 74
DEBUG _flat_to_channel: input shape=(None, 74, 74, 1), reshaping to (-1, 1, 74, 74)
DEBUG _flat_to_channel: gridsize from parameter: 1
DEBUG _flat_to_channel: N from parameter: 74
DEBUG _flat_to_channel: input shape=(None, 74, 74, 1), reshaping to (-1, 1, 74, 74)
input shape (None, 64, 64, 1)
DEBUG _flat_to_channel: gridsize from parameter: 1
DEBUG _flat_to_channel: N from parameter: 64
DEBUG _flat_to_channel: input shape=(None, 64, 64, 1), reshaping to (-1, 1, 64, 64)
2026-01-11 02:23:05,875 - WARNING - You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.
2026-01-11 02:23:07.995002: W tensorflow/core/common_runtime/type_inference.cc:340] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:
type_id: TFT_OPTIONAL
args {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_TENSOR
    args {
      type_id: TFT_INT32
    }
  }
}
 is neither a subtype nor a supertype of the combined inputs preceding it:
type_id: TFT_OPTIONAL
args {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_TENSOR
    args {
      type_id: TFT_FLOAT
    }
  }
}

	for Tuple type infernce function 0
	while inferring type of node 'StatefulPartitionedCall/functional_1/padded_obj_2_1/cond/output/_167'
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m32s[0m 11s/step - intensity_scaler_inv_loss: 0.6315 - loss: 0.5179 - pred_intensity_loss: 0.5179 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 351ms/step - intensity_scaler_inv_loss: 3.1756 - loss: 51.4791 - pred_intensity_loss: 50.7118 - trimmed_obj_loss: 0.0000e+00DEBUG _flat_to_channel: gridsize from parameter: 1
DEBUG _flat_to_channel: N from parameter: 74
DEBUG _flat_to_channel: input shape=(None, 74, 74, 1), reshaping to (-1, 1, 74, 74)
DEBUG _flat_to_channel: gridsize from parameter: 1
DEBUG _flat_to_channel: N from parameter: 74
DEBUG _flat_to_channel: input shape=(None, 74, 74, 1), reshaping to (-1, 1, 74, 74)
DEBUG _flat_to_channel: gridsize from parameter: 1
DEBUG _flat_to_channel: N from parameter: 74
DEBUG _flat_to_channel: input shape=(None, 74, 74, 1), reshaping to (-1, 1, 74, 74)
DEBUG _flat_to_channel: gridsize from parameter: 1
DEBUG _flat_to_channel: N from parameter: 74
DEBUG _flat_to_channel: input shape=(None, 74, 74, 1), reshaping to (-1, 1, 74, 74)
input shape (None, 64, 64, 1)
DEBUG _flat_to_channel: gridsize from parameter: 1
DEBUG _flat_to_channel: N from parameter: 64
DEBUG _flat_to_channel: input shape=(None, 64, 64, 1), reshaping to (-1, 1, 64, 64)
2026-01-11 02:23:14,144 - WARNING - You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.
[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m13s[0m 850ms/step - intensity_scaler_inv_loss: 3.0134 - loss: 50.0477 - pred_intensity_loss: 46.9785 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.7157 - val_loss: 0.9135 - val_pred_intensity_loss: 0.9135 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 2/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.7143 - loss: 0.8728 - pred_intensity_loss: 0.8728 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.7182 - loss: 0.7683 - pred_intensity_loss: 0.7657 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step - intensity_scaler_inv_loss: 0.7235 - loss: 0.6922 - pred_intensity_loss: 0.6819 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.7737 - val_loss: 0.5202 - val_pred_intensity_loss: 0.5202 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 3/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.7507 - loss: 0.4251 - pred_intensity_loss: 0.4251 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.7597 - loss: 0.4178 - pred_intensity_loss: 0.4157 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.7445 - loss: 0.3713 - pred_intensity_loss: 0.3631 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.6226 - val_loss: 0.0977 - val_pred_intensity_loss: 0.0977 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 4/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 35ms/step - intensity_scaler_inv_loss: 0.6096 - loss: 0.0612 - pred_intensity_loss: 0.0612 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.5737 - loss: -0.0543 - pred_intensity_loss: -0.0565 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.5537 - loss: -0.1138 - pred_intensity_loss: -0.1227 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5349 - val_loss: -0.1272 - val_pred_intensity_loss: -0.1272 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 5/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 32ms/step - intensity_scaler_inv_loss: 0.5225 - loss: -0.2060 - pred_intensity_loss: -0.2060 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.5287 - loss: -0.1966 - pred_intensity_loss: -0.1978 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.5227 - loss: -0.2095 - pred_intensity_loss: -0.2142 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5299 - val_loss: -0.1695 - val_pred_intensity_loss: -0.1695 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 6/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.4958 - loss: -0.2641 - pred_intensity_loss: -0.2641 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.5092 - loss: -0.2553 - pred_intensity_loss: -0.2557 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step - intensity_scaler_inv_loss: 0.5143 - loss: -0.2535 - pred_intensity_loss: -0.2550 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5261 - val_loss: -0.1721 - val_pred_intensity_loss: -0.1721 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 7/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 35ms/step - intensity_scaler_inv_loss: 0.5053 - loss: -0.2450 - pred_intensity_loss: -0.2450 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.5014 - loss: -0.2630 - pred_intensity_loss: -0.2630 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.4985 - loss: -0.2702 - pred_intensity_loss: -0.2701 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5264 - val_loss: -0.1680 - val_pred_intensity_loss: -0.1680 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 8/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 35ms/step - intensity_scaler_inv_loss: 0.4993 - loss: -0.3047 - pred_intensity_loss: -0.3047 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.5005 - loss: -0.2797 - pred_intensity_loss: -0.2792 - trimmed_obj_loss: 0.0000e+00
Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.5037 - loss: -0.2684 - pred_intensity_loss: -0.2665 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5320 - val_loss: -0.1678 - val_pred_intensity_loss: -0.1678 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 9/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.5099 - loss: -0.2373 - pred_intensity_loss: -0.2373 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.5048 - loss: -0.2560 - pred_intensity_loss: -0.2568 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.5018 - loss: -0.2734 - pred_intensity_loss: -0.2764 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5257 - val_loss: -0.1736 - val_pred_intensity_loss: -0.1736 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 10/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.4910 - loss: -0.3047 - pred_intensity_loss: -0.3047 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.4972 - loss: -0.2776 - pred_intensity_loss: -0.2771 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5205 - val_loss: -0.1770 - val_pred_intensity_loss: -0.1770 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 11/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.5034 - loss: -0.2404 - pred_intensity_loss: -0.2404 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.4943 - loss: -0.2709 - pred_intensity_loss: -0.2704 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.4950 - loss: -0.2790 - pred_intensity_loss: -0.2768 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5214 - val_loss: -0.1781 - val_pred_intensity_loss: -0.1781 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 12/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.5145 - loss: -0.2255 - pred_intensity_loss: -0.2255 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.5010 - loss: -0.2595 - pred_intensity_loss: -0.2603 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.4947 - loss: -0.2810 - pred_intensity_loss: -0.2844 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5232 - val_loss: -0.1793 - val_pred_intensity_loss: -0.1793 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 13/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.5020 - loss: -0.2675 - pred_intensity_loss: -0.2675 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.4941 - loss: -0.2832 - pred_intensity_loss: -0.2875 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5205 - val_loss: -0.1821 - val_pred_intensity_loss: -0.1821 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 14/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.4928 - loss: -0.2923 - pred_intensity_loss: -0.2923 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.4935 - loss: -0.2859 - pred_intensity_loss: -0.2865 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5209 - val_loss: -0.1841 - val_pred_intensity_loss: -0.1841 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 15/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.4951 - loss: -0.2762 - pred_intensity_loss: -0.2762 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.4938 - loss: -0.2883 - pred_intensity_loss: -0.2907 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5207 - val_loss: -0.1865 - val_pred_intensity_loss: -0.1865 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 16/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.5097 - loss: -0.2779 - pred_intensity_loss: -0.2779 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.4938 - loss: -0.2906 - pred_intensity_loss: -0.2890 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5195 - val_loss: -0.1897 - val_pred_intensity_loss: -0.1897 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 17/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.4878 - loss: -0.3074 - pred_intensity_loss: -0.3074 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.4925 - loss: -0.2933 - pred_intensity_loss: -0.2936 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5220 - val_loss: -0.1916 - val_pred_intensity_loss: -0.1916 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 18/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.4975 - loss: -0.2947 - pred_intensity_loss: -0.2947 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.4970 - loss: -0.2921 - pred_intensity_loss: -0.2924 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step - intensity_scaler_inv_loss: 0.4936 - loss: -0.2960 - pred_intensity_loss: -0.2970 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5169 - val_loss: -0.1973 - val_pred_intensity_loss: -0.1973 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 19/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.4799 - loss: -0.3319 - pred_intensity_loss: -0.3319 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.4868 - loss: -0.3141 - pred_intensity_loss: -0.3131 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.4940 - loss: -0.2993 - pred_intensity_loss: -0.2954 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5154 - val_loss: -0.2013 - val_pred_intensity_loss: -0.2013 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 20/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.4916 - loss: -0.2899 - pred_intensity_loss: -0.2899 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.4912 - loss: -0.3029 - pred_intensity_loss: -0.3026 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5169 - val_loss: -0.2057 - val_pred_intensity_loss: -0.2057 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 21/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.5001 - loss: -0.3019 - pred_intensity_loss: -0.3019 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.4922 - loss: -0.3069 - pred_intensity_loss: -0.3067 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step - intensity_scaler_inv_loss: 0.4897 - loss: -0.3082 - pred_intensity_loss: -0.3076 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5155 - val_loss: -0.2110 - val_pred_intensity_loss: -0.2110 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 22/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 36ms/step - intensity_scaler_inv_loss: 0.4949 - loss: -0.2980 - pred_intensity_loss: -0.2980 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.4883 - loss: -0.3106 - pred_intensity_loss: -0.3101 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.4884 - loss: -0.3145 - pred_intensity_loss: -0.3126 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5128 - val_loss: -0.2188 - val_pred_intensity_loss: -0.2188 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 23/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 32ms/step - intensity_scaler_inv_loss: 0.4766 - loss: -0.3232 - pred_intensity_loss: -0.3232 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.4821 - loss: -0.3161 - pred_intensity_loss: -0.3169 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.4839 - loss: -0.3239 - pred_intensity_loss: -0.3269 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5081 - val_loss: -0.2295 - val_pred_intensity_loss: -0.2295 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 24/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.4723 - loss: -0.3682 - pred_intensity_loss: -0.3682 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.4790 - loss: -0.3422 - pred_intensity_loss: -0.3419 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step - intensity_scaler_inv_loss: 0.4799 - loss: -0.3365 - pred_intensity_loss: -0.3352 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4974 - val_loss: -0.2422 - val_pred_intensity_loss: -0.2422 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 25/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 35ms/step - intensity_scaler_inv_loss: 0.4692 - loss: -0.3042 - pred_intensity_loss: -0.3042 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.4735 - loss: -0.3297 - pred_intensity_loss: -0.3305 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.4733 - loss: -0.3489 - pred_intensity_loss: -0.3521 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5039 - val_loss: -0.2543 - val_pred_intensity_loss: -0.2543 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 26/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.4807 - loss: -0.3614 - pred_intensity_loss: -0.3614 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.4749 - loss: -0.3639 - pred_intensity_loss: -0.3631 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step - intensity_scaler_inv_loss: 0.4757 - loss: -0.3536 - pred_intensity_loss: -0.3507 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4874 - val_loss: -0.2768 - val_pred_intensity_loss: -0.2768 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 27/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 32ms/step - intensity_scaler_inv_loss: 0.4659 - loss: -0.3841 - pred_intensity_loss: -0.3841 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.4687 - loss: -0.3716 - pred_intensity_loss: -0.3714 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.4700 - loss: -0.3649 - pred_intensity_loss: -0.3642 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4869 - val_loss: -0.2913 - val_pred_intensity_loss: -0.2913 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 28/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 32ms/step - intensity_scaler_inv_loss: 0.4629 - loss: -0.3701 - pred_intensity_loss: -0.3701 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.4671 - loss: -0.3734 - pred_intensity_loss: -0.3734 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.4681 - loss: -0.3730 - pred_intensity_loss: -0.3730 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4830 - val_loss: -0.3037 - val_pred_intensity_loss: -0.3037 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 29/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.4718 - loss: -0.3771 - pred_intensity_loss: -0.3771 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.4676 - loss: -0.3802 - pred_intensity_loss: -0.3807 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step - intensity_scaler_inv_loss: 0.4639 - loss: -0.3821 - pred_intensity_loss: -0.3841 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4768 - val_loss: -0.3154 - val_pred_intensity_loss: -0.3154 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 30/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.4545 - loss: -0.3726 - pred_intensity_loss: -0.3726 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.4599 - loss: -0.3753 - pred_intensity_loss: -0.3763 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.4619 - loss: -0.3879 - pred_intensity_loss: -0.3918 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4721 - val_loss: -0.3268 - val_pred_intensity_loss: -0.3268 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 31/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.4626 - loss: -0.4243 - pred_intensity_loss: -0.4243 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.4604 - loss: -0.4036 - pred_intensity_loss: -0.4037 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step - intensity_scaler_inv_loss: 0.4597 - loss: -0.3922 - pred_intensity_loss: -0.3927 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4706 - val_loss: -0.3384 - val_pred_intensity_loss: -0.3384 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 32/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.4606 - loss: -0.3690 - pred_intensity_loss: -0.3690 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.4595 - loss: -0.3895 - pred_intensity_loss: -0.3892 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.4601 - loss: -0.3964 - pred_intensity_loss: -0.3953 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4707 - val_loss: -0.3432 - val_pred_intensity_loss: -0.3432 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 33/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.4561 - loss: -0.4248 - pred_intensity_loss: -0.4248 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.4582 - loss: -0.4109 - pred_intensity_loss: -0.4109 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step - intensity_scaler_inv_loss: 0.4585 - loss: -0.3990 - pred_intensity_loss: -0.3989 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4688 - val_loss: -0.3475 - val_pred_intensity_loss: -0.3475 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 34/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.4572 - loss: -0.4237 - pred_intensity_loss: -0.4237 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.4579 - loss: -0.4011 - pred_intensity_loss: -0.3996 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4692 - val_loss: -0.3520 - val_pred_intensity_loss: -0.3520 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 35/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.4583 - loss: -0.4007 - pred_intensity_loss: -0.4007 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.4564 - loss: -0.4053 - pred_intensity_loss: -0.4044 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4683 - val_loss: -0.3545 - val_pred_intensity_loss: -0.3545 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 36/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 36ms/step - intensity_scaler_inv_loss: 0.4654 - loss: -0.3968 - pred_intensity_loss: -0.3968 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.4597 - loss: -0.4068 - pred_intensity_loss: -0.4067 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step - intensity_scaler_inv_loss: 0.4558 - loss: -0.4077 - pred_intensity_loss: -0.4070 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4675 - val_loss: -0.3574 - val_pred_intensity_loss: -0.3574 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 37/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.4619 - loss: -0.3876 - pred_intensity_loss: -0.3876 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.4573 - loss: -0.4008 - pred_intensity_loss: -0.4012 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.4546 - loss: -0.4088 - pred_intensity_loss: -0.4102 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4662 - val_loss: -0.3594 - val_pred_intensity_loss: -0.3594 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 38/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.4612 - loss: -0.3987 - pred_intensity_loss: -0.3987 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.4558 - loss: -0.4077 - pred_intensity_loss: -0.4078 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step - intensity_scaler_inv_loss: 0.4537 - loss: -0.4118 - pred_intensity_loss: -0.4120 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4651 - val_loss: -0.3636 - val_pred_intensity_loss: -0.3636 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 39/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 35ms/step - intensity_scaler_inv_loss: 0.4490 - loss: -0.4064 - pred_intensity_loss: -0.4064 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.4553 - loss: -0.4126 - pred_intensity_loss: -0.4148 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4627 - val_loss: -0.3659 - val_pred_intensity_loss: -0.3659 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 40/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.4474 - loss: -0.4056 - pred_intensity_loss: -0.4056 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.4499 - loss: -0.4187 - pred_intensity_loss: -0.4178 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step - intensity_scaler_inv_loss: 0.4535 - loss: -0.4136 - pred_intensity_loss: -0.4097 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4624 - val_loss: -0.3651 - val_pred_intensity_loss: -0.3651 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 41/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.4500 - loss: -0.4179 - pred_intensity_loss: -0.4179 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.4530 - loss: -0.4128 - pred_intensity_loss: -0.4130 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.4554 - loss: -0.4121 - pred_intensity_loss: -0.4127 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4650 - val_loss: -0.3683 - val_pred_intensity_loss: -0.3683 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 42/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.4507 - loss: -0.4229 - pred_intensity_loss: -0.4229 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.4540 - loss: -0.4137 - pred_intensity_loss: -0.4137 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4651 - val_loss: -0.3630 - val_pred_intensity_loss: -0.3630 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 43/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.4531 - loss: -0.4150 - pred_intensity_loss: -0.4150 - trimmed_obj_loss: 0.0000e+00
Epoch 43: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.4524 - loss: -0.4152 - pred_intensity_loss: -0.4143 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4663 - val_loss: -0.3664 - val_pred_intensity_loss: -0.3664 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
Epoch 44/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 32ms/step - intensity_scaler_inv_loss: 0.4542 - loss: -0.4167 - pred_intensity_loss: -0.4167 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.4535 - loss: -0.4183 - pred_intensity_loss: -0.4167 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4621 - val_loss: -0.3712 - val_pred_intensity_loss: -0.3712 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 2.5000e-04
Epoch 45/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.4540 - loss: -0.4476 - pred_intensity_loss: -0.4476 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.4533 - loss: -0.4287 - pred_intensity_loss: -0.4288 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.4505 - loss: -0.4205 - pred_intensity_loss: -0.4207 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4605 - val_loss: -0.3725 - val_pred_intensity_loss: -0.3725 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 2.5000e-04
Epoch 46/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 35ms/step - intensity_scaler_inv_loss: 0.4521 - loss: -0.4208 - pred_intensity_loss: -0.4208 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.4509 - loss: -0.4238 - pred_intensity_loss: -0.4234 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step - intensity_scaler_inv_loss: 0.4505 - loss: -0.4218 - pred_intensity_loss: -0.4202 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4614 - val_loss: -0.3737 - val_pred_intensity_loss: -0.3737 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 2.5000e-04
Epoch 47/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.4528 - loss: -0.4130 - pred_intensity_loss: -0.4130 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.4508 - loss: -0.4194 - pred_intensity_loss: -0.4193 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.4517 - loss: -0.4224 - pred_intensity_loss: -0.4217 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4618 - val_loss: -0.3737 - val_pred_intensity_loss: -0.3737 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 2.5000e-04
Epoch 48/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.4550 - loss: -0.4313 - pred_intensity_loss: -0.4313 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.4514 - loss: -0.4303 - pred_intensity_loss: -0.4302 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 35ms/step - intensity_scaler_inv_loss: 0.4505 - loss: -0.4232 - pred_intensity_loss: -0.4229 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4611 - val_loss: -0.3742 - val_pred_intensity_loss: -0.3742 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 2.5000e-04
Epoch 49/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.4524 - loss: -0.4013 - pred_intensity_loss: -0.4013 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.4503 - loss: -0.4135 - pred_intensity_loss: -0.4134 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 34ms/step - intensity_scaler_inv_loss: 0.4497 - loss: -0.4242 - pred_intensity_loss: -0.4239 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4607 - val_loss: -0.3751 - val_pred_intensity_loss: -0.3751 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 2.5000e-04
Epoch 50/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 33ms/step - intensity_scaler_inv_loss: 0.4526 - loss: -0.4079 - pred_intensity_loss: -0.4079 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - intensity_scaler_inv_loss: 0.4502 - loss: -0.4200 - pred_intensity_loss: -0.4200 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 36ms/step - intensity_scaler_inv_loss: 0.4506 - loss: -0.4248 - pred_intensity_loss: -0.4248 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4625 - val_loss: -0.3746 - val_pred_intensity_loss: -0.3746 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 2.5000e-04
W0000 00:00:1768127002.541008  898802 loop_optimizer.cc:934] Skipping loop optimization for Merge node with control input: functional_1/padded_obj_2_1/cond_2/branch_executed/_35
DEBUG: Setting probe to tf.Tensor(
[[[ 0.13510734-0.08366051j]
  [-0.08103182-0.0294797j ]
  [ 0.0637618 +0.02550233j]
  ...
  [-0.05742923+0.10465012j]
  [ 0.061039  +0.02731375j]
  [-0.07001488-0.0336627j ]]

 [[-0.08103182-0.0294797j ]
  [ 0.11364837-0.04388378j]
  [-0.08643168+0.03117809j]
  ...
  [ 0.02872955+0.03712314j]
  [-0.08476108+0.03176096j]
  [ 0.10533146-0.04452544j]]

 [[ 0.0637618 +0.02550233j]
  [-0.08643168+0.03117809j]
  [ 0.06238977-0.04183936j]
  ...
  [-0.01890127-0.04941669j]
  [ 0.06107059-0.04259919j]
  [-0.07948506+0.03188787j]]

 ...

 [[-0.05742923+0.10465012j]
  [ 0.02872955+0.03712314j]
  [-0.01890127-0.04941669j]
  ...
  [-0.00577373-0.08714116j]
  [-0.01781569-0.0517069j ]
  [ 0.02285527+0.04448525j]]

 [[ 0.061039  +0.02731375j]
  [-0.08476108+0.03176096j]
  [ 0.06107059-0.04259919j]
  ...
  [-0.01781569-0.0517069j ]
  [ 0.05980607-0.0434007j ]
  [-0.07803912+0.03257083j]]

 [[-0.07001488-0.0336627j ]
  [ 0.10533146-0.04452544j]
  [-0.07948506+0.03188787j]
  ...
  [ 0.02285527+0.04448525j]
  [-0.07803912+0.03257083j]
  [ 0.09786177-0.04529788j]]], shape=(64, 64, 1), dtype=complex64) in params
INFO: setting probe from test data container. It MUST be consistent with the training probe
DEBUG _flat_to_channel: gridsize from parameter: 1
DEBUG _flat_to_channel: N from parameter: 74
DEBUG _flat_to_channel: input shape=(32, 74, 74, 1), reshaping to (-1, 1, 74, 74)
DEBUG _flat_to_channel: gridsize from parameter: 1
DEBUG _flat_to_channel: N from parameter: 74
DEBUG _flat_to_channel: input shape=(32, 74, 74, 1), reshaping to (-1, 1, 74, 74)
DEBUG _flat_to_channel: gridsize from parameter: 1
DEBUG _flat_to_channel: N from parameter: 74
DEBUG _flat_to_channel: input shape=(32, 74, 74, 1), reshaping to (-1, 1, 74, 74)
DEBUG _flat_to_channel: gridsize from parameter: 1
DEBUG _flat_to_channel: N from parameter: 74
DEBUG _flat_to_channel: input shape=(32, 74, 74, 1), reshaping to (-1, 1, 74, 74)
input shape (None, 64, 64, 1)
DEBUG _flat_to_channel: gridsize from parameter: 1
DEBUG _flat_to_channel: N from parameter: 64
DEBUG _flat_to_channel: input shape=(None, 64, 64, 1), reshaping to (-1, 1, 64, 64)
[1m1/2[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1s/step[1m2/2[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 19ms/step
Object stitching failed: Grid-based stitching is not supported for gridsize=1 (non-grid mode). Individual patches cannot be arranged in a regular grid.
cannot reshape array of size 0 into shape (0,0,1)
2026-01-11 02:23:23,131 - INFO - Skipping image stitching (disabled or no test data available)
2026-01-11 02:23:23,131 - INFO - Backend dispatcher: workflow complete (backend=tensorflow)
2026-01-11 02:23:24,576 - INFO - Outputs saved to gs2_full_subsample_study_20260111_0222/train_subsample_64_groups_64/trial_1/pinn_run
2026-01-11 02:23:24,576 - INFO - TensorFlow artifacts saved via model_manager and save_outputs
[2026-01-11 02:23:26] SUCCESS: PtychoPINN training (subsample=64, groups=64, trial=1)
[2026-01-11 02:23:26] EXECUTING: Baseline training (subsample=64, groups=64, trial=1)
[2026-01-11 02:23:26] COMMAND: python scripts/run_baseline.py --config 'configs/comparison_config_gs2_quick.yaml' \
            --train_data_file 'prepare_1e4_photons_5k/dataset/train.npz' \
            --test_data 'prepare_1e4_photons_5k/dataset/test.npz' \
            --n_groups 64 --n_subsample 64 \
            --neighbor_count 7 \
            --output_dir 'gs2_full_subsample_study_20260111_0222/train_subsample_64_groups_64/trial_1/baseline_run'
2026-01-11 02:23:26.576671: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1768127006.588138  906402 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1768127006.591746  906402 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1768127006.601767  906402 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1768127006.601779  906402 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1768127006.601781  906402 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1768127006.601782  906402 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2026-01-11 02:23:26.604557: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2026-01-11 02:23:29,145 - INFO - Configuration setup complete
2026-01-11 02:23:29,145 - INFO - Final configuration: TrainingConfig(model=ModelConfig(N=64, gridsize=1, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', object_big=True, probe_big=True, probe_mask=False, pad_object=True, probe_scale=4.0, gaussian_smoothing_sigma=0.0), train_data_file=PosixPath('prepare_1e4_photons_5k/dataset/train.npz'), test_data_file=PosixPath('prepare_1e4_photons_5k/dataset/test.npz'), batch_size=16, nepochs=50, mae_weight=0.0, nll_weight=1.0, realspace_mae_weight=0.0, realspace_weight=0.0, nphotons=1000000000.0, n_groups=64, n_images=512, n_subsample=64, subsample_seed=None, neighbor_count=7, enable_oversampling=False, neighbor_pool_size=None, positions_provided=True, probe_trainable=False, intensity_scale_trainable=True, output_dir=PosixPath('gs2_full_subsample_study_20260111_0222/train_subsample_64_groups_64/trial_1/baseline_run'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
2026-01-11 02:23:29,145 - INFO - âœ… Validated model_type = 'supervised' for baseline training
2026-01-11 02:23:29,145 - INFO - --- Starting Supervised Baseline Run ---
2026-01-11 02:23:29,145 - INFO - Results will be saved to: gs2_full_subsample_study_20260111_0222/train_subsample_64_groups_64/trial_1/baseline_run/01-11-2026-02.23.29_baseline_gs1/
2026-01-11 02:23:29,145 - INFO - 
[1/6] Initializing probe...
I0000 00:00:1768127009.260141  906402 gpu_process_state.cc:208] Using CUDA malloc Async allocator for GPU: 0
I0000 00:00:1768127009.261477  906402 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22259 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:04:00.0, compute capability: 8.6
2026-01-11 02:23:29,295 - INFO - 
[2/6] Loading data...
2026-01-11 02:23:29,295 - INFO - Loading from .npz files: prepare_1e4_photons_5k/dataset/train.npz
2026-01-11 02:23:29,296 - INFO - Loading data from prepare_1e4_photons_5k/dataset/train.npz with n_images=512, n_subsample=None
2026-01-11 02:23:29,612 - INFO - Legacy sampling: using 512 images from 2576 total
2026-01-11 02:23:29,612 - INFO - Randomly subsampled 512 images
2026-01-11 02:23:29,617 - INFO - Loading data from prepare_1e4_photons_5k/dataset/test.npz with n_images=None, n_subsample=None
2026-01-11 02:23:29,914 - INFO - Using full dataset of 2424 images
2026-01-11 02:23:29,930 - INFO - [OVERSAMPLING DEBUG] generate_grouped_data called with: nsamples=64, n_points=512, C=1, K=7
2026-01-11 02:23:29,930 - INFO - [OVERSAMPLING DEBUG] Parameters: gridsize=1, N=64, sequential_sampling=False
2026-01-11 02:23:29,930 - INFO - [OVERSAMPLING DEBUG] Oversampling flags: enable_oversampling=False, neighbor_pool_size=None, effective_K=7
2026-01-11 02:23:29,930 - INFO - [OVERSAMPLING DEBUG] Oversampling check: nsamples > n_points = 64 > 512 = False
2026-01-11 02:23:29,930 - INFO - [OVERSAMPLING DEBUG] Oversampling check: C > 1 = 1 > 1 = False
2026-01-11 02:23:29,930 - INFO - [OVERSAMPLING DEBUG] needs_oversampling = False
2026-01-11 02:23:29,930 - INFO - Using efficient random sampling strategy for gridsize=1
2026-01-11 02:23:29,930 - INFO - [OVERSAMPLING DEBUG] Taking efficient branch: standard sample-then-group
2026-01-11 02:23:29,930 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently called with: nsamples=64, K=7, C=1
2026-01-11 02:23:29,930 - INFO - Generating 64 groups efficiently from 512 points (K=7, C=1)
2026-01-11 02:23:29,930 - INFO - [OVERSAMPLING DEBUG] Standard case: using 64 groups from 512 points
2026-01-11 02:23:29,930 - INFO - [OVERSAMPLING DEBUG] Randomly sampled 64 seed points
2026-01-11 02:23:29,930 - INFO - Sampled 64 seed points from 512 total points
2026-01-11 02:23:29,930 - INFO - Using seed indices directly for C=1 (gridsize=1) - no neighbor search needed
2026-01-11 02:23:29,930 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently completed: generated 64 groups
2026-01-11 02:23:29,930 - INFO - Successfully generated 64 groups with shape (64, 1)
2026-01-11 02:23:29,930 - INFO - [OVERSAMPLING DEBUG] Generated 64 groups in total
2026-01-11 02:23:29,930 - INFO - Generated 64 groups efficiently
2026-01-11 02:23:30,208 - INFO - [OVERSAMPLING DEBUG] generate_grouped_data called with: nsamples=64, n_points=2424, C=1, K=7
2026-01-11 02:23:30,208 - INFO - [OVERSAMPLING DEBUG] Parameters: gridsize=1, N=64, sequential_sampling=False
2026-01-11 02:23:30,208 - INFO - [OVERSAMPLING DEBUG] Oversampling flags: enable_oversampling=False, neighbor_pool_size=None, effective_K=7
2026-01-11 02:23:30,208 - INFO - [OVERSAMPLING DEBUG] Oversampling check: nsamples > n_points = 64 > 2424 = False
2026-01-11 02:23:30,208 - INFO - [OVERSAMPLING DEBUG] Oversampling check: C > 1 = 1 > 1 = False
2026-01-11 02:23:30,208 - INFO - [OVERSAMPLING DEBUG] needs_oversampling = False
2026-01-11 02:23:30,208 - INFO - Using efficient random sampling strategy for gridsize=1
2026-01-11 02:23:30,208 - INFO - [OVERSAMPLING DEBUG] Taking efficient branch: standard sample-then-group
2026-01-11 02:23:30,208 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently called with: nsamples=64, K=7, C=1
2026-01-11 02:23:30,208 - INFO - Generating 64 groups efficiently from 2424 points (K=7, C=1)
2026-01-11 02:23:30,208 - INFO - [OVERSAMPLING DEBUG] Standard case: using 64 groups from 2424 points
2026-01-11 02:23:30,208 - INFO - [OVERSAMPLING DEBUG] Randomly sampled 64 seed points
2026-01-11 02:23:30,209 - INFO - Sampled 64 seed points from 2424 total points
2026-01-11 02:23:30,209 - INFO - Using seed indices directly for C=1 (gridsize=1) - no neighbor search needed
2026-01-11 02:23:30,209 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently completed: generated 64 groups
2026-01-11 02:23:30,209 - INFO - Successfully generated 64 groups with shape (64, 1)
2026-01-11 02:23:30,209 - INFO - [OVERSAMPLING DEBUG] Generated 64 groups in total
2026-01-11 02:23:30,209 - INFO - Generated 64 groups efficiently
2026-01-11 02:23:30,215 - INFO - Globally set intensity_scale to: 988.2117309570312
2026-01-11 02:23:30,215 - INFO - 
[3/6] Shaping data for the baseline model...
2026-01-11 02:23:30,219 - INFO - Final training input shape: (64, 64, 64, 1)
2026-01-11 02:23:30,219 - INFO - 
[4/6] Training the baseline model for 50 epochs with batch size 16...
2026-01-11 02:23:30,219 - INFO - Training with 64 images
DEBUG: Setting timestamp to 01/11/2026, 02:23:29 in params
Current Parameters:
--------------------
N: 64
amp_activation: sigmoid
backend: tensorflow
batch_size: 16
bigN: 64
big_gridsize: 10
data_source: generic
debug: True
default_probe_scale: 0.7
enable_oversampling: False
gaussian_smoothing_sigma: 0.0
gridsize: 1
h5_path: wts.h5
intensity_scale.trainable: True
label: baseline_gs1
mae_weight: 0.0
max_position_jitter: 10
model_type: supervised
n_filters_scale: 2
n_groups: 64
n_images: 512
n_subsample: 64
neighbor_count: 7
nepochs: 50
nimgs_test: 3
nimgs_train: 9
nll_weight: 1.0
nphotons: 1000000000.0
npseed: 42
object.big: True
offset: 4
outer_offset_test: None
outer_offset_train: None
output_prefix: gs2_full_subsample_study_20260111_0222/train_subsample_64_groups_64/trial_1/baseline_run/01-11-2026-02.23.29_baseline_gs1/
pad_object: True
positions.provided: True
probe.big: True
probe.mask: False
probe.trainable: False
probe_scale: 4.0
realspace_mae_weight: 0.0
realspace_weight: 0.0
sequential_sampling: False
set_phi: False
sim_jitter_scale: 0.0
size: 392
test_data_file_path: prepare_1e4_photons_5k/dataset/test.npz
timestamp: 01/11/2026, 02:23:29
torch_loss_mode: poisson
train_data_file_path: prepare_1e4_photons_5k/dataset/train.npz
tv_weight: 0.0
use_xla_translate: True
DEBUG: Setting probe to tf.Tensor(
[[[2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  ...
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]]

 [[2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  ...
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]]

 [[2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  ...
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]]

 ...

 [[2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  ...
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]]

 [[2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  ...
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]]

 [[2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  ...
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]]], shape=(64, 64, 1), dtype=complex64) in params
diff3d shape: (512, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (512,)
objectGuess shape: (232, 232)
xcoords shape: (512,)
ycoords shape: (512,)
xcoords_start shape: (512,)
ycoords_start shape: (512,)
diff3d shape: (2424, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (2424,)
objectGuess shape: (232, 232)
xcoords shape: (2424,)
ycoords shape: (2424,)
xcoords_start shape: (2424,)
ycoords_start shape: (2424,)
DEBUG: nsamples: 64, gridsize: 1 (using efficient random sample-then-group strategy)
INFO: Using pre-computed 'Y' array from the input file.
neighbor-sampled diffraction shape (64, 64, 64, 1)
loader: using provided ground truth patches.
INFO: None
<PtychoDataContainer X=(64, 64, 64, 1) mean=0.407 Y_I=(64, 64, 64, 1) mean=0.564 Y_phi=(64, 64, 64, 1) mean=-0.052 coords_nominal=(64, 1, 2, 1) mean=0.000 coords_true=(64, 1, 2, 1) mean=0.000 probe=(64, 64) mean_amplitude=0.006 norm_Y_I=<scalar> nn_indices=(64, 1) global_offsets=(64, 1, 2, 1) local_offsets=(64, 1, 2, 1)>
DEBUG: nsamples: 64, gridsize: 1 (using efficient random sample-then-group strategy)
INFO: Using pre-computed 'Y' array from the input file.
neighbor-sampled diffraction shape (64, 64, 64, 1)
loader: using provided ground truth patches.
INFO: None
<PtychoDataContainer X=(64, 64, 64, 1) mean=0.406 Y_I=(64, 64, 64, 1) mean=0.562 Y_phi=(64, 64, 64, 1) mean=-0.037 coords_nominal=(64, 1, 2, 1) mean=0.000 coords_true=(64, 1, 2, 1) mean=0.000 probe=(64, 64) mean_amplitude=0.006 norm_Y_I=<scalar> nn_indices=(64, 1) global_offsets=(64, 1, 2, 1) local_offsets=(64, 1, 2, 1)>
DEBUG: Setting intensity_scale to tf.Tensor(988.21173, shape=(), dtype=float32) in params
Model: "functional"
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Layer (type)        â”ƒ Output Shape      â”ƒ    Param # â”ƒ Connected to      â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ input_layer         â”‚ (None, 64, 64, 1) â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d (Conv2D)     â”‚ (None, 64, 64,    â”‚        640 â”‚ input_layer[0][0] â”‚
â”‚                     â”‚ 64)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_1 (Conv2D)   â”‚ (None, 64, 64,    â”‚     36,928 â”‚ conv2d[0][0]      â”‚
â”‚                     â”‚ 64)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ max_pooling2d       â”‚ (None, 32, 32,    â”‚          0 â”‚ conv2d_1[0][0]    â”‚
â”‚ (MaxPooling2D)      â”‚ 64)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_2 (Conv2D)   â”‚ (None, 32, 32,    â”‚     73,856 â”‚ max_pooling2d[0]â€¦ â”‚
â”‚                     â”‚ 128)              â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_3 (Conv2D)   â”‚ (None, 32, 32,    â”‚    147,584 â”‚ conv2d_2[0][0]    â”‚
â”‚                     â”‚ 128)              â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ max_pooling2d_1     â”‚ (None, 16, 16,    â”‚          0 â”‚ conv2d_3[0][0]    â”‚
â”‚ (MaxPooling2D)      â”‚ 128)              â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_4 (Conv2D)   â”‚ (None, 16, 16,    â”‚    295,168 â”‚ max_pooling2d_1[â€¦ â”‚
â”‚                     â”‚ 256)              â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_5 (Conv2D)   â”‚ (None, 16, 16,    â”‚    590,080 â”‚ conv2d_4[0][0]    â”‚
â”‚                     â”‚ 256)              â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ max_pooling2d_2     â”‚ (None, 8, 8, 256) â”‚          0 â”‚ conv2d_5[0][0]    â”‚
â”‚ (MaxPooling2D)      â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_6 (Conv2D)   â”‚ (None, 8, 8, 256) â”‚    590,080 â”‚ max_pooling2d_2[â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_13 (Conv2D)  â”‚ (None, 8, 8, 256) â”‚    590,080 â”‚ max_pooling2d_2[â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_7 (Conv2D)   â”‚ (None, 8, 8, 256) â”‚    590,080 â”‚ conv2d_6[0][0]    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_14 (Conv2D)  â”‚ (None, 8, 8, 256) â”‚    590,080 â”‚ conv2d_13[0][0]   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling2d       â”‚ (None, 16, 16,    â”‚          0 â”‚ conv2d_7[0][0]    â”‚
â”‚ (UpSampling2D)      â”‚ 256)              â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling2d_3     â”‚ (None, 16, 16,    â”‚          0 â”‚ conv2d_14[0][0]   â”‚
â”‚ (UpSampling2D)      â”‚ 256)              â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_8 (Conv2D)   â”‚ (None, 16, 16,    â”‚    295,040 â”‚ up_sampling2d[0]â€¦ â”‚
â”‚                     â”‚ 128)              â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_15 (Conv2D)  â”‚ (None, 16, 16,    â”‚    295,040 â”‚ up_sampling2d_3[â€¦ â”‚
â”‚                     â”‚ 128)              â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_9 (Conv2D)   â”‚ (None, 16, 16,    â”‚    147,584 â”‚ conv2d_8[0][0]    â”‚
â”‚                     â”‚ 128)              â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_16 (Conv2D)  â”‚ (None, 16, 16,    â”‚    147,584 â”‚ conv2d_15[0][0]   â”‚
â”‚                     â”‚ 128)              â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling2d_1     â”‚ (None, 32, 32,    â”‚          0 â”‚ conv2d_9[0][0]    â”‚
â”‚ (UpSampling2D)      â”‚ 128)              â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling2d_4     â”‚ (None, 32, 32,    â”‚          0 â”‚ conv2d_16[0][0]   â”‚
â”‚ (UpSampling2D)      â”‚ 128)              â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_10 (Conv2D)  â”‚ (None, 32, 32,    â”‚     73,792 â”‚ up_sampling2d_1[â€¦ â”‚
â”‚                     â”‚ 64)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_17 (Conv2D)  â”‚ (None, 32, 32,    â”‚     73,792 â”‚ up_sampling2d_4[â€¦ â”‚
â”‚                     â”‚ 64)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_11 (Conv2D)  â”‚ (None, 32, 32,    â”‚     36,928 â”‚ conv2d_10[0][0]   â”‚
â”‚                     â”‚ 64)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_18 (Conv2D)  â”‚ (None, 32, 32,    â”‚     36,928 â”‚ conv2d_17[0][0]   â”‚
â”‚                     â”‚ 64)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling2d_2     â”‚ (None, 64, 64,    â”‚          0 â”‚ conv2d_11[0][0]   â”‚
â”‚ (UpSampling2D)      â”‚ 64)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling2d_5     â”‚ (None, 64, 64,    â”‚          0 â”‚ conv2d_18[0][0]   â”‚
â”‚ (UpSampling2D)      â”‚ 64)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_12 (Conv2D)  â”‚ (None, 64, 64, 1) â”‚        577 â”‚ up_sampling2d_2[â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_19 (Conv2D)  â”‚ (None, 64, 64, 1) â”‚        577 â”‚ up_sampling2d_5[â€¦ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 4,612,418 (17.59 MB)
 Trainable params: 4,612,418 (17.59 MB)
 Non-trainable params: 0 (0.00 B)
None
Training with 50 epochs and batch size 16
Epoch 1/50
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1768127012.997294  906538 service.cc:152] XLA service 0x7b0e38004280 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1768127012.997317  906538 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6
2026-01-11 02:23:33.064344: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
I0000 00:00:1768127013.472772  906538 cuda_dnn.cc:529] Loaded cuDNN version 91002
I0000 00:00:1768127016.351784  906538 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m16s[0m 6s/step - conv2d_12_loss: 0.5640 - conv2d_19_loss: 0.2066 - loss: 0.7706[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 981ms/step - conv2d_12_loss: 1.0459 - conv2d_19_loss: 0.2629 - loss: 1.3295[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m9s[0m 1s/step - conv2d_12_loss: 1.4162 - conv2d_19_loss: 0.2620 - loss: 1.7611 - val_conv2d_12_loss: 0.4966 - val_conv2d_19_loss: 0.2143 - val_loss: 0.7109 - learning_rate: 0.0010
Epoch 2/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 27ms/step - conv2d_12_loss: 0.4954 - conv2d_19_loss: 0.2118 - loss: 0.7072[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.5216 - conv2d_19_loss: 0.2150 - loss: 0.7361 - val_conv2d_12_loss: 0.5272 - val_conv2d_19_loss: 0.2133 - val_loss: 0.7405 - learning_rate: 0.0010
Epoch 3/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.5267 - conv2d_19_loss: 0.2182 - loss: 0.7449[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.5174 - conv2d_19_loss: 0.2149 - loss: 0.7325 - val_conv2d_12_loss: 0.4969 - val_conv2d_19_loss: 0.2127 - val_loss: 0.7096 - learning_rate: 0.0010
Epoch 4/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 27ms/step - conv2d_12_loss: 0.4958 - conv2d_19_loss: 0.2183 - loss: 0.7141[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 24ms/step - conv2d_12_loss: 0.4574 - conv2d_19_loss: 0.2137 - loss: 0.6751 - val_conv2d_12_loss: 0.2981 - val_conv2d_19_loss: 0.2127 - val_loss: 0.5108 - learning_rate: 0.0010
Epoch 5/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.2973 - conv2d_19_loss: 0.2109 - loss: 0.5082[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.1946 - conv2d_19_loss: 0.2142 - loss: 0.4128 - val_conv2d_12_loss: 0.1752 - val_conv2d_19_loss: 0.2139 - val_loss: 0.3891 - learning_rate: 0.0010
Epoch 6/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.1734 - conv2d_19_loss: 0.2203 - loss: 0.3937[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 24ms/step - conv2d_12_loss: 0.1477 - conv2d_19_loss: 0.2142 - loss: 0.3628 - val_conv2d_12_loss: 0.1177 - val_conv2d_19_loss: 0.2126 - val_loss: 0.3302 - learning_rate: 0.0010
Epoch 7/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.1161 - conv2d_19_loss: 0.2106 - loss: 0.3267[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.1118 - conv2d_19_loss: 0.2127 - loss: 0.3264 - val_conv2d_12_loss: 0.1086 - val_conv2d_19_loss: 0.2127 - val_loss: 0.3212 - learning_rate: 0.0010
Epoch 8/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.1079 - conv2d_19_loss: 0.2185 - loss: 0.3264[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0971 - conv2d_19_loss: 0.2141 - loss: 0.3111 - val_conv2d_12_loss: 0.0854 - val_conv2d_19_loss: 0.2128 - val_loss: 0.2982 - learning_rate: 0.0010
Epoch 9/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0830 - conv2d_19_loss: 0.2092 - loss: 0.2922[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0807 - conv2d_19_loss: 0.2138 - loss: 0.2948 - val_conv2d_12_loss: 0.0748 - val_conv2d_19_loss: 0.2124 - val_loss: 0.2872 - learning_rate: 0.0010
Epoch 10/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0749 - conv2d_19_loss: 0.2213 - loss: 0.2962[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0711 - conv2d_19_loss: 0.2134 - loss: 0.2847 - val_conv2d_12_loss: 0.0663 - val_conv2d_19_loss: 0.2126 - val_loss: 0.2789 - learning_rate: 0.0010
Epoch 11/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0653 - conv2d_19_loss: 0.2091 - loss: 0.2743[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0643 - conv2d_19_loss: 0.2135 - loss: 0.2778 - val_conv2d_12_loss: 0.0609 - val_conv2d_19_loss: 0.2123 - val_loss: 0.2732 - learning_rate: 0.0010
Epoch 12/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0592 - conv2d_19_loss: 0.2070 - loss: 0.2662[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0600 - conv2d_19_loss: 0.2131 - loss: 0.2734 - val_conv2d_12_loss: 0.0581 - val_conv2d_19_loss: 0.2123 - val_loss: 0.2704 - learning_rate: 0.0010
Epoch 13/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0567 - conv2d_19_loss: 0.2016 - loss: 0.2583[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0573 - conv2d_19_loss: 0.2138 - loss: 0.2707 - val_conv2d_12_loss: 0.0555 - val_conv2d_19_loss: 0.2123 - val_loss: 0.2678 - learning_rate: 0.0010
Epoch 14/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0570 - conv2d_19_loss: 0.2224 - loss: 0.2794[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0553 - conv2d_19_loss: 0.2132 - loss: 0.2687 - val_conv2d_12_loss: 0.0547 - val_conv2d_19_loss: 0.2123 - val_loss: 0.2670 - learning_rate: 0.0010
Epoch 15/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0551 - conv2d_19_loss: 0.2186 - loss: 0.2737[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 24ms/step - conv2d_12_loss: 0.0537 - conv2d_19_loss: 0.2132 - loss: 0.2671 - val_conv2d_12_loss: 0.0532 - val_conv2d_19_loss: 0.2122 - val_loss: 0.2654 - learning_rate: 0.0010
Epoch 16/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 27ms/step - conv2d_12_loss: 0.0539 - conv2d_19_loss: 0.2206 - loss: 0.2745[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 24ms/step - conv2d_12_loss: 0.0521 - conv2d_19_loss: 0.2129 - loss: 0.2656 - val_conv2d_12_loss: 0.0517 - val_conv2d_19_loss: 0.2122 - val_loss: 0.2639 - learning_rate: 0.0010
Epoch 17/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0499 - conv2d_19_loss: 0.2069 - loss: 0.2568[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0511 - conv2d_19_loss: 0.2137 - loss: 0.2643 - val_conv2d_12_loss: 0.0506 - val_conv2d_19_loss: 0.2122 - val_loss: 0.2629 - learning_rate: 0.0010
Epoch 18/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0510 - conv2d_19_loss: 0.2128 - loss: 0.2638[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 24ms/step - conv2d_12_loss: 0.0502 - conv2d_19_loss: 0.2134 - loss: 0.2635 - val_conv2d_12_loss: 0.0499 - val_conv2d_19_loss: 0.2122 - val_loss: 0.2621 - learning_rate: 0.0010
Epoch 19/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0506 - conv2d_19_loss: 0.2180 - loss: 0.2686[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0492 - conv2d_19_loss: 0.2127 - loss: 0.2626 - val_conv2d_12_loss: 0.0491 - val_conv2d_19_loss: 0.2122 - val_loss: 0.2613 - learning_rate: 0.0010
Epoch 20/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0478 - conv2d_19_loss: 0.2110 - loss: 0.2588[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 24ms/step - conv2d_12_loss: 0.0483 - conv2d_19_loss: 0.2131 - loss: 0.2615 - val_conv2d_12_loss: 0.0484 - val_conv2d_19_loss: 0.2122 - val_loss: 0.2605 - learning_rate: 0.0010
Epoch 21/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0488 - conv2d_19_loss: 0.2138 - loss: 0.2626[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 24ms/step - conv2d_12_loss: 0.0475 - conv2d_19_loss: 0.2137 - loss: 0.2607 - val_conv2d_12_loss: 0.0473 - val_conv2d_19_loss: 0.2122 - val_loss: 0.2595 - learning_rate: 0.0010
Epoch 22/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0477 - conv2d_19_loss: 0.2185 - loss: 0.2662[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 24ms/step - conv2d_12_loss: 0.0465 - conv2d_19_loss: 0.2131 - loss: 0.2598 - val_conv2d_12_loss: 0.0465 - val_conv2d_19_loss: 0.2122 - val_loss: 0.2587 - learning_rate: 0.0010
Epoch 23/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0429 - conv2d_19_loss: 0.2044 - loss: 0.2473[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 24ms/step - conv2d_12_loss: 0.0461 - conv2d_19_loss: 0.2139 - loss: 0.2592 - val_conv2d_12_loss: 0.0455 - val_conv2d_19_loss: 0.2121 - val_loss: 0.2576 - learning_rate: 0.0010
Epoch 24/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0457 - conv2d_19_loss: 0.2207 - loss: 0.2663[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0456 - conv2d_19_loss: 0.2127 - loss: 0.2589 - val_conv2d_12_loss: 0.0450 - val_conv2d_19_loss: 0.2122 - val_loss: 0.2572 - learning_rate: 0.0010
Epoch 25/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0461 - conv2d_19_loss: 0.2238 - loss: 0.2699[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 24ms/step - conv2d_12_loss: 0.0451 - conv2d_19_loss: 0.2138 - loss: 0.2582 - val_conv2d_12_loss: 0.0442 - val_conv2d_19_loss: 0.2121 - val_loss: 0.2564 - learning_rate: 0.0010
Epoch 26/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0437 - conv2d_19_loss: 0.2111 - loss: 0.2548[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0436 - conv2d_19_loss: 0.2128 - loss: 0.2570 - val_conv2d_12_loss: 0.0446 - val_conv2d_19_loss: 0.2122 - val_loss: 0.2568 - learning_rate: 0.0010
Epoch 27/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0440 - conv2d_19_loss: 0.2104 - loss: 0.2544[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 24ms/step - conv2d_12_loss: 0.0433 - conv2d_19_loss: 0.2131 - loss: 0.2566 - val_conv2d_12_loss: 0.0436 - val_conv2d_19_loss: 0.2122 - val_loss: 0.2558 - learning_rate: 0.0010
Epoch 28/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0423 - conv2d_19_loss: 0.2107 - loss: 0.2530[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0425 - conv2d_19_loss: 0.2135 - loss: 0.2557 - val_conv2d_12_loss: 0.0423 - val_conv2d_19_loss: 0.2121 - val_loss: 0.2544 - learning_rate: 0.0010
Epoch 29/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0411 - conv2d_19_loss: 0.2122 - loss: 0.2533[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 24ms/step - conv2d_12_loss: 0.0417 - conv2d_19_loss: 0.2135 - loss: 0.2549 - val_conv2d_12_loss: 0.0416 - val_conv2d_19_loss: 0.2122 - val_loss: 0.2538 - learning_rate: 0.0010
Epoch 30/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 27ms/step - conv2d_12_loss: 0.0411 - conv2d_19_loss: 0.2121 - loss: 0.2532[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0411 - conv2d_19_loss: 0.2128 - loss: 0.2543 - val_conv2d_12_loss: 0.0413 - val_conv2d_19_loss: 0.2121 - val_loss: 0.2534 - learning_rate: 0.0010
Epoch 31/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0385 - conv2d_19_loss: 0.2002 - loss: 0.2388[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0406 - conv2d_19_loss: 0.2131 - loss: 0.2538 - val_conv2d_12_loss: 0.0407 - val_conv2d_19_loss: 0.2121 - val_loss: 0.2528 - learning_rate: 0.0010
Epoch 32/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0409 - conv2d_19_loss: 0.2203 - loss: 0.2612[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0401 - conv2d_19_loss: 0.2133 - loss: 0.2533 - val_conv2d_12_loss: 0.0404 - val_conv2d_19_loss: 0.2122 - val_loss: 0.2526 - learning_rate: 0.0010
Epoch 33/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0407 - conv2d_19_loss: 0.2170 - loss: 0.2578[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0400 - conv2d_19_loss: 0.2136 - loss: 0.2531 - val_conv2d_12_loss: 0.0400 - val_conv2d_19_loss: 0.2121 - val_loss: 0.2522 - learning_rate: 0.0010
Epoch 34/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0395 - conv2d_19_loss: 0.2165 - loss: 0.2561[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0398 - conv2d_19_loss: 0.2133 - loss: 0.2530 - val_conv2d_12_loss: 0.0402 - val_conv2d_19_loss: 0.2122 - val_loss: 0.2524 - learning_rate: 0.0010
Epoch 35/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0402 - conv2d_19_loss: 0.2148 - loss: 0.2550
Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0395 - conv2d_19_loss: 0.2130 - loss: 0.2528 - val_conv2d_12_loss: 0.0403 - val_conv2d_19_loss: 0.2124 - val_loss: 0.2526 - learning_rate: 0.0010
Epoch 36/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0407 - conv2d_19_loss: 0.2204 - loss: 0.2612[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 24ms/step - conv2d_12_loss: 0.0397 - conv2d_19_loss: 0.2127 - loss: 0.2528 - val_conv2d_12_loss: 0.0397 - val_conv2d_19_loss: 0.2123 - val_loss: 0.2520 - learning_rate: 5.0000e-04
Epoch 37/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0391 - conv2d_19_loss: 0.2096 - loss: 0.2488[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0394 - conv2d_19_loss: 0.2132 - loss: 0.2526 - val_conv2d_12_loss: 0.0398 - val_conv2d_19_loss: 0.2123 - val_loss: 0.2521 - learning_rate: 5.0000e-04
Epoch 38/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0402 - conv2d_19_loss: 0.2149 - loss: 0.2550
Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0394 - conv2d_19_loss: 0.2139 - loss: 0.2524 - val_conv2d_12_loss: 0.0397 - val_conv2d_19_loss: 0.2123 - val_loss: 0.2520 - learning_rate: 5.0000e-04
Epoch 39/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0388 - conv2d_19_loss: 0.2159 - loss: 0.2547[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0394 - conv2d_19_loss: 0.2142 - loss: 0.2523 - val_conv2d_12_loss: 0.0396 - val_conv2d_19_loss: 0.2123 - val_loss: 0.2519 - learning_rate: 2.5000e-04
Epoch 40/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0394 - conv2d_19_loss: 0.2111 - loss: 0.2505[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0391 - conv2d_19_loss: 0.2126 - loss: 0.2523 - val_conv2d_12_loss: 0.0395 - val_conv2d_19_loss: 0.2123 - val_loss: 0.2518 - learning_rate: 2.5000e-04
Epoch 41/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0388 - conv2d_19_loss: 0.2126 - loss: 0.2514[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 24ms/step - conv2d_12_loss: 0.0391 - conv2d_19_loss: 0.2133 - loss: 0.2522 - val_conv2d_12_loss: 0.0396 - val_conv2d_19_loss: 0.2123 - val_loss: 0.2519 - learning_rate: 2.5000e-04
Epoch 42/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0406 - conv2d_19_loss: 0.2219 - loss: 0.2625
Epoch 42: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0392 - conv2d_19_loss: 0.2134 - loss: 0.2522 - val_conv2d_12_loss: 0.0396 - val_conv2d_19_loss: 0.2123 - val_loss: 0.2520 - learning_rate: 2.5000e-04
Epoch 43/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0369 - conv2d_19_loss: 0.1986 - loss: 0.2355[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0392 - conv2d_19_loss: 0.2139 - loss: 0.2522 - val_conv2d_12_loss: 0.0396 - val_conv2d_19_loss: 0.2124 - val_loss: 0.2519 - learning_rate: 1.2500e-04
Epoch 44/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0381 - conv2d_19_loss: 0.2096 - loss: 0.2477
Epoch 44: ReduceLROnPlateau reducing learning rate to 0.0001.
[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0391 - conv2d_19_loss: 0.2134 - loss: 0.2521 - val_conv2d_12_loss: 0.0395 - val_conv2d_19_loss: 0.2124 - val_loss: 0.2519 - learning_rate: 1.2500e-04
Epoch 45/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0388 - conv2d_19_loss: 0.2128 - loss: 0.2517[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 24ms/step - conv2d_12_loss: 0.0389 - conv2d_19_loss: 0.2130 - loss: 0.2521 - val_conv2d_12_loss: 0.0395 - val_conv2d_19_loss: 0.2124 - val_loss: 0.2519 - learning_rate: 1.0000e-04
Epoch 46/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0388 - conv2d_19_loss: 0.2111 - loss: 0.2498[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 24ms/step - conv2d_12_loss: 0.0390 - conv2d_19_loss: 0.2132 - loss: 0.2521 - val_conv2d_12_loss: 0.0395 - val_conv2d_19_loss: 0.2124 - val_loss: 0.2519 - learning_rate: 1.0000e-04
Epoch 47/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0393 - conv2d_19_loss: 0.2127 - loss: 0.2520[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0390 - conv2d_19_loss: 0.2129 - loss: 0.2521 - val_conv2d_12_loss: 0.0395 - val_conv2d_19_loss: 0.2124 - val_loss: 0.2519 - learning_rate: 1.0000e-04
Epoch 48/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0378 - conv2d_19_loss: 0.2085 - loss: 0.2463[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0391 - conv2d_19_loss: 0.2135 - loss: 0.2521 - val_conv2d_12_loss: 0.0395 - val_conv2d_19_loss: 0.2124 - val_loss: 0.2519 - learning_rate: 1.0000e-04
Epoch 49/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0392 - conv2d_19_loss: 0.2121 - loss: 0.2513[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0390 - conv2d_19_loss: 0.2134 - loss: 0.2521 - val_conv2d_12_loss: 0.0395 - val_conv2d_19_loss: 0.2124 - val_loss: 0.2519 - learning_rate: 1.0000e-04
Epoch 50/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0369 - conv2d_19_loss: 0.2040 - loss: 0.2409[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 24ms/step - conv2d_12_loss: 0.0390 - conv2d_19_loss: 0.2130 - loss: 0.2521 - val_conv2d_12_loss: 0.0395 - val_conv2d_19_loss: 0.2124 - val_loss: 0.2519 - learning_rate: 1.0000e-04
2026-01-11 02:23:45,143 - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
2026-01-11 02:23:45,261 - INFO - Trained model saved to gs2_full_subsample_study_20260111_0222/train_subsample_64_groups_64/trial_1/baseline_run/01-11-2026-02.23.29_baseline_gs1/baseline_model.h5
2026-01-11 02:23:45,261 - INFO - 
[5/6] Performing inference and stitching...
[1m1/2[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 794ms/step[1m2/2[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 15ms/step 
2026-01-11 02:23:48,017 - INFO - Stitched object shape: (1, 172, 172, 1, 1)
2026-01-11 02:23:48,017 - INFO - 
[6/6] Evaluating reconstruction and saving results...
2026-01-11 02:23:48,017 - INFO - Aligning ground truth to match reconstruction bounds...
2026-01-11 02:23:48,017 - INFO - --- Aligning reconstruction with ground truth for evaluation ---
2026-01-11 02:23:48,017 - INFO - Calculated ground truth crop region: rows [106:208], cols [35:205]
2026-01-11 02:23:48,017 - INFO - Initial shapes: Recon=(172, 172), Cropped GT=(102, 170)
2026-01-11 02:23:48,017 - INFO - Center-cropping from (172, 172) to (102, 170)
2026-01-11 02:23:48,017 - INFO - Final aligned shape: (102, 170)
2026-01-11 02:23:48,017 - INFO - --- Alignment complete ---
2026-01-11 02:23:48,017 - INFO - Final evaluation shapes: Reconstruction=(1, 102, 170, 1), Ground Truth=(102, 170, 1)
/home/ollie/Documents/PtychoPINN/ptycho/evaluation.py:168: RuntimeWarning: Negative SSIM value (-0.0196) encountered in MS-SSIM calculation. Clamping to 0.0001 to avoid NaN.
  warnings.warn(f"Negative SSIM value ({ssim_val:.4f}) encountered in MS-SSIM calculation. "
/home/ollie/Documents/PtychoPINN/ptycho/evaluation.py:168: RuntimeWarning: Negative SSIM value (-0.0124) encountered in MS-SSIM calculation. Clamping to 0.0001 to avoid NaN.
  warnings.warn(f"Negative SSIM value ({ssim_val:.4f}) encountered in MS-SSIM calculation. "
2026-01-11 02:23:48,038 - INFO - Evaluation Metrics (Amplitude, Phase):
2026-01-11 02:23:48,038 - INFO -   MAE:  (np.float32(0.28129277), np.float64(0.24197782516650052))
2026-01-11 02:23:48,038 - INFO -   PSNR: (57.8021473877266, 59.35360596977158)
2026-01-11 02:23:48,073 - INFO - Metrics and reconstruction images saved.
2026-01-11 02:23:48,073 - INFO - 
--- Baseline script finished successfully. ---
Amplitude normalization scale factor: 1.298134
mean scale adjustment: 1
mean scale adjustment: 1
DEBUG eval_reconstruction []: amp_target stats: mean=0.563129, std=0.045309, shape=(98, 166, 1)
DEBUG eval_reconstruction []: amp_pred stats: mean=0.433799, std=0.249715, shape=(98, 166, 1)
DEBUG eval_reconstruction []: phi_target stats: mean=-0.000000, std=0.271649, shape=(98, 166)
DEBUG eval_reconstruction []: phi_pred stats: mean=-0.000000, std=0.036443, shape=(98, 166)
performed by index method
performed by index method
performed by index method
mean scale adjustment: 1
mean scale adjustment: 1
Phase preprocessing: plane-fitted range [-0.672, 0.425] -> scaled range [0.393, 0.568]
performed by index method
performed by index method
performed by index method
Amplitude normalization scale factor: 1.298134
mean scale adjustment: 1
mean scale adjustment: 1
DEBUG eval_reconstruction [baseline_gs1]: amp_target stats: mean=0.563129, std=0.045309, shape=(98, 166, 1)
DEBUG eval_reconstruction [baseline_gs1]: amp_pred stats: mean=0.433799, std=0.249715, shape=(98, 166, 1)
DEBUG eval_reconstruction [baseline_gs1]: phi_target stats: mean=-0.000000, std=0.271649, shape=(98, 166)
DEBUG eval_reconstruction [baseline_gs1]: phi_pred stats: mean=-0.000000, std=0.036443, shape=(98, 166)
performed by index method
performed by index method
performed by index method
mean scale adjustment: 1
mean scale adjustment: 1
Phase preprocessing: plane-fitted range [-0.672, 0.425] -> scaled range [0.393, 0.568]
performed by index method
performed by index method
performed by index method
[2026-01-11 02:23:49] SUCCESS: Baseline training (subsample=64, groups=64, trial=1)
[2026-01-11 02:23:49] Completed training for train_groups=64 (Trial 1/1)
[2026-01-11 02:23:49] Completed all trials for train_groups=64
[2026-01-11 02:23:49] Model training phase completed
[2026-01-11 02:23:49] === STEP 3: Model Comparison ===
[2026-01-11 02:23:49] Running comparisons for train_subsample=64, train_groups=64 (1 trials)
[2026-01-11 02:23:49] EXECUTING: Model comparison (train_subsample=64, train_groups=64, trial=1)
[2026-01-11 02:23:49] COMMAND: bash scripts/run_comparison.sh \
                'prepare_1e4_photons_5k/dataset/train.npz' \
                'prepare_1e4_photons_5k/dataset/test.npz' \
                'gs2_full_subsample_study_20260111_0222/train_subsample_64_groups_64/trial_1' \
                --skip-training \
                --pinn-model 'gs2_full_subsample_study_20260111_0222/train_subsample_64_groups_64/trial_1/pinn_run' \
                --baseline-model 'gs2_full_subsample_study_20260111_0222/train_subsample_64_groups_64/trial_1/baseline_run/01-11-2026-02.23.29_baseline_gs1' \
                --n-test-subsample 2424 \
                --n-test-groups 606 \
                --neighbor-count 7
Creating output directories...
==========================================
Starting PtychoPINN vs Baseline Comparison
==========================================
Train data: prepare_1e4_photons_5k/dataset/train.npz
Test data: prepare_1e4_photons_5k/dataset/test.npz
Output directory: gs2_full_subsample_study_20260111_0222/train_subsample_64_groups_64/trial_1
Config file: configs/comparison_config.yaml
Training groups: 512 (from config)
Test subsample: 2424 images
Test groups: 606

Skipping training, using existing models:
  PtychoPINN: gs2_full_subsample_study_20260111_0222/train_subsample_64_groups_64/trial_1/pinn_run
  Baseline: gs2_full_subsample_study_20260111_0222/train_subsample_64_groups_64/trial_1/baseline_run/01-11-2026-02.23.29_baseline_gs1

Step 3/3: Running comparison analysis (subsample=2424, groups=606)...
---------------------------------------
2026-01-11 02:23:49.874910: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1768127029.886360  913823 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1768127029.890206  913823 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1768127029.900163  913823 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1768127029.900173  913823 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1768127029.900175  913823 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1768127029.900176  913823 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2026-01-11 02:23:49.902956: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2026-01-11 02:23:52,183 - INFO - Configuration: phase_align_method='plane', frc_sigma=0.0
2026-01-11 02:23:52,183 - INFO - Registration: enabled
2026-01-11 02:23:52,183 - INFO - NPZ output: raw=enabled, aligned=enabled
2026-01-11 02:23:52,183 - INFO - Loading PtychoPINN inference model (diffraction_to_obj) to restore configuration...
2026-01-11 02:23:52,183 - INFO - Loading model from: gs2_full_subsample_study_20260111_0222/train_subsample_64_groups_64/trial_1/pinn_run
I0000 00:00:1768127032.426528  913823 gpu_process_state.cc:208] Using CUDA malloc Async allocator for GPU: 0
I0000 00:00:1768127032.427737  913823 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22259 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:04:00.0, compute capability: 8.6
/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'center_mask_layer', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.
  warnings.warn(
2026-01-11 02:23:53,127 - ERROR - /home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'center_mask_layer', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.
  warnings.warn(
/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'center_mask_layer_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.
  warnings.warn(
2026-01-11 02:23:53,128 - ERROR - /home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'center_mask_layer_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.
  warnings.warn(
2026-01-11 02:23:53,786 - INFO - Successfully loaded model from gs2_full_subsample_study_20260111_0222/train_subsample_64_groups_64/trial_1/pinn_run
2026-01-11 02:23:53,788 - INFO - Restored configuration from model: gridsize=1, N=64
2026-01-11 02:23:53,788 - INFO - Loading test data from prepare_1e4_photons_5k/dataset/test.npz with restored gridsize=1...
2026-01-11 02:23:53,788 - INFO - Using independent sampling control: n_subsample=2424, n_groups=606
2026-01-11 02:23:53,788 - WARNING - n_test_subsample (2424) > n_test_groups (606), may use more data than expected
2026-01-11 02:23:53,788 - INFO - Loading data from prepare_1e4_photons_5k/dataset/test.npz with n_images=606, n_subsample=2424
2026-01-11 02:23:54,086 - INFO - Independent sampling: subsampling 2424 images from 2424 total
diff3d shape: (2424, 64, 64)2026-01-11 02:23:54,102 - INFO - diff3d shape: (2424, 64, 64)

probeGuess shape: (64, 64)2026-01-11 02:23:54,102 - INFO - probeGuess shape: (64, 64)

scan_index shape: (2424,)2026-01-11 02:23:54,102 - INFO - scan_index shape: (2424,)

objectGuess shape: (232, 232)2026-01-11 02:23:54,102 - INFO - objectGuess shape: (232, 232)

xcoords shape: (2424,)2026-01-11 02:23:54,102 - INFO - xcoords shape: (2424,)

ycoords shape: (2424,)2026-01-11 02:23:54,102 - INFO - ycoords shape: (2424,)

xcoords_start shape: (2424,)2026-01-11 02:23:54,102 - INFO - xcoords_start shape: (2424,)

ycoords_start shape: (2424,)2026-01-11 02:23:54,102 - INFO - ycoords_start shape: (2424,)

2026-01-11 02:23:54,103 - INFO - Final configuration: gridsize=1, N=64, n_images=2424
2026-01-11 02:23:54,103 - INFO - Using stitch_crop_size M=20 (N=64)
2026-01-11 02:23:54,103 - INFO - Loading Baseline model from gs2_full_subsample_study_20260111_0222/train_subsample_64_groups_64/trial_1/baseline_run/01-11-2026-02.23.29_baseline_gs1...
2026-01-11 02:23:54,103 - INFO - Found baseline model at: gs2_full_subsample_study_20260111_0222/train_subsample_64_groups_64/trial_1/baseline_run/01-11-2026-02.23.29_baseline_gs1/baseline_model.h5
2026-01-11 02:23:54,194 - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
2026-01-11 02:23:54,199 - INFO - Running two-way comparison (PtychoPINN vs. Baseline)
2026-01-11 02:23:54,199 - INFO - Running inference with PtychoPINN (diffraction_to_obj)...
2026-01-11 02:23:54,199 - INFO - Using single-shot PINN inference: 2424 groups (batch_size=32)
2026-01-11 02:23:54,199 - INFO - [OVERSAMPLING DEBUG] generate_grouped_data called with: nsamples=606, n_points=2424, C=1, K=7
2026-01-11 02:23:54,200 - INFO - [OVERSAMPLING DEBUG] Parameters: gridsize=1, N=64, sequential_sampling=False
2026-01-11 02:23:54,200 - INFO - [OVERSAMPLING DEBUG] Oversampling flags: enable_oversampling=False, neighbor_pool_size=None, effective_K=7
2026-01-11 02:23:54,200 - INFO - [OVERSAMPLING DEBUG] Oversampling check: nsamples > n_points = 606 > 2424 = False
2026-01-11 02:23:54,200 - INFO - [OVERSAMPLING DEBUG] Oversampling check: C > 1 = 1 > 1 = False
2026-01-11 02:23:54,200 - INFO - [OVERSAMPLING DEBUG] needs_oversampling = False
DEBUG:2026-01-11 02:23:54,200 - INFO - DEBUG:
 nsamples: 606, gridsize: 1 (using efficient random sample-then-group strategy)2026-01-11 02:23:54,200 - INFO - nsamples: 606, gridsize: 1 (using efficient random sample-then-group strategy)

2026-01-11 02:23:54,200 - INFO - Using efficient random sampling strategy for gridsize=1
2026-01-11 02:23:54,200 - INFO - [OVERSAMPLING DEBUG] Taking efficient branch: standard sample-then-group
2026-01-11 02:23:54,200 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently called with: nsamples=606, K=7, C=1
2026-01-11 02:23:54,200 - INFO - Generating 606 groups efficiently from 2424 points (K=7, C=1)
2026-01-11 02:23:54,200 - INFO - [OVERSAMPLING DEBUG] Standard case: using 606 groups from 2424 points
2026-01-11 02:23:54,200 - INFO - [OVERSAMPLING DEBUG] Randomly sampled 606 seed points
2026-01-11 02:23:54,200 - INFO - Sampled 606 seed points from 2424 total points
2026-01-11 02:23:54,200 - INFO - Using seed indices directly for C=1 (gridsize=1) - no neighbor search needed
2026-01-11 02:23:54,200 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently completed: generated 606 groups
2026-01-11 02:23:54,200 - INFO - Successfully generated 606 groups with shape (606, 1)
2026-01-11 02:23:54,200 - INFO - [OVERSAMPLING DEBUG] Generated 606 groups in total
2026-01-11 02:23:54,200 - INFO - Generated 606 groups efficiently
INFO: Using pre-computed 'Y' array from the input file.2026-01-11 02:23:54,201 - INFO - INFO: Using pre-computed 'Y' array from the input file.

neighbor-sampled diffraction shape2026-01-11 02:23:54,215 - INFO - neighbor-sampled diffraction shape
 (606, 64, 64, 1)2026-01-11 02:23:54,215 - INFO - (606, 64, 64, 1)

loader: using provided ground truth patches.2026-01-11 02:23:54,240 - INFO - loader: using provided ground truth patches.

INFO:2026-01-11 02:23:54,502 - INFO - INFO:
 None2026-01-11 02:23:54,503 - INFO - None

<PtychoDataContainer X=(606, 64, 64, 1) mean=0.406 Y_I=(606, 64, 64, 1) mean=0.563 Y_phi=(606, 64, 64, 1) mean=-0.031 coords_nominal=(606, 1, 2, 1) mean=0.000 coords_true=(606, 1, 2, 1) mean=0.000 probe=(64, 64) mean_amplitude=0.006 norm_Y_I=<scalar> nn_indices=(606, 1) global_offsets=(606, 1, 2, 1) local_offsets=(606, 1, 2, 1)>2026-01-11 02:23:54,504 - INFO - <PtychoDataContainer X=(606, 64, 64, 1) mean=0.406 Y_I=(606, 64, 64, 1) mean=0.563 Y_phi=(606, 64, 64, 1) mean=-0.031 coords_nominal=(606, 1, 2, 1) mean=0.000 coords_true=(606, 1, 2, 1) mean=0.000 probe=(64, 64) mean_amplitude=0.006 norm_Y_I=<scalar> nn_indices=(606, 1) global_offsets=(606, 1, 2, 1) local_offsets=(606, 1, 2, 1)>

2026-01-11 02:23:54,510 - INFO - Gridsize validated: params.cfg['gridsize']=1 matches 1 channels
DEBUG _flat_to_channel: gridsize from parameter: 12026-01-11 02:23:55,368 - INFO - DEBUG _flat_to_channel: gridsize from parameter: 1

DEBUG _flat_to_channel: N from parameter: 742026-01-11 02:23:55,368 - INFO - DEBUG _flat_to_channel: N from parameter: 74

DEBUG _flat_to_channel: input shape=(32, 74, 74, 1), reshaping to (-1, 1, 74, 74)2026-01-11 02:23:55,368 - INFO - DEBUG _flat_to_channel: input shape=(32, 74, 74, 1), reshaping to (-1, 1, 74, 74)

DEBUG _flat_to_channel: gridsize from parameter: 12026-01-11 02:23:55,537 - INFO - DEBUG _flat_to_channel: gridsize from parameter: 1

DEBUG _flat_to_channel: N from parameter: 742026-01-11 02:23:55,537 - INFO - DEBUG _flat_to_channel: N from parameter: 74

DEBUG _flat_to_channel: input shape=(32, 74, 74, 1), reshaping to (-1, 1, 74, 74)2026-01-11 02:23:55,537 - INFO - DEBUG _flat_to_channel: input shape=(32, 74, 74, 1), reshaping to (-1, 1, 74, 74)

DEBUG _flat_to_channel: gridsize from parameter: 12026-01-11 02:23:55,618 - INFO - DEBUG _flat_to_channel: gridsize from parameter: 1

DEBUG _flat_to_channel: N from parameter: 742026-01-11 02:23:55,618 - INFO - DEBUG _flat_to_channel: N from parameter: 74

DEBUG _flat_to_channel: input shape=(32, 74, 74, 1), reshaping to (-1, 1, 74, 74)2026-01-11 02:23:55,618 - INFO - DEBUG _flat_to_channel: input shape=(32, 74, 74, 1), reshaping to (-1, 1, 74, 74)

DEBUG _flat_to_channel: gridsize from parameter: 12026-01-11 02:23:55,712 - INFO - DEBUG _flat_to_channel: gridsize from parameter: 1

DEBUG _flat_to_channel: N from parameter: 742026-01-11 02:23:55,712 - INFO - DEBUG _flat_to_channel: N from parameter: 74

DEBUG _flat_to_channel: input shape=(32, 74, 74, 1), reshaping to (-1, 1, 74, 74)2026-01-11 02:23:55,712 - INFO - DEBUG _flat_to_channel: input shape=(32, 74, 74, 1), reshaping to (-1, 1, 74, 74)

WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1768127036.061390  913959 service.cc:152] XLA service 0x7635ac00d720 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1768127036.061414  913959 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6
2026-01-11 02:23:56.108612: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2026-01-11 02:23:56.230524: W tensorflow/core/framework/op_kernel.cc:1857] OP_REQUIRES failed at sequence_ops.cc:98 : INVALID_ARGUMENT: Input 0 to node `functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_1_0` with op Range must be a compile-time constant.

XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

2026-01-11 02:23:56.230586: W tensorflow/core/framework/op_kernel.cc:1857] OP_REQUIRES failed at if_op.cc:285 : INVALID_ARGUMENT: Input 0 to node `functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_1_0` with op Range must be a compile-time constant.

XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[{{node functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_1_0}}]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_while_cond_true_4914_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_while_cond_tfg_inlined_functional_1_1_padded_obj_2_1_cond_while_1_0_0_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

2026-01-11 02:23:56.230617: W tensorflow/core/framework/op_kernel.cc:1857] OP_REQUIRES failed at while_op.cc:403 : INVALID_ARGUMENT: Input 0 to node `functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_1_0` with op Range must be a compile-time constant.

XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[{{node functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_1_0}}]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_while_cond_true_4914_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_while_cond_tfg_inlined_functional_1_1_padded_obj_2_1_cond_while_1_0_0_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_1]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_while_body_4880_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_while_tfg_inlined_functional_1_1_padded_obj_2_1_cond_1_1_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

2026-01-11 02:23:56.230665: W tensorflow/core/framework/op_kernel.cc:1857] OP_REQUIRES failed at if_op.cc:291 : INVALID_ARGUMENT: Input 0 to node `functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_1_0` with op Range must be a compile-time constant.

XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[{{node functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_1_0}}]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_while_cond_true_4914_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_while_cond_tfg_inlined_functional_1_1_padded_obj_2_1_cond_while_1_0_0_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_1]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_while_body_4880_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_while_tfg_inlined_functional_1_1_padded_obj_2_1_cond_1_1_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[functional_1_1/padded_obj_2_1/cond/while_tfg_inlined_functional_1_1/padded_obj_2_1/cond_1]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_false_4474_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_1_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
File "Documents/PtychoPINN/scripts/compare_models.py", line 1774, in <module>
File "Documents/PtychoPINN/scripts/compare_models.py", line 1166, in main
File "Documents/PtychoPINN/ptycho/workflows/components.py", line 163, in predict
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 566, in predict
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 260, in one_step_on_data_distributed
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 250, in one_step_on_data
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 105, in predict_step
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py", line 941, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/operation.py", line 59, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 156, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/models/functional.py", line 183, in call
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/function.py", line 206, in _run_through_graph
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/models/functional.py", line 644, in call
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py", line 941, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/operation.py", line 59, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 156, in error_handler
File "Documents/PtychoPINN/ptycho/custom_layers.py", line 172, in call
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1100, in reassemble_patches
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 692, in newf
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1350, in reassemble_patches_position_batched_real
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1027, in _reassemble_position_batched

2026-01-11 02:23:56.234129: W tensorflow/core/framework/op_kernel.cc:1857] OP_REQUIRES failed at xla_ops.cc:591 : INVALID_ARGUMENT: Input 0 to node `functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_1_0` with op Range must be a compile-time constant.

XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[{{node functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_1_0}}]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_while_cond_true_4914_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_while_cond_tfg_inlined_functional_1_1_padded_obj_2_1_cond_while_1_0_0_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_1]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_while_body_4880_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_while_tfg_inlined_functional_1_1_padded_obj_2_1_cond_1_1_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[functional_1_1/padded_obj_2_1/cond/while_tfg_inlined_functional_1_1/padded_obj_2_1/cond_1]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_false_4474_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_1_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
File "Documents/PtychoPINN/scripts/compare_models.py", line 1774, in <module>
File "Documents/PtychoPINN/scripts/compare_models.py", line 1166, in main
File "Documents/PtychoPINN/ptycho/workflows/components.py", line 163, in predict
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 566, in predict
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 260, in one_step_on_data_distributed
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 250, in one_step_on_data
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 105, in predict_step
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py", line 941, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/operation.py", line 59, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 156, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/models/functional.py", line 183, in call
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/function.py", line 206, in _run_through_graph
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/models/functional.py", line 644, in call
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py", line 941, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/operation.py", line 59, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 156, in error_handler
File "Documents/PtychoPINN/ptycho/custom_layers.py", line 172, in call
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1100, in reassemble_patches
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 692, in newf
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1350, in reassemble_patches_position_batched_real
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1027, in _reassemble_position_batched

	 [[functional_1_1/padded_obj_2_1/cond]]
	tf2xla conversion failed while converting __inference_one_step_on_data_6492[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.
2026-01-11 02:23:56.234168: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Input 0 to node `functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_1_0` with op Range must be a compile-time constant.

XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[{{node functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_1_0}}]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_while_cond_true_4914_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_while_cond_tfg_inlined_functional_1_1_padded_obj_2_1_cond_while_1_0_0_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_1]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_while_body_4880_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_while_tfg_inlined_functional_1_1_padded_obj_2_1_cond_1_1_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[functional_1_1/padded_obj_2_1/cond/while_tfg_inlined_functional_1_1/padded_obj_2_1/cond_1]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_false_4474_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_1_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
File "Documents/PtychoPINN/scripts/compare_models.py", line 1774, in <module>
File "Documents/PtychoPINN/scripts/compare_models.py", line 1166, in main
File "Documents/PtychoPINN/ptycho/workflows/components.py", line 163, in predict
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 566, in predict
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 260, in one_step_on_data_distributed
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 250, in one_step_on_data
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 105, in predict_step
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py", line 941, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/operation.py", line 59, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 156, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/models/functional.py", line 183, in call
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/function.py", line 206, in _run_through_graph
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/models/functional.py", line 644, in call
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py", line 941, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/operation.py", line 59, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 156, in error_handler
File "Documents/PtychoPINN/ptycho/custom_layers.py", line 172, in call
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1100, in reassemble_patches
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 692, in newf
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1350, in reassemble_patches_position_batched_real
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1027, in _reassemble_position_batched

	 [[functional_1_1/padded_obj_2_1/cond]]
	tf2xla conversion failed while converting __inference_one_step_on_data_6492[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.
	 [[StatefulPartitionedCall]]
2026-01-11 02:23:56,307 - CRITICAL - Uncaught exception
Traceback (most recent call last):
  File "/home/ollie/Documents/PtychoPINN/scripts/compare_models.py", line 1774, in <module>
    main()
  File "/home/ollie/Documents/PtychoPINN/scripts/compare_models.py", line 1166, in main
    pinn_patches = pinn_model.predict(
                   ^^^^^^^^^^^^^^^^^^^
  File "/home/ollie/Documents/PtychoPINN/ptycho/workflows/components.py", line 163, in predict
    return self._model.predict(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/tensorflow/python/eager/execute.py", line 59, in quick_execute
    except TypeError as e:
tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:

Detected at node functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_1_0 defined at (most recent call last):
<stack traces unavailable>
Input 0 to node `functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_1_0` with op Range must be a compile-time constant.

XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[{{node functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_1_0}}]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_while_cond_true_4914_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_while_cond_tfg_inlined_functional_1_1_padded_obj_2_1_cond_while_1_0_0_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_1]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_while_body_4880_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_while_tfg_inlined_functional_1_1_padded_obj_2_1_cond_1_1_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[functional_1_1/padded_obj_2_1/cond/while_tfg_inlined_functional_1_1/padded_obj_2_1/cond_1]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_false_4474_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_1_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
File "Documents/PtychoPINN/scripts/compare_models.py", line 1774, in <module>
File "Documents/PtychoPINN/scripts/compare_models.py", line 1166, in main
File "Documents/PtychoPINN/ptycho/workflows/components.py", line 163, in predict
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 566, in predict
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 260, in one_step_on_data_distributed
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 250, in one_step_on_data
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 105, in predict_step
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py", line 941, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/operation.py", line 59, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 156, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/models/functional.py", line 183, in call
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/function.py", line 206, in _run_through_graph
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/models/functional.py", line 644, in call
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py", line 941, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/operation.py", line 59, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 156, in error_handler
File "Documents/PtychoPINN/ptycho/custom_layers.py", line 172, in call
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1100, in reassemble_patches
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 692, in newf
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1350, in reassemble_patches_position_batched_real
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1027, in _reassemble_position_batched

	 [[functional_1_1/padded_obj_2_1/cond]]
	tf2xla conversion failed while converting __inference_one_step_on_data_6492[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.
	 [[StatefulPartitionedCall]] [Op:__inference_one_step_on_data_distributed_6585]
Traceback (most recent call last):
2026-01-11 02:23:56,308 - ERROR - Traceback (most recent call last):
  File "/home/ollie/Documents/PtychoPINN/scripts/compare_models.py", line 1774, in <module>
2026-01-11 02:23:56,308 - ERROR - File "/home/ollie/Documents/PtychoPINN/scripts/compare_models.py", line 1774, in <module>
    main()2026-01-11 02:23:56,308 - ERROR - main()

  File "/home/ollie/Documents/PtychoPINN/scripts/compare_models.py", line 1166, in main
2026-01-11 02:23:56,308 - ERROR - File "/home/ollie/Documents/PtychoPINN/scripts/compare_models.py", line 1166, in main
    pinn_patches = pinn_model.predict(2026-01-11 02:23:56,308 - ERROR - pinn_patches = pinn_model.predict(

                   ^2026-01-11 02:23:56,308 - ERROR - ^
^2026-01-11 02:23:56,308 - ERROR - ^
^2026-01-11 02:23:56,308 - ERROR - ^
^2026-01-11 02:23:56,308 - ERROR - ^
^2026-01-11 02:23:56,308 - ERROR - ^
^2026-01-11 02:23:56,308 - ERROR - ^
^2026-01-11 02:23:56,308 - ERROR - ^
^2026-01-11 02:23:56,308 - ERROR - ^
^2026-01-11 02:23:56,308 - ERROR - ^
^2026-01-11 02:23:56,308 - ERROR - ^
^2026-01-11 02:23:56,308 - ERROR - ^
^2026-01-11 02:23:56,308 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^

  File "/home/ollie/Documents/PtychoPINN/ptycho/workflows/components.py", line 163, in predict
2026-01-11 02:23:56,309 - ERROR - File "/home/ollie/Documents/PtychoPINN/ptycho/workflows/components.py", line 163, in predict
    return self._model.predict(*args, **kwargs)2026-01-11 02:23:56,309 - ERROR - return self._model.predict(*args, **kwargs)

           ^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^
^2026-01-11 02:23:56,309 - ERROR - ^

  File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
2026-01-11 02:23:56,309 - ERROR - File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None2026-01-11 02:23:56,309 - ERROR - raise e.with_traceback(filtered_tb) from None

  File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
2026-01-11 02:23:56,309 - ERROR - File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,2026-01-11 02:23:56,310 - ERROR - tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,

  ^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,310 - ERROR - ^
^2026-01-11 02:23:56,311 - ERROR - ^
^2026-01-11 02:23:56,311 - ERROR - ^
^2026-01-11 02:23:56,311 - ERROR - ^
^2026-01-11 02:23:56,311 - ERROR - ^
^2026-01-11 02:23:56,311 - ERROR - ^
^2026-01-11 02:23:56,311 - ERROR - ^
^2026-01-11 02:23:56,311 - ERROR - ^
^2026-01-11 02:23:56,311 - ERROR - ^
^2026-01-11 02:23:56,311 - ERROR - ^
^2026-01-11 02:23:56,311 - ERROR - ^
^2026-01-11 02:23:56,311 - ERROR - ^

tensorflow.python.framework.errors_impl2026-01-11 02:23:56,311 - ERROR - tensorflow.python.framework.errors_impl
.2026-01-11 02:23:56,311 - ERROR - .
InvalidArgumentError2026-01-11 02:23:56,311 - ERROR - InvalidArgumentError
: 2026-01-11 02:23:56,311 - ERROR - :
Graph execution error:

Detected at node functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_1_0 defined at (most recent call last):
<stack traces unavailable>
Input 0 to node `functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_1_0` with op Range must be a compile-time constant.

XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[{{node functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_1_0}}]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_while_cond_true_4914_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_while_cond_tfg_inlined_functional_1_1_padded_obj_2_1_cond_while_1_0_0_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_1]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_while_body_4880_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_while_tfg_inlined_functional_1_1_padded_obj_2_1_cond_1_1_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[functional_1_1/padded_obj_2_1/cond/while_tfg_inlined_functional_1_1/padded_obj_2_1/cond_1]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_false_4474_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_1_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
File "Documents/PtychoPINN/scripts/compare_models.py", line 1774, in <module>
File "Documents/PtychoPINN/scripts/compare_models.py", line 1166, in main
File "Documents/PtychoPINN/ptycho/workflows/components.py", line 163, in predict
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 566, in predict
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 260, in one_step_on_data_distributed
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 250, in one_step_on_data
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 105, in predict_step
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py", line 941, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/operation.py", line 59, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 156, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/models/functional.py", line 183, in call
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/function.py", line 206, in _run_through_graph
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/models/functional.py", line 644, in call
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py", line 941, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/operation.py", line 59, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 156, in error_handler
File "Documents/PtychoPINN/ptycho/custom_layers.py", line 172, in call
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1100, in reassemble_patches
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 692, in newf
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1350, in reassemble_patches_position_batched_real
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1027, in _reassemble_position_batched

	 [[functional_1_1/padded_obj_2_1/cond]]
	tf2xla conversion failed while converting __inference_one_step_on_data_6492[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.
	 [[StatefulPartitionedCall]] [Op:__inference_one_step_on_data_distributed_6585]2026-01-11 02:23:56,311 - ERROR - Graph execution error:

Detected at node functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_1_0 defined at (most recent call last):
<stack traces unavailable>
Input 0 to node `functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_1_0` with op Range must be a compile-time constant.

XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[{{node functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_1_0}}]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_while_cond_true_4914_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_while_cond_tfg_inlined_functional_1_1_padded_obj_2_1_cond_while_1_0_0_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_1]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_while_body_4880_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_while_tfg_inlined_functional_1_1_padded_obj_2_1_cond_1_1_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[functional_1_1/padded_obj_2_1/cond/while_tfg_inlined_functional_1_1/padded_obj_2_1/cond_1]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_false_4474_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_1_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
File "Documents/PtychoPINN/scripts/compare_models.py", line 1774, in <module>
File "Documents/PtychoPINN/scripts/compare_models.py", line 1166, in main
File "Documents/PtychoPINN/ptycho/workflows/components.py", line 163, in predict
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 566, in predict
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 260, in one_step_on_data_distributed
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 250, in one_step_on_data
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 105, in predict_step
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py", line 941, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/operation.py", line 59, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 156, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/models/functional.py", line 183, in call
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/function.py", line 206, in _run_through_graph
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/models/functional.py", line 644, in call
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py", line 941, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/operation.py", line 59, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 156, in error_handler
File "Documents/PtychoPINN/ptycho/custom_layers.py", line 172, in call
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1100, in reassemble_patches
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 692, in newf
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1350, in reassemble_patches_position_batched_real
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1027, in _reassemble_position_batched

	 [[functional_1_1/padded_obj_2_1/cond]]
	tf2xla conversion failed while converting __inference_one_step_on_data_6492[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.
	 [[StatefulPartitionedCall]] [Op:__inference_one_step_on_data_distributed_6585]

[2026-01-11 02:23:57] SUCCESS: Model comparison (train_subsample=64, train_groups=64, trial=1)
[2026-01-11 02:23:57] Completed comparisons for train_groups=64
[2026-01-11 02:23:57] Model comparison phase completed
[2026-01-11 02:23:57] === STEP 4: Results Aggregation ===
[2026-01-11 02:23:57] EXECUTING: PSNR phase generalization plot
[2026-01-11 02:23:57] COMMAND: python scripts/studies/aggregate_and_plot_results.py \
        'gs2_full_subsample_study_20260111_0222' \
        --metric psnr \
        --part phase \
        --output psnr_phase_generalization.png
02:23:57 - INFO - Processing study directory: gs2_full_subsample_study_20260111_0222
02:23:57 - INFO - Analyzing psnr_phase
02:23:57 - ERROR - Failed to process results: No valid comparison files found in gs2_full_subsample_study_20260111_0222
[2026-01-11 02:23:57] SUCCESS: PSNR phase generalization plot
[2026-01-11 02:23:57] EXECUTING: FRC amplitude generalization plot
[2026-01-11 02:23:57] COMMAND: python scripts/studies/aggregate_and_plot_results.py \
        'gs2_full_subsample_study_20260111_0222' \
        --metric frc50 \
        --part amp \
        --output frc50_amp_generalization.png
02:23:58 - INFO - Processing study directory: gs2_full_subsample_study_20260111_0222
02:23:58 - INFO - Analyzing frc50_amp
02:23:58 - ERROR - Failed to process results: No valid comparison files found in gs2_full_subsample_study_20260111_0222
[2026-01-11 02:23:58] SUCCESS: FRC amplitude generalization plot
[2026-01-11 02:23:58] EXECUTING: MAE amplitude generalization plot
[2026-01-11 02:23:58] COMMAND: python scripts/studies/aggregate_and_plot_results.py \
        'gs2_full_subsample_study_20260111_0222' \
        --metric mae \
        --part amp \
        --output mae_amp_generalization.png
02:23:58 - INFO - Processing study directory: gs2_full_subsample_study_20260111_0222
02:23:58 - INFO - Analyzing mae_amp
02:23:58 - ERROR - Failed to process results: No valid comparison files found in gs2_full_subsample_study_20260111_0222
[2026-01-11 02:23:58] SUCCESS: MAE amplitude generalization plot
[2026-01-11 02:23:58] EXECUTING: SSIM amplitude generalization plot
[2026-01-11 02:23:58] COMMAND: python scripts/studies/aggregate_and_plot_results.py \
        'gs2_full_subsample_study_20260111_0222' \
        --metric ssim \
        --part amp \
        --output ssim_amp_generalization.png
02:23:59 - INFO - Processing study directory: gs2_full_subsample_study_20260111_0222
02:23:59 - INFO - Analyzing ssim_amp
02:23:59 - ERROR - Failed to process results: No valid comparison files found in gs2_full_subsample_study_20260111_0222
[2026-01-11 02:23:59] SUCCESS: SSIM amplitude generalization plot
[2026-01-11 02:23:59] EXECUTING: SSIM phase generalization plot
[2026-01-11 02:23:59] COMMAND: python scripts/studies/aggregate_and_plot_results.py \
        'gs2_full_subsample_study_20260111_0222' \
        --metric ssim \
        --part phase \
        --output ssim_phase_generalization.png
02:23:59 - INFO - Processing study directory: gs2_full_subsample_study_20260111_0222
02:23:59 - INFO - Analyzing ssim_phase
02:23:59 - ERROR - Failed to process results: No valid comparison files found in gs2_full_subsample_study_20260111_0222
[2026-01-11 02:23:59] SUCCESS: SSIM phase generalization plot
[2026-01-11 02:23:59] EXECUTING: MS-SSIM amplitude generalization plot
[2026-01-11 02:23:59] COMMAND: python scripts/studies/aggregate_and_plot_results.py \
        'gs2_full_subsample_study_20260111_0222' \
        --metric ms_ssim \
        --part amp \
        --output ms_ssim_amp_generalization.png
02:24:00 - INFO - Processing study directory: gs2_full_subsample_study_20260111_0222
02:24:00 - INFO - Analyzing ms_ssim_amp
02:24:00 - ERROR - Failed to process results: No valid comparison files found in gs2_full_subsample_study_20260111_0222
[2026-01-11 02:24:00] SUCCESS: MS-SSIM amplitude generalization plot
[2026-01-11 02:24:00] EXECUTING: MS-SSIM phase generalization plot
[2026-01-11 02:24:00] COMMAND: python scripts/studies/aggregate_and_plot_results.py \
        'gs2_full_subsample_study_20260111_0222' \
        --metric ms_ssim \
        --part phase \
        --output ms_ssim_phase_generalization.png
02:24:00 - INFO - Processing study directory: gs2_full_subsample_study_20260111_0222
02:24:00 - INFO - Analyzing ms_ssim_phase
02:24:00 - ERROR - Failed to process results: No valid comparison files found in gs2_full_subsample_study_20260111_0222
[2026-01-11 02:24:00] SUCCESS: MS-SSIM phase generalization plot
[2026-01-11 02:24:00] Results aggregation completed
[2026-01-11 02:24:00] === Generating Summary Report ===
[2026-01-11 02:24:00] Summary report generated: gs2_full_subsample_study_20260111_0222/STUDY_SUMMARY.md
[2026-01-11 02:24:00] === Study Completed Successfully ===
[2026-01-11 02:24:00] Training sizes tested: 1
[2026-01-11 02:24:00] Trials per size: 1
[2026-01-11 02:24:00] Total trials completed: 1
[2026-01-11 02:24:00] Total runtime: 00:01:05
[2026-01-11 02:24:00] Results directory: gs2_full_subsample_study_20260111_0222
[2026-01-11 02:24:00] Summary report: gs2_full_subsample_study_20260111_0222/STUDY_SUMMARY.md
