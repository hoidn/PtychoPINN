diff --git a/tests/test_patch_extraction_equivalence.py b/tests/test_patch_extraction_equivalence.py
index 6b5a392..9c8a340 100644
--- a/tests/test_patch_extraction_equivalence.py
+++ b/tests/test_patch_extraction_equivalence.py
@@ -4,11 +4,42 @@ patch extraction implementations.
 
 This module contains critical tests that prove the new high-performance batched
 implementation produces identical results to the legacy iterative implementation.
+
+Test Coverage Matrix:
+--------------------
+- Patch sizes (N): 32, 64, 128, 256
+- Grid sizes: 1, 2, 3 (corresponding to c=1, 4, 9 channels)
+- Batch sizes: 1, 10, 50, 100
+- Data types: complex64, complex128
+- Edge cases: border coordinates, zero offsets, single patch
+
+Performance Benchmarking:
+------------------------
+- Measures execution time for both implementations
+- Validates minimum 3x speedup requirement
+- Tests memory usage to ensure < 1.2x increase
+- Generates comprehensive performance reports
+
+Tolerance Justification:
+-----------------------
+We use atol=1e-6 for all equivalence tests. This tolerance:
+- Accounts for minor floating-point reordering effects
+- Is tight enough to catch meaningful algorithmic differences
+- Has been validated across diverse test configurations
+
+Running Performance Reports:
+---------------------------
+To generate a detailed performance report:
+    python -m unittest tests.test_patch_extraction_equivalence.TestPatchExtractionEquivalence.generate_performance_report
+
+The report will be saved to: test_outputs/patch_extraction_performance.txt
 """
 import unittest
 import numpy as np
 import tensorflow as tf
 import time
+import tracemalloc
+import os
 from ptycho.raw_data import _get_image_patches_iterative, _get_image_patches_batched
 from ptycho import tf_helper as hh
 
@@ -73,10 +104,34 @@ class TestPatchExtractionEquivalence(unittest.TestCase):
         # Run batched (new) implementation
         batched_result = _get_image_patches_batched(gt_padded, offsets_f, N, B, c)
         
+        # Calculate numerical differences for analysis
+        iterative_array = iterative_result.numpy()
+        batched_array = batched_result.numpy()
+        
+        # Compute difference metrics
+        abs_diff = np.abs(iterative_array - batched_array)
+        max_abs_diff = np.max(abs_diff)
+        mean_abs_diff = np.mean(abs_diff)
+        
+        # Compute relative differences where iterative is non-zero
+        mask = np.abs(iterative_array) > 1e-10
+        rel_diff = np.zeros_like(abs_diff, dtype=np.float32)
+        rel_diff[mask] = abs_diff[mask] / np.abs(iterative_array[mask])
+        max_rel_diff = np.max(rel_diff)
+        mean_rel_diff = np.mean(rel_diff[mask]) if np.any(mask) else 0.0
+        
+        # Log difference statistics
+        if max_abs_diff > 1e-8:  # Only log if there are meaningful differences
+            print(f"\nNumerical Difference Analysis:")
+            print(f"  Max absolute difference: {max_abs_diff:.2e}")
+            print(f"  Mean absolute difference: {mean_abs_diff:.2e}")
+            print(f"  Max relative difference: {max_rel_diff:.2e}")
+            print(f"  Mean relative difference: {mean_rel_diff:.2e}")
+        
         # Assert numerical equivalence with a tight tolerance
         np.testing.assert_allclose(
-            iterative_result.numpy(),
-            batched_result.numpy(),
+            iterative_array,
+            batched_array,
             atol=1e-6,
             err_msg="Batched implementation output does not match iterative implementation for standard case."
         )
@@ -231,6 +286,138 @@ class TestPatchExtractionEquivalence(unittest.TestCase):
             f"Expected at least 3x speedup, but got {speedup:.1f}x. "
             f"Iterative: {iterative_time:.4f}s, Batched: {batched_time:.4f}s"
         )
+        
+    def test_memory_usage(self):
+        """Test that batched implementation uses less than 1.2x memory of iterative."""
+        # Use moderate size to get meaningful memory measurements
+        gt_padded, offsets_f, N, B, c = self._generate_test_data(
+            obj_size=256, N=64, gridsize=2, B=50
+        )
+        
+        # Measure iterative implementation memory
+        tracemalloc.start()
+        _ = _get_image_patches_iterative(gt_padded, offsets_f, N, B, c)
+        iterative_current, iterative_peak = tracemalloc.get_traced_memory()
+        tracemalloc.stop()
+        
+        # Clear memory before next measurement
+        tf.keras.backend.clear_session()
+        
+        # Measure batched implementation memory
+        tracemalloc.start()
+        _ = _get_image_patches_batched(gt_padded, offsets_f, N, B, c)
+        batched_current, batched_peak = tracemalloc.get_traced_memory()
+        tracemalloc.stop()
+        
+        # Calculate memory ratio
+        memory_ratio = batched_peak / iterative_peak
+        
+        # Log memory usage
+        print(f"\nMemory Usage Results:")
+        print(f"  Iterative peak: {iterative_peak / 1024 / 1024:.2f} MB")
+        print(f"  Batched peak: {batched_peak / 1024 / 1024:.2f} MB")
+        print(f"  Memory ratio: {memory_ratio:.2f}x")
+        
+        # Assert memory efficiency
+        self.assertLess(
+            memory_ratio, 1.2,
+            f"Batched implementation uses {memory_ratio:.2f}x memory, "
+            f"exceeding 1.2x threshold"
+        )
+        
+    def generate_performance_report(self):
+        """Generate comprehensive performance report across multiple configurations."""
+        report_lines = [
+            "=" * 80,
+            "Patch Extraction Performance Report",
+            "=" * 80,
+            f"Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}",
+            "",
+            "Configuration Test Results:",
+            "-" * 80
+        ]
+        
+        # Test configurations
+        configs = [
+            {"N": 32, "gridsize": 1, "B": 100},
+            {"N": 64, "gridsize": 2, "B": 100},
+            {"N": 128, "gridsize": 2, "B": 50},
+            {"N": 64, "gridsize": 3, "B": 100},
+        ]
+        
+        for config in configs:
+            gt_padded, offsets_f, N, B, c = self._generate_test_data(
+                obj_size=config["N"] * 4, 
+                N=config["N"], 
+                gridsize=config["gridsize"], 
+                B=config["B"]
+            )
+            
+            # Time iterative
+            start = time.time()
+            for _ in range(5):
+                _ = _get_image_patches_iterative(gt_padded, offsets_f, N, B, c)
+            iterative_time = (time.time() - start) / 5
+            
+            # Time batched
+            start = time.time()
+            for _ in range(5):
+                _ = _get_image_patches_batched(gt_padded, offsets_f, N, B, c)
+            batched_time = (time.time() - start) / 5
+            
+            speedup = iterative_time / batched_time
+            
+            report_lines.extend([
+                f"\nConfiguration: N={N}, gridsize={config['gridsize']}, B={B}",
+                f"  Iterative: {iterative_time:.4f}s",
+                f"  Batched:   {batched_time:.4f}s", 
+                f"  Speedup:   {speedup:.1f}x"
+            ])
+        
+        # Memory usage summary
+        report_lines.extend([
+            "",
+            "-" * 80,
+            "Memory Usage Analysis:",
+            ""
+        ])
+        
+        # Run memory test
+        gt_padded, offsets_f, N, B, c = self._generate_test_data(
+            obj_size=256, N=64, gridsize=2, B=50
+        )
+        
+        tracemalloc.start()
+        _ = _get_image_patches_iterative(gt_padded, offsets_f, N, B, c)
+        _, iterative_peak = tracemalloc.get_traced_memory()
+        tracemalloc.stop()
+        
+        tf.keras.backend.clear_session()
+        
+        tracemalloc.start()
+        _ = _get_image_patches_batched(gt_padded, offsets_f, N, B, c)
+        _, batched_peak = tracemalloc.get_traced_memory()
+        tracemalloc.stop()
+        
+        report_lines.extend([
+            f"Peak Memory Usage:",
+            f"  Iterative: {iterative_peak / 1024 / 1024:.2f} MB",
+            f"  Batched:   {batched_peak / 1024 / 1024:.2f} MB",
+            f"  Ratio:     {batched_peak / iterative_peak:.2f}x",
+            "",
+            "=" * 80
+        ])
+        
+        # Save report
+        report_dir = "test_outputs"
+        os.makedirs(report_dir, exist_ok=True)
+        report_path = os.path.join(report_dir, "patch_extraction_performance.txt")
+        
+        with open(report_path, "w") as f:
+            f.write("\n".join(report_lines))
+            
+        print(f"\nPerformance report saved to: {report_path}")
+        return "\n".join(report_lines)
 
 
 if __name__ == '__main__':
