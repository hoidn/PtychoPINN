# PtychoPINN Test Suite Index

This document provides a comprehensive index of the automated tests in the `tests/` directory. Its purpose is to make the test suite discoverable, explain the scope of each test module, and provide direct commands for running specific tests.

## How to Run Tests

- **Run all tests:** `python -m unittest discover tests/`
- **Run a specific file:** `python -m unittest tests.image.test_cropping`
- **Run a specific test class:** `python -m unittest tests.test_integration_workflow.TestFullWorkflow`

---

## Test Modules

### Core Library Tests (`tests/`)

| Test File | Purpose / Scope | Key Tests | Usage / Command | Notes |
| :--- | :--- | :--- | :--- | :--- |

| `test_baselines.py` | No module docstring found. | `test_build_model_always_creates_single_channel_output` | `python -m unittest tests.test_baselines` | — |
| `test_benchmark_throughput.py` | Unit and Integration Tests for Inference Throughput Benchmarking  This module tests the benchmarking infrastructure for measuring and optimizing PtychoPINN inference throughput. | `test_calculate_efficiency`, `test_calculate_latency`, `test_calculate_throughput`, `test_clear_timings`, `test_config_custom_values`, `test_config_initialization`, `test_export_results`, `test_find_optimal_batch_size_no_success`, `test_find_optimal_batch_size`, `test_func`, `test_generate_summary`, `test_get_statistics`, `test_load_model_and_data`, `test_measure_function`, `test_profile_cpu_memory`, `test_snapshot`, `test_timing_profiler_performance`, `test_warmup_inference` | `python -m unittest tests.test_benchmark_throughput` | — |
| `test_cli_args.py` | Unit tests for the ptycho.cli_args module. | `test_add_logging_arguments`, `test_console_level_choices`, `test_get_logging_config_custom_level`, `test_get_logging_config_defaults`, `test_get_logging_config_quiet`, `test_get_logging_config_verbose`, `test_quiet_verbose_mutually_exclusive` | `python -m unittest tests.test_cli_args` | — |
| `test_coordinate_grouping.py` | Comprehensive test suite for efficient coordinate grouping implementation.  This module tests the new "sample-then-group" strategy for coordinate grouping in the RawData class, ensuring correctness, performance, and edge case handling. | `test_backward_compatibility`, `test_different_seeds_produce_different_results`, `test_edge_case_k_less_than_c`, `test_edge_case_more_samples_than_points`, `test_edge_case_small_dataset`, `test_efficient_grouping_output_shape`, `test_efficient_grouping_spatial_coherence`, `test_efficient_grouping_valid_indices`, `test_existing_tests_still_pass`, `test_generate_grouped_data_gridsize_1`, `test_generate_grouped_data_integration`, `test_memory_efficiency`, `test_no_cache_files_created`, `test_performance_improvement`, `test_reproducibility_with_seed` | `python -m unittest tests.test_coordinate_grouping` | — |
| `test_fno_hyperparam_study.py` | No module docstring found. | `test_sweep_writes_csv`, `test_write_pareto_plot` | `python -m unittest tests.test_fno_hyperparam_study` | — |
| `test_generator_registry.py` | No module docstring found. | `test_resolve_generator_cnn`, `test_resolve_generator_unknown_raises` | `python -m unittest tests.test_generator_registry` | — |
| `test_generic_loader.py` | No module docstring found. | `test_generic_loader_roundtrip`, `test_generic_loader` | `python -m unittest tests.test_generic_loader` | — |
| `test_grid_lines_compare_wrapper.py` | Tests for grid_lines_compare_wrapper orchestration. | `test_compare_wrapper_probe_mask_diameter_passthrough`, `test_wrapper_accepts_architecture_list`, `test_wrapper_accepts_plateau_params`, `test_wrapper_accepts_plateau_scheduler`, `test_wrapper_accepts_resnet_width`, `test_wrapper_defaults_torch_loss_mode_mae`, `test_wrapper_handles_new_architectures`, `test_wrapper_handles_stable_hybrid`, `test_wrapper_merges_metrics`, `test_wrapper_passes_grad_clip_algorithm`, `test_wrapper_passes_max_hidden_channels`, `test_wrapper_passes_optimizer`, `test_wrapper_passes_scheduler_knobs`, `test_wrapper_passes_torch_loss_mode_to_runner`, `test_wrapper_renders_visuals` | `python -m unittest tests.test_grid_lines_compare_wrapper` | — |
| `test_grid_lines_workflow.py` | Unit tests for ptycho.workflows.grid_lines_workflow.  Test strategy: plans/active/GRID-LINES-WORKFLOW-001/test_strategy.md | `test_apply_probe_mask_centered_disk`, `test_dataset_out_dir_gridsize1`, `test_dataset_out_dir_layout`, `test_do_not_share_when_nan_present`, `test_do_not_share_when_ranges_differ`, `test_grid_lines_cli_probe_mask_diameter`, `test_grid_lines_uses_ideal_disk_probe`, `test_metadata_includes_coords_type`, `test_metadata_includes_probe_mask_diameter`, `test_metadata_includes_probe_source`, `test_run_grid_lines_workflow_applies_mask`, `test_save_comparison_png_dynamic`, `test_save_comparison_png_skips_missing`, `test_save_recon_artifact_writes_npz`, `test_scale_probe_no_resize_when_same_size`, `test_scale_probe_pad_extrapolate_pads_amplitude_and_extrapolates_phase`, `test_scale_probe_pad_extrapolate_rejects_downscale`, `test_scale_probe_rejects_non_square`, `test_scale_probe_resizes_and_smooths`, `test_share_colorbar_when_ranges_match`, `test_share_when_single_subplot`, `test_stitch_predictions_gridsize1`, `test_stitch_predictions_gridsize2`, `test_stitch_predictions_phase` | `python -m unittest tests.test_grid_lines_workflow` | — |
| `test_integration_baseline_gs2.py` | No module docstring found. | `test_baseline_gridsize2_end_to_end` | `python -m unittest tests.test_integration_baseline_gs2` | — |
| `test_integration_workflow.py` | Validates the full train → save → load → infer workflow using subprocesses, ensuring model artifacts persist across CLI entrypoints. | `test_train_save_load_infer_cycle` | `python -m unittest tests.test_integration_workflow` | Critical integration coverage for TensorFlow persistence. |
| `test_lazy_loading.py` | Tests for lazy tensor allocation in loader.py.  These tests verify the OOM behavior with eager loading (baseline) and the fix via lazy tensor allocation (Phase B).  Spec reference: spec-ptycho-workflow.md Resource Constraints Finding: PINN-CHUNKED-001 (OOM blocker documentation) | `test_as_tf_dataset_yields_correct_structure`, `test_container_numpy_slicing_for_chunked_inference`, `test_dataset_batch_count`, `test_lazy_caching`, `test_lazy_container_backward_compatible`, `test_lazy_container_inference_integration`, `test_lazy_loading_avoids_oom`, `test_len_method`, `test_memory_usage_scales_with_dataset_size`, `test_oom_with_eager_loading`, `test_streaming_training_auto_detection`, `test_tensor_input_handled`, `test_train_accepts_use_streaming_parameter` | `python -m unittest tests.test_lazy_loading` | — |
| `test_log_config.py` | Unit tests for the ptycho.log_config module. | `test_backward_compatibility`, `test_conflicting_flags_verbose_overrides`, `test_custom_console_level`, `test_default_setup_logging_creates_log_directory_and_file`, `test_quiet_flag_overrides_console_level`, `test_quiet_mode_disables_console`, `test_setup_logging_clears_existing_handlers`, `test_string_path_support`, `test_verbose_mode_enables_debug_console` | `python -m unittest tests.test_log_config` | — |
| `test_misc.py` | No module docstring found. | `test_memoize_simulated_data` | `python -m unittest tests.test_misc` | — |
| `test_model_channel_consistency.py` | Regression tests for TensorFlow PINN channel handling. | `test_amp_head_matches_patch_channels`, `test_diffraction_to_obj_accepts_grouped_inputs` | `python -m unittest tests.test_model_channel_consistency` | — |
| `test_model_config_architecture.py` | No module docstring found. | `test_model_config_architecture_default_ok`, `test_model_config_architecture_invalid_raises` | `python -m unittest tests.test_model_config_architecture` | — |
| `test_model_factory.py` | Tests for model factory functions.  Tests MODULE-SINGLETON-001 fix: models created with different N values must work correctly in a single process.  Lazy loading in ptycho/model.py (via __getattr__) prevents import-time model creation, eliminating XLA trace caching conflicts when changing N values. No environment variable workarounds are needed.  Ref: REFACTOR-MODEL-SINGLETON-001, CONFIG-001 | `test_import_no_side_effects`, `test_multi_n_model_creation`, `test_multi_n_with_xla_enabled` | `python -m unittest tests.test_model_factory` | — |
| `test_nphotons_metadata_integration.py` | Comprehensive integration test for nphotons metadata system.  This test verifies the complete nphotons metadata workflow: 1. Simulation with different nphotons values saves metadata correctly 2. Training loads metadata and validates configurations  3. Inference uses correct nphotons from metadata 4. Parameter mismatch warnings work correctly 5. End-to-end workflow maintains nphotons consistency  The test follows the project's integration test patterns using subprocess calls to simulate real user workflows across separate processes. | `test_configuration_mismatch_warnings`, `test_end_to_end_workflow_consistency`, `test_metadata_backward_compatibility`, `test_metadata_persistence_single_nphotons`, `test_multiple_nphotons_metadata_consistency`, `test_training_with_mismatched_config_warns_but_continues` | `python -m unittest tests.test_nphotons_metadata_integration` | — |
| `test_oversampling.py` | Test automatic K choose C oversampling functionality. | `test_automatic_oversampling_triggers`, `test_enable_oversampling_flag_required`, `test_gridsize_1_no_oversampling`, `test_neighbor_pool_size_guard`, `test_oversampling_with_different_k_values`, `test_reproducibility_with_seed`, `test_standard_sampling_no_oversampling` | `python -m unittest tests.test_oversampling` | — |
| `test_projective_warp_xla.py` | Test cases for projective_warp_xla module. | `test_batch_processing`, `test_complex128_dtype`, `test_complex64_dtype`, `test_fill_modes`, `test_float32_dtype`, `test_float64_dtype`, `test_float64_with_translation`, `test_interpolation_modes`, `test_jit_compilation_float32`, `test_jit_compilation_float64`, `test_mixed_precision_translation`, `test_tfa_params_conversion` | `python -m unittest tests.test_projective_warp_xla` | — |
| `test_pytorch_tf_wrapper.py` | No module docstring found. | N/A | `python -m unittest tests.test_pytorch_tf_wrapper` | — |
| `test_raw_data_grouping.py` | Exhaustively checks the RawData sample-then-group implementation, covering shapes, bounds, locality, and oversized sampling requests. | `test_content_validity`, `test_edge_case_k_less_than_c`, `test_edge_case_more_samples_than_points`, `test_edge_case_small_dataset`, `test_memory_efficiency`, `test_output_shape`, `test_performance_improvement`, `test_reproducibility`, `test_uniform_sampling` | `python -m unittest tests.test_raw_data_grouping` | — |
| `test_run_baseline.py` | No module docstring found. | `test_prepare_baseline_data_flattens_gridsize2_data`, `test_prepare_baseline_data_passes_through_gridsize1` | `python -m unittest tests.test_run_baseline` | — |
| `test_scaling_regression.py` | Regression tests for scaling bugs in diffsim.py | `test_assertions_catch_invalid_intensity_scale`, `test_both_arrays_scaled_identically`, `test_different_nphotons_produce_proportional_scaling`, `test_intensity_scale_is_valid`, `test_phase_is_not_scaled`, `test_scaling_is_reversible`, `test_scaling_preserves_physics` | `python -m unittest tests.test_scaling_regression` | — |
| `test_sequential_sampling.py` | Test suite for sequential sampling functionality in coordinate grouping.  This module tests the sequential_sampling flag added to the RawData class to restore deterministic, sequential data subset selection capability. | `test_cli_argument_parsing`, `test_config_flag_exists`, `test_default_behavior_is_random`, `test_sequential_sampling_handles_edge_cases`, `test_sequential_sampling_is_deterministic`, `test_sequential_sampling_order`, `test_sequential_sampling_uses_first_n_points`, `test_sequential_sampling_with_gridsize_greater_than_1`, `test_sequential_sampling_with_seed_parameter`, `test_sequential_vs_random_coverage` | `python -m unittest tests.test_sequential_sampling` | — |
| `test_subsampling.py` | Unit tests for independent data subsampling functionality.  This module tests the new n_subsample parameter that enables independent control of data subsampling and neighbor grouping operations in PtychoPINN. | `test_different_seeds_produce_different_results`, `test_interaction_with_config_dataclass`, `test_legacy_n_images_behavior`, `test_n_subsample_overrides_n_images`, `test_no_subsample_uses_full_dataset`, `test_reproducible_subsampling_with_seed`, `test_sorted_indices_for_consistency`, `test_subsample_larger_than_dataset`, `test_subsample_with_n_subsample`, `test_subsample_zero_edge_case`, `test_y_patches_subsampled_consistently` | `python -m unittest tests.test_subsampling` | — |
| `test_tf_helper.py` | Unit tests for ptycho/tf_helper.py, focusing on patch reassembly logic.  This test suite validates the core functionality of the reassemble_position function by testing its fundamental properties without making assumptions about exact output sizes. | `test_basic_functionality`, `test_batch_translation`, `test_complex_tensor_translation`, `test_different_patch_values_blend`, `test_edge_cases`, `test_identical_patches_single_vs_double`, `test_integer_translation`, `test_perfect_overlap_averages_to_identity`, `test_subpixel_translation`, `test_translate_core_matches_addons`, `test_zero_translation` | `python -m unittest tests.test_tf_helper` | — |
| `test_tf_helper_edge_aware.py` | Edge-aware tests for translate functions that account for interpolation differences.  This test suite specifically handles the known differences between TFA and TF raw ops regarding edge interpolation while ensuring smooth patterns (as used in PtychoPINN) work correctly. | `test_batch_smooth_patterns`, `test_boundary_behavior`, `test_complex_smooth_translation`, `test_document_edge_differences`, `test_gaussian_probe_translation`, `test_smooth_object_translation`, `test_typical_probe_sizes` | `python -m unittest tests.test_tf_helper_edge_aware` | — |
| `test_workflow_components.py` | Unit tests for ptycho.workflows.components module. | `test_exception_propagation`, `test_load_valid_model_directory`, `test_missing_diffraction_model`, `test_missing_model_archive`, `test_nonexistent_directory`, `test_not_a_directory`, `test_path_conversion` | `python -m unittest tests.test_workflow_components` | — |
| `test_workflow_generator_integration.py` | Tests for generator registry integration with workflows. | `test_train_cdi_model_calls_resolve_generator`, `test_train_with_lightning_calls_resolve_generator` | `python -m unittest tests.test_workflow_generator_integration` | — |

### Image Tests (`tests/image/`)

| Test File | Purpose / Scope | Key Tests | Usage / Command | Notes |
| :--- | :--- | :--- | :--- | :--- |

| `test_cropping.py` | Tests for the cropping module, focusing on the align_for_evaluation function. | `test_align_for_evaluation_bounding_box`, `test_align_for_evaluation_coordinates_format`, `test_align_for_evaluation_shapes`, `test_align_for_evaluation_with_squeeze`, `test_center_crop_exact_size` | `python -m unittest tests.image.test_cropping` | — |
| `test_registration.py` | Covers registration alignment helpers, including translation detection, shift application, and complex-valued data handling. | `test_apply_shift_and_crop_basic`, `test_apply_shift_and_crop_zero_offset`, `test_different_image_content`, `test_edge_case_maximum_shift`, `test_edge_case_single_pixel_shift`, `test_find_offset_known_shift_complex`, `test_find_offset_known_shift_real`, `test_input_validation_2d_requirement`, `test_input_validation_excessive_offset`, `test_input_validation_shape_matching`, `test_noise_robustness`, `test_register_and_align_convenience`, `test_registration_sign_verification`, `test_round_trip_registration`, `test_shift_and_crop_preserves_data_type` | `python -m unittest tests.image.test_registration` | — |

### Io Tests (`tests/io/`)

| Test File | Purpose / Scope | Key Tests | Usage / Command | Notes |
| :--- | :--- | :--- | :--- | :--- |

| `test_ptychodus_interop_h5.py` | No module docstring found. | `test_interop_h5_reader` | `python -m unittest tests.io.test_ptychodus_interop_h5` | — |
| `test_ptychodus_product_io.py` | No module docstring found. | `test_cli_convert_run1084_smoke`, `test_export_writes_minimal_hdf5`, `test_import_reads_hdf5_to_rawdata` | `python -m unittest tests.io.test_ptychodus_product_io` | — |

### Scripts Tests (`tests/scripts/`)

| Test File | Purpose / Scope | Key Tests | Usage / Command | Notes |
| :--- | :--- | :--- | :--- | :--- |

| `test_inference_backend_selector.py` | Unit tests for backend selector integration in scripts/inference/inference.py  This module tests that the inference CLI correctly dispatches to the backend selector when using PyTorch backend, and that TensorFlow-specific visualization helpers are skipped for PyTorch runs.  Test Coverage: 1. Inference CLI with backend='pytorch' dispatches to backend_selector 2. TensorFlow-specific tf.keras.backend.clear_session() is handled gracefully 3. TensorFlow backend continues to use legacy inference paths  References: - Phase R (reactivation): plans/ptychodus_pytorch_integration_plan.md - Backend selector: ptycho/workflows/backend_selector.py - Inference CLI: scripts/inference/inference.py | `test_backend_selector_preserves_config_001_compliance`, `test_cli_backend_argument_parsing`, `test_pytorch_backend_defaults_auto_execution_config`, `test_pytorch_backend_dispatch`, `test_pytorch_backend_moves_model_to_execution_device`, `test_pytorch_execution_config_flags`, `test_pytorch_inference_execution_path`, `test_setup_inference_configuration_uses_backend`, `test_tensorflow_backend_dispatch` | `python -m unittest tests.scripts.test_inference_backend_selector` | — |
| `test_ptychi_reconstruct_tike.py` | Tests for ptychi_reconstruct_tike.py CLI interface. | `test_main_uses_cli_arguments` | `python -m unittest tests.scripts.test_ptychi_reconstruct_tike` | — |
| `test_tf_inference_helper.py` | Tests for TensorFlow inference helper extraction (PARALLEL-API-INFERENCE).  This module tests the extracted `_run_tf_inference_and_reconstruct()` helper and related utilities for API parity with PyTorch inference.  See: plans/active/PARALLEL-API-INFERENCE/reports/2026-01-09T010000Z/extraction_design.md | `test_deprecated_wrapper_has_deprecation_docstring`, `test_deprecated_wrapper_still_works`, `test_extract_ground_truth_is_importable`, `test_extract_ground_truth_signature`, `test_helper_has_correct_defaults`, `test_helper_is_importable`, `test_helper_signature_matches_spec` | `python -m unittest tests.scripts.test_tf_inference_helper` | — |
| `test_training_backend_selector.py` | Unit tests for backend selector integration in scripts/training/train.py  This module tests that the training CLI correctly dispatches to the backend selector when using PyTorch backend, and that TensorFlow-only persistence helpers (model_manager.save, save_outputs) are skipped for PyTorch runs.  Test Coverage: 1. Training CLI with backend='pytorch' dispatches to backend_selector 2. TensorFlow-only persistence is guarded and skipped for PyTorch 3. TensorFlow backend continues to use legacy persistence paths  References: - Phase R (reactivation): plans/ptychodus_pytorch_integration_plan.md - Backend selector: ptycho/workflows/backend_selector.py - Training CLI: scripts/training/train.py | `test_manual_accumulation_guard`, `test_pytorch_backend_defaults_auto_execution_config`, `test_pytorch_backend_dispatch`, `test_pytorch_execution_config_flags`, `test_supervised_mode_enforces_mae_loss`, `test_tensorflow_backend_persistence`, `test_torch_scheduler_plateau_params_roundtrip`, `test_torch_scheduler_plateau_roundtrip` | `python -m unittest tests.scripts.test_training_backend_selector` | — |

### Studies Tests (`tests/studies/`)

| Test File | Purpose / Scope | Key Tests | Usage / Command | Notes |
| :--- | :--- | :--- | :--- | :--- |

| `test_aggregate_nan_msssim.py` | Test script to verify NaN handling and MS-SSIM filtering in aggregate_and_plot_results.py  This script creates mock data directories with controlled test cases: 1. A trial with good MS-SSIM (0.8) 2. A trial with bad MS-SSIM (0.1)  3. A trial with NaN values  Then runs the aggregation script with different thresholds to verify behavior. | N/A | `python -m unittest tests.studies.test_aggregate_nan_msssim` | — |
| `test_filtering_order.py` | Enhanced test script to verify filtering order in aggregate_and_plot_results.py  This script creates mock data with specific MS-SSIM values to test if filtering happens before or after statistical aggregation.  Test scenario: - 4 trials with ms_ssim_phase values: 0.8, 0.7, 0.6, 0.1 - With threshold 0.3, the outlier (0.1) should be filtered out - Correct median after filtering: 0.7 (from [0.8, 0.7, 0.6]) - Incorrect median if filtering after aggregation: 0.65 (from [0.8, 0.7, 0.6, 0.1]) | N/A | `python -m unittest tests.studies.test_filtering_order` | — |
| `test_mean_vs_median.py` | Test script to verify that aggregate_and_plot_results.py now uses mean instead of median  This script creates mock data with specific values to test the statistical aggregation: - 3 trials with values: 10, 20, 30 - Expected mean: 20.0 - Expected 25th percentile: 15.0 - Expected 75th percentile: 25.0 - Expected median (old): 20.0 (same as mean in this case) | N/A | `python -m unittest tests.studies.test_mean_vs_median` | — |

### Study Tests (`tests/study/`)

| Test File | Purpose / Scope | Key Tests | Usage / Command | Notes |
| :--- | :--- | :--- | :--- | :--- |

| `test_check_dense_highlights_match.py` | Tests for check_dense_highlights_match.py highlights checker (STUDY-SYNTH-FLY64-DOSE-OVERLAP-001)  Validates that the checker correctly enforces: - PREVIEW-PHASE-001: Preview metadata contains phase-only indicator - STUDY-001: MS-SSIM ±0.000 / MAE ±0.000000 precision in ssim_grid_summary.md table - Highlights/preview text values match metrics_delta_summary.json  Coverage: - test_summary_mismatch_fails: RED test with tampered ssim_grid table value - test_summary_matches_json: GREEN test with aligned artifacts | `test_missing_phase_only_metadata_fails`, `test_summary_matches_json`, `test_summary_mismatch_fails` | `python -m unittest tests.study.test_check_dense_highlights_match` | — |
| `test_dose_overlap_comparison.py` | Test Phase G comparison orchestration for STUDY-SYNTH-FLY64-DOSE-OVERLAP-001.  Validates build_comparison_jobs function produces deterministic jobs with pointers to Phase C/E/F artifacts and metric configuration. | `test_baseline_complex_output_converts_to_amplitude_phase`, `test_baseline_model_predict_receives_both_inputs`, `test_build_comparison_jobs_creates_all_conditions`, `test_build_comparison_jobs_uses_dose_specific_phase_e_paths`, `test_execute_comparison_jobs_appends_tike_recon_path`, `test_execute_comparison_jobs_invokes_compare_models`, `test_execute_comparison_jobs_records_summary`, `test_pinn_reconstruction_reassembles_batched_predictions`, `test_pinn_reconstruction_reassembles_full_train_split`, `test_prepare_baseline_inference_data_grouped_flatten_helper` | `python -m unittest tests.study.test_dose_overlap_comparison` | — |
| `test_dose_overlap_dataset_contract.py` | Phase B test coverage for dataset contract validation (STUDY-SYNTH-FLY64-DOSE-OVERLAP-001).  Tests validate_dataset_contract() against DATA-001, spacing thresholds, and oversampling preconditions using small in-memory fixtures (no production datasets).  Test Strategy: - Use native pytest (no unittest.TestCase mixins) per project guidance - Small synthetic fixtures for pass/fail scenarios - Actionable ValueError messages for debugging  References: - plans/active/STUDY-SYNTH-FLY64-DOSE-OVERLAP-001/test_strategy.md:48-60 - docs/findings.md:DATA-001, CONFIG-001, OVERSAMPLING-001 | `test_validate_dataset_contract_happy_path`, `test_validate_dataset_contract_missing_key`, `test_validate_dataset_contract_oversampling_missing_neighbor_count`, `test_validate_dataset_contract_oversampling_precondition_fail`, `test_validate_dataset_contract_oversampling_precondition_pass`, `test_validate_dataset_contract_shape_mismatch`, `test_validate_dataset_contract_spacing_dense`, `test_validate_dataset_contract_spacing_violation`, `test_validate_dataset_contract_unknown_view`, `test_validate_dataset_contract_wrong_dtype_diffraction`, `test_validate_dataset_contract_wrong_dtype_object` | `python -m unittest tests.study.test_dose_overlap_dataset_contract` | — |
| `test_dose_overlap_design.py` | Tests for the fly64 dose/overlap study design constants.  Validates that Phase A design parameters are correctly encoded and satisfy all documented constraints from the implementation plan.  Test tier: Unit Test strategy: plans/active/STUDY-SYNTH-FLY64-DOSE-OVERLAP-001/test_strategy.md | `test_study_design_constants`, `test_study_design_to_dict`, `test_study_design_validation` | `python -m unittest tests.study.test_dose_overlap_design` | — |
| `test_dose_overlap_generation.py` | Phase C tests for fly64 dose/overlap dataset generation pipeline.  These tests verify: 1. build_simulation_plan constructs dose-specific configs correctly 2. generate_dataset_for_dose orchestrates all stages with proper dependency injection 3. Pipeline invokes simulation, canonicalization, patching, splitting, and validation  References: - plans/active/STUDY-SYNTH-FLY64-DOSE-OVERLAP-001/reports/2025-11-04T032018Z/phase_c_dataset_generation/plan.md - specs/data_contracts.md §2 (canonical NPZ format) - CONFIG-001, DATA-001, OVERSAMPLING-001 | `test_build_simulation_plan_handles_metadata_pickle_guard`, `test_build_simulation_plan`, `test_generate_dataset_config_construction`, `test_generate_dataset_for_dose_handles_metadata_splits`, `test_generate_dataset_pipeline_orchestration`, `test_generate_dataset_validates_with_real_contract`, `test_load_data_for_sim_handles_metadata_pickle_guard` | `python -m unittest tests.study.test_dose_overlap_generation` | — |
| `test_dose_overlap_overlap.py` | Phase D tests for overlap metrics per specs/overlap_metrics.md.  This module tests the overlap metrics pipeline: - disc_overlap_area and disc_overlap_fraction correctness (analytical cases) - Metric 1 (group-based, gs=2 only) aggregation - Metric 2 (image-based with deduplication) - Metric 3 (group↔group COM-based) - compute_overlap_metrics orchestration (gs=1 and gs=2) - generate_overlap_views CLI integration - Bundle schema validation  References: - specs/overlap_metrics.md (normative spec) - plans/active/STUDY-SYNTH-FLY64-DOSE-OVERLAP-001/test_strategy.md - docs/GRIDSIZE_N_GROUPS_GUIDE.md (unified n_groups semantics) | `test_compute_overlap_metrics_degenerate_s_img`, `test_compute_overlap_metrics_gs1`, `test_compute_overlap_metrics_gs2`, `test_compute_overlap_metrics_invalid_gridsize`, `test_disc_overlap_area_symmetry`, `test_disc_overlap_fraction_half_diameter`, `test_disc_overlap_fraction_no_overlap`, `test_disc_overlap_fraction_perfect_overlap`, `test_filter_dataset_by_mask_handles_scalar_metadata`, `test_form_groups_gs1`, `test_form_groups_gs2`, `test_generate_overlap_views_basic`, `test_generate_overlap_views_dense_acceptance_floor`, `test_metric_1_group_based_synthetic`, `test_metric_2_image_based_deduplication`, `test_metric_2_image_based_single_image`, `test_metric_3_group_to_group_no_overlap`, `test_metric_3_group_to_group_overlapping`, `test_overlap_metrics_bundle`, `test_subsample_images_deterministic` | `python -m unittest tests.study.test_dose_overlap_overlap` | — |
| `test_dose_overlap_reconstruction.py` | Tests for Phase F pty-chi LSQML reconstruction job builder.  Validates that build_ptychi_jobs() correctly constructs a manifest with: - 3 doses × 3 views (baseline, dense, sparse) × 2 splits = 18 jobs total - CLI arguments for scripts/reconstruction/ptychi_reconstruct_tike.py - DATA-001 compliant NPZ path validation  Test tier: Unit Test strategy: plans/active/STUDY-SYNTH-FLY64-DOSE-OVERLAP-001/test_strategy.md | `test_build_ptychi_jobs_manifest`, `test_cli_executes_selected_jobs`, `test_cli_filters_dry_run`, `test_cli_skips_missing_phase_d`, `test_run_ptychi_job_invokes_script` | `python -m unittest tests.study.test_dose_overlap_reconstruction` | — |
| `test_dose_overlap_training.py` | Tests for Phase E training job matrix builder.  Validates that build_training_jobs() correctly enumerates 9 jobs per dose (3 doses × 3 variants: baseline gs1, dense gs2, sparse gs2) with proper metadata and dataset path validation.  Test tier: Unit Test strategy: plans/active/STUDY-SYNTH-FLY64-DOSE-OVERLAP-001/test_strategy.md | `test_build_training_jobs_matrix`, `test_build_training_jobs_skips_missing_view`, `test_execute_training_job_delegates_to_pytorch_trainer`, `test_execute_training_job_dispatches_pytorch_when_requested`, `test_execute_training_job_dispatches_tensorflow_by_default`, `test_execute_training_job_persists_bundle`, `test_execute_training_job_tensorflow_persists_bundle`, `test_run_training_job_dry_run`, `test_run_training_job_invokes_runner`, `test_train_cdi_model_normalizes_history`, `test_training_cli_filters_jobs`, `test_training_cli_invokes_real_runner`, `test_training_cli_manifest_and_bridging`, `test_training_cli_records_bundle_path` | `python -m unittest tests.study.test_dose_overlap_training` | — |
| `test_phase_g_dense_artifacts_verifier.py` | Tests for Phase G dense pipeline artifact inventory validation.  Covers the TDD cycle for validate_artifact_inventory() in verify_dense_pipeline_artifacts.py | `test_verify_dense_pipeline_artifact_inventory_blocks_missing_entries`, `test_verify_dense_pipeline_artifact_inventory_passes_with_complete_bundle`, `test_verify_dense_pipeline_cli_logs_complete`, `test_verify_dense_pipeline_cli_logs_missing`, `test_verify_dense_pipeline_cli_logs_require_ssim_grid_log`, `test_verify_dense_pipeline_cli_phase_logs_complete`, `test_verify_dense_pipeline_cli_phase_logs_incomplete`, `test_verify_dense_pipeline_cli_phase_logs_missing`, `test_verify_dense_pipeline_cli_phase_logs_wrong_pattern`, `test_verify_dense_pipeline_highlights_complete`, `test_verify_dense_pipeline_highlights_delta_mismatch`, `test_verify_dense_pipeline_highlights_mismatched_value`, `test_verify_dense_pipeline_highlights_missing_model`, `test_verify_dense_pipeline_highlights_missing_preview`, `test_verify_dense_pipeline_highlights_preview_contains_amplitude`, `test_verify_dense_pipeline_highlights_preview_mismatch`, `test_verify_dense_pipeline_requires_ssim_grid_summary` | `python -m unittest tests.study.test_phase_g_dense_artifacts_verifier` | — |
| `test_phase_g_dense_metrics_report.py` | Tests for Phase G dense metrics reporting helper. | `test_analyze_dense_metrics_flags_failures`, `test_analyze_dense_metrics_success_digest`, `test_report_phase_g_dense_metrics_missing_model_fails`, `test_report_phase_g_dense_metrics` | `python -m unittest tests.study.test_phase_g_dense_metrics_report` | — |
| `test_phase_g_dense_orchestrator.py` | Tests for Phase G dense orchestrator summary helper. | `test_persist_delta_highlights_creates_preview`, `test_prepare_hub_clobbers_previous_outputs`, `test_prepare_hub_detects_stale_outputs`, `test_run_phase_g_dense_collect_only_generates_commands`, `test_run_phase_g_dense_collect_only_post_verify_only`, `test_run_phase_g_dense_exec_invokes_reporting_helper`, `test_run_phase_g_dense_exec_prints_highlights_preview`, `test_run_phase_g_dense_exec_runs_analyze_digest`, `test_run_phase_g_dense_post_verify_hooks`, `test_run_phase_g_dense_post_verify_only_executes_chain`, `test_summarize_phase_g_outputs_fails_on_execution_failures`, `test_summarize_phase_g_outputs_fails_on_missing_csv`, `test_summarize_phase_g_outputs_fails_on_missing_manifest`, `test_summarize_phase_g_outputs`, `test_validate_phase_c_metadata_accepts_valid_metadata`, `test_validate_phase_c_metadata_handles_patched_layout`, `test_validate_phase_c_metadata_requires_canonical_transform`, `test_validate_phase_c_metadata_requires_metadata` | `python -m unittest tests.study.test_phase_g_dense_orchestrator` | — |
| `test_ssim_grid.py` | Tests for ssim_grid.py helper — Phase G MS-SSIM/MAE delta table generation.  Acceptance: - RED test: preview guard must reject when metrics_delta_highlights_preview.txt contains "amplitude" - GREEN test: helper succeeds with phase-only preview and emits ssim_grid_summary.md with ±0.000/±0.000000 tables - Test must execute via subprocess to prove CLI contract (not just imports) | `test_smoke_ssim_grid` | `python -m unittest tests.study.test_ssim_grid` | — |

### Tf helper Tests (`tests/tf_helper/`)

| Test File | Purpose / Scope | Key Tests | Usage / Command | Notes |
| :--- | :--- | :--- | :--- | :--- |

| `test_translation_shape_guard.py` | Regression test for non-XLA translation shape guard (FIX-PYTORCH-FORWARD-PARITY-001 Phase C1d).  This module tests the shape consistency guard added to ptycho/tf_helper.py::translate_core to prevent crashes when images and translations have mismatched batch dimensions.  Reference: - Issue: plans/active/FIX-PYTORCH-FORWARD-PARITY-001/reports/.../tf_baseline/phase_c1/red/blocked_20251114T074039Z_tf_non_xla_shape_error.md - Finding: POLICY-001 (PyTorch mandatory), CONFIG-001 (update_legacy_dict bridge)  The bug occurred when USE_XLA_TRANSLATE=0 and gridsize=2, causing _channel_to_flat to produce images with batch dimension (b*gridsize^2) while translations kept batch dimension (b). | `test_non_xla_translation_guard`, `test_non_xla_translation_matching_batch`, `test_reassemble_patches_position_real_gridsize2`, `test_translate_xla_gridsize_broadcast_jit`, `test_translate_xla_gridsize_broadcast` | `python -m unittest tests.tf_helper.test_translation_shape_guard` | — |

### Tools Tests (`tests/tools/`)

| Test File | Purpose / Scope | Key Tests | Usage / Command | Notes |
| :--- | :--- | :--- | :--- | :--- |

| `test_generate_patches_tool.py` | Test suite for generate_patches_tool.py with metadata preservation.  This module tests the patch generation tool's ability to preserve and extend metadata through NPZ transformations. | `test_generate_patches_preserves_metadata`, `test_generate_patches_without_metadata` | `python -m unittest tests.tools.test_generate_patches_tool` | — |
| `test_generate_test_index.py` | Tests for the generate_test_index automation script. | `test_get_module_docstring_handles_missing_docstring`, `test_get_module_docstring_reads_existing_docstring`, `test_get_test_functions_lists_key_tests` | `python -m unittest tests.tools.test_generate_test_index` | — |
| `test_tail_interleave_logs.py` | No module docstring found. | `test_interleave_summaries` | `python -m unittest tests.tools.test_tail_interleave_logs` | — |
| `test_transpose_rename_convert_tool.py` | Test suite for transpose_rename_convert_tool.py with metadata preservation.  This module tests the canonicalization tool's ability to preserve and extend metadata through NPZ transformations. | `test_canonicalize_preserves_metadata`, `test_canonicalize_without_metadata` | `python -m unittest tests.tools.test_transpose_rename_convert_tool` | — |
| `test_update_tool.py` | Test script for update_tool.py | `test_update_function`, `test_update_function` | `python -m unittest tests.tools.test_update_tool` | — |

### Torch Tests (`tests/torch/`)

| Test File | Purpose / Scope | Key Tests | Usage / Command | Notes |
| :--- | :--- | :--- | :--- | :--- |

| `test_agc.py` | Tests for Adaptive Gradient Clipping (AGC).  See: Brock et al., 2021 (NFNet), Algorithm 2. Contract: ptycho_torch/train_utils.py::adaptive_gradient_clip_ | `test_agc_clips_large_gradients`, `test_agc_handles_zero_params`, `test_agc_preserves_small_gradients` | `python -m unittest tests.torch.test_agc` | — |
| `test_api_deprecation.py` | Test suite for ptycho_torch.api deprecation warnings per ADR-003 Phase E.C1.  Validates that legacy API entry points emit DeprecationWarning with migration guidance steering users toward factory-driven workflows documented in docs/workflows/pytorch.md.  Reference: - SPEC: specs/ptychodus_api_spec.md (no explicit deprecation contract, but   follows CLI logger deprecation semantics at §4.9) - ARCH: plans/active/ADR-003-BACKEND-API/reports/2025-10-20T134500Z/         phase_e_governance_adr_addendum/adr_addendum.md:295-334   (defers legacy API decision to Phase E.C1) - WORKFLOW: docs/workflows/pytorch.md:188-196 flags ptycho_torch/api as             deprecated surface needing migration instructions  Test Strategy: - RED Phase: Import legacy modules expecting DeprecationWarning via   warnings.catch_warnings context manager (stacklevel=2 ensures caller sees   accurate origin). - GREEN Phase: After warn_legacy_api_import implementation, validate warning   message content includes migration keywords (ptycho_train_torch, config_factory). - No behavior changes: legacy modules remain functional; only messaging added. | `test_api_package_import_is_idempotent`, `test_example_train_import_emits_deprecation_warning` | `python -m unittest tests.torch.test_api_deprecation` | — |
| `test_backend_selection.py` | Phase E1.B Backend Selection Tests (INTEGRATE-PYTORCH-001)  This module documents the expected backend selection mechanism for PyTorch vs TensorFlow workflows. These tests define the desired behavior before implementation (TDD red phase).  Test Coverage: 1. Default backend behavior (backward compatibility) 2. PyTorch backend selection via config flag 3. CONFIG-001 compliance (update_legacy_dict before workflow dispatch) 4. Torch unavailability handling 5. API parity between backends  Implementation Status: These are FAILING tests (Phase E1.B red phase). They document the expected behavior when a backend selection mechanism is added to the configuration system. The tests will pass once Phase E1.C implementation completes.  References: - Phase E plan: plans/active/INTEGRATE-PYTORCH-001/phase_e_integration.md - Callchain analysis: plans/active/INTEGRATE-PYTORCH-001/reports/2025-10-17T173826Z/phase_e_callchain/ - Spec: specs/ptychodus_api_spec.md §4.1-4.6 | `test_backend_selection_preserves_api_parity`, `test_defaults_to_tensorflow_backend`, `test_inference_config_supports_backend_selection`, `test_pytorch_backend_calls_update_legacy_dict`, `test_pytorch_unavailable_raises_error`, `test_selects_pytorch_backend` | `python -m unittest tests.torch.test_backend_selection` | — |
| `test_cli_inference_torch.py` | GREEN Phase pytest tests for inference CLI execution config integration (ADR-003 Phase C4.D1).  This module validates the inference CLI's ability to accept and forward execution config flags to the factory and workflow layers. Tests verify argparse→PyTorchExecutionConfig→factory propagation chain.  Phase C4 Implementation: - Inference CLI accepts --accelerator, --num-workers, --inference-batch-size flags (✓ C4.C5) - CLI args map to PyTorchExecutionConfig fields (✓ C4.C6) - Factory receives correct execution config values (✓ validated) - Tests mock both factory and bundle loader to prevent IO  References: - Plan: plans/active/ADR-003-BACKEND-API/reports/2025-10-20T033100Z/phase_c4_cli_integration/plan.md §C4.D1 - Implementation: ptycho_torch/inference.py:380-442 (argparse), :455-546 (factory integration) - Factory Design: .../2025-10-19T232336Z/phase_b_factories/factory_design.md | `test_accelerator_flag_roundtrip`, `test_accelerator_flag_roundtrip`, `test_cli_calls_save_individual_reconstructions`, `test_cli_delegates_to_helper_for_data_loading`, `test_cli_delegates_to_inference_helper`, `test_cli_delegates_to_validate_paths`, `test_inference_batch_size_flag_roundtrip`, `test_multiple_execution_config_flags`, `test_num_workers_flag_roundtrip`, `test_quiet_flag_suppresses_progress_output` | `python -m unittest tests.torch.test_cli_inference_torch` | — |
| `test_cli_shared.py` | Unit tests for CLI shared helper functions (ADR-003 Phase D.B3, GREEN as of 2025-10-20).  This module tests the shared helper functions in `ptycho_torch/cli/shared.py` that support the training and inference CLI thin-wrapper refactor. These helpers were introduced in Phase D.B3 and are now fully implemented and GREEN.  Helper Functions Under Test: - resolve_accelerator(): Handle --device → --accelerator backward compatibility - build_execution_config_from_args(): Construct PyTorchExecutionConfig with validation - validate_paths(): Check file existence and create output directory  Test Coverage: - Unit tests for each helper function in isolation - Verify deprecation warnings are emitted correctly - Verify validation errors are raised with clear messages - Use tmp_path fixtures for file I/O tests  GREEN Status (Phase D.B3, 2025-10-20): - All 20 tests PASSING (helpers implemented per training_refactor.md blueprint) - Deprecation semantics verified (--device → --accelerator mapping with warnings) - Validation logic tested (path checks, execution config field constraints) - Evidence: plans/.../phase_d_cli_wrappers_training_impl/pytest_cli_shared_green.log  References: - Blueprint: plans/.../phase_d_cli_wrappers_training/training_refactor.md §Component 1 - Design Notes: plans/.../phase_d_cli_wrappers_baseline/design_notes.md §D1-D8 - Spec: specs/ptychodus_api_spec.md §7 (CLI execution config flags) - Implementation: ptycho_torch/cli/shared.py (3 helper functions, ~150 lines) | `test_accepts_none_test_file`, `test_accepts_none_train_file_for_inference_mode`, `test_all_accelerator_values_passthrough`, `test_conflict_accelerator_wins`, `test_creates_output_dir`, `test_default_no_device`, `test_emits_deterministic_warning`, `test_handles_disable_mlflow_flag`, `test_handles_quiet_flag`, `test_inference_mode_custom_batch_size`, `test_inference_mode_defaults`, `test_inference_mode_respects_quiet`, `test_inference_mode`, `test_invalid_mode_raises_value_error`, `test_legacy_device_cpu`, `test_legacy_device_cuda_maps_to_gpu`, `test_quiet_or_disable_mlflow_both_true`, `test_raises_if_test_file_missing`, `test_raises_if_train_file_missing`, `test_resolve_accelerator_auto_defaults`, `test_resolves_accelerator_from_device_flag`, `test_training_mode_custom_values`, `test_training_mode_defaults`, `test_works_with_pathlib_path_objects` | `python -m unittest tests.torch.test_cli_shared` | — |
| `test_cli_train_torch.py` | RED Phase pytest scaffolds for training CLI execution config integration (ADR-003 Phase C4.B1).  This module tests the training CLI's ability to accept and forward execution config flags to the factory and workflow layers. Tests are expected to FAIL in RED phase because the CLI implementation does not yet exist.  Phase C4 Requirements: - Training CLI accepts --accelerator, --deterministic, --num-workers, --learning-rate flags - CLI args map to PyTorchExecutionConfig fields - Factory receives correct execution config values - Workflow helpers receive execution config from factory payload  References: - Plan: plans/active/ADR-003-BACKEND-API/reports/2025-10-20T033100Z/phase_c4_cli_integration/plan.md §C4.B1 - Argparse Schema: .../phase_c4_cli_integration/argparse_schema.md - Factory Design: .../2025-10-19T232336Z/phase_b_factories/factory_design.md | `test_accelerator_flag_roundtrip`, `test_accumulate_grad_batches_roundtrip`, `test_bundle_persistence`, `test_checkpoint_mode_flag`, `test_checkpoint_monitor_flag`, `test_checkpoint_save_top_k_flag`, `test_deterministic_flag_roundtrip`, `test_disable_mlflow_deprecation_warning`, `test_early_stop_patience_flag`, `test_enable_checkpointing_flag`, `test_learning_rate_flag_roundtrip`, `test_logger_backend_csv_default`, `test_logger_backend_none`, `test_logger_backend_tensorboard`, `test_multiple_execution_config_flags`, `test_no_deterministic_flag_roundtrip`, `test_num_workers_flag_roundtrip`, `test_patch_stats_dump`, `test_scheduler_flag_roundtrip` | `python -m unittest tests.torch.test_cli_train_torch` | — |
| `test_config_bridge.py` | Tests for PyTorch config bridge adapter translating PyTorch configs to TensorFlow spec format.  This module validates Phase B.B2 of INTEGRATE-PYTORCH-001: the configuration bridge that translates PyTorch singleton configs to TensorFlow dataclass configs, enabling population of the legacy params.cfg dictionary through the standard update_legacy_dict() function.  MVP Test Coverage (9 fields): - N, gridsize, model_type (model essentials) - train_data_file, test_data_file, model_path (lifecycle paths) - n_groups, neighbor_count (data grouping) - nphotons (physics scaling)  Test Strategy: 1. Instantiate PyTorch configs with MVP-aligned values 2. Call adapter functions to convert to TensorFlow dataclass configs 3. Use update_legacy_dict() to populate params.cfg 4. Assert params.cfg contains expected keys with correct values  Implementation Status: Adapter module (ptycho_torch.config_bridge) implemented in Phase B.B3. This test validates the MVP translation functions convert PyTorch configs to TensorFlow dataclasses. | `test_activation_error_handling`, `test_config_bridge_architecture_default`, `test_config_bridge_architecture_from_pt_model`, `test_config_bridge_architecture_override`, `test_config_bridge_fno_input_transform`, `test_default_divergence_detection`, `test_gridsize_error_handling`, `test_inference_config_n_subsample_explicit_override`, `test_inference_config_n_subsample_missing_override_uses_none`, `test_inference_config_override_fields`, `test_model_config_direct_fields`, `test_model_config_override_fields`, `test_model_config_probe_mask_override`, `test_model_config_probe_mask_translation`, `test_model_config_transform_fields`, `test_model_path_required_error`, `test_model_type_error_handling`, `test_mvp_config_bridge_populates_params_cfg`, `test_n_groups_missing_override_warning`, `test_nphotons_default_divergence_error`, `test_nphotons_override_passes_validation`, `test_params_cfg_matches_baseline`, `test_test_data_file_training_missing_warning`, `test_train_data_file_required_error`, `test_training_config_direct_fields`, `test_training_config_gradient_clip_algorithm_roundtrip`, `test_training_config_lr_scheduler_roundtrip`, `test_training_config_n_subsample_explicit_override`, `test_training_config_n_subsample_missing_override_uses_none`, `test_training_config_optimizer_roundtrip`, `test_training_config_override_fields`, `test_training_config_subsample_seed_from_dataconfig`, `test_training_config_subsample_seed_override`, `test_training_config_transform_fields` | `python -m unittest tests.torch.test_config_bridge` | — |
| `test_config_factory.py` | RED Phase Tests for PyTorch Config Factory Functions  This test module encodes the expected behavior of the configuration factory functions defined in ptycho_torch/config_factory.py. These tests are written BEFORE implementation (TDD RED phase) and will fail with NotImplementedError until Phase B3.a implementation.  Test Strategy:     Phase B2 (RED): All tests fail with NotImplementedError from factory stubs     Phase B3 (GREEN): Implementation added, tests pass  Test Coverage:     1. Factory Returns Correct Payload Structure     2. Config Bridge Integration (TF dataclass translation)     3. params.cfg Population (CONFIG-001 compliance)     4. Override Precedence Rules     5. Validation Errors (missing n_groups, invalid paths)  Design Reference:     plans/active/ADR-003-BACKEND-API/reports/2025-10-19T232336Z/phase_b_factories/factory_design.md §5  Override Matrix Reference:     plans/active/ADR-003-BACKEND-API/reports/2025-10-19T232336Z/phase_b_factories/override_matrix.md | `test_accum_steps_override_applied`, `test_checkpoint_defaults_respected`, `test_checkpoint_knobs_propagate_through_factory`, `test_epochs_to_nepochs_conversion`, `test_execution_config_defaults_applied`, `test_execution_config_explicit_instance_propagates`, `test_execution_config_fields_accessible`, `test_factory_populates_params_cfg`, `test_generator_output_mode_override_propagates`, `test_grid_size_tuple_to_gridsize_int`, `test_gridsize_sets_channel_count`, `test_infer_probe_size_from_npz`, `test_infer_probe_size_missing_file_fallback`, `test_inference_payload_contains_pytorch_configs`, `test_inference_payload_contains_tf_config`, `test_inference_payload_execution_config_not_none`, `test_inference_payload_returns_dataclass`, `test_k_to_neighbor_count_conversion`, `test_logger_backend_csv_default`, `test_logger_backend_tensorboard`, `test_missing_checkpoint_raises_error`, `test_missing_n_groups_raises_error`, `test_nonexistent_train_data_file_raises_error`, `test_override_dict_wins_over_defaults`, `test_overrides_applied_records_execution_knobs`, `test_populate_legacy_params_helper`, `test_probe_size_override_wins_over_inference`, `test_scheduler_override_applied`, `test_training_payload_contains_overrides_dict`, `test_training_payload_contains_pytorch_configs`, `test_training_payload_contains_tf_config`, `test_training_payload_execution_config_not_none`, `test_training_payload_infers_probe_size_for_pt_data_config`, `test_training_payload_returns_dataclass` | `python -m unittest tests.torch.test_config_factory` | — |
| `test_coords_relative_contract.py` | No module docstring found. | `test_coords_relative_from_nominal_c1_is_zero`, `test_coords_relative_from_nominal_c4_matches_tf_sign` | `python -m unittest tests.torch.test_coords_relative_contract` | — |
| `test_data_pipeline.py` | Torch-optional parity tests for RawDataTorch and PtychoDataContainerTorch adapters.  This module is whitelisted in conftest.py to run without PyTorch installed, validating that adapters correctly delegate to TensorFlow RawData when torch unavailable.  Test source citations (from test_blueprint.md and data_contract.md): - RawData API: ptycho/raw_data.py:365-486 - PtychoDataContainer API: ptycho/loader.py:93-138 - Data contract: specs/data_contracts.md:7-70  Phase C.B2/C.B3 Goals (TDD Red Phase): - Document expected RawDataTorch wrapper behavior - Document expected PtychoDataContainerTorch API surface - Capture baseline failure logs for implementation guidance | `test_data_container_shapes_and_dtypes`, `test_deterministic_generation_validation`, `test_memmap_bridge_accepts_diffraction_legacy`, `test_memmap_loader_matches_raw_data_torch`, `test_raw_data_torch_matches_tensorflow`, `test_y_patches_are_complex64` | `python -m unittest tests.torch.test_data_pipeline` | — |
| `test_dataloader.py` | Unit tests for PyTorch dataloader DATA-001 compliance.  This module tests the PyTorch memory-mapped dataloader's ability to load canonical NPZ datasets with the `diffraction` key and maintain backward compatibility with legacy `diff3d` key.  Test Coverage: 1. Canonical diffraction key loading (DATA-001 spec compliance) 2. Legacy diff3d key backward compatibility 3. Error handling when neither key exists  References: - specs/data_contracts.md §1 — Canonical NPZ schema - docs/findings.md#DATA-001 — diffraction key requirement - plans/active/INTEGRATE-PYTORCH-001/reports/2025-10-17T223200Z/dataloader_triage.md | `test_auto_transposes_legacy_hwn_format`, `test_backward_compat_legacy_diff3d`, `test_error_when_no_diffraction_key`, `test_handles_edge_case_square_dataset`, `test_loads_canonical_diffraction`, `test_npz_headers_also_transposes_shape`, `test_preserves_canonical_nwh_format`, `test_real_dataset_dimensions`, `test_works_with_diff3d_legacy_key` | `python -m unittest tests.torch.test_dataloader` | — |
| `test_debug_fno_activations.py` | No module docstring found. | `test_debug_fno_activations_emits_report`, `test_debug_fno_activations_stable_hybrid_checkpoint` | `python -m unittest tests.torch.test_debug_fno_activations` | — |
| `test_debug_fno_gradients.py` | No module docstring found. | `test_debug_fno_gradients_emits_report` | `python -m unittest tests.torch.test_debug_fno_gradients` | — |
| `test_execution_config_defaults.py` | Test PyTorchExecutionConfig accelerator auto-resolution and GPU-first defaults.  This test module validates the auto→cuda resolution logic introduced to make PyTorchExecutionConfig GPU-first by default per POLICY-001 compliance.  Coverage:     1. Auto-resolution prefers CUDA when available     2. Auto-resolution falls back to CPU with POLICY-001 warning when unavailable     3. Backend selector inherits GPU-first behavior when execution_config=None     4. Explicit accelerator values (cpu, cuda) bypass auto-resolution  Related:     - POLICY-001: PyTorch backend defaults to GPU execution     - ptycho/config/config.py:PyTorchExecutionConfig.__post_init__     - ptycho_torch/workflows/components.py (execution_config=None call sites) | `test_auto_prefers_cuda`, `test_auto_warns_and_falls_back_to_cpu`, `test_backend_selector_cpu_fallback_with_warning`, `test_backend_selector_inherits_gpu_first_defaults`, `test_backend_selector_warns_on_cpu_only_hosts`, `test_explicit_cpu_bypasses_auto_resolution`, `test_explicit_cuda_bypasses_auto_resolution`, `test_workflow_auto_instantiates_with_hardware_detection` | `python -m unittest tests.torch.test_execution_config_defaults` | — |
| `test_fixture_pytorch_integration.py` | Regression test for PyTorch integration fixture contract compliance.  This module validates that the generated fixture (minimal_dataset_v1.npz) conforms to the acceptance criteria defined in Phase B1 fixture_scope.md §3.  Test Strategy (TDD RED → GREEN):     Phase B2.B: Write failing test asserting fixture contract (this file)     Phase B2.C: Implement generator to satisfy contract (make test GREEN)  Acceptance Criteria Reference:     plans/active/TEST-PYTORCH-001/reports/2025-10-19T215300Z/phase_b_fixture/fixture_scope.md §3  Data Contract:     specs/data_contracts.md §1 (canonical NPZ format requirements)  Author: Ralph (Phase B2.B TDD RED) Date: 2025-10-19 | `test_coordinate_coverage`, `test_fixture_compatible_with_pytorch_dataloader`, `test_fixture_file_exists`, `test_fixture_loads_with_rawdata`, `test_fixture_outputs_match_contract`, `test_metadata_content_valid`, `test_metadata_sidecar_exists` | `python -m unittest tests.torch.test_fixture_pytorch_integration` | — |
| `test_fno_fallback_init.py` | No module docstring found. | `test_fallback_spectral_init_scale` | `python -m unittest tests.torch.test_fno_fallback_init` | — |
| `test_fno_generators.py` | Tests for FNO/Hybrid generator implementations. | `test_amp_phase_bounds`, `test_block_has_residual`, `test_block_preserves_dims`, `test_fno_generator_builds_model`, `test_forward_shape`, `test_hybrid_generator_builds_model`, `test_identity_init`, `test_input_transform_log1p_matches_expected`, `test_input_transform_sqrt_matches_expected`, `test_layerscale_grad_flow`, `test_lifter_preserves_spatial_dims`, `test_lifter_with_custom_hidden`, `test_max_hidden_channel_cap`, `test_output_contract_real_imag`, `test_output_is_differentiable`, `test_output_shape_real_imag`, `test_output_shape_real_imag`, `test_output_shape`, `test_output_shape`, `test_resolve_fno_generator`, `test_resolve_hybrid_generator`, `test_resolve_stable_hybrid_generator`, `test_stable_hybrid_generator_output_shape`, `test_zero_mean_update` | `python -m unittest tests.torch.test_fno_generators` | — |
| `test_fno_integration.py` | Integration tests for FNO/Hybrid end-to-end training and inference. | `test_fno_generator_forward_pass`, `test_fno_generator_training_loop`, `test_fno_hyperparams_propagate_to_generator`, `test_hybrid_generator_forward_pass`, `test_neuraloperator_spectral_conv_available`, `test_pinn_uses_fno_generator_when_selected`, `test_ptychoblock_uses_neuraloperator_when_available`, `test_synthetic_npz_fixture_contract` | `python -m unittest tests.torch.test_fno_integration` | — |
| `test_fno_lightning_integration.py` | Integration tests for Lightning training with loss history tracking. | `test_loss_history_callback_collects_per_epoch`, `test_loss_history_callback_handles_missing_metrics`, `test_reassemble_cdi_image_torch_handles_real_imag_outputs`, `test_train_history_collects_epochs_for_fno_hybrid`, `test_train_history_collects_epochs` | `python -m unittest tests.torch.test_fno_lightning_integration` | — |
| `test_fno_reconstruction_quality.py` | Quality comparison tests for FNO/Hybrid vs CNN generators. | `test_fno_quality_comparable_to_random_baseline`, `test_generator_output_improves_with_training`, `test_generators_handle_different_input_sizes`, `test_hybrid_produces_different_output_than_fno` | `python -m unittest tests.torch.test_fno_reconstruction_quality` | — |
| `test_generator_adapter.py` | Tests for generator adapter path in ptycho_torch/model.py.  These tests verify the real/imag to complex channel-first conversion used by FNO/Hybrid generators to integrate with PtychoPINN's physics pipeline. | `test_ptychopinn_predict_complex_real_imag`, `test_ptychopinn_with_custom_generator`, `test_real_imag_to_complex_channel_first_invalid_shape`, `test_real_imag_to_complex_channel_first_with_imag`, `test_real_imag_to_complex_channel_first` | `python -m unittest tests.torch.test_generator_adapter` | — |
| `test_generator_registry.py` | No module docstring found. | `test_resolve_generator_cnn`, `test_resolve_generator_fno_vanilla`, `test_resolve_generator_hybrid_resnet`, `test_resolve_generator_unknown_raises` | `python -m unittest tests.torch.test_generator_registry` | — |
| `test_grad_norm_logging_flag.py` | No module docstring found. | `test_training_config_has_grad_norm_flags` | `python -m unittest tests.torch.test_grad_norm_logging_flag` | — |
| `test_grad_norm_utils.py` | No module docstring found. | `test_compute_grad_norm_l2_matches_manual` | `python -m unittest tests.torch.test_grad_norm_utils` | — |
| `test_grid_lines_hybrid_resnet_integration.py` | No module docstring found. | `test_grid_lines_dataset_stats`, `test_grid_lines_hybrid_resnet_metrics`, `test_grid_lines_scratch_root_created` | `python -m unittest tests.torch.test_grid_lines_hybrid_resnet_integration` | — |
| `test_grid_lines_torch_runner.py` | Smoke tests for the Torch grid-lines runner. | `test_config_creation`, `test_coords_relative_wins_over_nominal`, `test_coords_type_nominal_normalizes`, `test_coords_type_relative_passthrough`, `test_create_training_payload_sets_channels_from_gridsize`, `test_creates_execution_config`, `test_creates_training_config`, `test_default_loss_mode_is_mae`, `test_fno_inference_uses_forward_predict`, `test_fno_input_transform_passed`, `test_generator_output_mode_override`, `test_gradient_clip_algorithm_default`, `test_gradient_clip_algorithm_forwarded`, `test_load_missing_key_raises`, `test_load_valid_npz`, `test_metrics_stitch_predictions_to_ground_truth`, `test_runner_accepts_capped_channels`, `test_runner_accepts_fno_vanilla`, `test_runner_accepts_hybrid_resnet`, `test_runner_accepts_optimizer`, `test_runner_accepts_stable_hybrid`, `test_runner_channels_derived_from_gridsize`, `test_runner_creates_run_directory_structure`, `test_runner_emits_metrics_json`, `test_runner_passes_resnet_width`, `test_runner_rejects_hybrid_resnet_shallow_blocks`, `test_runner_rejects_invalid_resnet_width`, `test_runner_reports_model_params_and_inference_time`, `test_runner_returns_predictions_complex`, `test_runner_sets_gridsize_from_config`, `test_runner_sets_torch_loss_mode`, `test_runner_uses_lightning_training`, `test_runner_writes_recon_artifact`, `test_setup_configs_threads_plateau_params`, `test_setup_configs_threads_scheduler_fields`, `test_to_complex_patches_basic`, `test_to_complex_patches_preserves_values`, `test_training_payload_receives_architecture` | `python -m unittest tests.torch.test_grid_lines_torch_runner` | — |
| `test_grid_lines_torch_runner_grad_norm_flag.py` | No module docstring found. | `test_runner_config_propagates_grad_clip_to_training_config`, `test_runner_config_supports_grad_norm_logging` | `python -m unittest tests.torch.test_grid_lines_torch_runner_grad_norm_flag` | — |
| `test_inference_reassembly_parity.py` | No module docstring found. | `test_inference_helper_uses_reassembly`, `test_reassembly_canvas_padding_invariants` | `python -m unittest tests.torch.test_inference_reassembly_parity` | — |
| `test_integration_workflow_torch.py` | Phase C PyTorch Integration Workflow Tests (TEST-PYTORCH-001)  This module provides end-to-end integration tests for the PyTorch backend, mirroring the TensorFlow integration test structure in tests/test_integration_workflow.py.  Test Coverage: 1. PyTorch train → save → load → infer workflow using subprocess calls 2. Artifact persistence (Lightning checkpoint, model bundle) 3. Reconstruction output validation 4. CONFIG-001 compliance (params.cfg synchronization)  Implementation Status: Phase C2 (GREEN) — Pytest modernization complete. Helper function `_run_pytorch_workflow` now executes train/infer subprocesses.  References: - Phase C plan: plans/active/TEST-PYTORCH-001/reports/2025-10-19T120415Z/phase_c_modernization/plan.md - Implementation plan: plans/active/TEST-PYTORCH-001/implementation.md - TensorFlow baseline: tests/test_integration_workflow.py - Fixture inventory: plans/active/TEST-PYTORCH-001/reports/2025-10-19T115303Z/baseline/inventory.md | `test_bundle_loader_returns_modules`, `test_pytorch_tf_output_parity`, `test_pytorch_train_save_load_infer_cycle_legacy`, `test_run_pytorch_train_save_load_infer` | `python -m unittest tests.torch.test_integration_workflow_torch` | — |
| `test_lightning_checkpoint.py` | Red-then-green tests for Lightning checkpoint hyperparameter serialization.  Per Phase D1c requirements in plans/active/INTEGRATE-PYTORCH-001/phase_d2_completion.md, this module validates that PtychoPINN_Lightning checkpoints include serialized hyperparameters so that load_from_checkpoint() works without explicit config kwargs.  Expected failure mode (RED phase): - checkpoint['hyper_parameters'] returns None - load_from_checkpoint raises TypeError for missing 4 positional arguments  Implementation target (GREEN phase): - self.save_hyperparameters() called in PtychoPINN_Lightning.__init__() - Config objects serialized as dicts (Path/Tensor fields converted) - load_from_checkpoint reconstructs module from checkpoint alone | `test_checkpoint_configs_are_serializable`, `test_checkpoint_contains_hyperparameters`, `test_load_from_checkpoint_without_kwargs` | `python -m unittest tests.torch.test_lightning_checkpoint` | — |
| `test_lightning_dataloader_coords_guard.py` | No module docstring found. | `test_coords_nominal_allowed_when_object_big_false`, `test_coords_relative_required_when_object_big_true` | `python -m unittest tests.torch.test_lightning_dataloader_coords_guard` | — |
| `test_loss_modes.py` | No module docstring found. | `test_mae_loss_mode_logs_mae_metrics_only`, `test_poisson_loss_mode_logs_poisson_metrics` | `python -m unittest tests.torch.test_loss_modes` | — |
| `test_loss_units.py` | No module docstring found. | `test_mae_loss_operates_on_amplitude`, `test_poisson_intensity_layer_squares_observations` | `python -m unittest tests.torch.test_loss_units` | — |
| `test_lr_scheduler.py` | Tests for warmup+cosine LR scheduler (Phase 6 of FNO-STABILITY-OVERHAUL-001). | `test_warmup_cosine_no_warmup`, `test_warmup_cosine_scheduler_progression` | `python -m unittest tests.torch.test_lr_scheduler` | — |
| `test_mlflow_recon_logging.py` | Tests for PtychoReconLoggingCallback.  Covers: index selection determinism, epoch gating, DDP guard, missing-data skip paths, MLflow artifact logging calls. | `test_artifact_paths_contain_epoch_and_patch`, `test_auto_indices_deterministic`, `test_auto_indices_evenly_spaced`, `test_build_stitch_config_from_dataset_metadata`, `test_build_stitch_config_returns_none_without_metadata`, `test_empty_dataset`, `test_execution_config_custom`, `test_execution_config_defaults`, `test_explicit_indices_clamped_to_dataset_size`, `test_explicit_indices_respected`, `test_falls_back_to_train_dataloader`, `test_handles_val_dataloader_list`, `test_indices_cached_after_first_call`, `test_logs_artifacts_for_unsupervised`, `test_logs_at_correct_interval`, `test_logs_gt_and_error_for_supervised`, `test_multi_channel_patch_logging`, `test_no_logging_when_no_val_dataloader`, `test_runner_propagates_recon_log_max_stitch_samples`, `test_skips_non_interval_epoch`, `test_skips_when_no_logger`, `test_skips_when_not_global_zero`, `test_stitched_logging_skips_when_no_metadata`, `test_stitched_logging_uses_reassemble_patches`, `test_uses_rms_scale_for_output` | `python -m unittest tests.torch.test_mlflow_recon_logging` | — |
| `test_model_manager.py` | Tests for ptycho_torch/model_manager.py — PyTorch model persistence layer.  This module validates that the PyTorch persistence functions (`save_torch_bundle`, `load_torch_bundle`) satisfy the reconstructor persistence contract defined in specs/ptychodus_api_spec.md §4.6 and maintain archival format parity with ptycho/model_manager.py TensorFlow implementation.  Critical Behavioral Requirements (from spec §4.6 + Phase D3 callchain): 1. save_torch_bundle MUST produce wts.h5.zip-compatible archives with dual-model structure 2. Archive MUST contain manifest.dill, per-model subdirectories with params.dill snapshots 3. params.dill MUST capture full params.cfg state via dataclass_to_legacy_dict (CONFIG-001) 4. Archive format MUST enable cross-backend loading (PyTorch archives loadable by TF loaders) 5. All persistence functions must be torch-optional (importable when PyTorch unavailable)  Test Strategy: - Red-phase: document required archive structure via failing tests using zip inspection - Green-phase: implement save_torch_bundle producing spec-compliant archives - torch-optional: module structure follows test_config_bridge.py pattern (guarded imports)  Artifacts (Phase D3.B): - plans/active/INTEGRATE-PYTORCH-001/reports/2025-10-17T110500Z/phase_d3_writer.md - plans/active/INTEGRATE-PYTORCH-001/reports/2025-10-17T110500Z/pytest_red.log - plans/active/INTEGRATE-PYTORCH-001/reports/2025-10-17T110500Z/pytest_green.log | `test_archive_structure`, `test_load_round_trip_returns_model_stub`, `test_load_round_trip_updates_params_cfg`, `test_missing_params_raises_value_error`, `test_params_snapshot`, `test_reconstructs_models_from_bundle`, `test_save_bundle_with_intensity_scale` | `python -m unittest tests.torch.test_model_manager` | — |
| `test_model_output_modes.py` | No module docstring found. | `test_amp_phase_logits_bounds`, `test_amp_phase_mode_accepts_tuple` | `python -m unittest tests.torch.test_model_output_modes` | — |
| `test_model_training.py` | Tests for PyTorch model training functionality. | `test_configure_optimizers_selects_warmup_scheduler`, `test_configure_optimizers_supports_plateau`, `test_configures_adam`, `test_configures_adamw`, `test_configures_sgd`, `test_invalid_optimizer_raises` | `python -m unittest tests.torch.test_model_training` | — |
| `test_patch_stats_cli.py` | Minimal pytest for patch stats CLI flag plumbing (FIX-PYTORCH-FORWARD-PARITY-001 Phase A).  Tests that --log-patch-stats and --patch-stats-limit are accepted by the training CLI and forwarded through the configuration stack.  Usage:     pytest tests/torch/test_patch_stats_cli.py::TestPatchStatsCLI::test_patch_stats_flags_accepted -v  References:     - input.md (2025-11-16): Brief for Phase A instrumentation     - Phase A plan: plans/active/FIX-PYTORCH-FORWARD-PARITY-001/implementation.md §A2 | `test_factory_creates_inference_config_with_patch_stats`, `test_factory_inference_config_defaults`, `test_patch_stats_default_disabled`, `test_patch_stats_flags_accepted` | `python -m unittest tests.torch.test_patch_stats_cli` | — |
| `test_padded_size_no_jitter.py` | Regression test to ensure Torch padded size ignores max position jitter buffer. | `test_get_padded_size_ignores_max_position_jitter` | `python -m unittest tests.torch.test_padded_size_no_jitter` | — |
| `test_reassemble_patches_position_real_c1.py` | Regression test for Torch patch reassembly normalization when C=1 (centermask + epsilon, no hard masking). | `test_reassemble_patches_position_real_c1_uses_centermask_norm` | `python -m unittest tests.torch.test_reassemble_patches_position_real_c1` | — |
| `test_tf_helper.py` | No module docstring found. | `test_combine_complex`, `test_get_mask`, `test_placeholder_torch_functions` | `python -m unittest tests.torch.test_tf_helper` | — |
| `test_train_probe_size.py` | Unit tests for PyTorch CLI probe size inference (INTEGRATE-PYTORCH-001-PROBE-SIZE).  This module tests the PyTorch train.py CLI's ability to automatically infer probe size (DataConfig.N) from NPZ metadata, eliminating hardcoded defaults that cause tensor shape mismatches.  Test Coverage: 1. NPZ probe shape extraction utility function 2. DataConfig.N correctly derived from probeGuess metadata 3. Integration with existing npz_headers pattern  Red Phase Target: Tests should fail because utility function doesn't exist yet.  References: - specs/data_contracts.md §1 — probeGuess schema requirement - ptycho_torch/train.py:420 — Current hardcoded DataConfig(N=128) - ptycho_torch/dataloader.py:29-83 — Existing npz_headers() pattern to extend - docs/fix_plan.md#INTEGRATE-PYTORCH-001-PROBE-SIZE - input.md Do Now #1 | `test_infer_probe_size_128`, `test_infer_probe_size_from_npz`, `test_infer_probe_size_missing_probe`, `test_infer_probe_size_real_dataset`, `test_infer_probe_size_rectangular` | `python -m unittest tests.torch.test_train_probe_size` | — |
| `test_workflows_components.py` | Tests for ptycho_torch/workflows/components.py — PyTorch workflow orchestration module.  This module validates that the PyTorch workflow functions (`run_cdi_example_torch`, `train_cdi_model_torch`, `load_inference_bundle_torch`) satisfy the reconstructor contract defined in specs/ptychodus_api_spec.md §4 and maintain parity with ptycho/workflows/components.py.  Critical Behavioral Requirements (from CONFIG-001 + spec §4.5): 1. Entry points MUST call `update_legacy_dict(params.cfg, config)` before delegating    to core modules (prevents silent params.cfg drift). 2. All workflow functions must be torch-optional (importable when PyTorch unavailable). 3. Signatures must match TensorFlow equivalents to enable transparent backend selection.  Test Strategy: - Red-phase: document required API via failing tests using monkeypatch spies - Green-phase: implement stubs that invoke update_legacy_dict and raise NotImplementedError - torch-optional: module structure follows test_config_bridge.py pattern (guarded imports)  Artifacts (Phase D2.A): - plans/active/INTEGRATE-PYTORCH-001/reports/2025-10-17T091450Z/phase_d2_scaffold.md - plans/active/INTEGRATE-PYTORCH-001/reports/2025-10-17T091450Z/pytest_scaffold.log | `test_batches_remain_float32`, `test_coords_relative_layout`, `test_dataloader_casts_float64_to_float32`, `test_disable_checkpointing_skips_callbacks`, `test_early_stopping_callback_configured`, `test_execution_config_controls_determinism`, `test_execution_config_overrides_trainer`, `test_inference_uses_execution_batch_size`, `test_lightning_dataloader_tensor_dict_structure`, `test_lightning_poisson_count_contract`, `test_lightning_training_respects_gridsize`, `test_load_inference_bundle_handles_bundle`, `test_model_checkpoint_callback_configured`, `test_monitor_uses_val_loss_name`, `test_probe_big_false_no_mismatch`, `test_probe_big_shape_alignment`, `test_reassemble_cdi_image_torch_flip_transpose_contract`, `test_reassemble_cdi_image_torch_guard_without_train_results`, `test_reassemble_cdi_image_torch_return_contract`, `test_run_cdi_example_calls_update_legacy_dict`, `test_run_cdi_example_invokes_training`, `test_run_cdi_example_persists_models`, `test_run_cdi_example_torch_do_stitching_delegates_to_reassemble`, `test_train_cdi_model_torch_invokes_lightning`, `test_train_with_lightning_instantiates_module`, `test_train_with_lightning_passes_fno_input_transform`, `test_train_with_lightning_returns_models_dict`, `test_train_with_lightning_runs_trainer_fit`, `test_trainer_receives_accumulation`, `test_trainer_receives_logger` | `python -m unittest tests.torch.test_workflows_components` | — |

---
*This document can be automatically updated. Run ``python scripts/tools/generate_test_index.py`` to regenerate.*
