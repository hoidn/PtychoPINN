# PtychoPINN Test Suite Index

This document provides a comprehensive index of the automated tests in the `tests/` directory. Its purpose is to make the test suite discoverable, explain the scope of each test module, and provide direct commands for running specific tests.

## How to Run Tests

- **Run all tests:** `python -m unittest discover tests/`
- **Run a specific file:** `python -m unittest tests.image.test_cropping`
- **Run a specific test class:** `python -m unittest tests.test_integration_workflow.TestFullWorkflow`

### Markers (pytest)

- `integration`: Project‑level end‑to‑end integration smoke tests.
  - Run: `pytest -v -m integration`
  - Canonical NodeID (TensorFlow workflow in this repo): `tests/test_integration_workflow.py::TestFullWorkflow::test_train_save_load_infer_cycle`
  - Alias: `tf_integration`

---

## Test Modules

### Core Library Tests (`tests/`)

| Test File | Purpose / Scope | Key Tests | Usage / Command | Notes |
| :--- | :--- | :--- | :--- | :--- |

| `test_baselines.py` | No module docstring found. | `test_build_model_always_creates_single_channel_output` | `python -m unittest tests.test_baselines` | — |
| `test_benchmark_throughput.py` | Unit and Integration Tests for Inference Throughput Benchmarking  This module tests the benchmarking infrastructure for measuring and optimizing PtychoPINN inference throughput. | `test_calculate_efficiency`, `test_calculate_latency`, `test_calculate_throughput`, `test_clear_timings`, `test_config_custom_values`, `test_config_initialization`, `test_export_results`, `test_find_optimal_batch_size_no_success`, `test_find_optimal_batch_size`, `test_func`, `test_generate_summary`, `test_get_statistics`, `test_load_model_and_data`, `test_measure_function`, `test_profile_cpu_memory`, `test_snapshot`, `test_timing_profiler_performance`, `test_warmup_inference` | `python -m unittest tests.test_benchmark_throughput` | — |
| `test_cli_args.py` | Unit tests for the ptycho.cli_args module. | `test_add_logging_arguments`, `test_console_level_choices`, `test_get_logging_config_custom_level`, `test_get_logging_config_defaults`, `test_get_logging_config_quiet`, `test_get_logging_config_verbose`, `test_quiet_verbose_mutually_exclusive` | `python -m unittest tests.test_cli_args` | — |
| `test_coordinate_grouping.py` | Comprehensive test suite for efficient coordinate grouping implementation.  This module tests the new "sample-then-group" strategy for coordinate grouping in the RawData class, ensuring correctness, performance, and edge case handling. | `test_backward_compatibility`, `test_different_seeds_produce_different_results`, `test_edge_case_k_less_than_c`, `test_edge_case_more_samples_than_points`, `test_edge_case_small_dataset`, `test_efficient_grouping_output_shape`, `test_efficient_grouping_spatial_coherence`, `test_efficient_grouping_valid_indices`, `test_existing_tests_still_pass`, `test_generate_grouped_data_gridsize_1`, `test_generate_grouped_data_integration`, `test_memory_efficiency`, `test_no_cache_files_created`, `test_performance_improvement`, `test_reproducibility_with_seed` | `python -m unittest tests.test_coordinate_grouping` | — |
| `test_generic_loader.py` | No module docstring found. | `test_generic_loader_roundtrip`, `test_generic_loader` | `python -m unittest tests.test_generic_loader` | — |
| `test_integration_baseline_gs2.py` | No module docstring found. | `test_baseline_gridsize2_end_to_end` | `python -m unittest tests.test_integration_baseline_gs2` | — |
| `test_integration_workflow.py` | Validates the full train → save → load → infer workflow using subprocesses, ensuring model artifacts persist across CLI entrypoints. | `test_train_save_load_infer_cycle` | `python -m unittest tests.test_integration_workflow` | Critical integration coverage for TensorFlow persistence. |
| `test_log_config.py` | Unit tests for the ptycho.log_config module. | `test_backward_compatibility`, `test_conflicting_flags_verbose_overrides`, `test_custom_console_level`, `test_default_setup_logging_creates_log_directory_and_file`, `test_quiet_flag_overrides_console_level`, `test_quiet_mode_disables_console`, `test_setup_logging_clears_existing_handlers`, `test_string_path_support`, `test_verbose_mode_enables_debug_console` | `python -m unittest tests.test_log_config` | — |
| `test_misc.py` | No module docstring found. | `test_memoize_simulated_data` | `python -m unittest tests.test_misc` | — |
| `test_model_factory.py` | Tests for model factory functions. Tests MODULE-SINGLETON-001 fix: models created with different N values must work correctly in a single process. Also tests lazy loading (no import-time side effects) and XLA re-enablement. | `test_multi_n_model_creation`, `test_import_no_side_effects`, `test_multi_n_with_xla_enabled` | `pytest tests/test_model_factory.py -vv` | Phase A/B/C tests for REFACTOR-MODEL-SINGLETON-001. |
| `test_nphotons_metadata_integration.py` | Comprehensive integration test for nphotons metadata system.  This test verifies the complete nphotons metadata workflow: 1. Simulation with different nphotons values saves metadata correctly 2. Training loads metadata and validates configurations  3. Inference uses correct nphotons from metadata 4. Parameter mismatch warnings work correctly 5. End-to-end workflow maintains nphotons consistency  The test follows the project's integration test patterns using subprocess calls to simulate real user workflows across separate processes. | `test_configuration_mismatch_warnings`, `test_end_to_end_workflow_consistency`, `test_metadata_backward_compatibility`, `test_metadata_persistence_single_nphotons`, `test_multiple_nphotons_metadata_consistency`, `test_training_with_mismatched_config_warns_but_continues` | `python -m unittest tests.test_nphotons_metadata_integration` | — |
| `test_oversampling.py` | Test automatic K choose C oversampling functionality. | `test_automatic_oversampling_triggers`, `test_gridsize_1_no_oversampling`, `test_oversampling_with_different_k_values`, `test_reproducibility_with_seed`, `test_standard_sampling_no_oversampling` | `python -m unittest tests.test_oversampling` | — |
| `test_projective_warp_xla.py` | Test cases for projective_warp_xla module. | `test_batch_processing`, `test_complex128_dtype`, `test_complex64_dtype`, `test_fill_modes`, `test_float32_dtype`, `test_float64_dtype`, `test_float64_with_translation`, `test_interpolation_modes`, `test_jit_compilation_float32`, `test_jit_compilation_float64`, `test_mixed_precision_translation`, `test_tfa_params_conversion` | `python -m unittest tests.test_projective_warp_xla` | — |
| `test_pytorch_tf_wrapper.py` | No module docstring found. | N/A | `python -m unittest tests.test_pytorch_tf_wrapper` | — |
| `test_raw_data_grouping.py` | Exhaustively checks the RawData sample-then-group implementation, covering shapes, bounds, locality, and oversized sampling requests. | `test_content_validity`, `test_edge_case_k_less_than_c`, `test_edge_case_more_samples_than_points`, `test_edge_case_small_dataset`, `test_memory_efficiency`, `test_output_shape`, `test_performance_improvement`, `test_reproducibility`, `test_uniform_sampling` | `python -m unittest tests.test_raw_data_grouping` | — |
| `test_run_baseline.py` | No module docstring found. | `test_prepare_baseline_data_flattens_gridsize2_data`, `test_prepare_baseline_data_passes_through_gridsize1` | `python -m unittest tests.test_run_baseline` | — |
| `test_scaling_regression.py` | Regression tests for scaling bugs in diffsim.py | `test_assertions_catch_invalid_intensity_scale`, `test_both_arrays_scaled_identically`, `test_different_nphotons_produce_proportional_scaling`, `test_intensity_scale_is_valid`, `test_phase_is_not_scaled`, `test_scaling_is_reversible`, `test_scaling_preserves_physics` | `python -m unittest tests.test_scaling_regression` | — |
| `test_sequential_sampling.py` | Test suite for sequential sampling functionality in coordinate grouping.  This module tests the sequential_sampling flag added to the RawData class to restore deterministic, sequential data subset selection capability. | `test_cli_argument_parsing`, `test_config_flag_exists`, `test_default_behavior_is_random`, `test_sequential_sampling_handles_edge_cases`, `test_sequential_sampling_is_deterministic`, `test_sequential_sampling_order`, `test_sequential_sampling_uses_first_n_points`, `test_sequential_sampling_with_gridsize_greater_than_1`, `test_sequential_sampling_with_seed_parameter`, `test_sequential_vs_random_coverage` | `python -m unittest tests.test_sequential_sampling` | — |
| `test_subsampling.py` | Unit tests for independent data subsampling functionality.  This module tests the new n_subsample parameter that enables independent control of data subsampling and neighbor grouping operations in PtychoPINN. | `test_different_seeds_produce_different_results`, `test_interaction_with_config_dataclass`, `test_legacy_n_images_behavior`, `test_n_subsample_overrides_n_images`, `test_no_subsample_uses_full_dataset`, `test_reproducible_subsampling_with_seed`, `test_sorted_indices_for_consistency`, `test_subsample_larger_than_dataset`, `test_subsample_with_n_subsample`, `test_subsample_zero_edge_case`, `test_y_patches_subsampled_consistently` | `python -m unittest tests.test_subsampling` | — |
| `test_tf_helper.py` | Unit tests for ptycho/tf_helper.py, focusing on patch reassembly logic.  This test suite validates the core functionality of the reassemble_position function by testing its fundamental properties without making assumptions about exact output sizes. | `test_basic_functionality`, `test_batch_translation`, `test_complex_tensor_translation`, `test_different_patch_values_blend`, `test_edge_cases`, `test_identical_patches_single_vs_double`, `test_integer_translation`, `test_perfect_overlap_averages_to_identity`, `test_subpixel_translation`, `test_translate_core_matches_addons`, `test_zero_translation` | `python -m unittest tests.test_tf_helper` | — |
| `test_tf_helper_edge_aware.py` | Edge-aware tests for translate functions that account for interpolation differences.  This test suite specifically handles the known differences between TFA and TF raw ops regarding edge interpolation while ensuring smooth patterns (as used in PtychoPINN) work correctly. | `test_batch_smooth_patterns`, `test_boundary_behavior`, `test_complex_smooth_translation`, `test_document_edge_differences`, `test_gaussian_probe_translation`, `test_smooth_object_translation`, `test_typical_probe_sizes` | `python -m unittest tests.test_tf_helper_edge_aware` | — |
| `test_utilities.py` | Test utilities for PtychoPINN tests. | N/A | `python -m unittest tests.test_utilities` | — |
| `test_workflow_components.py` | Unit tests for ptycho.workflows.components module. | `test_exception_propagation`, `test_load_valid_model_directory`, `test_missing_diffraction_model`, `test_missing_model_archive`, `test_nonexistent_directory`, `test_not_a_directory`, `test_path_conversion` | `python -m unittest tests.test_workflow_components` | — |

### Image Tests (`tests/image/`)

| Test File | Purpose / Scope | Key Tests | Usage / Command | Notes |
| :--- | :--- | :--- | :--- | :--- |

| `test_cropping.py` | Tests for the cropping module, focusing on the align_for_evaluation function. | `test_align_for_evaluation_bounding_box`, `test_align_for_evaluation_coordinates_format`, `test_align_for_evaluation_shapes`, `test_align_for_evaluation_with_squeeze`, `test_center_crop_exact_size` | `python -m unittest tests.image.test_cropping` | — |
| `test_registration.py` | Covers registration alignment helpers, including translation detection, shift application, and complex-valued data handling. | `test_apply_shift_and_crop_basic`, `test_apply_shift_and_crop_zero_offset`, `test_different_image_content`, `test_edge_case_maximum_shift`, `test_edge_case_single_pixel_shift`, `test_find_offset_known_shift_complex`, `test_find_offset_known_shift_real`, `test_input_validation_2d_requirement`, `test_input_validation_excessive_offset`, `test_input_validation_shape_matching`, `test_noise_robustness`, `test_register_and_align_convenience`, `test_registration_sign_verification`, `test_round_trip_registration`, `test_shift_and_crop_preserves_data_type` | `python -m unittest tests.image.test_registration` | — |

### Study Tests (`tests/study/`)

| Test File | Purpose / Scope | Key Tests | Usage / Command | Notes |
| :--- | :--- | :--- | :--- | :--- |

| `test_dose_overlap_design.py` | Tests study design configuration and validation for STUDY-SYNTH-FLY64-DOSE-OVERLAP-001 initiative. Validates study parameters and overlap metric configuration (spacing thresholds are reference-only, not acceptance gates). | `test_study_design_constants`, `test_study_design_validation`, `test_study_design_to_dict` | `pytest tests/study/test_dose_overlap_design.py -v` | Validates study design dataclass and constants. |
| `test_dose_overlap_dataset_contract.py` | Tests DATA-001 contract enforcement for Phase C/D datasets. Validates dataset structure, dtypes, and oversampling preconditions (no spacing-based acceptance gate; overlap is measured in Phase D). | `test_validate_dataset_contract_happy_path`, `test_validate_dataset_contract_oversampling_precondition_pass` | `pytest tests/study/test_dose_overlap_dataset_contract.py -v` | Enforces canonical NPZ format and study constraints. |
| `test_dose_overlap_generation.py` | Tests Phase C dataset generation pipeline for dose sweep. Validates simulation configuration, dataset orchestration, and output structure. | `test_build_simulation_plan`, `test_generate_dataset_pipeline_orchestration`, `test_generate_dataset_config_construction` | `pytest tests/study/test_dose_overlap_generation.py -v` | Tests dataset generation workflows (dose_1000, dose_10000, dose_100000). |
| `test_dose_overlap_overlap.py` | Tests Phase D overlap metrics. Validates metric computations and metrics bundle emission (no geometry acceptance gating). | `test_generate_overlap_views_metrics_manifest` | `pytest tests/study/test_dose_overlap_overlap.py -v` | Tests overlap metrics pipeline and metrics aggregation. Key selector: `pytest tests/study/test_dose_overlap_overlap.py::test_generate_overlap_views_metrics_manifest -vv`. |
| `test_dose_overlap_training.py` | Tests Phase E training job matrix enumeration, run helper execution, and CLI skip reporting. Validates that build_training_jobs() produces correct job count (9 jobs = 3 doses × 3 variants) and run_training_job() orchestrates CONFIG-001 bridge, directory creation, runner invocation, and dry-run mode. Phase E5 tests validate skip summary persistence (`skip_summary.json`) and manifest integration when views are absent. | `test_build_training_jobs_matrix`, `test_run_training_job_invokes_runner`, `test_run_training_job_dry_run`, `test_training_cli_manifest_and_bridging`, `test_build_training_jobs_skips_missing_view` | `pytest tests/study/test_dose_overlap_training.py -v` or `pytest tests/study/test_dose_overlap_training.py::test_build_training_jobs_matrix -vv` or `pytest tests/study/test_dose_overlap_training.py -k run_training_job -vv` or `pytest tests/study/test_dose_overlap_training.py -k training_cli -vv` | Tests job builder logic with dependency injection (tmp_path fixtures for Phase C/D datasets). Validates baseline (gs1) and overlap (gs2 dense/sparse) job generation. E3 run helper tests use spies to validate CONFIG-001 compliance, directory creation, and dry-run behavior. E5 CLI tests validate skip summary file existence, schema (`{timestamp, skipped_views, skipped_count}`), and manifest consistency. See Phase E5 evidence at `plans/active/STUDY-SYNTH-FLY64-DOSE-OVERLAP-001/reports/2025-11-04T170500Z/phase_e_training_e5_real_run_baseline/`. |
| `test_dose_overlap_reconstruction.py` | Tests Phase F pty-chi LSQML baseline reconstruction orchestration. Validates that build_ptychi_jobs() enumerates 18 jobs (3 doses × 2 views × 3 splits), run_ptychi_job() dispatches subprocess with CLI argument handoff (`--input-npz`, `--output-dir`, `--algorithm`, `--num-epochs`, `--n-images`), dry-run filtering emits manifest + skip summary, and live execution captures per-job logging with execution telemetry (`returncode`, `log_path`) in manifest. Phase F dense/test baseline validates script argument parsing and reconstruction output persistence. | `test_build_ptychi_jobs_manifest`, `test_run_ptychi_job_invokes_script`, `test_cli_filters_dry_run`, `test_cli_executes_selected_jobs` | `pytest tests/study/test_dose_overlap_reconstruction.py -v` or `pytest tests/study/test_dose_overlap_reconstruction.py::test_build_ptychi_jobs_manifest -vv` or `pytest tests/study/test_dose_overlap_reconstruction.py -k "ptychi" -vv` or `pytest tests/study/test_dose_overlap_reconstruction.py --collect-only -vv` | Phase F tests use subprocess mocking to validate CLI handoff without heavy pty-chi dependencies. Script test (`tests/scripts/test_ptychi_reconstruct_tike.py`) validates `ptychi_reconstruct_tike.py::main()` honors CLI arguments via argparse. Deterministic CLI command: `AUTHORITATIVE_CMDS_DOC=./docs/TESTING_GUIDE.md python -m studies.fly64_dose_overlap.reconstruction --phase-c-root tmp/phase_c_f2_cli --phase-d-root tmp/phase_d_f2_cli --artifact-root tmp/reconstruction_artifacts --dose 1000 --view dense --split test --allow-missing-phase-d`. See Phase F dense/test evidence at `plans/active/STUDY-SYNTH-FLY64-DOSE-OVERLAP-001/reports/2025-11-04T230000Z/phase_f_ptychi_baseline_f2_dense_test_run/`. |
| `test_phase_g_dense_orchestrator.py` | Tests Phase G orchestrator helpers in `bin/run_phase_g_dense.py`: (1) `prepare_hub()` helper for stale output detection/archival, (2) `validate_phase_c_metadata()` guard enforcing DATA-001 metadata contract and canonical transformation history, (3) `summarize_phase_g_outputs()` metrics aggregation helper, (4) highlights preview emission to stdout, and (5) automated digest generation via `analyze_dense_metrics.py` invocation. Hub preparation helper detects existing Phase C outputs, raises RuntimeError with `--clobber` guidance (default read-only), or archives stale outputs to timestamped `{hub}/archive/` directory (clobber mode). Metadata guard validates Phase C NPZ outputs contain `_metadata` field and `transpose_rename_convert` transformation in `data_transformations` list (read-only checks, TYPE-PATH-001 compliant). Summary helper parses `comparison_manifest.json`, extracts metrics from per-job `comparison_metrics.csv` (tidy format: model, metric, amplitude, phase, value), and emits JSON/Markdown summaries to `{hub}/analysis/` with aggregate statistics (mean/best per model). Highlights preview reads `analysis/aggregate_highlights.txt` (generated by reporting helper) and prints content to stdout with banner for quick sanity-checks of MS-SSIM/MAE deltas without opening files. Digest invocation test validates orchestrator automatically calls `analyze_dense_metrics.py` after reporting helper with correct flags (`--metrics`, `--highlights`, `--output`) and log path. | `test_prepare_hub_detects_stale_outputs`, `test_prepare_hub_clobbers_previous_outputs`, `test_validate_phase_c_metadata_requires_metadata`, `test_validate_phase_c_metadata_requires_canonical_transform`, `test_validate_phase_c_metadata_accepts_valid_metadata`, `test_summarize_phase_g_outputs`, `test_summarize_phase_g_outputs_fails_on_missing_manifest`, `test_summarize_phase_g_outputs_fails_on_execution_failures`, `test_summarize_phase_g_outputs_fails_on_missing_csv`, `test_run_phase_g_dense_collect_only_generates_commands`, `test_run_phase_g_dense_exec_invokes_reporting_helper`, `test_run_phase_g_dense_exec_prints_highlights_preview`, `test_run_phase_g_dense_exec_runs_analyze_digest` | `pytest tests/study/test_phase_g_dense_orchestrator.py::test_prepare_hub_detects_stale_outputs -vv` or `pytest tests/study/test_phase_g_dense_orchestrator.py::test_prepare_hub_clobbers_previous_outputs -vv` or `pytest tests/study/test_phase_g_dense_orchestrator.py -k metadata -vv` or `pytest tests/study/test_phase_g_dense_orchestrator.py::test_validate_phase_c_metadata_requires_canonical_transform -vv` or `pytest tests/study/test_phase_g_dense_orchestrator.py::test_summarize_phase_g_outputs -vv` or `pytest tests/study/test_phase_g_dense_orchestrator.py -k fails -vv` or `pytest tests/study/test_phase_g_dense_orchestrator.py::test_run_phase_g_dense_exec_prints_highlights_preview -vv` or `pytest tests/study/test_phase_g_dense_orchestrator.py::test_run_phase_g_dense_exec_runs_analyze_digest -vv` or `pytest tests/study/test_phase_g_dense_orchestrator.py --collect-only -vv` | Tests use `importlib.util.spec_from_file_location()` to load helpers from orchestrator script. Hub preparation tests validate stale detection with read-only error (no file deletions) and clobber mode with clean hub state (all .npz files removed/archived). Metadata guard tests validate three behaviors: missing metadata baseline (raises RuntimeError mentioning `_metadata`), missing canonical transformation (raises mentioning `transpose_rename_convert`), and valid metadata acceptance (uses `MetadataManager.add_transformation_record()` to embed transformation). Summary helper tests validate manifest parsing, fail-fast error handling, metrics extraction, JSON/Markdown emission, and aggregate computation (mean/best MS-SSIM/MAE per model). Highlights preview test validates stdout contains "Aggregate highlights preview" banner and sample MS-SSIM/MAE delta content after reporting helper runs. Digest invocation test validates `analyze_dense_metrics.py` is called after reporting helper, checks command order (reporting then digest), and validates CLI flags (--metrics metrics_summary.json, --highlights aggregate_highlights.txt, --output metrics_digest.md) with log path cli/metrics_digest_cli.log. Full orchestrator script runs 8-command Phase C→G pipeline (generation → overlap → training baseline/dense → reconstruction train/test → comparison train/test) with CLI logs under `{hub}/cli/`, prepares hub before execution (detects/archives stale outputs), validates Phase C metadata after generation, calls summary helper + reporting helper after success, prints highlights preview to stdout, and invokes digest script to generate final `metrics_digest.md`. Deterministic CLI command: `AUTHORITATIVE_CMDS_DOC=./docs/TESTING_GUIDE.md python plans/active/STUDY-SYNTH-FLY64-DOSE-OVERLAP-001/bin/run_phase_g_dense.py --hub <hub-path> --dose 1000 --view dense --splits train test --clobber`. Dry-run: add `--collect-only` flag. 13 tests collected (2 prepare_hub + 3 metadata guard + 4 summary + 1 collect-only smoke + 3 execution flow). See Phase G evidence at `plans/active/STUDY-SYNTH-FLY64-DOSE-OVERLAP-001/reports/2025-11-09T050500Z/phase_g_dense_full_execution_real_run/`. |
| `test_phase_g_dense_metrics_report.py` | Tests Phase G metrics reporting helper (`bin/report_phase_g_dense_metrics.py`) and analysis script (`bin/analyze_dense_metrics.py`): validates (1) aggregate metrics report generation from `metrics_summary.json`, delta computation, 3-decimal formatting, stdout+Markdown emission, highlights file generation, and fail-fast on missing models; (2) digest generation combining metrics summary with highlights, success banner emission when `n_failed == 0`, failure banner when `n_failed > 0`, exit code correctness, and digest file persistence. Reporting helper tests validate successful report generation with all models present (aggregate tables, delta sections for MS-SSIM/MAE amplitude/phase, signed +/- deltas) and graceful failure (exit 1, actionable stderr) when models missing. Analysis script tests validate success path (exit 0, success banner **✓ ALL COMPARISONS SUCCESSFUL ✓** present, no failure warning) and failure path (exit 1, failure banner **⚠️ FAILURES PRESENT ⚠️** in stderr/digest, digest still written). Both helpers documented in docs/TESTING_GUIDE.md:284-329 with usage examples. | `test_report_phase_g_dense_metrics`, `test_report_phase_g_dense_metrics_missing_model_fails`, `test_analyze_dense_metrics_flags_failures`, `test_analyze_dense_metrics_success_digest` | `pytest tests/study/test_phase_g_dense_metrics_report.py::test_report_phase_g_dense_metrics -vv` or `pytest tests/study/test_phase_g_dense_metrics_report.py::test_analyze_dense_metrics_success_digest -vv` or `pytest tests/study/test_phase_g_dense_metrics_report.py::test_analyze_dense_metrics_flags_failures -vv` or `pytest tests/study/test_phase_g_dense_metrics_report.py --collect-only -vv` | Tests invoke helpers as subprocess via `subprocess.run()` to validate CLI interfaces end-to-end. Reporting helper CLI: `python plans/active/.../bin/report_phase_g_dense_metrics.py --metrics <metrics_summary.json> --output <aggregate_report.md> --highlights <aggregate_highlights.txt>`. Analysis script CLI: `python plans/active/.../bin/analyze_dense_metrics.py --metrics <metrics_summary.json> --highlights <aggregate_highlights.txt> --output <metrics_digest.md>`. Exit codes: 0 (success), 1 (failures/missing files), 2 (invalid format). 4 tests collected (up from 2). See Phase G evidence at `plans/active/STUDY-SYNTH-FLY64-DOSE-OVERLAP-001/reports/2025-11-09T030500Z/phase_g_dense_full_execution_real_run/`. |
| `test_phase_g_dense_artifacts_verifier.py` | Tests Phase G dense pipeline artifact verification script (`bin/verify_dense_pipeline_artifacts.py`): validates (1) presence of all required artifacts (Phase C NPZ datasets, Phase D/E/F intermediate outputs, Phase G comparison CSVs, analysis summaries/highlights/preview/digest/ssim_grid), (2) metrics delta summary JSON schema (TYPE-PATH-001 compliance with POSIX-relative paths), (3) highlights vs preview value matching (both sourced from same JSON deltas with MS-SSIM ±0.000 and MAE ±0.000000 precision), (4) PREVIEW-PHASE-001 guard enforcing phase-only formatting (preview MUST NOT contain "amplitude" keyword), (5) CLI logs including ssim_grid_cli.log helper, and (6) ssim_grid_summary.md presence with phase-only preview metadata. Preview guard tests validate RED fixture (amplitude contamination fails validation with `preview_phase_only=False` and `preview_format_errors` metadata) and GREEN fixture (phase-only format passes with `preview_phase_only=True`). SSIM grid tests validate helper log requirement (`test_verify_dense_pipeline_cli_logs_require_ssim_grid_log`) and summary markdown requirement (`test_verify_dense_pipeline_requires_ssim_grid_summary`). Verifier emits JSON report with pass/fail status, artifact inventory, validation errors, and provenance metadata. | `test_verify_dense_pipeline_highlights_preview_contains_amplitude`, `test_verify_dense_pipeline_highlights_complete`, `test_verify_dense_pipeline_cli_logs_require_ssim_grid_log`, `test_verify_dense_pipeline_requires_ssim_grid_summary` | `pytest tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_highlights_preview_contains_amplitude -vv` or `pytest tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_highlights_complete -vv` or `pytest tests/study/test_phase_g_dense_artifacts_verifier.py -k ssim_grid -vv` or `pytest tests/study/test_phase_g_dense_artifacts_verifier.py --collect-only -vv` | Preview guard tests validate that `verify_dense_pipeline_artifacts.py::validate_metrics_delta_highlights()` enforces PREVIEW-PHASE-001: preview lines must follow `<prefix>: <delta>` format without "amplitude" keyword (phase values only). RED test creates hub with amplitude-contaminated preview (`"amplitude +0.010 phase +0.015"`), invokes verifier, asserts non-zero exit + `preview_phase_only=False` + `preview_format_errors` list in JSON report. GREEN test uses valid phase-only preview (`"+0.015"` only), asserts zero exit + `preview_phase_only=True` + empty `preview_format_errors`. SSIM grid helper tests validate RED fixtures (missing ssim_grid_cli.log or ssim_grid_summary.md) fail validation with actionable error messages citing PREVIEW-PHASE-001, and GREEN fixtures with complete artifacts pass. The `validate_ssim_grid_summary()` function checks for preview metadata line containing "phase-only: ✓" indicator. Verifier CLI: `python plans/active/.../bin/verify_dense_pipeline_artifacts.py --hub <hub-path> --report <report.json>`. Exit codes: 0 (all validations pass), 1 (validation failures). 17 tests collected (2 ssim_grid + 15 existing). See evidence at `plans/active/STUDY-SYNTH-FLY64-DOSE-OVERLAP-001/reports/2025-11-12T010500Z/phase_g_dense_full_run_verifier/`. |
| `test_check_dense_highlights_match.py` | Tests Phase G highlights checker script (`bin/check_dense_highlights_match.py`): validates (1) highlights/preview text values match metrics_delta_summary.json with correct precision (MS-SSIM ±0.000, MAE ±0.000000 per STUDY-001), (2) ssim_grid_summary.md table values match JSON deltas exactly, and (3) PREVIEW-PHASE-001 enforcement via preview metadata containing "phase-only" indicator. RED tests validate failure on tampered ssim_grid table values (test_summary_mismatch_fails) and missing phase-only metadata (test_missing_phase_only_metadata_fails). GREEN test validates success with aligned artifacts (test_summary_matches_json). Checker parses Markdown tables with regex, extracts ±formatted deltas, and cross-validates against JSON source of truth. | `test_summary_mismatch_fails`, `test_summary_matches_json`, `test_missing_phase_only_metadata_fails` | `pytest tests/study/test_check_dense_highlights_match.py -vv` or `pytest tests/study/test_check_dense_highlights_match.py::test_summary_mismatch_fails -vv` or `pytest tests/study/test_check_dense_highlights_match.py::test_summary_matches_json -vv` | Tests use tmp_path fixtures to create minimal hubs with synthesized metrics_delta_summary.json, highlights, preview, and ssim_grid_summary.md artifacts. Tamper flags (`tamper_ssim_grid`, `remove_phase_only`) enable RED/GREEN test scenarios. Checker CLI: `python plans/active/.../bin/check_dense_highlights_match.py --hub <hub-path>`. Exit codes: 0 (all validations pass), 1 (validation failures). 3 tests collected (2 RED + 1 GREEN). Checker invoked as subprocess via `subprocess.run()` to validate CLI interface. See evidence at hub path `plans/active/STUDY-SYNTH-FLY64-DOSE-OVERLAP-001/reports/2025-11-12T010500Z/phase_g_dense_full_run_verifier/green/pytest_highlights_checker.log`. |

### Studies Tests (`tests/studies/`)

| Test File | Purpose / Scope | Key Tests | Usage / Command | Notes |
| :--- | :--- | :--- | :--- | :--- |

| `test_aggregate_nan_msssim.py` | Test script to verify NaN handling and MS-SSIM filtering in aggregate_and_plot_results.py  This script creates mock data directories with controlled test cases: 1. A trial with good MS-SSIM (0.8) 2. A trial with bad MS-SSIM (0.1)  3. A trial with NaN values  Then runs the aggregation script with different thresholds to verify behavior. | N/A | `python -m unittest tests.studies.test_aggregate_nan_msssim` | — |
| `test_filtering_order.py` | Enhanced test script to verify filtering order in aggregate_and_plot_results.py  This script creates mock data with specific MS-SSIM values to test if filtering happens before or after statistical aggregation.  Test scenario: - 4 trials with ms_ssim_phase values: 0.8, 0.7, 0.6, 0.1 - With threshold 0.3, the outlier (0.1) should be filtered out - Correct median after filtering: 0.7 (from [0.8, 0.7, 0.6]) - Incorrect median if filtering after aggregation: 0.65 (from [0.8, 0.7, 0.6, 0.1]) | N/A | `python -m unittest tests.studies.test_filtering_order` | — |
| `test_mean_vs_median.py` | Test script to verify that aggregate_and_plot_results.py now uses mean instead of median  This script creates mock data with specific values to test the statistical aggregation: - 3 trials with values: 10, 20, 30 - Expected mean: 20.0 - Expected 25th percentile: 15.0 - Expected 75th percentile: 25.0 - Expected median (old): 20.0 (same as mean in this case) | N/A | `python -m unittest tests.studies.test_mean_vs_median` | — |

### Tools Tests (`tests/tools/`)

| Test File | Purpose / Scope | Key Tests | Usage / Command | Notes |
| :--- | :--- | :--- | :--- | :--- |

| `test_generate_test_index.py` | Tests for the generate_test_index automation script. | `test_get_module_docstring_handles_missing_docstring`, `test_get_module_docstring_reads_existing_docstring`, `test_get_test_functions_lists_key_tests` | `python -m unittest tests.tools.test_generate_test_index` | — |
| `test_update_tool.py` | Test script for update_tool.py | `test_update_function`, `test_update_function` | `python -m unittest tests.tools.test_update_tool` | — |

### Torch Tests (`tests/torch/`)

| Test File | Purpose / Scope | Key Tests | Usage / Command | Notes |
| :--- | :--- | :--- | :--- | :--- |

| `test_api_deprecation.py` | Test suite for ptycho_torch.api deprecation warnings per ADR-003 Phase E.C1. Validates that legacy API entry points emit DeprecationWarning with migration guidance steering users toward factory-driven workflows documented in docs/workflows/pytorch.md. | `test_example_train_import_emits_deprecation_warning`, `test_api_package_import_is_idempotent` | `pytest tests/torch/test_api_deprecation.py -vv` | Validates legacy API deprecation messaging. Uses native pytest style. |
| `test_debug_fno_activations.py` | Smoke test for the one-off FNO activation debug script. Verifies the script runs on a tiny NPZ and emits `activation_report.json` with expected keys. | `test_debug_fno_activations_emits_report` | `pytest tests/torch/test_debug_fno_activations.py -vv` | Script-level smoke coverage. |
| `test_fno_fallback_init.py` | Validates fallback spectral weight initialization uses variance-preserving scaling. | `test_fallback_spectral_init_scale` | `pytest tests/torch/test_fno_fallback_init.py -vv` | Covers fallback spectral init scale. |
| `test_fno_generators.py` | Tests for FNO/Hybrid generator components including SpatialLifter, PtychoBlock, HybridUNOGenerator, CascadedFNOGenerator, and generator registry integration. | `test_lifter_preserves_spatial_dims`, `test_block_preserves_dims`, `test_output_shape`, `test_resolve_fno_generator`, `test_resolve_hybrid_generator` | `pytest tests/torch/test_fno_generators.py -vv` | Core FNO architecture unit tests. |
| `test_fno_integration.py` | Integration tests for FNO/Hybrid end-to-end training and inference. Tests synthetic NPZ fixture, forward pass verification, neuraloperator integration flag, and training loop functionality. | `test_synthetic_npz_fixture_contract`, `test_fno_generator_forward_pass`, `test_hybrid_generator_forward_pass`, `test_neuraloperator_spectral_conv_available`, `test_fno_generator_training_loop` | `pytest tests/torch/test_fno_integration.py -vv` | FNO integration tests. Skips neuraloperator test if package not installed. |
| `test_fno_lightning_integration.py` | Integration tests for Lightning training with loss history tracking. Tests _LossHistoryCallback and full training pipeline history collection. | `test_loss_history_callback_collects_per_epoch`, `test_loss_history_callback_handles_missing_metrics`, `test_train_history_collects_epochs` | `pytest tests/torch/test_fno_lightning_integration.py -vv` | Lightning callback tests. `test_train_history_collects_epochs` is marked slow. |
| `test_fno_reconstruction_quality.py` | Quality comparison tests for FNO/Hybrid vs CNN generators. Tests output validity, architecture differences, training improvement, and input size handling. | `test_fno_quality_comparable_to_random_baseline`, `test_hybrid_produces_different_output_than_fno`, `test_generator_output_improves_with_training`, `test_generators_handle_different_input_sizes` | `pytest tests/torch/test_fno_reconstruction_quality.py -vv` | FNO quality/validation tests. All marked slow. |
| `test_loss_units.py` | Unit tests enforcing amplitude/intensity contracts for Poisson and MAE losses. | `test_poisson_intensity_layer_squares_observations`, `test_mae_loss_operates_on_amplitude` | `pytest tests/torch/test_loss_units.py -vv` | Guards loss unit regressions in PyTorch. |
| `test_tf_helper.py` | No module docstring found. | `test_combine_complex`, `test_get_mask`, `test_placeholder_torch_functions` | `python -m unittest tests.torch.test_tf_helper` | — |

---
*This document can be automatically updated. Run ``python scripts/tools/generate_test_index.py`` to regenerate.*
