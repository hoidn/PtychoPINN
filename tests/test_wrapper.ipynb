{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 14:46:17.506646: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-03 14:46:17.506805: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-03 14:46:17.526375: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-03 14:46:17.588541: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-03 14:46:18.873221: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/root/miniconda3/envs/ptychopinn_ptychodus_pytorch/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from test_pytorch_tf_wrapper import debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define random 4D numpy array\n",
    "np_array = np.random.rand(2,3,4,4)\n",
    "\n",
    "\n",
    "#Tensorflow tensor\n",
    "tf_tensor = tf.convert_to_tensor(np_array)\n",
    "#Move second dimension to end of tf_tensor\n",
    "tf_tensor = tf.transpose(tf_tensor, perm=[0,2,3,1])\n",
    "#Pytorch tensor from np_array\n",
    "pt_tensor = torch.from_numpy(np_array)\n",
    "\n",
    "#Define simple test functions and run them in sequence\n",
    "\n",
    "#Tensorflow\n",
    "@debug\n",
    "def test_tf_relu(tensor):\n",
    "    #Check if tensor is instance of tensorflow\n",
    "    if isinstance(tensor, tf.Tensor):\n",
    "        return tf.nn.relu(tensor)\n",
    "    else:\n",
    "        return None\n",
    "#Pytorch\n",
    "@debug\n",
    "def test_pt_relu(tensor):\n",
    "    if isinstance(tensor, torch.Tensor):\n",
    "        # return torch.nn.functional.log_softmax(tensor) #This will throw an error\n",
    "        return torch.nn.functional.relu(tensor) #This should make tensors self consistent (equivalent relu\n",
    "    else:\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'test_tf_relu' called with args (<tf.Tensor: shape=(2, 4, 4, 3), dtype=float64, numpy=\n",
      "array([[[[0.02727996, 0.74935995, 0.3692678 ],\n",
      "         [0.93183584, 0.41277401, 0.42904819],\n",
      "         [0.80817074, 0.62759374, 0.9335988 ],\n",
      "         [0.14014139, 0.08970998, 0.9986459 ]],\n",
      "\n",
      "        [[0.39768923, 0.09388694, 0.50113571],\n",
      "         [0.65529919, 0.77202963, 0.36178317],\n",
      "         [0.99238636, 0.60772949, 0.14501327],\n",
      "         [0.33572844, 0.36886858, 0.99924363]],\n",
      "\n",
      "        [[0.03449322, 0.03107256, 0.45341695],\n",
      "         [0.88979372, 0.53874732, 0.32420513],\n",
      "         [0.86731281, 0.0359714 , 0.93107651],\n",
      "         [0.51784269, 0.41799242, 0.72397077]],\n",
      "\n",
      "        [[0.37276547, 0.65548798, 0.51203239],\n",
      "         [0.08517467, 0.05050482, 0.08441395],\n",
      "         [0.76487357, 0.24146425, 0.64161029],\n",
      "         [0.95285114, 0.81344323, 0.76008956]]],\n",
      "\n",
      "\n",
      "       [[[0.0800115 , 0.50126908, 0.46001906],\n",
      "         [0.21987174, 0.80481368, 0.82800734],\n",
      "         [0.76476917, 0.40671079, 0.36835298],\n",
      "         [0.66901303, 0.2749788 , 0.32523124]],\n",
      "\n",
      "        [[0.72148769, 0.38067094, 0.13157792],\n",
      "         [0.92349191, 0.46770485, 0.84284305],\n",
      "         [0.26336238, 0.72010331, 0.60679746],\n",
      "         [0.70968321, 0.30305411, 0.28646433]],\n",
      "\n",
      "        [[0.86102918, 0.19233046, 0.47399769],\n",
      "         [0.49305904, 0.98750742, 0.74222302],\n",
      "         [0.71172429, 0.47521144, 0.82497157],\n",
      "         [0.20416908, 0.52678716, 0.44682648]],\n",
      "\n",
      "        [[0.07134271, 0.42145744, 0.57137703],\n",
      "         [0.51670351, 0.74934809, 0.79650922],\n",
      "         [0.13088839, 0.20887163, 0.81129485],\n",
      "         [0.73067258, 0.27406806, 0.80668126]]]])>,) and kwargs {}\n",
      "Function 'test_pt_relu' called with args (tensor([[[[0.0273, 0.9318, 0.8082, 0.1401],\n",
      "          [0.3977, 0.6553, 0.9924, 0.3357],\n",
      "          [0.0345, 0.8898, 0.8673, 0.5178],\n",
      "          [0.3728, 0.0852, 0.7649, 0.9529]],\n",
      "\n",
      "         [[0.7494, 0.4128, 0.6276, 0.0897],\n",
      "          [0.0939, 0.7720, 0.6077, 0.3689],\n",
      "          [0.0311, 0.5387, 0.0360, 0.4180],\n",
      "          [0.6555, 0.0505, 0.2415, 0.8134]],\n",
      "\n",
      "         [[0.3693, 0.4290, 0.9336, 0.9986],\n",
      "          [0.5011, 0.3618, 0.1450, 0.9992],\n",
      "          [0.4534, 0.3242, 0.9311, 0.7240],\n",
      "          [0.5120, 0.0844, 0.6416, 0.7601]]],\n",
      "\n",
      "\n",
      "        [[[0.0800, 0.2199, 0.7648, 0.6690],\n",
      "          [0.7215, 0.9235, 0.2634, 0.7097],\n",
      "          [0.8610, 0.4931, 0.7117, 0.2042],\n",
      "          [0.0713, 0.5167, 0.1309, 0.7307]],\n",
      "\n",
      "         [[0.5013, 0.8048, 0.4067, 0.2750],\n",
      "          [0.3807, 0.4677, 0.7201, 0.3031],\n",
      "          [0.1923, 0.9875, 0.4752, 0.5268],\n",
      "          [0.4215, 0.7493, 0.2089, 0.2741]],\n",
      "\n",
      "         [[0.4600, 0.8280, 0.3684, 0.3252],\n",
      "          [0.1316, 0.8428, 0.6068, 0.2865],\n",
      "          [0.4740, 0.7422, 0.8250, 0.4468],\n",
      "          [0.5714, 0.7965, 0.8113, 0.8067]]]], dtype=torch.float64),) and kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_103860/2624005.py:25: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.nn.functional.log_softmax(tensor)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Output mismatch for function '{func.__name__}' with id '{func_id}'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#Pytorch tensor\u001b[39;00m\n\u001b[1;32m     10\u001b[0m pt_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(tensor)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtest_pt_relu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpt_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/av_linux/PtychoPINN/tests/test_pytorch_tf_wrapper.py:92\u001b[0m, in \u001b[0;36minvocation_wrapper.<locals>.wrapper.<locals>.inner_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m#If function calls are above 2, do not save function outputs (simply pass func through)\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m invocations\u001b[38;5;241m.\u001b[39mcounts[f] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mouter_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/home/av_linux/PtychoPINN/tests/test_pytorch_tf_wrapper.py:121\u001b[0m, in \u001b[0;36mdebug.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m#Check if output is consistent\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(output\u001b[38;5;241m.\u001b[39mnumpy(), existing_output):\n\u001b[0;32m--> 121\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput mismatch for function \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{func.__name__}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with id \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{func_id}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput for function \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with id \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is consistent\u001b[39m\u001b[38;5;124m\"\u001b[39m)   \n",
      "\u001b[0;31mValueError\u001b[0m: Output mismatch for function '{func.__name__}' with id '{func_id}'"
     ]
    }
   ],
   "source": [
    "#Testing 10 iterations of tf tensors\n",
    "for i in range(10):\n",
    "    tensor = np.random.rand(2,3,4,4)\n",
    "    #TensorFlow Tensor\n",
    "    tf_tensor = tf.convert_to_tensor(tensor)\n",
    "    #Move second dimension to end of tf_tensor\n",
    "    tf_tensor = tf.transpose(tf_tensor, perm=[0,2,3,1])\n",
    "    test_tf_relu(tf_tensor)\n",
    "    #Pytorch tensor\n",
    "    pt_tensor = torch.from_numpy(tensor)\n",
    "    test_pt_relu(pt_tensor)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptychopinn_ptychodus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
