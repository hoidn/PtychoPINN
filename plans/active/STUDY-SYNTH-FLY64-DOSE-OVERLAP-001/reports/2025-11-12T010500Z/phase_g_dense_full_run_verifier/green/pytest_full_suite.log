============================= test session starts ==============================
platform linux -- Python 3.11.13, pytest-8.4.1, pluggy-1.6.0 -- /home/ollie/miniconda3/envs/ptycho311/bin/python3.11
cachedir: .pytest_cache
PyTorch: available
PyTorch version: 2.8.0+cu128
rootdir: /home/ollie/Documents/PtychoPINN
configfile: pyproject.toml
plugins: anyio-4.9.0
collecting ... collected 513 items / 2 skipped

tests/image/test_cropping.py::TestCroppingAlignment::test_align_for_evaluation_bounding_box PASSED [  0%]
tests/image/test_cropping.py::TestCroppingAlignment::test_align_for_evaluation_coordinates_format PASSED [  0%]
tests/image/test_cropping.py::TestCroppingAlignment::test_align_for_evaluation_shapes PASSED [  0%]
tests/image/test_cropping.py::TestCroppingAlignment::test_align_for_evaluation_with_squeeze PASSED [  0%]
tests/image/test_cropping.py::TestCroppingAlignment::test_center_crop_exact_size PASSED [  0%]
tests/image/test_registration.py::TestRegistration::test_apply_shift_and_crop_basic PASSED [  1%]
tests/image/test_registration.py::TestRegistration::test_apply_shift_and_crop_zero_offset PASSED [  1%]
tests/image/test_registration.py::TestRegistration::test_different_image_content PASSED [  1%]
tests/image/test_registration.py::TestRegistration::test_edge_case_maximum_shift PASSED [  1%]
tests/image/test_registration.py::TestRegistration::test_edge_case_single_pixel_shift PASSED [  1%]
tests/image/test_registration.py::TestRegistration::test_find_offset_known_shift_complex PASSED [  2%]
tests/image/test_registration.py::TestRegistration::test_find_offset_known_shift_real PASSED [  2%]
tests/image/test_registration.py::TestRegistration::test_input_validation_2d_requirement PASSED [  2%]
tests/image/test_registration.py::TestRegistration::test_input_validation_excessive_offset PASSED [  2%]
tests/image/test_registration.py::TestRegistration::test_input_validation_shape_matching PASSED [  2%]
tests/image/test_registration.py::TestRegistration::test_noise_robustness PASSED [  3%]
tests/image/test_registration.py::TestRegistration::test_register_and_align_convenience PASSED [  3%]
tests/image/test_registration.py::TestRegistration::test_registration_sign_verification PASSED [  3%]
tests/image/test_registration.py::TestRegistration::test_round_trip_registration PASSED [  3%]
tests/image/test_registration.py::TestRegistration::test_shift_and_crop_preserves_data_type PASSED [  3%]
tests/io/test_ptychodus_interop_h5.py::test_interop_h5_reader SKIPPED    [  4%]
tests/io/test_ptychodus_product_io.py::test_export_writes_minimal_hdf5 PASSED [  4%]
tests/io/test_ptychodus_product_io.py::test_import_reads_hdf5_to_rawdata PASSED [  4%]
tests/io/test_ptychodus_product_io.py::test_cli_convert_run1084_smoke PASSED [  4%]
tests/scripts/test_inference_backend_selector.py::TestInferenceCliBackendDispatch::test_pytorch_backend_dispatch PASSED [  4%]
tests/scripts/test_inference_backend_selector.py::TestInferenceCliBackendDispatch::test_tensorflow_backend_dispatch PASSED [  5%]
tests/scripts/test_inference_backend_selector.py::TestInferenceCliBackendDispatch::test_backend_selector_preserves_config_001_compliance PASSED [  5%]
tests/scripts/test_inference_backend_selector.py::TestInferenceCliBackendDispatch::test_cli_backend_argument_parsing PASSED [  5%]
tests/scripts/test_inference_backend_selector.py::TestInferenceCliBackendDispatch::test_setup_inference_configuration_uses_backend PASSED [  5%]
tests/scripts/test_inference_backend_selector.py::TestInferenceCliBackendDispatch::test_pytorch_inference_execution_path PASSED [  5%]
tests/scripts/test_inference_backend_selector.py::TestInferenceCliBackendDispatch::test_pytorch_execution_config_flags PASSED [  6%]
tests/scripts/test_inference_backend_selector.py::TestInferenceCliBackendDispatch::test_pytorch_backend_moves_model_to_execution_device PASSED [  6%]
tests/scripts/test_inference_backend_selector.py::TestInferenceCliBackendDispatch::test_pytorch_backend_defaults_auto_execution_config PASSED [  6%]
tests/scripts/test_ptychi_reconstruct_tike.py::test_main_uses_cli_arguments PASSED [  6%]
tests/scripts/test_training_backend_selector.py::TestTrainingCliBackendDispatch::test_pytorch_backend_dispatch PASSED [  6%]
tests/scripts/test_training_backend_selector.py::TestTrainingCliBackendDispatch::test_tensorflow_backend_persistence PASSED [  7%]
tests/scripts/test_training_backend_selector.py::TestTrainingCliBackendDispatch::test_pytorch_execution_config_flags PASSED [  7%]
tests/scripts/test_training_backend_selector.py::TestTrainingCliBackendDispatch::test_supervised_mode_enforces_mae_loss PASSED [  7%]
tests/scripts/test_training_backend_selector.py::TestTrainingCliBackendDispatch::test_manual_accumulation_guard PASSED [  7%]
tests/scripts/test_training_backend_selector.py::TestTrainingCliBackendDispatch::test_pytorch_backend_defaults_auto_execution_config PASSED [  7%]
tests/study/test_check_dense_highlights_match.py::test_summary_mismatch_fails PASSED [  7%]
tests/study/test_check_dense_highlights_match.py::test_summary_matches_json PASSED [  8%]
tests/study/test_check_dense_highlights_match.py::test_missing_phase_only_metadata_fails PASSED [  8%]
tests/study/test_dose_overlap_comparison.py::test_build_comparison_jobs_creates_all_conditions PASSED [  8%]
tests/study/test_dose_overlap_comparison.py::test_execute_comparison_jobs_invokes_compare_models PASSED [  8%]
tests/study/test_dose_overlap_comparison.py::test_execute_comparison_jobs_records_summary PASSED [  8%]
tests/study/test_dose_overlap_comparison.py::test_build_comparison_jobs_uses_dose_specific_phase_e_paths PASSED [  9%]
tests/study/test_dose_overlap_comparison.py::test_execute_comparison_jobs_appends_tike_recon_path PASSED [  9%]
tests/study/test_dose_overlap_comparison.py::test_prepare_baseline_inference_data_grouped_flatten_helper PASSED [  9%]
tests/study/test_dose_overlap_comparison.py::test_baseline_model_predict_receives_both_inputs PASSED [  9%]
tests/study/test_dose_overlap_comparison.py::test_baseline_complex_output_converts_to_amplitude_phase PASSED [  9%]
tests/study/test_dose_overlap_comparison.py::test_pinn_reconstruction_reassembles_batched_predictions PASSED [ 10%]
tests/study/test_dose_overlap_dataset_contract.py::test_validate_dataset_contract_happy_path PASSED [ 10%]
tests/study/test_dose_overlap_dataset_contract.py::test_validate_dataset_contract_missing_key PASSED [ 10%]
tests/study/test_dose_overlap_dataset_contract.py::test_validate_dataset_contract_wrong_dtype_diffraction PASSED [ 10%]
tests/study/test_dose_overlap_dataset_contract.py::test_validate_dataset_contract_wrong_dtype_object PASSED [ 10%]
tests/study/test_dose_overlap_dataset_contract.py::test_validate_dataset_contract_shape_mismatch PASSED [ 11%]
tests/study/test_dose_overlap_dataset_contract.py::test_validate_dataset_contract_spacing_dense PASSED [ 11%]
tests/study/test_dose_overlap_dataset_contract.py::test_validate_dataset_contract_spacing_violation PASSED [ 11%]
tests/study/test_dose_overlap_dataset_contract.py::test_validate_dataset_contract_oversampling_precondition_pass PASSED [ 11%]
tests/study/test_dose_overlap_dataset_contract.py::test_validate_dataset_contract_oversampling_precondition_fail PASSED [ 11%]
tests/study/test_dose_overlap_dataset_contract.py::test_validate_dataset_contract_oversampling_missing_neighbor_count PASSED [ 12%]
tests/study/test_dose_overlap_dataset_contract.py::test_validate_dataset_contract_unknown_view PASSED [ 12%]
tests/study/test_dose_overlap_design.py::test_study_design_constants PASSED [ 12%]
tests/study/test_dose_overlap_design.py::test_study_design_validation PASSED [ 12%]
tests/study/test_dose_overlap_design.py::test_study_design_to_dict PASSED [ 12%]
tests/study/test_dose_overlap_generation.py::test_build_simulation_plan PASSED [ 13%]
tests/study/test_dose_overlap_generation.py::test_generate_dataset_pipeline_orchestration[1000.0] PASSED [ 13%]
tests/study/test_dose_overlap_generation.py::test_generate_dataset_pipeline_orchestration[10000.0] PASSED [ 13%]
tests/study/test_dose_overlap_generation.py::test_generate_dataset_pipeline_orchestration[100000.0] PASSED [ 13%]
tests/study/test_dose_overlap_generation.py::test_generate_dataset_config_construction PASSED [ 13%]
tests/study/test_dose_overlap_generation.py::test_generate_dataset_validates_with_real_contract PASSED [ 14%]
tests/study/test_dose_overlap_generation.py::test_build_simulation_plan_handles_metadata_pickle_guard PASSED [ 14%]
tests/study/test_dose_overlap_generation.py::test_load_data_for_sim_handles_metadata_pickle_guard PASSED [ 14%]
tests/study/test_dose_overlap_generation.py::test_generate_dataset_for_dose_handles_metadata_splits PASSED [ 14%]
tests/study/test_dose_overlap_overlap.py::test_disc_overlap_fraction_perfect_overlap PASSED [ 14%]
tests/study/test_dose_overlap_overlap.py::test_disc_overlap_fraction_half_diameter PASSED [ 15%]
tests/study/test_dose_overlap_overlap.py::test_disc_overlap_fraction_no_overlap PASSED [ 15%]
tests/study/test_dose_overlap_overlap.py::test_disc_overlap_area_symmetry PASSED [ 15%]
tests/study/test_dose_overlap_overlap.py::test_subsample_images_deterministic PASSED [ 15%]
tests/study/test_dose_overlap_overlap.py::test_filter_dataset_by_mask_handles_scalar_metadata PASSED [ 15%]
tests/study/test_dose_overlap_overlap.py::test_form_groups_gs1 PASSED    [ 15%]
tests/study/test_dose_overlap_overlap.py::test_form_groups_gs2 PASSED    [ 16%]
tests/study/test_dose_overlap_overlap.py::test_metric_1_group_based_synthetic PASSED [ 16%]
tests/study/test_dose_overlap_overlap.py::test_metric_2_image_based_deduplication PASSED [ 16%]
tests/study/test_dose_overlap_overlap.py::test_metric_2_image_based_single_image PASSED [ 16%]
tests/study/test_dose_overlap_overlap.py::test_metric_3_group_to_group_overlapping PASSED [ 16%]
tests/study/test_dose_overlap_overlap.py::test_metric_3_group_to_group_no_overlap PASSED [ 17%]
tests/study/test_dose_overlap_overlap.py::test_compute_overlap_metrics_gs1 PASSED [ 17%]
tests/study/test_dose_overlap_overlap.py::test_compute_overlap_metrics_gs2 PASSED [ 17%]
tests/study/test_dose_overlap_overlap.py::test_compute_overlap_metrics_degenerate_s_img PASSED [ 17%]
tests/study/test_dose_overlap_overlap.py::test_compute_overlap_metrics_invalid_gridsize PASSED [ 17%]
tests/study/test_dose_overlap_overlap.py::test_generate_overlap_views_basic PASSED [ 18%]
tests/study/test_dose_overlap_overlap.py::test_overlap_metrics_bundle PASSED [ 18%]
tests/study/test_dose_overlap_overlap.py::test_generate_overlap_views_dense_acceptance_floor PASSED [ 18%]
tests/study/test_dose_overlap_reconstruction.py::test_build_ptychi_jobs_manifest PASSED [ 18%]
tests/study/test_dose_overlap_reconstruction.py::test_run_ptychi_job_invokes_script PASSED [ 18%]
tests/study/test_dose_overlap_reconstruction.py::test_cli_filters_dry_run PASSED [ 19%]
tests/study/test_dose_overlap_reconstruction.py::test_cli_executes_selected_jobs PASSED [ 19%]
tests/study/test_dose_overlap_reconstruction.py::test_cli_skips_missing_phase_d PASSED [ 19%]
tests/study/test_dose_overlap_training.py::test_train_cdi_model_normalizes_history PASSED [ 19%]
tests/study/test_dose_overlap_training.py::test_build_training_jobs_matrix PASSED [ 19%]
tests/study/test_dose_overlap_training.py::test_run_training_job_invokes_runner PASSED [ 20%]
tests/study/test_dose_overlap_training.py::test_run_training_job_dry_run PASSED [ 20%]
tests/study/test_dose_overlap_training.py::test_execute_training_job_dispatches_tensorflow_by_default PASSED [ 20%]
tests/study/test_dose_overlap_training.py::test_execute_training_job_dispatches_pytorch_when_requested PASSED [ 20%]
tests/study/test_dose_overlap_training.py::test_execute_training_job_tensorflow_persists_bundle PASSED [ 20%]
tests/study/test_dose_overlap_training.py::test_training_cli_filters_jobs PASSED [ 21%]
tests/study/test_dose_overlap_training.py::test_training_cli_manifest_and_bridging PASSED [ 21%]
tests/study/test_dose_overlap_training.py::test_execute_training_job_delegates_to_pytorch_trainer PASSED [ 21%]
tests/study/test_dose_overlap_training.py::test_execute_training_job_persists_bundle PASSED [ 21%]
tests/study/test_dose_overlap_training.py::test_training_cli_invokes_real_runner PASSED [ 21%]
tests/study/test_dose_overlap_training.py::test_build_training_jobs_skips_missing_view PASSED [ 22%]
tests/study/test_dose_overlap_training.py::test_training_cli_records_bundle_path PASSED [ 22%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_artifact_inventory_blocks_missing_entries PASSED [ 22%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_artifact_inventory_passes_with_complete_bundle PASSED [ 22%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_cli_logs_missing PASSED [ 22%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_cli_logs_complete PASSED [ 23%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_cli_phase_logs_missing PASSED [ 23%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_cli_phase_logs_wrong_pattern PASSED [ 23%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_cli_phase_logs_incomplete PASSED [ 23%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_cli_phase_logs_complete PASSED [ 23%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_highlights_missing_model PASSED [ 23%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_highlights_mismatched_value PASSED [ 24%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_highlights_complete PASSED [ 24%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_highlights_missing_preview PASSED [ 24%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_highlights_preview_contains_amplitude PASSED [ 24%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_highlights_preview_mismatch PASSED [ 24%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_highlights_delta_mismatch PASSED [ 25%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_cli_logs_require_ssim_grid_log PASSED [ 25%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_requires_ssim_grid_summary PASSED [ 25%]
tests/study/test_phase_g_dense_metrics_report.py::test_report_phase_g_dense_metrics PASSED [ 25%]
tests/study/test_phase_g_dense_metrics_report.py::test_report_phase_g_dense_metrics_missing_model_fails PASSED [ 25%]
tests/study/test_phase_g_dense_metrics_report.py::test_analyze_dense_metrics_flags_failures PASSED [ 26%]
tests/study/test_phase_g_dense_metrics_report.py::test_analyze_dense_metrics_success_digest PASSED [ 26%]
tests/study/test_phase_g_dense_orchestrator.py::test_run_phase_g_dense_collect_only_generates_commands FAILED [ 26%]
tests/study/test_phase_g_dense_orchestrator.py::test_run_phase_g_dense_post_verify_hooks FAILED [ 26%]
tests/study/test_phase_g_dense_orchestrator.py::test_summarize_phase_g_outputs PASSED [ 26%]
tests/study/test_phase_g_dense_orchestrator.py::test_summarize_phase_g_outputs_fails_on_missing_manifest PASSED [ 27%]
tests/study/test_phase_g_dense_orchestrator.py::test_summarize_phase_g_outputs_fails_on_execution_failures PASSED [ 27%]
tests/study/test_phase_g_dense_orchestrator.py::test_summarize_phase_g_outputs_fails_on_missing_csv PASSED [ 27%]
tests/study/test_phase_g_dense_orchestrator.py::test_validate_phase_c_metadata_requires_metadata PASSED [ 27%]
tests/study/test_phase_g_dense_orchestrator.py::test_validate_phase_c_metadata_requires_canonical_transform PASSED [ 27%]
tests/study/test_phase_g_dense_orchestrator.py::test_validate_phase_c_metadata_accepts_valid_metadata PASSED [ 28%]
tests/study/test_phase_g_dense_orchestrator.py::test_validate_phase_c_metadata_handles_patched_layout PASSED [ 28%]
tests/study/test_phase_g_dense_orchestrator.py::test_prepare_hub_detects_stale_outputs PASSED [ 28%]
tests/study/test_phase_g_dense_orchestrator.py::test_prepare_hub_clobbers_previous_outputs PASSED [ 28%]
tests/study/test_phase_g_dense_orchestrator.py::test_run_phase_g_dense_exec_invokes_reporting_helper FAILED [ 28%]
tests/study/test_phase_g_dense_orchestrator.py::test_run_phase_g_dense_exec_prints_highlights_preview FAILED [ 29%]
tests/study/test_phase_g_dense_orchestrator.py::test_run_phase_g_dense_exec_runs_analyze_digest FAILED [ 29%]
tests/study/test_phase_g_dense_orchestrator.py::test_persist_delta_highlights_creates_preview PASSED [ 29%]
tests/study/test_phase_g_dense_orchestrator.py::test_run_phase_g_dense_collect_only_post_verify_only PASSED [ 29%]
tests/study/test_phase_g_dense_orchestrator.py::test_run_phase_g_dense_post_verify_only_executes_chain PASSED [ 29%]
tests/study/test_ssim_grid.py::test_smoke_ssim_grid PASSED               [ 30%]
tests/test_baselines.py::TestBaselines::test_build_model_always_creates_single_channel_output PASSED [ 30%]
tests/test_cli_args.py::TestCliArgs::test_add_logging_arguments PASSED   [ 30%]
tests/test_cli_args.py::TestCliArgs::test_console_level_choices PASSED   [ 30%]
tests/test_cli_args.py::TestCliArgs::test_get_logging_config_custom_level PASSED [ 30%]
tests/test_cli_args.py::TestCliArgs::test_get_logging_config_defaults PASSED [ 30%]
tests/test_cli_args.py::TestCliArgs::test_get_logging_config_quiet PASSED [ 31%]
tests/test_cli_args.py::TestCliArgs::test_get_logging_config_verbose PASSED [ 31%]
tests/test_cli_args.py::TestCliArgs::test_quiet_verbose_mutually_exclusive PASSED [ 31%]
tests/test_coordinate_grouping.py::TestCoordinateGrouping::test_backward_compatibility PASSED [ 31%]
tests/test_coordinate_grouping.py::TestCoordinateGrouping::test_different_seeds_produce_different_results PASSED [ 31%]
tests/test_coordinate_grouping.py::TestCoordinateGrouping::test_edge_case_k_less_than_c PASSED [ 32%]
tests/test_coordinate_grouping.py::TestCoordinateGrouping::test_edge_case_more_samples_than_points PASSED [ 32%]
tests/test_coordinate_grouping.py::TestCoordinateGrouping::test_edge_case_small_dataset PASSED [ 32%]
tests/test_coordinate_grouping.py::TestCoordinateGrouping::test_efficient_grouping_output_shape PASSED [ 32%]
tests/test_coordinate_grouping.py::TestCoordinateGrouping::test_efficient_grouping_spatial_coherence PASSED [ 32%]
tests/test_coordinate_grouping.py::TestCoordinateGrouping::test_efficient_grouping_valid_indices PASSED [ 33%]
tests/test_coordinate_grouping.py::TestCoordinateGrouping::test_generate_grouped_data_gridsize_1 PASSED [ 33%]
tests/test_coordinate_grouping.py::TestCoordinateGrouping::test_generate_grouped_data_integration PASSED [ 33%]
tests/test_coordinate_grouping.py::TestCoordinateGrouping::test_memory_efficiency PASSED [ 33%]
tests/test_coordinate_grouping.py::TestCoordinateGrouping::test_no_cache_files_created PASSED [ 33%]
tests/test_coordinate_grouping.py::TestCoordinateGrouping::test_performance_improvement PASSED [ 34%]
tests/test_coordinate_grouping.py::TestCoordinateGrouping::test_reproducibility_with_seed PASSED [ 34%]
tests/test_coordinate_grouping.py::TestIntegrationWithExistingCode::test_existing_tests_still_pass PASSED [ 34%]
tests/test_generic_loader.py::TestGenericLoader::test_generic_loader_roundtrip SKIPPED [ 34%]
tests/test_generic_loader.py::test_generic_loader PASSED                 [ 34%]
tests/test_integration_baseline_gs2.py::TestBaselineGridsize2Integration::test_baseline_gridsize2_end_to_end PASSED [ 35%]
tests/test_integration_workflow.py::TestFullWorkflow::test_train_save_load_infer_cycle PASSED [ 35%]
tests/test_log_config.py::TestLogConfig::test_backward_compatibility PASSED [ 35%]
tests/test_log_config.py::TestLogConfig::test_conflicting_flags_verbose_overrides PASSED [ 35%]
tests/test_log_config.py::TestLogConfig::test_custom_console_level PASSED [ 35%]
tests/test_log_config.py::TestLogConfig::test_default_setup_logging_creates_log_directory_and_file PASSED [ 36%]
tests/test_log_config.py::TestLogConfig::test_quiet_flag_overrides_console_level PASSED [ 36%]
tests/test_log_config.py::TestLogConfig::test_quiet_mode_disables_console PASSED [ 36%]
tests/test_log_config.py::TestLogConfig::test_setup_logging_clears_existing_handlers PASSED [ 36%]
tests/test_log_config.py::TestLogConfig::test_string_path_support PASSED [ 36%]
tests/test_log_config.py::TestLogConfig::test_verbose_mode_enables_debug_console PASSED [ 37%]
tests/test_misc.py::test_memoize_simulated_data SKIPPED (Deprecated:...) [ 37%]
tests/test_model_channel_consistency.py::test_amp_head_matches_patch_channels PASSED [ 37%]
tests/test_model_channel_consistency.py::test_diffraction_to_obj_accepts_grouped_inputs PASSED [ 37%]
tests/test_nphotons_metadata_integration.py::TestNphotonsMetadataIntegration::test_configuration_mismatch_warnings PASSED [ 37%]
tests/test_nphotons_metadata_integration.py::TestNphotonsMetadataIntegration::test_end_to_end_workflow_consistency PASSED [ 38%]
tests/test_nphotons_metadata_integration.py::TestNphotonsMetadataIntegration::test_metadata_backward_compatibility PASSED [ 38%]
tests/test_nphotons_metadata_integration.py::TestNphotonsMetadataIntegration::test_metadata_persistence_single_nphotons PASSED [ 38%]
tests/test_nphotons_metadata_integration.py::TestNphotonsMetadataIntegration::test_multiple_nphotons_metadata_consistency PASSED [ 38%]
tests/test_nphotons_metadata_integration.py::TestNphotonsMetadataIntegration::test_training_with_mismatched_config_warns_but_continues PASSED [ 38%]
tests/test_oversampling.py::TestAutomaticOversampling::test_automatic_oversampling_triggers PASSED [ 38%]
tests/test_oversampling.py::TestAutomaticOversampling::test_enable_oversampling_flag_required PASSED [ 39%]
tests/test_oversampling.py::TestAutomaticOversampling::test_gridsize_1_no_oversampling PASSED [ 39%]
tests/test_oversampling.py::TestAutomaticOversampling::test_neighbor_pool_size_guard PASSED [ 39%]
tests/test_oversampling.py::TestAutomaticOversampling::test_oversampling_with_different_k_values PASSED [ 39%]
tests/test_oversampling.py::TestAutomaticOversampling::test_reproducibility_with_seed PASSED [ 39%]
tests/test_oversampling.py::TestAutomaticOversampling::test_standard_sampling_no_oversampling PASSED [ 40%]
tests/test_projective_warp_xla.py::TestProjectiveWarpXLA::test_batch_processing PASSED [ 40%]
tests/test_projective_warp_xla.py::TestProjectiveWarpXLA::test_complex128_dtype PASSED [ 40%]
tests/test_projective_warp_xla.py::TestProjectiveWarpXLA::test_complex64_dtype PASSED [ 40%]
tests/test_projective_warp_xla.py::TestProjectiveWarpXLA::test_fill_modes PASSED [ 40%]
tests/test_projective_warp_xla.py::TestProjectiveWarpXLA::test_float32_dtype PASSED [ 41%]
tests/test_projective_warp_xla.py::TestProjectiveWarpXLA::test_float64_dtype PASSED [ 41%]
tests/test_projective_warp_xla.py::TestProjectiveWarpXLA::test_float64_with_translation PASSED [ 41%]
tests/test_projective_warp_xla.py::TestProjectiveWarpXLA::test_interpolation_modes PASSED [ 41%]
tests/test_projective_warp_xla.py::TestProjectiveWarpXLA::test_jit_compilation_float32 PASSED [ 41%]
tests/test_projective_warp_xla.py::TestProjectiveWarpXLA::test_jit_compilation_float64 PASSED [ 42%]
tests/test_projective_warp_xla.py::TestProjectiveWarpXLA::test_mixed_precision_translation PASSED [ 42%]
tests/test_projective_warp_xla.py::TestProjectiveWarpXLA::test_tfa_params_conversion PASSED [ 42%]
tests/test_raw_data_grouping.py::TestRawDataGrouping::test_content_validity PASSED [ 42%]
tests/test_raw_data_grouping.py::TestRawDataGrouping::test_edge_case_k_less_than_c PASSED [ 42%]
tests/test_raw_data_grouping.py::TestRawDataGrouping::test_edge_case_more_samples_than_points PASSED [ 43%]
tests/test_raw_data_grouping.py::TestRawDataGrouping::test_edge_case_small_dataset PASSED [ 43%]
tests/test_raw_data_grouping.py::TestRawDataGrouping::test_memory_efficiency PASSED [ 43%]
tests/test_raw_data_grouping.py::TestRawDataGrouping::test_output_shape PASSED [ 43%]
tests/test_raw_data_grouping.py::TestRawDataGrouping::test_performance_improvement PASSED [ 43%]
tests/test_raw_data_grouping.py::TestRawDataGrouping::test_reproducibility PASSED [ 44%]
tests/test_raw_data_grouping.py::TestRawDataGrouping::test_uniform_sampling PASSED [ 44%]
tests/test_scaling_regression.py::TestScalingRegression::test_both_arrays_scaled_identically PASSED [ 44%]
tests/test_scaling_regression.py::TestScalingRegression::test_different_nphotons_produce_proportional_scaling PASSED [ 44%]
tests/test_scaling_regression.py::TestScalingRegression::test_intensity_scale_is_valid PASSED [ 44%]
tests/test_scaling_regression.py::TestScalingRegression::test_phase_is_not_scaled PASSED [ 45%]
tests/test_scaling_regression.py::TestScalingRegression::test_scaling_is_reversible PASSED [ 45%]
tests/test_scaling_regression.py::TestScalingRegression::test_scaling_preserves_physics PASSED [ 45%]
tests/test_scaling_regression.py::TestScalingAssertions::test_assertions_catch_invalid_intensity_scale PASSED [ 45%]
tests/test_sequential_sampling.py::TestSequentialSampling::test_default_behavior_is_random PASSED [ 45%]
tests/test_sequential_sampling.py::TestSequentialSampling::test_sequential_sampling_handles_edge_cases PASSED [ 46%]
tests/test_sequential_sampling.py::TestSequentialSampling::test_sequential_sampling_is_deterministic PASSED [ 46%]
tests/test_sequential_sampling.py::TestSequentialSampling::test_sequential_sampling_order PASSED [ 46%]
tests/test_sequential_sampling.py::TestSequentialSampling::test_sequential_sampling_uses_first_n_points PASSED [ 46%]
tests/test_sequential_sampling.py::TestSequentialSampling::test_sequential_sampling_with_gridsize_greater_than_1 PASSED [ 46%]
tests/test_sequential_sampling.py::TestSequentialSampling::test_sequential_sampling_with_seed_parameter PASSED [ 46%]
tests/test_sequential_sampling.py::TestSequentialSampling::test_sequential_vs_random_coverage PASSED [ 47%]
tests/test_sequential_sampling.py::TestIntegrationWithWorkflow::test_cli_argument_parsing PASSED [ 47%]
tests/test_sequential_sampling.py::TestIntegrationWithWorkflow::test_config_flag_exists PASSED [ 47%]
tests/test_subsampling.py::TestSubsampling::test_different_seeds_produce_different_results PASSED [ 47%]
tests/test_subsampling.py::TestSubsampling::test_interaction_with_config_dataclass PASSED [ 47%]
tests/test_subsampling.py::TestSubsampling::test_legacy_n_images_behavior PASSED [ 48%]
tests/test_subsampling.py::TestSubsampling::test_n_subsample_overrides_n_images PASSED [ 48%]
tests/test_subsampling.py::TestSubsampling::test_no_subsample_uses_full_dataset PASSED [ 48%]
tests/test_subsampling.py::TestSubsampling::test_reproducible_subsampling_with_seed PASSED [ 48%]
tests/test_subsampling.py::TestSubsampling::test_sorted_indices_for_consistency PASSED [ 48%]
tests/test_subsampling.py::TestSubsampling::test_subsample_larger_than_dataset PASSED [ 49%]
tests/test_subsampling.py::TestSubsampling::test_subsample_with_n_subsample PASSED [ 49%]
tests/test_subsampling.py::TestSubsampling::test_subsample_zero_edge_case PASSED [ 49%]
tests/test_subsampling.py::TestSubsampling::test_y_patches_subsampled_consistently PASSED [ 49%]
tests/test_tf_helper.py::TestReassemblePosition::test_basic_functionality PASSED [ 49%]
tests/test_tf_helper.py::TestReassemblePosition::test_different_patch_values_blend PASSED [ 50%]
tests/test_tf_helper.py::TestReassemblePosition::test_identical_patches_single_vs_double PASSED [ 50%]
tests/test_tf_helper.py::TestReassemblePosition::test_perfect_overlap_averages_to_identity PASSED [ 50%]
tests/test_tf_helper.py::TestTranslateFunction::test_batch_translation PASSED [ 50%]
tests/test_tf_helper.py::TestTranslateFunction::test_complex_tensor_translation PASSED [ 50%]
tests/test_tf_helper.py::TestTranslateFunction::test_edge_cases PASSED   [ 51%]
tests/test_tf_helper.py::TestTranslateFunction::test_integer_translation PASSED [ 51%]
tests/test_tf_helper.py::TestTranslateFunction::test_subpixel_translation PASSED [ 51%]
tests/test_tf_helper.py::TestTranslateFunction::test_translate_core_matches_addons SKIPPED [ 51%]
tests/test_tf_helper.py::TestTranslateFunction::test_zero_translation PASSED [ 51%]
tests/test_tf_helper_edge_aware.py::TestTranslateSmoothPatterns::test_complex_smooth_translation PASSED [ 52%]
tests/test_tf_helper_edge_aware.py::TestTranslateSmoothPatterns::test_gaussian_probe_translation SKIPPED [ 52%]
tests/test_tf_helper_edge_aware.py::TestTranslateSmoothPatterns::test_smooth_object_translation SKIPPED [ 52%]
tests/test_tf_helper_edge_aware.py::TestTranslateEdgeCases::test_boundary_behavior SKIPPED [ 52%]
tests/test_tf_helper_edge_aware.py::TestTranslateEdgeCases::test_document_edge_differences SKIPPED [ 52%]
tests/test_tf_helper_edge_aware.py::TestPtychoPINNRelevantCases::test_batch_smooth_patterns SKIPPED [ 53%]
tests/test_tf_helper_edge_aware.py::TestPtychoPINNRelevantCases::test_typical_probe_sizes SKIPPED [ 53%]
tests/test_workflow_components.py::TestLoadInferenceBundle::test_exception_propagation PASSED [ 53%]
tests/test_workflow_components.py::TestLoadInferenceBundle::test_load_valid_model_directory FAILED [ 53%]
tests/test_workflow_components.py::TestLoadInferenceBundle::test_missing_diffraction_model PASSED [ 53%]
tests/test_workflow_components.py::TestLoadInferenceBundle::test_missing_model_archive PASSED [ 53%]
tests/test_workflow_components.py::TestLoadInferenceBundle::test_nonexistent_directory PASSED [ 54%]
tests/test_workflow_components.py::TestLoadInferenceBundle::test_not_a_directory PASSED [ 54%]
tests/test_workflow_components.py::TestLoadInferenceBundle::test_path_conversion PASSED [ 54%]
tests/tools/test_generate_patches_tool.py::test_generate_patches_preserves_metadata PASSED [ 54%]
tests/tools/test_generate_patches_tool.py::test_generate_patches_without_metadata PASSED [ 54%]
tests/tools/test_generate_test_index.py::GenerateTestIndexTests::test_get_module_docstring_handles_missing_docstring PASSED [ 55%]
tests/tools/test_generate_test_index.py::GenerateTestIndexTests::test_get_module_docstring_reads_existing_docstring PASSED [ 55%]
tests/tools/test_generate_test_index.py::GenerateTestIndexTests::test_get_test_functions_lists_key_tests PASSED [ 55%]
tests/tools/test_tail_interleave_logs.py::test_interleave_summaries PASSED [ 55%]
tests/tools/test_transpose_rename_convert_tool.py::test_canonicalize_preserves_metadata PASSED [ 55%]
tests/tools/test_transpose_rename_convert_tool.py::test_canonicalize_without_metadata PASSED [ 56%]
tests/tools/test_update_tool.py::TestUpdateTool::test_update_function PASSED [ 56%]
tests/tools/test_update_tool.py::test_update_function PASSED             [ 56%]
tests/torch/test_api_deprecation.py::TestLegacyAPIDeprecation::test_example_train_import_emits_deprecation_warning PASSED [ 56%]
tests/torch/test_api_deprecation.py::TestLegacyAPIDeprecation::test_api_package_import_is_idempotent PASSED [ 56%]
tests/torch/test_backend_selection.py::TestBackendSelection::test_defaults_to_tensorflow_backend PASSED [ 57%]
tests/torch/test_backend_selection.py::TestBackendSelection::test_selects_pytorch_backend PASSED [ 57%]
tests/torch/test_backend_selection.py::TestBackendSelection::test_pytorch_backend_calls_update_legacy_dict PASSED [ 57%]
tests/torch/test_backend_selection.py::TestBackendSelection::test_pytorch_unavailable_raises_error PASSED [ 57%]
tests/torch/test_backend_selection.py::TestBackendSelection::test_inference_config_supports_backend_selection PASSED [ 57%]
tests/torch/test_backend_selection.py::TestBackendSelection::test_backend_selection_preserves_api_parity PASSED [ 58%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLI::test_accelerator_flag_roundtrip PASSED [ 58%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLI::test_num_workers_flag_roundtrip PASSED [ 58%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLI::test_inference_batch_size_flag_roundtrip PASSED [ 58%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLI::test_multiple_execution_config_flags PASSED [ 58%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLIThinWrapper::test_cli_delegates_to_validate_paths PASSED [ 59%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLIThinWrapper::test_cli_delegates_to_helper_for_data_loading PASSED [ 59%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLIThinWrapper::test_cli_delegates_to_inference_helper PASSED [ 59%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLIThinWrapper::test_cli_calls_save_individual_reconstructions PASSED [ 59%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLIThinWrapper::test_quiet_flag_suppresses_progress_output PASSED [ 59%]
tests/torch/test_cli_shared.py::TestResolveAccelerator::test_default_no_device PASSED [ 60%]
tests/torch/test_cli_shared.py::TestResolveAccelerator::test_legacy_device_cpu PASSED [ 60%]
tests/torch/test_cli_shared.py::TestResolveAccelerator::test_legacy_device_cuda_maps_to_gpu PASSED [ 60%]
tests/torch/test_cli_shared.py::TestResolveAccelerator::test_conflict_accelerator_wins PASSED [ 60%]
tests/torch/test_cli_shared.py::TestResolveAccelerator::test_all_accelerator_values_passthrough PASSED [ 60%]
tests/torch/test_cli_shared.py::TestResolveAccelerator::test_resolve_accelerator_auto_defaults PASSED [ 61%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfig::test_training_mode_defaults PASSED [ 61%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfig::test_training_mode_custom_values PASSED [ 61%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfig::test_inference_mode PASSED [ 61%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfig::test_emits_deterministic_warning PASSED [ 61%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfig::test_handles_quiet_flag PASSED [ 61%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfig::test_handles_disable_mlflow_flag PASSED [ 62%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfig::test_quiet_or_disable_mlflow_both_true PASSED [ 62%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfig::test_invalid_mode_raises_value_error PASSED [ 62%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfig::test_resolves_accelerator_from_device_flag PASSED [ 62%]
tests/torch/test_cli_shared.py::TestValidatePaths::test_creates_output_dir PASSED [ 62%]
tests/torch/test_cli_shared.py::TestValidatePaths::test_raises_if_train_file_missing PASSED [ 63%]
tests/torch/test_cli_shared.py::TestValidatePaths::test_raises_if_test_file_missing PASSED [ 63%]
tests/torch/test_cli_shared.py::TestValidatePaths::test_accepts_none_test_file PASSED [ 63%]
tests/torch/test_cli_shared.py::TestValidatePaths::test_works_with_pathlib_path_objects PASSED [ 63%]
tests/torch/test_cli_shared.py::TestValidatePaths::test_accepts_none_train_file_for_inference_mode PASSED [ 63%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfigInferenceMode::test_inference_mode_defaults PASSED [ 64%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfigInferenceMode::test_inference_mode_custom_batch_size PASSED [ 64%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfigInferenceMode::test_inference_mode_respects_quiet PASSED [ 64%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_accelerator_flag_roundtrip PASSED [ 64%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_deterministic_flag_roundtrip PASSED [ 64%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_no_deterministic_flag_roundtrip PASSED [ 65%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_num_workers_flag_roundtrip PASSED [ 65%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_learning_rate_flag_roundtrip PASSED [ 65%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_multiple_execution_config_flags PASSED [ 65%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_bundle_persistence PASSED [ 65%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_enable_checkpointing_flag PASSED [ 66%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_checkpoint_save_top_k_flag PASSED [ 66%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_checkpoint_monitor_flag PASSED [ 66%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_checkpoint_mode_flag PASSED [ 66%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_early_stop_patience_flag PASSED [ 66%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_scheduler_flag_roundtrip PASSED [ 67%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_accumulate_grad_batches_roundtrip PASSED [ 67%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_logger_backend_csv_default PASSED [ 67%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_logger_backend_tensorboard PASSED [ 67%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_logger_backend_none PASSED [ 67%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_disable_mlflow_deprecation_warning PASSED [ 68%]
tests/torch/test_config_bridge.py::TestConfigBridgeMVP::test_mvp_config_bridge_populates_params_cfg PASSED [ 68%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_direct_fields[N-direct] PASSED [ 68%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_direct_fields[n_filters_scale-direct] PASSED [ 68%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_direct_fields[object_big-direct] PASSED [ 68%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_direct_fields[probe_big-direct] PASSED [ 69%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_direct_fields[batch_size-direct] PASSED [ 69%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_transform_fields[gridsize-tuple-to-int] PASSED [ 69%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_transform_fields[model_type-unsupervised] PASSED [ 69%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_transform_fields[model_type-supervised] PASSED [ 69%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_transform_fields[amp_activation-silu] PASSED [ 69%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_transform_fields[amp_activation-SiLU] PASSED [ 70%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_transform_fields[amp_activation-passthrough] PASSED [ 70%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_transform_fields[nepochs-rename] PASSED [ 70%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_transform_fields[nll_weight-true] PASSED [ 70%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_transform_fields[nll_weight-false] PASSED [ 70%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_override_fields[pad_object-default] PASSED [ 71%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_override_fields[pad_object-override] PASSED [ 71%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_override_fields[gaussian_smoothing_sigma-default] PASSED [ 71%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_override_fields[gaussian_smoothing_sigma-override] PASSED [ 71%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[mae_weight-default] PASSED [ 71%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[mae_weight-override] PASSED [ 72%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[realspace_mae_weight-default] PASSED [ 72%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[realspace_weight-default] PASSED [ 72%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[positions_provided-default] PASSED [ 72%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[probe_trainable-default] PASSED [ 72%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[sequential_sampling-default] PASSED [ 73%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_inference_config_override_fields[debug-default] PASSED [ 73%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_inference_config_override_fields[debug-override] PASSED [ 73%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_probe_mask_translation[probe_mask-default] PASSED [ 73%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_probe_mask_override PASSED [ 73%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_default_divergence_detection[nphotons-divergence] PASSED [ 74%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_default_divergence_detection[probe_scale-divergence] PASSED [ 74%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_gridsize_error_handling[gridsize-non-square] PASSED [ 74%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_type_error_handling[model_type-invalid-enum] PASSED [ 74%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_activation_error_handling[amp_activation-unknown] PASSED [ 74%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_train_data_file_required_error PASSED [ 75%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_path_required_error PASSED [ 75%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_nphotons_default_divergence_error PASSED [ 75%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_nphotons_override_passes_validation PASSED [ 75%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_n_subsample_missing_override_uses_none PASSED [ 75%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_n_subsample_explicit_override PASSED [ 76%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_inference_config_n_subsample_missing_override_uses_none PASSED [ 76%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_inference_config_n_subsample_explicit_override PASSED [ 76%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_subsample_seed_from_dataconfig PASSED [ 76%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_subsample_seed_override PASSED [ 76%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_n_groups_missing_override_warning PASSED [ 76%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_test_data_file_training_missing_warning PASSED [ 77%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_params_cfg_matches_baseline PASSED [ 77%]
tests/torch/test_config_factory.py::TestTrainingPayloadStructure::test_training_payload_returns_dataclass PASSED [ 77%]
tests/torch/test_config_factory.py::TestTrainingPayloadStructure::test_training_payload_contains_tf_config PASSED [ 77%]
tests/torch/test_config_factory.py::TestTrainingPayloadStructure::test_training_payload_contains_pytorch_configs PASSED [ 77%]
tests/torch/test_config_factory.py::TestTrainingPayloadStructure::test_training_payload_contains_overrides_dict PASSED [ 78%]
tests/torch/test_config_factory.py::TestTrainingPayloadStructure::test_gridsize_sets_channel_count PASSED [ 78%]
tests/torch/test_config_factory.py::TestInferencePayloadStructure::test_inference_payload_returns_dataclass PASSED [ 78%]
tests/torch/test_config_factory.py::TestInferencePayloadStructure::test_inference_payload_contains_tf_config PASSED [ 78%]
tests/torch/test_config_factory.py::TestInferencePayloadStructure::test_inference_payload_contains_pytorch_configs PASSED [ 78%]
tests/torch/test_config_factory.py::TestConfigBridgeTranslation::test_grid_size_tuple_to_gridsize_int PASSED [ 79%]
tests/torch/test_config_factory.py::TestConfigBridgeTranslation::test_epochs_to_nepochs_conversion PASSED [ 79%]
tests/torch/test_config_factory.py::TestConfigBridgeTranslation::test_k_to_neighbor_count_conversion PASSED [ 79%]
tests/torch/test_config_factory.py::TestLegacyParamsPopulation::test_factory_populates_params_cfg PASSED [ 79%]
tests/torch/test_config_factory.py::TestLegacyParamsPopulation::test_populate_legacy_params_helper PASSED [ 79%]
tests/torch/test_config_factory.py::TestOverridePrecedence::test_override_dict_wins_over_defaults PASSED [ 80%]
tests/torch/test_config_factory.py::TestOverridePrecedence::test_probe_size_override_wins_over_inference PASSED [ 80%]
tests/torch/test_config_factory.py::TestFactoryValidation::test_missing_n_groups_raises_error PASSED [ 80%]
tests/torch/test_config_factory.py::TestFactoryValidation::test_nonexistent_train_data_file_raises_error PASSED [ 80%]
tests/torch/test_config_factory.py::TestFactoryValidation::test_missing_checkpoint_raises_error PASSED [ 80%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_training_payload_execution_config_not_none PASSED [ 81%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_inference_payload_execution_config_not_none PASSED [ 81%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_execution_config_defaults_applied PASSED [ 81%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_execution_config_explicit_instance_propagates PASSED [ 81%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_execution_config_fields_accessible PASSED [ 81%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_overrides_applied_records_execution_knobs PASSED [ 82%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_checkpoint_knobs_propagate_through_factory PASSED [ 82%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_checkpoint_defaults_respected PASSED [ 82%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_scheduler_override_applied PASSED [ 82%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_accum_steps_override_applied PASSED [ 82%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_logger_backend_csv_default PASSED [ 83%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_logger_backend_tensorboard PASSED [ 83%]
tests/torch/test_config_factory.py::TestProbeSizeInference::test_infer_probe_size_from_npz PASSED [ 83%]
tests/torch/test_config_factory.py::TestProbeSizeInference::test_infer_probe_size_missing_file_fallback PASSED [ 83%]
tests/torch/test_data_pipeline.py::TestRawDataTorchAdapter::test_raw_data_torch_matches_tensorflow PASSED [ 83%]
tests/torch/test_data_pipeline.py::TestDataContainerParity::test_data_container_shapes_and_dtypes PASSED [ 84%]
tests/torch/test_data_pipeline.py::TestGroundTruthLoading::test_y_patches_are_complex64 PASSED [ 84%]
tests/torch/test_data_pipeline.py::TestMemmapBridgeParity::test_memmap_loader_matches_raw_data_torch PASSED [ 84%]
tests/torch/test_data_pipeline.py::TestMemmapBridgeParity::test_deterministic_generation_validation PASSED [ 84%]
tests/torch/test_data_pipeline.py::TestMemmapBridgeParity::test_memmap_bridge_accepts_diffraction_legacy PASSED [ 84%]
tests/torch/test_dataloader.py::TestDataloaderCanonicalKeySupport::test_backward_compat_legacy_diff3d PASSED [ 84%]
tests/torch/test_dataloader.py::TestDataloaderCanonicalKeySupport::test_error_when_no_diffraction_key PASSED [ 85%]
tests/torch/test_dataloader.py::TestDataloaderCanonicalKeySupport::test_loads_canonical_diffraction PASSED [ 85%]
tests/torch/test_dataloader.py::TestDataloaderFormatAutoTranspose::test_auto_transposes_legacy_hwn_format PASSED [ 85%]
tests/torch/test_dataloader.py::TestDataloaderFormatAutoTranspose::test_handles_edge_case_square_dataset PASSED [ 85%]
tests/torch/test_dataloader.py::TestDataloaderFormatAutoTranspose::test_npz_headers_also_transposes_shape PASSED [ 85%]
tests/torch/test_dataloader.py::TestDataloaderFormatAutoTranspose::test_preserves_canonical_nwh_format PASSED [ 86%]
tests/torch/test_dataloader.py::TestDataloaderFormatAutoTranspose::test_real_dataset_dimensions PASSED [ 86%]
tests/torch/test_dataloader.py::TestDataloaderFormatAutoTranspose::test_works_with_diff3d_legacy_key PASSED [ 86%]
tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_auto_prefers_cuda PASSED [ 86%]
tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_auto_warns_and_falls_back_to_cpu PASSED [ 86%]
tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_explicit_cpu_bypasses_auto_resolution PASSED [ 87%]
tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_explicit_cuda_bypasses_auto_resolution PASSED [ 87%]
tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_workflow_auto_instantiates_with_hardware_detection PASSED [ 87%]
tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_backend_selector_warns_on_cpu_only_hosts SKIPPED [ 87%]
tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_backend_selector_inherits_gpu_first_defaults PASSED [ 87%]
tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_backend_selector_cpu_fallback_with_warning PASSED [ 88%]
tests/torch/test_fixture_pytorch_integration.py::TestFixtureContract::test_fixture_file_exists PASSED [ 88%]
tests/torch/test_fixture_pytorch_integration.py::TestFixtureContract::test_fixture_outputs_match_contract PASSED [ 88%]
tests/torch/test_fixture_pytorch_integration.py::TestFixtureContract::test_metadata_sidecar_exists PASSED [ 88%]
tests/torch/test_fixture_pytorch_integration.py::TestFixtureContract::test_metadata_content_valid PASSED [ 88%]
tests/torch/test_fixture_pytorch_integration.py::TestFixtureContract::test_coordinate_coverage PASSED [ 89%]
tests/torch/test_fixture_pytorch_integration.py::TestFixtureIntegrationSmoke::test_fixture_loads_with_rawdata PASSED [ 89%]
tests/torch/test_fixture_pytorch_integration.py::TestFixtureIntegrationSmoke::test_fixture_compatible_with_pytorch_dataloader PASSED [ 89%]
tests/torch/test_integration_workflow_torch.py::test_bundle_loader_returns_modules PASSED [ 89%]
tests/torch/test_integration_workflow_torch.py::test_run_pytorch_train_save_load_infer PASSED [ 89%]
tests/torch/test_integration_workflow_torch.py::TestPyTorchIntegrationWorkflow::test_pytorch_train_save_load_infer_cycle_legacy SKIPPED [ 90%]
tests/torch/test_integration_workflow_torch.py::TestPyTorchIntegrationWorkflow::test_pytorch_tf_output_parity SKIPPED [ 90%]
tests/torch/test_lightning_checkpoint.py::TestLightningCheckpointSerialization::test_checkpoint_contains_hyperparameters PASSED [ 90%]
tests/torch/test_lightning_checkpoint.py::TestLightningCheckpointSerialization::test_load_from_checkpoint_without_kwargs PASSED [ 90%]
tests/torch/test_lightning_checkpoint.py::TestLightningCheckpointSerialization::test_checkpoint_configs_are_serializable PASSED [ 90%]
tests/torch/test_model_manager.py::TestSaveTorchBundle::test_archive_structure PASSED [ 91%]
tests/torch/test_model_manager.py::TestSaveTorchBundle::test_params_snapshot PASSED [ 91%]
tests/torch/test_model_manager.py::TestLoadTorchBundle::test_load_round_trip_updates_params_cfg PASSED [ 91%]
tests/torch/test_model_manager.py::TestLoadTorchBundle::test_missing_params_raises_value_error PASSED [ 91%]
tests/torch/test_model_manager.py::TestLoadTorchBundle::test_load_round_trip_returns_model_stub PASSED [ 91%]
tests/torch/test_model_manager.py::TestLoadTorchBundle::test_reconstructs_models_from_bundle PASSED [ 92%]
tests/torch/test_tf_helper.py::TestTorchTFHelper::test_combine_complex SKIPPED [ 92%]
tests/torch/test_tf_helper.py::TestTorchTFHelper::test_get_mask SKIPPED  [ 92%]
tests/torch/test_tf_helper.py::TestTorchTFHelper::test_placeholder_torch_functions SKIPPED [ 92%]
tests/torch/test_train_probe_size.py::TestNPZProbeSizeExtraction::test_infer_probe_size_128 PASSED [ 92%]
tests/torch/test_train_probe_size.py::TestNPZProbeSizeExtraction::test_infer_probe_size_from_npz PASSED [ 92%]
tests/torch/test_train_probe_size.py::TestNPZProbeSizeExtraction::test_infer_probe_size_missing_probe PASSED [ 93%]
tests/torch/test_train_probe_size.py::TestNPZProbeSizeExtraction::test_infer_probe_size_real_dataset PASSED [ 93%]
tests/torch/test_train_probe_size.py::TestNPZProbeSizeExtraction::test_infer_probe_size_rectangular PASSED [ 93%]
tests/torch/test_workflows_components.py::TestWorkflowsComponentsScaffold::test_run_cdi_example_calls_update_legacy_dict FAILED [ 93%]
tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_train_cdi_model_torch_invokes_lightning FAILED [ 93%]
tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_lightning_dataloader_tensor_dict_structure PASSED [ 94%]
tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_lightning_poisson_count_contract PASSED [ 94%]
tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_lightning_training_respects_gridsize PASSED [ 94%]
tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_coords_relative_layout PASSED [ 94%]
tests/torch/test_workflows_components.py::TestWorkflowsComponentsRun::test_run_cdi_example_invokes_training FAILED [ 94%]
tests/torch/test_workflows_components.py::TestWorkflowsComponentsRun::test_run_cdi_example_persists_models FAILED [ 95%]
tests/torch/test_workflows_components.py::TestWorkflowsComponentsRun::test_load_inference_bundle_handles_bundle PASSED [ 95%]
tests/torch/test_workflows_components.py::TestTrainWithLightningRed::test_train_with_lightning_instantiates_module PASSED [ 95%]
tests/torch/test_workflows_components.py::TestTrainWithLightningRed::test_train_with_lightning_runs_trainer_fit FAILED [ 95%]
tests/torch/test_workflows_components.py::TestTrainWithLightningRed::test_train_with_lightning_returns_models_dict FAILED [ 95%]
tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_reassemble_cdi_image_torch_guard_without_train_results PASSED [ 96%]
tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_reassemble_cdi_image_torch_flip_transpose_contract[False-False-False] PASSED [ 96%]
tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_reassemble_cdi_image_torch_flip_transpose_contract[True-False-False] PASSED [ 96%]
tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_reassemble_cdi_image_torch_flip_transpose_contract[False-True-False] PASSED [ 96%]
tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_reassemble_cdi_image_torch_flip_transpose_contract[False-False-True] PASSED [ 96%]
tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_reassemble_cdi_image_torch_flip_transpose_contract[True-True-True] PASSED [ 97%]
tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_run_cdi_example_torch_do_stitching_delegates_to_reassemble FAILED [ 97%]
tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_reassemble_cdi_image_torch_return_contract PASSED [ 97%]
tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchFloat32::test_batches_remain_float32 PASSED [ 97%]
tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchFloat32::test_dataloader_casts_float64_to_float32 PASSED [ 97%]
tests/torch/test_workflows_components.py::TestDecoderLastShapeParity::test_probe_big_shape_alignment PASSED [ 98%]
tests/torch/test_workflows_components.py::TestDecoderLastShapeParity::test_probe_big_false_no_mismatch PASSED [ 98%]
tests/torch/test_workflows_components.py::TestTrainWithLightningGreen::test_execution_config_overrides_trainer FAILED [ 98%]
tests/torch/test_workflows_components.py::TestTrainWithLightningGreen::test_execution_config_controls_determinism FAILED [ 98%]
tests/torch/test_workflows_components.py::TestInferenceExecutionConfig::test_inference_uses_execution_batch_size PASSED [ 98%]
tests/torch/test_workflows_components.py::TestLightningCheckpointCallbacks::test_model_checkpoint_callback_configured PASSED [ 99%]
tests/torch/test_workflows_components.py::TestLightningCheckpointCallbacks::test_early_stopping_callback_configured PASSED [ 99%]
tests/torch/test_workflows_components.py::TestLightningCheckpointCallbacks::test_disable_checkpointing_skips_callbacks PASSED [ 99%]
tests/torch/test_workflows_components.py::TestLightningExecutionConfig::test_trainer_receives_accumulation FAILED [ 99%]
tests/torch/test_workflows_components.py::TestLightningExecutionConfig::test_monitor_uses_val_loss_name PASSED [ 99%]
tests/torch/test_workflows_components.py::TestLightningExecutionConfig::test_trainer_receives_logger PASSED [100%]

=================================== FAILURES ===================================
____________ test_run_phase_g_dense_collect_only_generates_commands ____________

tmp_path = PosixPath('/tmp/pytest-of-ollie/pytest-705/test_run_phase_g_dense_collect0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x74d776620650>
capsys = <_pytest.capture.CaptureFixture object at 0x74d776622c90>

    def test_run_phase_g_dense_collect_only_generates_commands(tmp_path: Path, monkeypatch: pytest.MonkeyPatch, capsys) -> None:
        """
        Test that main() with --collect-only prints planned commands without executing them.
    
        Acceptance:
        - Loads main() from orchestrator script via importlib
        - Runs with --collect-only into a tmp hub directory
        - Asserts stdout contains expected command substrings (Phase C/D/E/F/G markers)
        - Verifies no Phase C outputs are created (dry-run mode, no filesystem side effects)
        - Ensures AUTHORITATIVE_CMDS_DOC environment variable is respected
        - Returns 0 exit code on success
    
        Follows TYPE-PATH-001 (Path normalization).
        """
        # Import main() from orchestrator
        module = _import_orchestrator_module()
        main = module.main
    
        # Setup: Create tmp hub directory
        hub = tmp_path / "collect_only_hub"
        hub.mkdir(parents=True)
    
        # Set AUTHORITATIVE_CMDS_DOC to satisfy orchestrator env check
        monkeypatch.setenv("AUTHORITATIVE_CMDS_DOC", "./docs/TESTING_GUIDE.md")
    
        # Prepare sys.argv for argparse
        monkeypatch.setattr(
            sys,
            "argv",
            [
                "run_phase_g_dense.py",
                "--hub", str(hub),
                "--dose", "1000",
                "--view", "dense",
                "--splits", "train", "test",
                "--collect-only",
            ],
        )
    
        # Execute: Call main() (should print commands and return 0)
        exit_code = main()
    
        # Assert: Exit code should be 0
        assert exit_code == 0, f"Expected exit code 0 from --collect-only mode, got {exit_code}"
    
        # Assert: Capture stdout and verify expected command substrings
        captured = capsys.readouterr()
        stdout = captured.out
    
        # Check for phase markers in stdout
        assert "Phase C: Dataset Generation" in stdout, "Missing Phase C command in --collect-only output"
        assert "Phase D: Overlap View Generation" in stdout, "Missing Phase D command in --collect-only output"
        assert "Phase E: Training Baseline (gs1)" in stdout, "Missing Phase E baseline command in --collect-only output"
        assert "Phase E: Training Dense (gs2)" in stdout, "Missing Phase E dense command in --collect-only output"
        assert "Phase F: Reconstruction" in stdout, "Missing Phase F command in --collect-only output"
        assert "Phase G: Comparison" in stdout, "Missing Phase G command in --collect-only output"
    
        # Check for specific command keywords
        assert "studies.fly64_dose_overlap.generation" in stdout, "Missing generation module in command output"
>       assert "studies.fly64_dose_overlap.overlap" in stdout, "Missing overlap module in command output"
E       AssertionError: Missing overlap module in command output
E       assert 'studies.fly64_dose_overlap.overlap' in '[run_phase_g_dense] Collect-only mode: planned commands:\n\n1. Phase C: Dataset Generation\n   Command: /home/ollie/miniconda3/envs/ptycho311/bin/python3.11 -m studies.fly64_dose_overlap.generation --base-npz tike_outputs/fly001_reconstructed_final_prepared/fly001_reconstructed_interp_smooth_both.npz --output-root /tmp/pytest-of-ollie/pytest-705/test_run_phase_g_dense_collect0/collect_only_hub/data/phase_c --dose 1000\n   Log: /tmp/pytest-of-ollie/pytest-705/test_run_phase_g_dense_collect0/collect_only_hub/cli/phase_c_generation.log\n\n2. Phase D: Overlap View Generation\n   Command: __PHASE_D_PROGRAMMATIC__\n   Log: /tmp/pytest-of-ollie/pytest-705/test_run_phase_g_dense_collect0/collect_only_hub/cli/phase_d_dense.log\n\n3. Phase E: Training Baseline (gs1)\n   Command: /home/ollie/miniconda3/envs/ptycho311/bin/python3.11 -m studies.fly64_dose_overlap.training --phase-c-root /tmp/pytest-of-ollie/pytest-705/test_run_phase_g_dense_collect0/collect_only_hub/data/phase_c --phase-d-root /tmp/pytest-of-ollie/pytest-705/test_run_phase_g_dense_collect0/collect_only_hub/data/phase_d --artifact-root /tmp/pytest-of-ollie/pytest-705/test_run_phase_g_dense_collect0/collect_only_hub/data/phase...t-705/test_run_phase_g_dense_collect0/collect_only_hub\n   Log: /tmp/pytest-of-ollie/pytest-705/test_run_phase_g_dense_collect0/collect_only_hub/cli/ssim_grid_cli.log\n\n12. Post-Verify: Verify pipeline artifacts\n   Command: /home/ollie/miniconda3/envs/ptycho311/bin/python3.11 /home/ollie/Documents/PtychoPINN/plans/active/STUDY-SYNTH-FLY64-DOSE-OVERLAP-001/bin/verify_dense_pipeline_artifacts.py --hub /tmp/pytest-of-ollie/pytest-705/test_run_phase_g_dense_collect0/collect_only_hub --report /tmp/pytest-of-ollie/pytest-705/test_run_phase_g_dense_collect0/collect_only_hub/analysis/verification_report.json --dose 1000 --view dense\n   Log: /tmp/pytest-of-ollie/pytest-705/test_run_phase_g_dense_collect0/collect_only_hub/analysis/verify_dense_stdout.log\n\n13. Post-Verify: Check highlights match\n   Command: /home/ollie/miniconda3/envs/ptycho311/bin/python3.11 /home/ollie/Documents/PtychoPINN/plans/active/STUDY-SYNTH-FLY64-DOSE-OVERLAP-001/bin/check_dense_highlights_match.py --hub /tmp/pytest-of-ollie/pytest-705/test_run_phase_g_dense_collect0/collect_only_hub\n   Log: /tmp/pytest-of-ollie/pytest-705/test_run_phase_g_dense_collect0/collect_only_hub/analysis/check_dense_highlights.log\n'

tests/study/test_phase_g_dense_orchestrator.py:88: AssertionError
___________________ test_run_phase_g_dense_post_verify_hooks ___________________

tmp_path = PosixPath('/tmp/pytest-of-ollie/pytest-705/test_run_phase_g_dense_post_ve0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x74d7767cf450>

    def test_run_phase_g_dense_post_verify_hooks(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
        """
        Test that main() invokes post-verify commands (verify_dense_pipeline_artifacts.py + check_dense_highlights_match.py) with correct paths.
    
        Acceptance:
        - Loads main() from orchestrator script via importlib
        - Stubs prepare_hub, validate_phase_c_metadata, summarize_phase_g_outputs (no-op for speed)
        - Monkeypatches run_command to record invocations
        - Runs main() without --skip-post-verify (default: post-verify enabled)
        - Asserts post-verify commands are invoked AFTER ssim_grid
        - Validates verify command includes --hub, --report, --dose, --view flags
        - Validates check command includes --hub flag
        - Validates log paths point to analysis/verify_dense_stdout.log and analysis/check_dense_highlights.log
        - Ensures AUTHORITATIVE_CMDS_DOC environment variable is respected
        - Returns 0 exit code on success
    
        Follows TYPE-PATH-001 (Path normalization), DATA-001, TEST-CLI-001.
        """
        # Import main() and helper functions from orchestrator
        module = _import_orchestrator_module()
        main = module.main
    
        # Setup: Create tmp hub directory
        hub = tmp_path / "post_verify_hub"
        hub.mkdir(parents=True)
    
        # Create expected directory structure for Phase CG
        phase_c_root = hub / "data" / "phase_c"
        phase_c_root.mkdir(parents=True)
        cli_log_dir = hub / "cli"
        cli_log_dir.mkdir(parents=True)
        phase_g_root = hub / "analysis"
        phase_g_root.mkdir(parents=True)
    
        # Set AUTHORITATIVE_CMDS_DOC to satisfy orchestrator env check
        monkeypatch.setenv("AUTHORITATIVE_CMDS_DOC", "./docs/TESTING_GUIDE.md")
    
        # Prepare sys.argv for argparse (NO --skip-post-verify, so post-verify enabled)
        monkeypatch.setattr(
            sys,
            "argv",
            [
                "run_phase_g_dense.py",
                "--hub", str(hub),
                "--dose", "1000",
                "--view", "dense",
                "--splits", "train", "test",
                "--clobber",  # Required to pass prepare_hub check
            ],
        )
    
        # Stub heavy helpers to no-op (we only care about run_command invocations)
        def stub_prepare_hub(hub_path, clobber):
            """No-op stub for prepare_hub."""
            pass
    
        def stub_validate_phase_c_metadata(hub_path):
            """No-op stub for validate_phase_c_metadata."""
            pass
    
        def stub_summarize_phase_g_outputs(hub_path):
            """Create metrics_summary.json with test data for delta computation."""
            analysis = Path(hub_path) / "analysis"
            analysis.mkdir(parents=True, exist_ok=True)
    
            # Create metrics_summary.json with aggregate_metrics for delta computation
            summary_data = {
                "n_jobs": 2,
                "n_success": 2,
                "n_failed": 0,
                "jobs": [],
                "aggregate_metrics": {
                    "PtychoPINN": {
                        "ms_ssim": {"mean_amplitude": 0.950, "mean_phase": 0.920},
                        "mae": {"mean_amplitude": 0.025, "mean_phase": 0.035}
                    },
                    "Baseline": {
                        "ms_ssim": {"mean_amplitude": 0.930, "mean_phase": 0.900},
                        "mae": {"mean_amplitude": 0.030, "mean_phase": 0.040}
                    },
                    "PtyChi": {
                        "ms_ssim": {"mean_amplitude": 0.940, "mean_phase": 0.910},
                        "mae": {"mean_amplitude": 0.027, "mean_phase": 0.037}
                    }
                }
            }
    
            import json
            metrics_summary_path = analysis / "metrics_summary.json"
            with metrics_summary_path.open("w", encoding="utf-8") as f:
                json.dump(summary_data, f, indent=2)
    
        def stub_generate_artifact_inventory(hub_path):
            """Create artifact_inventory.txt to satisfy orchestrator validation."""
            analysis = Path(hub_path) / "analysis"
            analysis.mkdir(parents=True, exist_ok=True)
            inventory_path = analysis / "artifact_inventory.txt"
            inventory_path.write_text("# Stub artifact inventory\n", encoding="utf-8")
    
        monkeypatch.setattr(module, "prepare_hub", stub_prepare_hub)
        monkeypatch.setattr(module, "validate_phase_c_metadata", stub_validate_phase_c_metadata)
        monkeypatch.setattr(module, "summarize_phase_g_outputs", stub_summarize_phase_g_outputs)
        monkeypatch.setattr(module, "generate_artifact_inventory", stub_generate_artifact_inventory)
    
        # Record run_command invocations
        run_command_calls = []
    
        def stub_run_command(cmd, log_path):
            """Record cmd and log_path, create required files for orchestrator progression."""
            run_command_calls.append((cmd, log_path))
            cmd_str = " ".join(str(c) for c in cmd)
    
            # When reporting helper is invoked, create highlights file
            if "report_phase_g_dense_metrics.py" in cmd_str and "--highlights" in cmd_str:
                for i, part in enumerate(cmd):
                    if str(part) == "--highlights" and i + 1 < len(cmd):
                        highlights_path = Path(cmd[i + 1])
                        highlights_path.parent.mkdir(parents=True, exist_ok=True)
                        highlights_path.write_text("MS-SSIM Deltas\n", encoding="utf-8")
                        break
    
            # When analyze digest is invoked, create digest file
            if "analyze_dense_metrics.py" in cmd_str and "--output" in cmd_str:
                for i, part in enumerate(cmd):
                    if str(part) == "--output" and i + 1 < len(cmd):
                        digest_path = Path(cmd[i + 1])
                        digest_path.parent.mkdir(parents=True, exist_ok=True)
                        digest_path.write_text("# Digest\n", encoding="utf-8")
                        break
    
            # When ssim_grid is invoked, create summary file
            if "ssim_grid.py" in cmd_str and "--hub" in cmd_str:
                for i, part in enumerate(cmd):
                    if str(part) == "--hub" and i + 1 < len(cmd):
                        hub_path = Path(cmd[i + 1])
                        ssim_grid_summary_path = hub_path / "analysis" / "ssim_grid_summary.md"
                        ssim_grid_summary_path.parent.mkdir(parents=True, exist_ok=True)
                        ssim_grid_summary_path.write_text("# SSIM Grid\n", encoding="utf-8")
                        break
    
            # When verify_dense_pipeline_artifacts is invoked, create report file
            if "verify_dense_pipeline_artifacts.py" in cmd_str and "--report" in cmd_str:
                for i, part in enumerate(cmd):
                    if str(part) == "--report" and i + 1 < len(cmd):
                        report_path = Path(cmd[i + 1])
                        report_path.parent.mkdir(parents=True, exist_ok=True)
                        report_path.write_text('{"valid": true}\n', encoding="utf-8")
                        break
    
            # When check_dense_highlights_match is invoked, no file creation needed (just stdout)
            # (check_dense_highlights_match.py outputs to stdout, which is captured by run_command log_path)
    
        monkeypatch.setattr(module, "run_command", stub_run_command)
    
        # Execute: Call main() (should execute Phase CG pipeline + reporting helper + analyze digest + ssim_grid + post-verify)
        exit_code = main()
    
        # Assert: Exit code should be 0
>       assert exit_code == 0, f"Expected exit code 0 from real execution mode, got {exit_code}"
E       AssertionError: Expected exit code 0 from real execution mode, got 1
E       assert 1 == 0

tests/study/test_phase_g_dense_orchestrator.py:278: AssertionError
----------------------------- Captured stdout call -----------------------------

================================================================================
[run_phase_g_dense] Preparing hub...
================================================================================


[run_phase_g_dense] Starting Phase CG pipeline for dose=1000, view=dense, splits=['train', 'test']
[run_phase_g_dense] Hub: /tmp/pytest-of-ollie/pytest-705/test_run_phase_g_dense_post_ve0/post_verify_hub
[run_phase_g_dense] Total commands: 8


================================================================================
[1/8] Phase C: Dataset Generation
================================================================================


================================================================================
[run_phase_g_dense] Cleaning up unwanted dose directories (keeping dose_1000 only)...
================================================================================


================================================================================
[run_phase_g_dense] Validating Phase C metadata...
================================================================================


================================================================================
[2/8] Phase D: Overlap View Generation
================================================================================

_____________ test_run_phase_g_dense_exec_invokes_reporting_helper _____________

tmp_path = PosixPath('/tmp/pytest-of-ollie/pytest-705/test_run_phase_g_dense_exec_in0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x74d8dc347610>

    def test_run_phase_g_dense_exec_invokes_reporting_helper(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
        """
        Test that main() in real execution mode invokes the reporting helper after Phase CG pipeline.
    
        Acceptance:
        - Loads main() from orchestrator script via importlib
        - Stubs prepare_hub, validate_phase_c_metadata, summarize_phase_g_outputs (no-op for speed)
        - Monkeypatches run_command to record invocations
        - Runs main() without --collect-only to trigger real execution path
        - Asserts final run_command call targets report_phase_g_dense_metrics.py script
        - Validates command includes --metrics metrics_summary.json and --output aggregate_report.md
        - Validates log_path points to cli/aggregate_report_cli.log
        - Ensures AUTHORITATIVE_CMDS_DOC environment variable is respected
        - Returns 0 exit code on success
    
        Follows TYPE-PATH-001 (Path normalization).
        """
        # Import main() and helper functions from orchestrator
        module = _import_orchestrator_module()
        main = module.main
    
        # Setup: Create tmp hub directory
        hub = tmp_path / "exec_hub"
        hub.mkdir(parents=True)
    
        # Create expected directory structure for Phase CG
        phase_c_root = hub / "data" / "phase_c"
        phase_c_root.mkdir(parents=True)
        cli_log_dir = hub / "cli"
        cli_log_dir.mkdir(parents=True)
        phase_g_root = hub / "analysis"
        phase_g_root.mkdir(parents=True)
    
        # Set AUTHORITATIVE_CMDS_DOC to satisfy orchestrator env check
        monkeypatch.setenv("AUTHORITATIVE_CMDS_DOC", "./docs/TESTING_GUIDE.md")
    
        # Prepare sys.argv for argparse (NO --collect-only, so real execution)
        monkeypatch.setattr(
            sys,
            "argv",
            [
                "run_phase_g_dense.py",
                "--hub", str(hub),
                "--dose", "1000",
                "--view", "dense",
                "--splits", "train", "test",
                "--clobber",  # Required to pass prepare_hub check
            ],
        )
    
        # Stub heavy helpers to no-op (we only care about run_command invocations)
        def stub_prepare_hub(hub_path, clobber):
            """No-op stub for prepare_hub."""
            pass
    
        def stub_validate_phase_c_metadata(hub_path):
            """No-op stub for validate_phase_c_metadata."""
            pass
    
        def stub_summarize_phase_g_outputs(hub_path):
            """No-op stub for summarize_phase_g_outputs."""
            pass
    
        monkeypatch.setattr(module, "prepare_hub", stub_prepare_hub)
        monkeypatch.setattr(module, "validate_phase_c_metadata", stub_validate_phase_c_metadata)
        monkeypatch.setattr(module, "summarize_phase_g_outputs", stub_summarize_phase_g_outputs)
    
        # Record run_command invocations
        run_command_calls = []
    
        def stub_run_command(cmd, log_path):
            """Record cmd and log_path for assertions, and create highlights file when reporting helper is invoked."""
            run_command_calls.append((cmd, log_path))
            # When reporting helper is invoked, create the highlights file to satisfy orchestrator expectations
            cmd_str = " ".join(str(c) for c in cmd)
            if "report_phase_g_dense_metrics.py" in cmd_str and "--highlights" in cmd_str:
                # Extract highlights path from command
                for i, part in enumerate(cmd):
                    if str(part) == "--highlights" and i + 1 < len(cmd):
                        highlights_path = Path(cmd[i + 1])
                        highlights_path.parent.mkdir(parents=True, exist_ok=True)
                        # Write minimal highlights content
                        highlights_path.write_text("Minimal highlights for test\n", encoding="utf-8")
                        break
    
        monkeypatch.setattr(module, "run_command", stub_run_command)
    
        # Execute: Call main() (should execute Phase CG pipeline + reporting helper)
        exit_code = main()
    
        # Assert: Exit code should be 0
>       assert exit_code == 0, f"Expected exit code 0 from real execution mode, got {exit_code}"
E       AssertionError: Expected exit code 0 from real execution mode, got 1
E       assert 1 == 0

tests/study/test_phase_g_dense_orchestrator.py:1055: AssertionError
----------------------------- Captured stdout call -----------------------------

================================================================================
[run_phase_g_dense] Preparing hub...
================================================================================


[run_phase_g_dense] Starting Phase CG pipeline for dose=1000, view=dense, splits=['train', 'test']
[run_phase_g_dense] Hub: /tmp/pytest-of-ollie/pytest-705/test_run_phase_g_dense_exec_in0/exec_hub
[run_phase_g_dense] Total commands: 8


================================================================================
[1/8] Phase C: Dataset Generation
================================================================================


================================================================================
[run_phase_g_dense] Cleaning up unwanted dose directories (keeping dose_1000 only)...
================================================================================


================================================================================
[run_phase_g_dense] Validating Phase C metadata...
================================================================================


================================================================================
[2/8] Phase D: Overlap View Generation
================================================================================

____________ test_run_phase_g_dense_exec_prints_highlights_preview _____________

tmp_path = PosixPath('/tmp/pytest-of-ollie/pytest-705/test_run_phase_g_dense_exec_pr0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x74d774514350>
capsys = <_pytest.capture.CaptureFixture object at 0x74d7766bea50>

    def test_run_phase_g_dense_exec_prints_highlights_preview(tmp_path: Path, monkeypatch: pytest.MonkeyPatch, capsys) -> None:
        """
        Test that main() in real execution mode prints an "Aggregate highlights" preview after reporting helper.
    
        Acceptance:
        - Loads main() from orchestrator script via importlib
        - Stubs prepare_hub, validate_phase_c_metadata, summarize_phase_g_outputs (no-op for speed)
        - Stubs run_command to write deterministic highlights text when reporting helper is invoked
        - Runs main() without --collect-only to trigger real execution path
        - Captures stdout via capsys
        - Asserts stdout contains "Aggregate highlights preview" banner
        - Asserts stdout contains sample highlights content (MS-SSIM/MAE deltas)
        - Returns 0 exit code on success
    
        Follows TYPE-PATH-001 (Path normalization).
        """
        # Import main() and helper functions from orchestrator
        module = _import_orchestrator_module()
        main = module.main
    
        # Setup: Create tmp hub directory
        hub = tmp_path / "exec_hub"
        hub.mkdir(parents=True)
    
        # Create expected directory structure for Phase CG
        phase_c_root = hub / "data" / "phase_c"
        phase_c_root.mkdir(parents=True)
        cli_log_dir = hub / "cli"
        cli_log_dir.mkdir(parents=True)
        phase_g_root = hub / "analysis"
        phase_g_root.mkdir(parents=True)
    
        # Set AUTHORITATIVE_CMDS_DOC to satisfy orchestrator env check
        monkeypatch.setenv("AUTHORITATIVE_CMDS_DOC", "./docs/TESTING_GUIDE.md")
    
        # Prepare sys.argv for argparse (NO --collect-only, so real execution)
        monkeypatch.setattr(
            sys,
            "argv",
            [
                "run_phase_g_dense.py",
                "--hub", str(hub),
                "--dose", "1000",
                "--view", "dense",
                "--splits", "train", "test",
                "--clobber",  # Required to pass prepare_hub check
            ],
        )
    
        # Stub heavy helpers to no-op (we only care about run_command invocations)
        def stub_prepare_hub(hub_path, clobber):
            """No-op stub for prepare_hub."""
            pass
    
        def stub_validate_phase_c_metadata(hub_path):
            """No-op stub for validate_phase_c_metadata."""
            pass
    
        def stub_summarize_phase_g_outputs(hub_path):
            """No-op stub for summarize_phase_g_outputs."""
            pass
    
        monkeypatch.setattr(module, "prepare_hub", stub_prepare_hub)
        monkeypatch.setattr(module, "validate_phase_c_metadata", stub_validate_phase_c_metadata)
        monkeypatch.setattr(module, "summarize_phase_g_outputs", stub_summarize_phase_g_outputs)
    
        # Create deterministic highlights file when reporting helper is invoked
        def stub_run_command(cmd, log_path):
            """Stub that writes highlights file when reporting helper is invoked."""
            cmd_str = " ".join(str(c) for c in cmd)
            if "report_phase_g_dense_metrics.py" in cmd_str and "--highlights" in cmd_str:
                # Extract highlights path from command
                # Command format: [..., '--highlights', 'path/to/aggregate_highlights.txt', ...]
                for i, part in enumerate(cmd):
                    if str(part) == "--highlights" and i + 1 < len(cmd):
                        highlights_path = Path(cmd[i + 1])
                        highlights_path.parent.mkdir(parents=True, exist_ok=True)
                        # Write deterministic highlights content
                        highlights_path.write_text(
                            "Phase G Dense Metrics  Highlights\n"
                            "==================================================\n"
                            "\n"
                            "MS-SSIM Deltas (PtychoPINN - Baseline):\n"
                            "  Amplitude (mean): +0.123\n"
                            "  Phase (mean):     +0.045\n"
                            "\n"
                            "MS-SSIM Deltas (PtychoPINN - PtyChi):\n"
                            "  Amplitude (mean): +0.067\n"
                            "  Phase (mean):     +0.012\n"
                            "\n"
                            "MAE Deltas (PtychoPINN - Baseline):\n"
                            "  [Note: Negative = PtychoPINN better (lower error)]\n"
                            "  Amplitude (mean): -0.008\n"
                            "  Phase (mean):     -0.003\n"
                            "\n"
                            "MAE Deltas (PtychoPINN - PtyChi):\n"
                            "  [Note: Negative = PtychoPINN better (lower error)]\n"
                            "  Amplitude (mean): -0.005\n"
                            "  Phase (mean):     -0.001\n",
                            encoding="utf-8"
                        )
                        break
    
        monkeypatch.setattr(module, "run_command", stub_run_command)
    
        # Execute: Call main() (should execute Phase CG pipeline + reporting helper + highlights preview)
        exit_code = main()
    
        # Assert: Exit code should be 0
>       assert exit_code == 0, f"Expected exit code 0 from real execution mode, got {exit_code}"
E       AssertionError: Expected exit code 0 from real execution mode, got 1
E       assert 1 == 0

tests/study/test_phase_g_dense_orchestrator.py:1201: AssertionError
----------------------------- Captured stdout call -----------------------------

================================================================================
[run_phase_g_dense] Preparing hub...
================================================================================


[run_phase_g_dense] Starting Phase CG pipeline for dose=1000, view=dense, splits=['train', 'test']
[run_phase_g_dense] Hub: /tmp/pytest-of-ollie/pytest-705/test_run_phase_g_dense_exec_pr0/exec_hub
[run_phase_g_dense] Total commands: 8


================================================================================
[1/8] Phase C: Dataset Generation
================================================================================


================================================================================
[run_phase_g_dense] Cleaning up unwanted dose directories (keeping dose_1000 only)...
================================================================================


================================================================================
[run_phase_g_dense] Validating Phase C metadata...
================================================================================


================================================================================
[2/8] Phase D: Overlap View Generation
================================================================================

_______________ test_run_phase_g_dense_exec_runs_analyze_digest ________________

tmp_path = PosixPath('/tmp/pytest-of-ollie/pytest-705/test_run_phase_g_dense_exec_ru0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x74d8dc3470d0>

    def test_run_phase_g_dense_exec_runs_analyze_digest(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
        """
        Test that main() in real execution mode invokes analyze_dense_metrics.py after reporting helper
        and emits MS-SSIM/MAE delta summary to stdout.
    
        Acceptance:
        - Loads main() from orchestrator script via importlib
        - Stubs prepare_hub, validate_phase_c_metadata, summarize_phase_g_outputs (seeds metrics_summary.json)
        - Monkeypatches run_command to record invocations and create required files
        - Runs main() without --collect-only to trigger real execution path
        - Asserts analyze_dense_metrics.py is invoked after report_phase_g_dense_metrics.py
        - Validates analyze command includes --metrics, --highlights, --output flags
        - Validates log_path points to cli/metrics_digest_cli.log
        - Validates stdout contains delta block with four lines (MS-SSIM vs Baseline/PtyChi, MAE vs Baseline/PtyChi)
        - Ensures AUTHORITATIVE_CMDS_DOC environment variable is respected
        - Returns 0 exit code on success
    
        Follows TYPE-PATH-001 (Path normalization).
        """
        # Import main() and helper functions from orchestrator
        module = _import_orchestrator_module()
        main = module.main
    
        # Setup: Create tmp hub directory
        hub = tmp_path / "exec_hub"
        hub.mkdir(parents=True)
    
        # Create expected directory structure for Phase CG
        phase_c_root = hub / "data" / "phase_c"
        phase_c_root.mkdir(parents=True)
        cli_log_dir = hub / "cli"
        cli_log_dir.mkdir(parents=True)
        phase_g_root = hub / "analysis"
        phase_g_root.mkdir(parents=True)
    
        # Set AUTHORITATIVE_CMDS_DOC to satisfy orchestrator env check
        monkeypatch.setenv("AUTHORITATIVE_CMDS_DOC", "./docs/TESTING_GUIDE.md")
    
        # Prepare sys.argv for argparse (NO --collect-only, so real execution)
        monkeypatch.setattr(
            sys,
            "argv",
            [
                "run_phase_g_dense.py",
                "--hub", str(hub),
                "--dose", "1000",
                "--view", "dense",
                "--splits", "train", "test",
                "--clobber",  # Required to pass prepare_hub check
            ],
        )
    
        # Stub heavy helpers to no-op (we only care about run_command invocations)
        def stub_prepare_hub(hub_path, clobber):
            """No-op stub for prepare_hub."""
            pass
    
        def stub_validate_phase_c_metadata(hub_path):
            """No-op stub for validate_phase_c_metadata."""
            pass
    
        def stub_summarize_phase_g_outputs(hub_path):
            """Create metrics_summary.json with test data for delta computation."""
            analysis = Path(hub_path) / "analysis"
            analysis.mkdir(parents=True, exist_ok=True)
    
            # Create metrics_summary.json with aggregate_metrics for delta computation
            summary_data = {
                "n_jobs": 2,
                "n_success": 2,
                "n_failed": 0,
                "jobs": [],
                "aggregate_metrics": {
                    "PtychoPINN": {
                        "ms_ssim": {
                            "mean_amplitude": 0.950,
                            "best_amplitude": 0.955,
                            "mean_phase": 0.920,
                            "best_phase": 0.925
                        },
                        "mae": {
                            "mean_amplitude": 0.025,
                            "mean_phase": 0.035
                        }
                    },
                    "Baseline": {
                        "ms_ssim": {
                            "mean_amplitude": 0.930,
                            "best_amplitude": 0.935,
                            "mean_phase": 0.900,
                            "best_phase": 0.905
                        },
                        "mae": {
                            "mean_amplitude": 0.030,
                            "mean_phase": 0.040
                        }
                    },
                    "PtyChi": {
                        "ms_ssim": {
                            "mean_amplitude": 0.940,
                            "best_amplitude": 0.945,
                            "mean_phase": 0.910,
                            "best_phase": 0.915
                        },
                        "mae": {
                            "mean_amplitude": 0.027,
                            "mean_phase": 0.037
                        }
                    }
                }
            }
    
            import json
            metrics_summary_path = analysis / "metrics_summary.json"
            with metrics_summary_path.open("w", encoding="utf-8") as f:
                json.dump(summary_data, f, indent=2)
    
        def stub_generate_artifact_inventory(hub_path):
            """Create artifact_inventory.txt with test data listing key artifacts."""
            analysis = Path(hub_path) / "analysis"
            analysis.mkdir(parents=True, exist_ok=True)
    
            inventory_path = analysis / "artifact_inventory.txt"
    
            # List key artifacts that should be present after pipeline execution
            artifacts = [
                "analysis/aggregate_highlights.txt",
                "analysis/aggregate_report.md",
                "analysis/comparison_manifest.json",
                "analysis/metrics_delta_highlights.txt",
                "analysis/metrics_delta_summary.json",
                "analysis/metrics_digest.md",
                "analysis/metrics_summary.json",
                "analysis/metrics_summary.md",
                "cli/metrics_digest_cli.log",
                "cli/aggregate_report_cli.log",
            ]
    
            # Write sorted artifact list (deterministic)
            with inventory_path.open("w", encoding="utf-8") as f:
                for artifact in sorted(artifacts):
                    f.write(f"{artifact}\n")
    
        monkeypatch.setattr(module, "prepare_hub", stub_prepare_hub)
        monkeypatch.setattr(module, "validate_phase_c_metadata", stub_validate_phase_c_metadata)
        monkeypatch.setattr(module, "summarize_phase_g_outputs", stub_summarize_phase_g_outputs)
        monkeypatch.setattr(module, "generate_artifact_inventory", stub_generate_artifact_inventory)
    
        # Record run_command invocations
        run_command_calls = []
    
        def stub_run_command(cmd, log_path):
            """Record cmd and log_path, create required files for orchestrator progression."""
            run_command_calls.append((cmd, log_path))
            cmd_str = " ".join(str(c) for c in cmd)
    
            # Create log file for every invocation (TEST-CLI-001)
            log_path.parent.mkdir(parents=True, exist_ok=True)
            log_path.write_text(f"Stub log for: {cmd_str}\n", encoding="utf-8")
    
            # When reporting helper is invoked, create highlights file
            if "report_phase_g_dense_metrics.py" in cmd_str and "--highlights" in cmd_str:
                for i, part in enumerate(cmd):
                    if str(part) == "--highlights" and i + 1 < len(cmd):
                        highlights_path = Path(cmd[i + 1])
                        highlights_path.parent.mkdir(parents=True, exist_ok=True)
                        highlights_path.write_text("MS-SSIM Deltas (PtychoPINN - Baseline):\n  Amplitude (mean): +0.050\n", encoding="utf-8")
                        break
    
            # When analyze digest is invoked, create digest file
            if "analyze_dense_metrics.py" in cmd_str and "--output" in cmd_str:
                for i, part in enumerate(cmd):
                    if str(part) == "--output" and i + 1 < len(cmd):
                        digest_path = Path(cmd[i + 1])
                        digest_path.parent.mkdir(parents=True, exist_ok=True)
                        digest_path.write_text("# Phase G Dense Metrics Digest\n", encoding="utf-8")
                        break
    
            # When ssim_grid is invoked, create summary file and log
            if "ssim_grid.py" in cmd_str and "--hub" in cmd_str:
                # ssim_grid.py creates analysis/ssim_grid_summary.md
                for i, part in enumerate(cmd):
                    if str(part) == "--hub" and i + 1 < len(cmd):
                        hub_path = Path(cmd[i + 1])
                        ssim_grid_summary_path = hub_path / "analysis" / "ssim_grid_summary.md"
                        ssim_grid_summary_path.parent.mkdir(parents=True, exist_ok=True)
                        ssim_grid_summary_path.write_text("# SSIM Grid Summary (Phase-Only)\n", encoding="utf-8")
                        # Create ssim_grid CLI log
                        ssim_grid_log_path = hub_path / "cli" / "ssim_grid_cli.log"
                        ssim_grid_log_path.parent.mkdir(parents=True, exist_ok=True)
                        ssim_grid_log_path.write_text("SSIM grid generation complete\n", encoding="utf-8")
                        break
    
            # When verify_dense_pipeline_artifacts is invoked, create report and log files
            if "verify_dense_pipeline_artifacts.py" in cmd_str and "--report" in cmd_str:
                for i, part in enumerate(cmd):
                    if str(part) == "--report" and i + 1 < len(cmd):
                        report_path = Path(cmd[i + 1])
                        report_path.parent.mkdir(parents=True, exist_ok=True)
                        report_path.write_text('{"valid": true}\n', encoding="utf-8")
                        # Create verification log in analysis/
                        hub_path = report_path.parent.parent  # analysis -> hub
                        verify_log_path = hub_path / "analysis" / "verify_dense_stdout.log"
                        verify_log_path.parent.mkdir(parents=True, exist_ok=True)
                        verify_log_path.write_text("Verification complete\n", encoding="utf-8")
                        break
    
            # When check_dense_highlights_match is invoked, create log file
            if "check_dense_highlights_match.py" in cmd_str:
                # Extract hub path from command to create log in analysis/
                for i, part in enumerate(cmd):
                    if str(part) == "--hub" and i + 1 < len(cmd):
                        hub_path = Path(cmd[i + 1])
                        check_log_path = hub_path / "analysis" / "check_dense_highlights.log"
                        check_log_path.parent.mkdir(parents=True, exist_ok=True)
                        check_log_path.write_text("Highlights check complete\n", encoding="utf-8")
                        break
    
        monkeypatch.setattr(module, "run_command", stub_run_command)
    
        # Execute: Call main() with stdout capture (should execute Phase CG pipeline + reporting helper + analyze digest)
        import io
        import contextlib
    
        stdout_buffer = io.StringIO()
        with contextlib.redirect_stdout(stdout_buffer):
            exit_code = main()
    
        stdout = stdout_buffer.getvalue()
    
        # Assert: Exit code should be 0
>       assert exit_code == 0, f"Expected exit code 0 from real execution mode, got {exit_code}"
E       AssertionError: Expected exit code 0 from real execution mode, got 1
E       assert 1 == 0

tests/study/test_phase_g_dense_orchestrator.py:1459: AssertionError
___________ TestLoadInferenceBundle.test_load_valid_model_directory ____________

self = <tests.test_workflow_components.TestLoadInferenceBundle testMethod=test_load_valid_model_directory>

    def test_load_valid_model_directory(self):
        """Test loading from a valid model directory."""
        # Create mock model archive
        self.create_mock_model_archive(include_diffraction_model=True)
    
        # Mock ModelManager.load_multiple_models
        with patch('ptycho.workflows.components.ModelManager.load_multiple_models') as mock_load:
            # Create a mock model
            mock_model = MagicMock(spec=tf.keras.Model)
            mock_load.return_value = {'diffraction_to_obj': mock_model}
    
            # Test loading
            model, config = load_inference_bundle(self.model_dir)
    
            # Verify results
            self.assertIsNotNone(model)
>           self.assertEqual(model, mock_model)
E           AssertionError: <DiffractionToObjectAdapter name=diffraction_to_obj, built=False> != <MagicMock spec='Model' id='128468718136976'>

tests/test_workflow_components.py:82: AssertionError
------------------------------ Captured log call -------------------------------
WARNING  absl:saving_api.py:83 You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`.
_ TestWorkflowsComponentsScaffold.test_run_cdi_example_calls_update_legacy_dict _

self = <test_workflows_components.TestWorkflowsComponentsScaffold object at 0x74d7767999d0>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x74d76a20cd10>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'sigmoid', 'backend': 'tensorflow', 'batch_size': 16, ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj...ensity_scale_trainable=True, output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow')

    def test_run_cdi_example_calls_update_legacy_dict(
        self,
        monkeypatch,
        params_cfg_snapshot,
        minimal_training_config
    ):
        """
        CRITICAL PARITY TEST: run_cdi_example_torch must call update_legacy_dict.
    
        Requirement: specs/ptychodus_api_spec.md:187 mandates that PyTorch entry
        points must synchronize params.cfg via update_legacy_dict() to prevent
        silent CONFIG-001 violations (shape mismatch errors from empty params.cfg).
    
        Red-phase contract:
        - Entry signature: run_cdi_example_torch(train_data, test_data, config, ...)
        - MUST call ptycho.config.config.update_legacy_dict(params.cfg, config)
        - Stub implementation may raise NotImplementedError for paths Phase D2.B/C fill
    
        Test mechanism:
        - Use monkeypatch to spy on update_legacy_dict calls
        - Pass minimal dummy data (no actual training execution required)
        - Assert update_legacy_dict was invoked with correct params.cfg + config args
        """
        # Import the module under test
        # This import must succeed even when torch unavailable (torch-optional)
        from ptycho_torch.workflows import components as torch_components
        from ptycho.config.config import update_legacy_dict
        from ptycho.raw_data import RawData
    
        # Spy flag to track update_legacy_dict invocation
        update_legacy_dict_called = {"called": False, "args": None}
    
        def mock_update_legacy_dict(cfg_dict, config_obj):
            """Spy that records invocation and delegates to real function."""
            update_legacy_dict_called["called"] = True
            update_legacy_dict_called["args"] = (cfg_dict, config_obj)
            # Call the real function to populate params.cfg for validation
            update_legacy_dict(cfg_dict, config_obj)
    
        # Patch update_legacy_dict with spy
        monkeypatch.setattr(
            "ptycho.config.config.update_legacy_dict",
            mock_update_legacy_dict
        )
    
        # Create minimal dummy train_data (RawData-compatible stub)
        # For scaffold test, we don't need valid NPZ data  just RawData structure
        dummy_coords = np.array([0.0, 1.0, 2.0])
        dummy_diff = np.random.rand(3, 64, 64).astype(np.float32)
        dummy_probe = np.ones((64, 64), dtype=np.complex64)
        dummy_scan_index = np.array([0, 1, 2], dtype=int)
    
        train_data = RawData(
            xcoords=dummy_coords,
            ycoords=dummy_coords,
            xcoords_start=dummy_coords,
            ycoords_start=dummy_coords,
            diff3d=dummy_diff,
            probeGuess=dummy_probe,
            scan_index=dummy_scan_index,
        )
    
        # Attempt to call run_cdi_example_torch
        # Phase D2.A: expects NotImplementedError (scaffold only)  COMPLETED
        # Phase D2.B/C: IMPLEMENTED  now returns results tuple
        # Test validates update_legacy_dict was called, but doesn't fully exercise training
        # (training path tested separately in TestWorkflowsComponentsTraining)
    
        # Monkeypatch train_cdi_model_torch to prevent full training execution in this test
        def mock_train_cdi_model_torch(train_data, test_data, config):
            """Minimal stub to prevent full training in scaffold test."""
            return {"history": {"train_loss": [0.5]}, "train_container": None, "test_container": None}
    
        monkeypatch.setattr(
            "ptycho_torch.workflows.components.train_cdi_model_torch",
            mock_train_cdi_model_torch
        )
    
        # Call should now succeed (Phase D2.C implemented)
>       recon_amp, recon_phase, results = torch_components.run_cdi_example_torch(
            train_data=train_data,
            test_data=None,  # Optional
            config=minimal_training_config,
            flip_x=False,
            flip_y=False,
            transpose=False,
            M=20,
            do_stitching=False,
        )

tests/torch/test_workflows_components.py:156: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

train_data = <ptycho.raw_data.RawData object at 0x74d76a30e8d0>
test_data = None
config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj...ensity_scale_trainable=True, output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow')
flip_x = False, flip_y = False, transpose = False, M = 20, do_stitching = False
execution_config = None

    def run_cdi_example_torch(
        train_data: Union[RawData, 'RawDataTorch', 'PtychoDataContainerTorch'],
        test_data: Optional[Union[RawData, 'RawDataTorch', 'PtychoDataContainerTorch']],
        config: TrainingConfig,
        flip_x: bool = False,
        flip_y: bool = False,
        transpose: bool = False,
        M: int = 20,
        do_stitching: bool = False,
        execution_config: Optional[Any] = None
    ) -> Tuple[Optional[Any], Optional[Any], Dict[str, Any]]:
        """
        Run the main CDI example execution flow using PyTorch backend.
    
        This function provides API parity with ptycho.workflows.components.run_cdi_example,
        enabling transparent backend selection from Ptychodus per specs/ptychodus_api_spec.md 4.5.
    
        CRITICAL: This function MUST call update_legacy_dict(params.cfg, config) before
        delegating to core modules to prevent CONFIG-001 violations (empty params.cfg
        causing silent shape mismatches downstream).
    
        Args:
            train_data: Training data (RawData, RawDataTorch, or PtychoDataContainerTorch)
            test_data: Optional test data (same type constraints as train_data)
            config: TrainingConfig instance (TensorFlow dataclass, translated via config_bridge)
            flip_x: Whether to flip the x coordinates during reconstruction
            flip_y: Whether to flip the y coordinates during reconstruction
            transpose: Whether to transpose the image by swapping dimensions
            M: Parameter for reassemble_position function (default: 20)
            do_stitching: Whether to perform image stitching after training
            execution_config: Optional PyTorchExecutionConfig for runtime control (accelerator,
                             num_workers, learning_rate, scheduler, logger, checkpointing).
                             See CONFIG-002, CONFIG-LOGGER-001.
    
        Returns:
            Tuple containing:
            - reconstructed amplitude (or None if stitching disabled)
            - reconstructed phase (or None if stitching disabled)
            - results dictionary (training history, containers, metrics)
    
        Raises:
            NotImplementedError: Phase D2.B/C not yet implemented (scaffold only)
    
        Phase D2.A Scaffold Status:
            - Entry signature:  COMPLETE (matches TensorFlow)
            - update_legacy_dict call:  COMPLETE (CONFIG-001 compliance)
            - Placeholder logic:  COMPLETE (raises NotImplementedError)
            - Torch-optional:  COMPLETE (importable without torch)
    
        Phase D2.B/C TODO:
            - Implement train_cdi_model_torch delegation (Lightning trainer orchestration)
            - Implement reassemble_cdi_image_torch (optional stitching path)
            - Add MLflow disable flag handling
            - Validate deterministic seeds from config
    
        Example (Post D2.B/C):
            >>> from ptycho_torch.workflows.components import run_cdi_example_torch
            >>> from ptycho.config.config import TrainingConfig, ModelConfig
            >>> from ptycho.raw_data import RawData
            >>>
            >>> # Load data
            >>> train_data = RawData.from_file("train.npz")
            >>> config = TrainingConfig(model=ModelConfig(N=64), ...)
            >>>
            >>> # Execute PyTorch pipeline
            >>> amp, phase, results = run_cdi_example_torch(
            ...     train_data, None, config, do_stitching=False
            ... )
        """
        # CRITICAL: Update params.cfg before delegating (CONFIG-001 compliance)
        # This ensures legacy modules invoked downstream observe correct configuration state
        ptycho_config.update_legacy_dict(params.cfg, config)
        logger.info("PyTorch workflow: params.cfg synchronized with TrainingConfig")
    
        # Step 1: Train the model (Phase D2.B  delegates to Lightning trainer stub)
        logger.info("Invoking PyTorch training orchestration via train_cdi_model_torch")
        # Note: train_cdi_model_torch will need to be updated to accept execution_config
        # For now, we pass it as a keyword argument for forward compatibility
>       train_results = train_cdi_model_torch(train_data, test_data, config, execution_config=execution_config)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: TestWorkflowsComponentsScaffold.test_run_cdi_example_calls_update_legacy_dict.<locals>.mock_train_cdi_model_torch() got an unexpected keyword argument 'execution_config'

ptycho_torch/workflows/components.py:161: TypeError
----------------------------- Captured stdout call -----------------------------
diff3d shape: (3, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (3,)
xcoords shape: (3,)
ycoords shape: (3,)
xcoords_start shape: (3,)
ycoords_start shape: (3,)
_ TestWorkflowsComponentsTraining.test_train_cdi_model_torch_invokes_lightning _

self = <test_workflows_components.TestWorkflowsComponentsTraining object at 0x74d77679a3d0>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x74d76a342e50>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'sigmoid', 'backend': 'tensorflow', 'batch_size': 16, ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=1, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj...ensity_scale_trainable=True, output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x74d76a302e50>

    def test_train_cdi_model_torch_invokes_lightning(
        self,
        monkeypatch,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data
    ):
        """
        CRITICAL TRAINING PATH TEST: train_cdi_model_torch must delegate to Lightning.
    
        Requirement: Phase D2.B must implement training orchestration following the
        pattern in plans/active/INTEGRATE-PYTORCH-001/reports/2025-10-17T093500Z/
        phase_d2_training_analysis.md.
    
        Red-phase contract:
        - Entry signature: train_cdi_model_torch(train_data, test_data, config)
        - MUST call _ensure_container(data, config) for train/test inputs
        - MUST delegate to Lightning trainer with normalized config
        - MUST return dict with keys: history, train_container, test_container
        - Stub implementation may raise NotImplementedError initially
    
        Test mechanism:
        - Use monkeypatch to spy on _ensure_container and Lightning orchestration calls
        - Pass minimal RawData (no actual training execution required)
        - Assert expected orchestration order without running full training
        """
        # Import the module under test
        from ptycho_torch.workflows import components as torch_components
    
        # Spy flags to track internal calls
        ensure_container_calls = []
        lightning_trainer_called = {"called": False, "config": None}
    
        def mock_ensure_container(data, config):
            """Spy that records _ensure_container invocations."""
            ensure_container_calls.append({
                "data": data,
                "config": config
            })
            # Return a sentinel PtychoDataContainerTorch-like object
            # In Phase D2.B implementation, this would be a real container
            return {"X": np.ones((2, 64, 64)), "Y": np.ones((2, 64, 64), dtype=np.complex64)}
    
        def mock_lightning_orchestrator(train_container, test_container, config):
            """Spy that records Lightning trainer invocation."""
            lightning_trainer_called["called"] = True
            lightning_trainer_called["config"] = config
            # Return minimal training results dict
            return {
                "history": {"train_loss": [0.5, 0.3], "val_loss": [0.6, 0.4]},
                "train_container": train_container,
                "test_container": test_container,
            }
    
        # Patch internal helpers (Phase D2.B implemented)
        monkeypatch.setattr(
            "ptycho_torch.workflows.components._ensure_container",
            mock_ensure_container
        )
        monkeypatch.setattr(
            "ptycho_torch.workflows.components._train_with_lightning",
            mock_lightning_orchestrator
        )
    
        # Call train_cdi_model_torch (Phase D2.B green phase)
>       results = torch_components.train_cdi_model_torch(
            train_data=dummy_raw_data,
            test_data=None,  # Optional
            config=minimal_training_config
        )

tests/torch/test_workflows_components.py:318: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

train_data = <ptycho.raw_data.RawData object at 0x74d76a302e50>
test_data = None
config = TrainingConfig(model=ModelConfig(N=64, gridsize=1, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj...ensity_scale_trainable=True, output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow')
execution_config = None

    def train_cdi_model_torch(
        train_data: Union[RawData, 'RawDataTorch', 'PtychoDataContainerTorch'],
        test_data: Optional[Union[RawData, 'RawDataTorch', 'PtychoDataContainerTorch']],
        config: TrainingConfig,
        execution_config: Optional[Any] = None
    ) -> Dict[str, Any]:
        """
        Train the CDI model using PyTorch Lightning backend.
    
        This function provides API parity with ptycho.workflows.components.train_cdi_model,
        orchestrating data preparation, probe initialization, and Lightning trainer execution.
    
        Args:
            train_data: Training data (RawData, RawDataTorch, or PtychoDataContainerTorch)
            test_data: Optional test data for validation
            config: TrainingConfig instance (TensorFlow dataclass)
            execution_config: Optional PyTorchExecutionConfig for runtime control
    
        Returns:
            Dict[str, Any]: Results dictionary containing:
            - 'history': Training history (losses, metrics)
            - 'train_container': PtychoDataContainerTorch for training data
            - 'test_container': Optional PtychoDataContainerTorch for test data
            - Additional outputs from Lightning trainer
    
        Raises:
            ImportError: If Phase C adapters not available
            TypeError: If input data types are invalid
    
        Phase D2.B Status:
            - Entry signature:  COMPLETE (matches TensorFlow)
            - _ensure_container helper:  COMPLETE (normalizes inputs via Phase C adapters)
            - Lightning orchestration:  STUB (returns minimal dict, full impl pending)
            - Torch-optional:  COMPLETE (importable without torch)
    
        Example:
            >>> config = TrainingConfig(model=ModelConfig(N=64), nepochs=10, ...)
            >>> results = train_cdi_model_torch(train_data, test_data, config)
            >>> print(results['history']['train_loss'][-1])
        """
        # Step 1: Normalize train_data to PtychoDataContainerTorch
        logger.info("Normalizing training data via _ensure_container")
        train_container = _ensure_container(train_data, config)
    
        # Step 2: Normalize test_data if provided
        test_container = None
        if test_data is not None:
            logger.info("Normalizing test data via _ensure_container")
            test_container = _ensure_container(test_data, config)
    
        # Step 3: Initialize probe (TODO: implement probe handling for PyTorch)
        # TensorFlow baseline: probe.set_probe_guess(None, train_container.probe)
        # For Phase D2.B stub, skip probe initialization
        logger.debug("Probe initialization deferred to full Lightning implementation")
    
        # Step 4: Delegate to Lightning trainer
        logger.info("Delegating to Lightning trainer via _train_with_lightning")
>       results = _train_with_lightning(train_container, test_container, config, execution_config=execution_config)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: TestWorkflowsComponentsTraining.test_train_cdi_model_torch_invokes_lightning.<locals>.mock_lightning_orchestrator() got an unexpected keyword argument 'execution_config'

ptycho_torch/workflows/components.py:1114: TypeError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (10, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (10,)
xcoords shape: (10,)
ycoords shape: (10,)
xcoords_start shape: (10,)
ycoords_start shape: (10,)
_______ TestWorkflowsComponentsRun.test_run_cdi_example_invokes_training _______

self = <test_workflows_components.TestWorkflowsComponentsRun object at 0x74d7767a4a90>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x74d768732790>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'sigmoid', 'backend': 'tensorflow', 'batch_size': 16, ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj...ensity_scale_trainable=True, output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x74d77407a390>

    def test_run_cdi_example_invokes_training(
        self,
        monkeypatch,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data
    ):
        """
        CRITICAL PARITY TEST: run_cdi_example_torch must invoke training orchestration.
    
        Requirement: Phase D2.C must implement full workflow orchestration following
        TensorFlow baseline ptycho/workflows/components.py:676-723 and mirroring
        the reconstructor lifecycle per specs/ptychodus_api_spec.md 4.5.
    
        Red-phase contract:
        - Entry signature: run_cdi_example_torch(train_data, test_data, config, do_stitching=False, ...)
        - MUST call train_cdi_model_torch(train_data, test_data, config) first
        - When do_stitching=False: return (None, None, results_dict)
        - When do_stitching=True + test_data: invoke reassemble helper, return (amp, phase, results)
        - results dict MUST contain keys from training (history, containers)
    
        Test mechanism:
        - Use monkeypatch to spy on train_cdi_model_torch call
        - Pass minimal RawData + do_stitching=False (no inference path required)
        - Assert train_cdi_model_torch was invoked with correct args
        - Validate return signature matches TensorFlow baseline
        """
        # Import the module under test
        from ptycho_torch.workflows import components as torch_components
    
        # Spy flag to track train_cdi_model_torch invocation
        train_cdi_model_torch_called = {"called": False, "args": None}
    
        def mock_train_cdi_model_torch(train_data, test_data, config):
            """Spy that records train_cdi_model_torch invocation."""
            train_cdi_model_torch_called["called"] = True
            train_cdi_model_torch_called["args"] = (train_data, test_data, config)
            # Return minimal training results dict
            return {
                "history": {"train_loss": [0.5, 0.3]},
                "train_container": {"sentinel": "train"},
                "test_container": None,
            }
    
        # Patch train_cdi_model_torch
        monkeypatch.setattr(
            "ptycho_torch.workflows.components.train_cdi_model_torch",
            mock_train_cdi_model_torch
        )
    
        # Call run_cdi_example_torch with do_stitching=False (Phase D2.C red phase)
>       recon_amp, recon_phase, results = torch_components.run_cdi_example_torch(
            train_data=dummy_raw_data,
            test_data=None,
            config=minimal_training_config,
            flip_x=False,
            flip_y=False,
            transpose=False,
            M=20,
            do_stitching=False,
        )

tests/torch/test_workflows_components.py:925: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

train_data = <ptycho.raw_data.RawData object at 0x74d77407a390>
test_data = None
config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj...ensity_scale_trainable=True, output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow')
flip_x = False, flip_y = False, transpose = False, M = 20, do_stitching = False
execution_config = None

    def run_cdi_example_torch(
        train_data: Union[RawData, 'RawDataTorch', 'PtychoDataContainerTorch'],
        test_data: Optional[Union[RawData, 'RawDataTorch', 'PtychoDataContainerTorch']],
        config: TrainingConfig,
        flip_x: bool = False,
        flip_y: bool = False,
        transpose: bool = False,
        M: int = 20,
        do_stitching: bool = False,
        execution_config: Optional[Any] = None
    ) -> Tuple[Optional[Any], Optional[Any], Dict[str, Any]]:
        """
        Run the main CDI example execution flow using PyTorch backend.
    
        This function provides API parity with ptycho.workflows.components.run_cdi_example,
        enabling transparent backend selection from Ptychodus per specs/ptychodus_api_spec.md 4.5.
    
        CRITICAL: This function MUST call update_legacy_dict(params.cfg, config) before
        delegating to core modules to prevent CONFIG-001 violations (empty params.cfg
        causing silent shape mismatches downstream).
    
        Args:
            train_data: Training data (RawData, RawDataTorch, or PtychoDataContainerTorch)
            test_data: Optional test data (same type constraints as train_data)
            config: TrainingConfig instance (TensorFlow dataclass, translated via config_bridge)
            flip_x: Whether to flip the x coordinates during reconstruction
            flip_y: Whether to flip the y coordinates during reconstruction
            transpose: Whether to transpose the image by swapping dimensions
            M: Parameter for reassemble_position function (default: 20)
            do_stitching: Whether to perform image stitching after training
            execution_config: Optional PyTorchExecutionConfig for runtime control (accelerator,
                             num_workers, learning_rate, scheduler, logger, checkpointing).
                             See CONFIG-002, CONFIG-LOGGER-001.
    
        Returns:
            Tuple containing:
            - reconstructed amplitude (or None if stitching disabled)
            - reconstructed phase (or None if stitching disabled)
            - results dictionary (training history, containers, metrics)
    
        Raises:
            NotImplementedError: Phase D2.B/C not yet implemented (scaffold only)
    
        Phase D2.A Scaffold Status:
            - Entry signature:  COMPLETE (matches TensorFlow)
            - update_legacy_dict call:  COMPLETE (CONFIG-001 compliance)
            - Placeholder logic:  COMPLETE (raises NotImplementedError)
            - Torch-optional:  COMPLETE (importable without torch)
    
        Phase D2.B/C TODO:
            - Implement train_cdi_model_torch delegation (Lightning trainer orchestration)
            - Implement reassemble_cdi_image_torch (optional stitching path)
            - Add MLflow disable flag handling
            - Validate deterministic seeds from config
    
        Example (Post D2.B/C):
            >>> from ptycho_torch.workflows.components import run_cdi_example_torch
            >>> from ptycho.config.config import TrainingConfig, ModelConfig
            >>> from ptycho.raw_data import RawData
            >>>
            >>> # Load data
            >>> train_data = RawData.from_file("train.npz")
            >>> config = TrainingConfig(model=ModelConfig(N=64), ...)
            >>>
            >>> # Execute PyTorch pipeline
            >>> amp, phase, results = run_cdi_example_torch(
            ...     train_data, None, config, do_stitching=False
            ... )
        """
        # CRITICAL: Update params.cfg before delegating (CONFIG-001 compliance)
        # This ensures legacy modules invoked downstream observe correct configuration state
        ptycho_config.update_legacy_dict(params.cfg, config)
        logger.info("PyTorch workflow: params.cfg synchronized with TrainingConfig")
    
        # Step 1: Train the model (Phase D2.B  delegates to Lightning trainer stub)
        logger.info("Invoking PyTorch training orchestration via train_cdi_model_torch")
        # Note: train_cdi_model_torch will need to be updated to accept execution_config
        # For now, we pass it as a keyword argument for forward compatibility
>       train_results = train_cdi_model_torch(train_data, test_data, config, execution_config=execution_config)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: TestWorkflowsComponentsRun.test_run_cdi_example_invokes_training.<locals>.mock_train_cdi_model_torch() got an unexpected keyword argument 'execution_config'

ptycho_torch/workflows/components.py:161: TypeError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (10, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (10,)
xcoords shape: (10,)
ycoords shape: (10,)
xcoords_start shape: (10,)
ycoords_start shape: (10,)
_______ TestWorkflowsComponentsRun.test_run_cdi_example_persists_models ________

self = <test_workflows_components.TestWorkflowsComponentsRun object at 0x74d7767a5190>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x74d76a206a90>
tmp_path = PosixPath('/tmp/pytest-of-ollie/pytest-705/test_run_cdi_example_persists_0')
params_cfg_snapshot = {'N': 64, 'amp_activation': 'sigmoid', 'backend': 'tensorflow', 'batch_size': 16, ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj...ensity_scale_trainable=True, output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x74d76a3cdc50>

    def test_run_cdi_example_persists_models(
        self,
        monkeypatch,
        tmp_path,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data
    ):
        """
        REGRESSION TEST: run_cdi_example_torch must persist models when config.output_dir set.
    
        Requirement: Phase D4.B2  validate PyTorch orchestration maintains persistence
        parity with TensorFlow baseline per specs/ptychodus_api_spec.md:4.6.
    
        TensorFlow baseline (ptycho/workflows/components.py:709-723):
        - When config.output_dir is provided, calls save_model() or ModelManager.save()
        - Produces wts.h5.zip archive in output_dir with dual-model bundle
        - Persistence happens after training completes successfully
    
        Red-phase expectation:
        - run_cdi_example_torch currently does NOT call save_torch_bundle
        - Once Phase D4.C1 complete, SHOULD invoke save_torch_bundle when output_dir set
        - Test will FAIL until orchestration wiring is complete
    
        Test mechanism:
        - Monkeypatch save_torch_bundle to spy on invocation
        - Set config.output_dir to tmp_path
        - Call run_cdi_example_torch
        - Validate save_torch_bundle was called with correct models dict + base_path
        """
        # Import the module under test
        from ptycho_torch.workflows import components as torch_components
        from ptycho.config.config import TrainingConfig, ModelConfig
    
        # Spy flag to track save_torch_bundle invocation
        save_torch_bundle_called = {"called": False, "args": None, "kwargs": None}
    
        def mock_save_torch_bundle(models_dict, base_path, config, **kwargs):
            """Spy that records save_torch_bundle invocation."""
            save_torch_bundle_called["called"] = True
            save_torch_bundle_called["args"] = (models_dict, base_path, config)
            save_torch_bundle_called["kwargs"] = kwargs
    
        # Monkeypatch save_torch_bundle
        monkeypatch.setattr(
            "ptycho_torch.workflows.components.save_torch_bundle",
            mock_save_torch_bundle
        )
    
        # Monkeypatch train_cdi_model_torch to return minimal results with models
        def mock_train_cdi_model_torch(train_data, test_data, config):
            """Return stub results including models dict for persistence."""
            return {
                "history": {"train_loss": [0.5, 0.3]},
                "train_container": {"sentinel": "train"},
                "test_container": None,
                "models": {
                    'autoencoder': {'_sentinel': 'trained_autoencoder'},
                    'diffraction_to_obj': {'_sentinel': 'trained_diffraction'},
                },
            }
    
        monkeypatch.setattr(
            "ptycho_torch.workflows.components.train_cdi_model_torch",
            mock_train_cdi_model_torch
        )
    
        # Create config with output_dir set
        model_config = ModelConfig(N=64, gridsize=2, model_type='pinn')
        config_with_output = TrainingConfig(
            model=model_config,
            train_data_file=Path("/tmp/dummy_train.npz"),
            test_data_file=Path("/tmp/dummy_test.npz"),
            n_groups=10,
            neighbor_count=4,
            nphotons=1e9,
            output_dir=tmp_path,  # Enable persistence
        )
    
        # Call run_cdi_example_torch
>       recon_amp, recon_phase, results = torch_components.run_cdi_example_torch(
            train_data=dummy_raw_data,
            test_data=None,
            config=config_with_output,
            flip_x=False,
            flip_y=False,
            transpose=False,
            M=20,
            do_stitching=False,
        )

tests/torch/test_workflows_components.py:1036: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

train_data = <ptycho.raw_data.RawData object at 0x74d76a3cdc50>
test_data = None
config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj...th('/tmp/pytest-of-ollie/pytest-705/test_run_cdi_example_persists_0'), sequential_sampling=False, backend='tensorflow')
flip_x = False, flip_y = False, transpose = False, M = 20, do_stitching = False
execution_config = None

    def run_cdi_example_torch(
        train_data: Union[RawData, 'RawDataTorch', 'PtychoDataContainerTorch'],
        test_data: Optional[Union[RawData, 'RawDataTorch', 'PtychoDataContainerTorch']],
        config: TrainingConfig,
        flip_x: bool = False,
        flip_y: bool = False,
        transpose: bool = False,
        M: int = 20,
        do_stitching: bool = False,
        execution_config: Optional[Any] = None
    ) -> Tuple[Optional[Any], Optional[Any], Dict[str, Any]]:
        """
        Run the main CDI example execution flow using PyTorch backend.
    
        This function provides API parity with ptycho.workflows.components.run_cdi_example,
        enabling transparent backend selection from Ptychodus per specs/ptychodus_api_spec.md 4.5.
    
        CRITICAL: This function MUST call update_legacy_dict(params.cfg, config) before
        delegating to core modules to prevent CONFIG-001 violations (empty params.cfg
        causing silent shape mismatches downstream).
    
        Args:
            train_data: Training data (RawData, RawDataTorch, or PtychoDataContainerTorch)
            test_data: Optional test data (same type constraints as train_data)
            config: TrainingConfig instance (TensorFlow dataclass, translated via config_bridge)
            flip_x: Whether to flip the x coordinates during reconstruction
            flip_y: Whether to flip the y coordinates during reconstruction
            transpose: Whether to transpose the image by swapping dimensions
            M: Parameter for reassemble_position function (default: 20)
            do_stitching: Whether to perform image stitching after training
            execution_config: Optional PyTorchExecutionConfig for runtime control (accelerator,
                             num_workers, learning_rate, scheduler, logger, checkpointing).
                             See CONFIG-002, CONFIG-LOGGER-001.
    
        Returns:
            Tuple containing:
            - reconstructed amplitude (or None if stitching disabled)
            - reconstructed phase (or None if stitching disabled)
            - results dictionary (training history, containers, metrics)
    
        Raises:
            NotImplementedError: Phase D2.B/C not yet implemented (scaffold only)
    
        Phase D2.A Scaffold Status:
            - Entry signature:  COMPLETE (matches TensorFlow)
            - update_legacy_dict call:  COMPLETE (CONFIG-001 compliance)
            - Placeholder logic:  COMPLETE (raises NotImplementedError)
            - Torch-optional:  COMPLETE (importable without torch)
    
        Phase D2.B/C TODO:
            - Implement train_cdi_model_torch delegation (Lightning trainer orchestration)
            - Implement reassemble_cdi_image_torch (optional stitching path)
            - Add MLflow disable flag handling
            - Validate deterministic seeds from config
    
        Example (Post D2.B/C):
            >>> from ptycho_torch.workflows.components import run_cdi_example_torch
            >>> from ptycho.config.config import TrainingConfig, ModelConfig
            >>> from ptycho.raw_data import RawData
            >>>
            >>> # Load data
            >>> train_data = RawData.from_file("train.npz")
            >>> config = TrainingConfig(model=ModelConfig(N=64), ...)
            >>>
            >>> # Execute PyTorch pipeline
            >>> amp, phase, results = run_cdi_example_torch(
            ...     train_data, None, config, do_stitching=False
            ... )
        """
        # CRITICAL: Update params.cfg before delegating (CONFIG-001 compliance)
        # This ensures legacy modules invoked downstream observe correct configuration state
        ptycho_config.update_legacy_dict(params.cfg, config)
        logger.info("PyTorch workflow: params.cfg synchronized with TrainingConfig")
    
        # Step 1: Train the model (Phase D2.B  delegates to Lightning trainer stub)
        logger.info("Invoking PyTorch training orchestration via train_cdi_model_torch")
        # Note: train_cdi_model_torch will need to be updated to accept execution_config
        # For now, we pass it as a keyword argument for forward compatibility
>       train_results = train_cdi_model_torch(train_data, test_data, config, execution_config=execution_config)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: TestWorkflowsComponentsRun.test_run_cdi_example_persists_models.<locals>.mock_train_cdi_model_torch() got an unexpected keyword argument 'execution_config'

ptycho_torch/workflows/components.py:161: TypeError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (10, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (10,)
xcoords shape: (10,)
ycoords shape: (10,)
xcoords_start shape: (10,)
ycoords_start shape: (10,)
_____ TestTrainWithLightningRed.test_train_with_lightning_runs_trainer_fit _____

self = <test_workflows_components.TestTrainWithLightningRed object at 0x74d7767a6910>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x74d76a204d90>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'sigmoid', 'backend': 'pytorch', 'batch_size': 16, ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj...ensity_scale_trainable=True, output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x74d76a206310>

    def test_train_with_lightning_runs_trainer_fit(
        self,
        monkeypatch,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data
    ):
        """
        RED TEST 2: _train_with_lightning MUST invoke Trainer.fit with dataloaders.
    
        Requirement: docs/workflows/pytorch.md 5 Lightning trainer expectations
        require Trainer.fit orchestration with train/val dataloaders.
    
        Design contract (phase_b_test_design.md 2):
        - _train_with_lightning MUST construct lightning.pytorch.Trainer
        - MUST invoke trainer.fit(module, train_dataloader, val_dataloader)
        - Dataloaders MUST be derived from provided train/test containers
        - Validation dataloader is None when test_container is None
    
        Test mechanism:
        - Monkeypatch Trainer constructor to return stub exposing fit_called flag
        - Monkeypatch dataloader builders (future helpers) with sentinels
        - Invoke _train_with_lightning
        - Assert Trainer.fit was called with correct dataloaders
    
        Expected red-phase failure:
        - Stub never constructs Trainer or calls fit
        - fit_called flag remains False  assertion fails
        """
        from ptycho_torch.workflows import components as torch_components
    
        # Spy to track Trainer.fit invocation
        trainer_fit_called = {"called": False, "args": None, "kwargs": None}
    
        class MockTrainer:
            """Stub Trainer that records fit() calls."""
            def fit(self, module, train_dataloaders=None, val_dataloaders=None, **kwargs):
                trainer_fit_called["called"] = True
                trainer_fit_called["args"] = (module, train_dataloaders, val_dataloaders)
                trainer_fit_called["kwargs"] = kwargs
    
        # Monkeypatch Lightning Trainer
        monkeypatch.setattr(
            "lightning.pytorch.Trainer",
            lambda **kwargs: MockTrainer()
        )
    
        # Monkeypatch Lightning module to prevent import errors
        class StubLightningModule:
            def save_hyperparameters(self):
                pass
    
        monkeypatch.setattr(
            "ptycho_torch.model.PtychoPINN_Lightning",
            lambda *args, **kwargs: StubLightningModule()
        )
    
        # Create sentinel dataloaders (Phase B2 will wire real loader builders)
        sentinel_train_loader = {"_sentinel": "train_dataloader"}
        sentinel_val_loader = None  # test_container is None
    
        # Monkeypatch future dataloader builder helper
        # (Phase B2 will add _build_lightning_dataloaders or similar)
        def mock_build_dataloaders(container, config, shuffle=True):
            """Sentinel that returns mock dataloader."""
            if container is not None:
                return sentinel_train_loader
            return None
    
        # For red phase, assume _train_with_lightning will eventually call helper
        # For now, test just validates fit() invocation pattern
    
        # Create minimal containers
        train_container = {"X": np.ones((10, 64, 64))}
    
        # Call _train_with_lightning
>       results = torch_components._train_with_lightning(
            train_container=train_container,
            test_container=None,
            config=minimal_training_config
        )

tests/torch/test_workflows_components.py:1433: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

train_container = {'X': array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., ...        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.]]])}
test_container = None
config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj...ensity_scale_trainable=True, output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow')
execution_config = PyTorchExecutionConfig(accelerator='cuda', strategy='auto', deterministic=True, gradient_clip_val=None, accum_steps=1,...nt_mode='min', early_stop_patience=100, logger_backend='csv', inference_batch_size=None, middle_trim=0, pad_eval=False)

    def _train_with_lightning(
        train_container: 'PtychoDataContainerTorch',
        test_container: Optional['PtychoDataContainerTorch'],
        config: TrainingConfig,
        execution_config: Optional['PyTorchExecutionConfig'] = None
    ) -> Dict[str, Any]:
        """
        Orchestrate Lightning trainer execution for PyTorch model training.
    
        This function implements the Lightning training workflow per Phase D2.B blueprint:
        1. Derives PyTorch config objects from TensorFlow TrainingConfig
        2. Instantiates PtychoPINN_Lightning module with all four config dependencies
        3. Builds train/val dataloaders via _build_lightning_dataloaders helper
        4. Configures Lightning Trainer with checkpoint/logging settings (ADR-003 Phase C3)
        5. Executes training via trainer.fit()
        6. Returns structured results dict with history, containers, and module handle
    
        Args:
            train_container: Normalized training data container
            test_container: Optional normalized test data container
            config: TrainingConfig with training hyperparameters
            execution_config: Optional PyTorchExecutionConfig with runtime knobs (Phase C3.A2)
    
        Returns:
            Dict[str, Any]: Training results including:
                - history: Dict with train_loss and optional val_loss trajectories
                - train_container: Original training container
                - test_container: Original test container
                - models: Dict with 'diffraction_to_obj' (Lightning module) and 'autoencoder' (sentinel)
                          for dual-model bundle persistence per spec 4.6
    
        Raises:
            RuntimeError: If torch or lightning packages are not installed (POLICY-001)
    
        References:
            - Blueprint: plans/active/INTEGRATE-PYTORCH-001/reports/2025-10-18T020940Z/phase_d2_completion/phase_b2_implementation.md
            - Spec: specs/ptychodus_api_spec.md:187 (reconstructor lifecycle contract)
            - Findings: POLICY-001 (PyTorch mandatory), CONFIG-001 (params.cfg already populated by caller)
            - ADR-003 Phase C3: execution_config controls Trainer kwargs (accelerator, deterministic, gradient_clip_val)
        """
        # B2.2: torch-optional imports with POLICY-001 compliant error messaging
        try:
            import torch
            import lightning.pytorch as L
            from ptycho_torch.model import PtychoPINN_Lightning
            from ptycho_torch.config_params import (
                DataConfig as PTDataConfig,
                ModelConfig as PTModelConfig,
                TrainingConfig as PTTrainingConfig,
                InferenceConfig as PTInferenceConfig
            )
        except ImportError as e:
            raise RuntimeError(
                "PyTorch backend requires torch>=2.2 and lightning. "
                "Install with: pip install -e .[torch]\n"
                "See docs/findings.md#policy-001 for PyTorch requirement policy."
            ) from e
    
        logger.info("_train_with_lightning orchestrating Lightning training")
        logger.info(f"Training config: nepochs={config.nepochs}, n_groups={config.n_groups}")
    
        # B2.1: Use config_factory to derive PyTorch configs with correct channel propagation
        # CRITICAL (Phase C4.D B2): Factory ensures C = gridsize**2 is propagated to
        # pt_model_config.C_model and pt_model_config.C_forward, preventing channel mismatch
        # when gridsize > 1 (see docs/findings.md#BUG-TF-001).
        from ptycho_torch.config_factory import create_training_payload
    
        # Build factory overrides from TrainingConfig fields
        # Factory requires n_groups in overrides dict; train_data_file and output_dir as positional
        # Note: Factory expects model_type in PyTorch naming ('Unsupervised'/'Supervised')
        #       but TrainingConfig uses TensorFlow naming ('pinn'/'supervised')
        mode_map = {'pinn': 'Unsupervised', 'supervised': 'Supervised'}
        factory_overrides = {
            'n_groups': config.n_groups,  # Required by factory validation
            'gridsize': config.model.gridsize,
            'model_type': mode_map.get(config.model.model_type, 'Unsupervised'),
            'amp_activation': config.model.amp_activation,
            'n_filters_scale': config.model.n_filters_scale,
            'nphotons': config.nphotons,
            'neighbor_count': config.neighbor_count,
            'max_epochs': config.nepochs,
            'batch_size': getattr(config, 'batch_size', 16),
        }
    
        # Create payload with factory-derived PyTorch configs
        payload = create_training_payload(
            train_data_file=Path(config.train_data_file),
            output_dir=Path(getattr(config, 'output_dir', './outputs')),
            execution_config=execution_config,  # Pass through from caller
            overrides=factory_overrides
        )
    
        # Extract PyTorch configs from payload (gridsize  C propagation already applied)
        pt_data_config = payload.pt_data_config
        pt_model_config = payload.pt_model_config
        pt_training_config = payload.pt_training_config
    
        # CRITICAL: Supervised mode REQUIRES a compatible loss function (MAE)
        # The Lightning module expects loss_name to be defined, which only happens when:
        #   1. mode='Unsupervised' AND loss_function='Poisson'  sets loss_name='poisson_train'
        #   2. mode='Unsupervised' AND loss_function='MAE'  sets loss_name='mae_train'
        #   3. mode='Supervised' AND loss_function='MAE'  sets loss_name='mae_train'
        # Without this override, supervised mode with default loss_function='Poisson' causes
        # AttributeError: 'PtychoPINN_Lightning' object has no attribute 'loss_name'
        # See: ptycho_torch/model.py:1052-1066
        if pt_model_config.mode == 'Supervised' and pt_model_config.loss_function != 'MAE':
            logger.info(
                f"Backend override: supervised mode requires MAE loss "
                f"(was {pt_model_config.loss_function}), forcing loss_function='MAE'"
            )
            # Create new ModelConfig with corrected loss_function
            from dataclasses import replace
            pt_model_config = replace(pt_model_config, loss_function='MAE')
    
        # Create minimal InferenceConfig for Lightning module (training payload doesn't include it)
        pt_inference_config = PTInferenceConfig()
    
        # B2.4: Instantiate PtychoPINN_Lightning with factory-derived config objects
        model = PtychoPINN_Lightning(
            model_config=pt_model_config,
            data_config=pt_data_config,
            training_config=pt_training_config,
            inference_config=pt_inference_config
        )
    
        # Save hyperparameters so checkpoint can reconstruct module without external state
        model.save_hyperparameters()
    
        # B2.3: Build dataloaders via helper
        train_loader, val_loader = _build_lightning_dataloaders(
            train_container, test_container, config
        )
    
        # DATA-SUP-001: Supervised mode requires labeled data
        # Check if supervised mode is requested but training data lacks required labels
        if pt_model_config.mode == 'Supervised':
            # Inspect first batch to verify label keys exist
            try:
                first_batch = next(iter(train_loader))
                batch_dict = first_batch[0]  # Extract tensor dict from batch tuple
                if 'label_amp' not in batch_dict or 'label_phase' not in batch_dict:
                    raise RuntimeError(
                        f"Supervised mode (model_type='supervised') requires labeled datasets with "
                        f"'label_amp' and 'label_phase' keys, but training data lacks these fields. "
                        f"Either: (1) Use a labeled NPZ dataset (see ptycho_torch/notebooks/create_supervised_datasets.ipynb), "
                        f"or (2) Switch to PINN mode (--model_type pinn) for self-supervised physics-based training. "
                        f"See DATA-SUP-001 in docs/findings.md for details."
                    )
            except StopIteration:
                raise RuntimeError(
                    f"Training dataloader is empty. Check dataset path and n_groups configuration."
                )
    
        # B2.5: Configure Trainer with settings from config
        # C3.A3: Thread execution config values to Trainer kwargs
        output_dir = Path(getattr(config, 'output_dir', './outputs'))
        debug_mode = getattr(config, 'debug', False)
    
        # Import execution config defaults if not provided
        if execution_config is None:
            from ptycho.config.config import PyTorchExecutionConfig
            execution_config = PyTorchExecutionConfig()
            logger.info(f"PyTorchExecutionConfig auto-instantiated for Lightning training (accelerator resolved to '{execution_config.accelerator}')")
    
        # EB1.D: Configure checkpoint/early-stop callbacks (ADR-003 Phase EB1)
        callbacks = []
        if execution_config.enable_checkpointing:
            from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping
    
            # Determine if we have validation data to use val metrics
            has_validation = test_container is not None
    
            # EB2.B: Derive monitor metric from model.val_loss_name (ADR-003 Phase EB2)
            # The model's val_loss_name is dynamically constructed based on model_type and loss configuration
            # (e.g., 'poisson_val_Amp_loss' for PINN with amplitude loss, 'mae_val_Phase_loss' for supervised)
            # This ensures checkpoint/early-stop callbacks watch the correct logged metric
            if has_validation and hasattr(model, 'val_loss_name'):
                # Use the model's dynamic validation loss name
                monitor_metric = model.val_loss_name
            else:
                # Fall back to execution config default or train loss
                monitor_metric = execution_config.checkpoint_monitor_metric
                if 'val_' in monitor_metric and not has_validation:
                    # Fall back to train_loss if val metric requested but no validation data
                    monitor_metric = monitor_metric.replace('val_', 'train_')
    
            # Build checkpoint filename template using dynamic metric name
            # Format: epoch={epoch:02d}-<metric_short_name>={<full_metric_name>:.4f}
            if has_validation:
                # Extract short name for filename (remove '_loss' suffix if present)
                metric_short_name = monitor_metric.replace('_loss', '')
                filename_template = f'epoch={{epoch:02d}}-{metric_short_name}={{{monitor_metric}:.4f}}'
            else:
                filename_template = 'epoch={epoch:02d}'
    
            checkpoint_callback = ModelCheckpoint(
                dirpath=str(output_dir / "checkpoints"),
                filename=filename_template,
                monitor=monitor_metric,
                mode=execution_config.checkpoint_mode,
                save_top_k=execution_config.checkpoint_save_top_k,
                save_last=True,  # Always keep last checkpoint for recovery
                verbose=False,
            )
            callbacks.append(checkpoint_callback)
    
            # EarlyStopping callback (ADR-003 Phase EB1.D)
            # Only add early stopping if validation data is available (otherwise no metric to monitor)
            if has_validation:
                early_stop_callback = EarlyStopping(
                    monitor=monitor_metric,
                    mode=execution_config.checkpoint_mode,
                    patience=execution_config.early_stop_patience,
                    verbose=False,
                )
                callbacks.append(early_stop_callback)
    
        # Instantiate logger based on execution config (Phase EB3.B - ADR-003)
        lightning_logger = False  # Default: no logger
        if execution_config.logger_backend is not None:
            try:
                if execution_config.logger_backend == 'csv':
                    from lightning.pytorch.loggers import CSVLogger
                    lightning_logger = CSVLogger(
                        save_dir=str(output_dir),
                        name='lightning_logs',
                    )
                    logger.info(f"Enabled CSVLogger: metrics saved to {output_dir}/lightning_logs/")
                elif execution_config.logger_backend == 'tensorboard':
                    from lightning.pytorch.loggers import TensorBoardLogger
                    lightning_logger = TensorBoardLogger(
                        save_dir=str(output_dir),
                        name='lightning_logs',
                    )
                    logger.info(f"Enabled TensorBoardLogger: run `tensorboard --logdir={output_dir}/lightning_logs/`")
                elif execution_config.logger_backend == 'mlflow':
                    from lightning.pytorch.loggers import MLFlowLogger
                    lightning_logger = MLFlowLogger(
                        experiment_name=getattr(config, 'experiment_name', 'PtychoPINN'),
                        tracking_uri=str(output_dir / 'mlruns'),
                    )
                    logger.info(f"Enabled MLFlowLogger: tracking URI={output_dir}/mlruns")
                else:
                    logger.warning(
                        f"Unknown logger_backend '{execution_config.logger_backend}'. "
                        f"Falling back to logger=False. Supported: 'csv', 'tensorboard', 'mlflow'."
                    )
            except ImportError as e:
                logger.warning(
                    f"Failed to import Lightning logger '{execution_config.logger_backend}': {e}. "
                    f"Metrics logging disabled. Install the required package to enable logging."
                )
                lightning_logger = False
        else:
            logger.info("Logger disabled (logger_backend=None). Loss metrics will not be saved to disk.")
    
        # EXEC-ACCUM-001: Guard against manual optimization + gradient accumulation
        # Lightning's manual optimization (automatic_optimization=False) is incompatible with
        # Trainer(accumulate_grad_batches>1). The PtychoPINN_Lightning module uses manual optimization
        # for custom physics loss integration, so gradient accumulation must be disabled.
>       if not model.automatic_optimization and execution_config.accum_steps > 1:
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'StubLightningModule' object has no attribute 'automatic_optimization'

ptycho_torch/workflows/components.py:831: AttributeError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (10, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (10,)
xcoords shape: (10,)
ycoords shape: (10,)
xcoords_start shape: (10,)
ycoords_start shape: (10,)
----------------------------- Captured stderr call -----------------------------
INFO: Seed set to 42
------------------------------ Captured log call -------------------------------
INFO     lightning.fabric.utilities.seed:seed.py:57 Seed set to 42
___ TestTrainWithLightningRed.test_train_with_lightning_returns_models_dict ____

self = <test_workflows_components.TestTrainWithLightningRed object at 0x74d7767a6fd0>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x74d76a23a550>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'sigmoid', 'backend': 'pytorch', 'batch_size': 16, ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj...ensity_scale_trainable=True, output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x74d76a23b9d0>

    def test_train_with_lightning_returns_models_dict(
        self,
        monkeypatch,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data
    ):
        """
        RED TEST 3: _train_with_lightning MUST return results dict with 'models' key.
    
        Requirement: Phase D4 persistence tests require trained module handles
        for save_torch_bundle orchestration (mirrors TensorFlow train_cdi_model).
    
        Design contract (phase_b_test_design.md 3):
        - _train_with_lightning returns Dict[str, Any]
        - Results dict MUST contain 'models' key
        - models['lightning_module'] (or models['diffraction_to_obj']) MUST point to trained module
        - This enables downstream save_torch_bundle to persist checkpoint
    
        Test mechanism:
        - Monkeypatch Lightning components to return stub module
        - Invoke _train_with_lightning
        - Assert results dict contains 'models' key with module handle
    
        Expected red-phase failure:
        - Stub returns only history/containers  missing 'models' key
        - Assertion fails
        """
        from ptycho_torch.workflows import components as torch_components
    
        # Stub Lightning module with sentinel identity
        class StubLightningModule:
            def save_hyperparameters(self):
                pass
    
            _sentinel = "trained_lightning_module"
    
        stub_module = StubLightningModule()
    
        # Monkeypatch Lightning module constructor
        monkeypatch.setattr(
            "ptycho_torch.model.PtychoPINN_Lightning",
            lambda *args, **kwargs: stub_module
        )
    
        # Monkeypatch Trainer to skip actual training
        class MockTrainer:
            def fit(self, module, train_dataloaders=None, val_dataloaders=None, **kwargs):
                pass
    
        monkeypatch.setattr(
            "lightning.pytorch.Trainer",
            lambda **kwargs: MockTrainer()
        )
    
        # Create minimal containers
        train_container = {"X": np.ones((10, 64, 64))}
    
        # Call _train_with_lightning
>       results = torch_components._train_with_lightning(
            train_container=train_container,
            test_container=None,
            config=minimal_training_config
        )

tests/torch/test_workflows_components.py:1513: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

train_container = {'X': array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., ...        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.]]])}
test_container = None
config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj...ensity_scale_trainable=True, output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow')
execution_config = PyTorchExecutionConfig(accelerator='cuda', strategy='auto', deterministic=True, gradient_clip_val=None, accum_steps=1,...nt_mode='min', early_stop_patience=100, logger_backend='csv', inference_batch_size=None, middle_trim=0, pad_eval=False)

    def _train_with_lightning(
        train_container: 'PtychoDataContainerTorch',
        test_container: Optional['PtychoDataContainerTorch'],
        config: TrainingConfig,
        execution_config: Optional['PyTorchExecutionConfig'] = None
    ) -> Dict[str, Any]:
        """
        Orchestrate Lightning trainer execution for PyTorch model training.
    
        This function implements the Lightning training workflow per Phase D2.B blueprint:
        1. Derives PyTorch config objects from TensorFlow TrainingConfig
        2. Instantiates PtychoPINN_Lightning module with all four config dependencies
        3. Builds train/val dataloaders via _build_lightning_dataloaders helper
        4. Configures Lightning Trainer with checkpoint/logging settings (ADR-003 Phase C3)
        5. Executes training via trainer.fit()
        6. Returns structured results dict with history, containers, and module handle
    
        Args:
            train_container: Normalized training data container
            test_container: Optional normalized test data container
            config: TrainingConfig with training hyperparameters
            execution_config: Optional PyTorchExecutionConfig with runtime knobs (Phase C3.A2)
    
        Returns:
            Dict[str, Any]: Training results including:
                - history: Dict with train_loss and optional val_loss trajectories
                - train_container: Original training container
                - test_container: Original test container
                - models: Dict with 'diffraction_to_obj' (Lightning module) and 'autoencoder' (sentinel)
                          for dual-model bundle persistence per spec 4.6
    
        Raises:
            RuntimeError: If torch or lightning packages are not installed (POLICY-001)
    
        References:
            - Blueprint: plans/active/INTEGRATE-PYTORCH-001/reports/2025-10-18T020940Z/phase_d2_completion/phase_b2_implementation.md
            - Spec: specs/ptychodus_api_spec.md:187 (reconstructor lifecycle contract)
            - Findings: POLICY-001 (PyTorch mandatory), CONFIG-001 (params.cfg already populated by caller)
            - ADR-003 Phase C3: execution_config controls Trainer kwargs (accelerator, deterministic, gradient_clip_val)
        """
        # B2.2: torch-optional imports with POLICY-001 compliant error messaging
        try:
            import torch
            import lightning.pytorch as L
            from ptycho_torch.model import PtychoPINN_Lightning
            from ptycho_torch.config_params import (
                DataConfig as PTDataConfig,
                ModelConfig as PTModelConfig,
                TrainingConfig as PTTrainingConfig,
                InferenceConfig as PTInferenceConfig
            )
        except ImportError as e:
            raise RuntimeError(
                "PyTorch backend requires torch>=2.2 and lightning. "
                "Install with: pip install -e .[torch]\n"
                "See docs/findings.md#policy-001 for PyTorch requirement policy."
            ) from e
    
        logger.info("_train_with_lightning orchestrating Lightning training")
        logger.info(f"Training config: nepochs={config.nepochs}, n_groups={config.n_groups}")
    
        # B2.1: Use config_factory to derive PyTorch configs with correct channel propagation
        # CRITICAL (Phase C4.D B2): Factory ensures C = gridsize**2 is propagated to
        # pt_model_config.C_model and pt_model_config.C_forward, preventing channel mismatch
        # when gridsize > 1 (see docs/findings.md#BUG-TF-001).
        from ptycho_torch.config_factory import create_training_payload
    
        # Build factory overrides from TrainingConfig fields
        # Factory requires n_groups in overrides dict; train_data_file and output_dir as positional
        # Note: Factory expects model_type in PyTorch naming ('Unsupervised'/'Supervised')
        #       but TrainingConfig uses TensorFlow naming ('pinn'/'supervised')
        mode_map = {'pinn': 'Unsupervised', 'supervised': 'Supervised'}
        factory_overrides = {
            'n_groups': config.n_groups,  # Required by factory validation
            'gridsize': config.model.gridsize,
            'model_type': mode_map.get(config.model.model_type, 'Unsupervised'),
            'amp_activation': config.model.amp_activation,
            'n_filters_scale': config.model.n_filters_scale,
            'nphotons': config.nphotons,
            'neighbor_count': config.neighbor_count,
            'max_epochs': config.nepochs,
            'batch_size': getattr(config, 'batch_size', 16),
        }
    
        # Create payload with factory-derived PyTorch configs
        payload = create_training_payload(
            train_data_file=Path(config.train_data_file),
            output_dir=Path(getattr(config, 'output_dir', './outputs')),
            execution_config=execution_config,  # Pass through from caller
            overrides=factory_overrides
        )
    
        # Extract PyTorch configs from payload (gridsize  C propagation already applied)
        pt_data_config = payload.pt_data_config
        pt_model_config = payload.pt_model_config
        pt_training_config = payload.pt_training_config
    
        # CRITICAL: Supervised mode REQUIRES a compatible loss function (MAE)
        # The Lightning module expects loss_name to be defined, which only happens when:
        #   1. mode='Unsupervised' AND loss_function='Poisson'  sets loss_name='poisson_train'
        #   2. mode='Unsupervised' AND loss_function='MAE'  sets loss_name='mae_train'
        #   3. mode='Supervised' AND loss_function='MAE'  sets loss_name='mae_train'
        # Without this override, supervised mode with default loss_function='Poisson' causes
        # AttributeError: 'PtychoPINN_Lightning' object has no attribute 'loss_name'
        # See: ptycho_torch/model.py:1052-1066
        if pt_model_config.mode == 'Supervised' and pt_model_config.loss_function != 'MAE':
            logger.info(
                f"Backend override: supervised mode requires MAE loss "
                f"(was {pt_model_config.loss_function}), forcing loss_function='MAE'"
            )
            # Create new ModelConfig with corrected loss_function
            from dataclasses import replace
            pt_model_config = replace(pt_model_config, loss_function='MAE')
    
        # Create minimal InferenceConfig for Lightning module (training payload doesn't include it)
        pt_inference_config = PTInferenceConfig()
    
        # B2.4: Instantiate PtychoPINN_Lightning with factory-derived config objects
        model = PtychoPINN_Lightning(
            model_config=pt_model_config,
            data_config=pt_data_config,
            training_config=pt_training_config,
            inference_config=pt_inference_config
        )
    
        # Save hyperparameters so checkpoint can reconstruct module without external state
        model.save_hyperparameters()
    
        # B2.3: Build dataloaders via helper
        train_loader, val_loader = _build_lightning_dataloaders(
            train_container, test_container, config
        )
    
        # DATA-SUP-001: Supervised mode requires labeled data
        # Check if supervised mode is requested but training data lacks required labels
        if pt_model_config.mode == 'Supervised':
            # Inspect first batch to verify label keys exist
            try:
                first_batch = next(iter(train_loader))
                batch_dict = first_batch[0]  # Extract tensor dict from batch tuple
                if 'label_amp' not in batch_dict or 'label_phase' not in batch_dict:
                    raise RuntimeError(
                        f"Supervised mode (model_type='supervised') requires labeled datasets with "
                        f"'label_amp' and 'label_phase' keys, but training data lacks these fields. "
                        f"Either: (1) Use a labeled NPZ dataset (see ptycho_torch/notebooks/create_supervised_datasets.ipynb), "
                        f"or (2) Switch to PINN mode (--model_type pinn) for self-supervised physics-based training. "
                        f"See DATA-SUP-001 in docs/findings.md for details."
                    )
            except StopIteration:
                raise RuntimeError(
                    f"Training dataloader is empty. Check dataset path and n_groups configuration."
                )
    
        # B2.5: Configure Trainer with settings from config
        # C3.A3: Thread execution config values to Trainer kwargs
        output_dir = Path(getattr(config, 'output_dir', './outputs'))
        debug_mode = getattr(config, 'debug', False)
    
        # Import execution config defaults if not provided
        if execution_config is None:
            from ptycho.config.config import PyTorchExecutionConfig
            execution_config = PyTorchExecutionConfig()
            logger.info(f"PyTorchExecutionConfig auto-instantiated for Lightning training (accelerator resolved to '{execution_config.accelerator}')")
    
        # EB1.D: Configure checkpoint/early-stop callbacks (ADR-003 Phase EB1)
        callbacks = []
        if execution_config.enable_checkpointing:
            from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping
    
            # Determine if we have validation data to use val metrics
            has_validation = test_container is not None
    
            # EB2.B: Derive monitor metric from model.val_loss_name (ADR-003 Phase EB2)
            # The model's val_loss_name is dynamically constructed based on model_type and loss configuration
            # (e.g., 'poisson_val_Amp_loss' for PINN with amplitude loss, 'mae_val_Phase_loss' for supervised)
            # This ensures checkpoint/early-stop callbacks watch the correct logged metric
            if has_validation and hasattr(model, 'val_loss_name'):
                # Use the model's dynamic validation loss name
                monitor_metric = model.val_loss_name
            else:
                # Fall back to execution config default or train loss
                monitor_metric = execution_config.checkpoint_monitor_metric
                if 'val_' in monitor_metric and not has_validation:
                    # Fall back to train_loss if val metric requested but no validation data
                    monitor_metric = monitor_metric.replace('val_', 'train_')
    
            # Build checkpoint filename template using dynamic metric name
            # Format: epoch={epoch:02d}-<metric_short_name>={<full_metric_name>:.4f}
            if has_validation:
                # Extract short name for filename (remove '_loss' suffix if present)
                metric_short_name = monitor_metric.replace('_loss', '')
                filename_template = f'epoch={{epoch:02d}}-{metric_short_name}={{{monitor_metric}:.4f}}'
            else:
                filename_template = 'epoch={epoch:02d}'
    
            checkpoint_callback = ModelCheckpoint(
                dirpath=str(output_dir / "checkpoints"),
                filename=filename_template,
                monitor=monitor_metric,
                mode=execution_config.checkpoint_mode,
                save_top_k=execution_config.checkpoint_save_top_k,
                save_last=True,  # Always keep last checkpoint for recovery
                verbose=False,
            )
            callbacks.append(checkpoint_callback)
    
            # EarlyStopping callback (ADR-003 Phase EB1.D)
            # Only add early stopping if validation data is available (otherwise no metric to monitor)
            if has_validation:
                early_stop_callback = EarlyStopping(
                    monitor=monitor_metric,
                    mode=execution_config.checkpoint_mode,
                    patience=execution_config.early_stop_patience,
                    verbose=False,
                )
                callbacks.append(early_stop_callback)
    
        # Instantiate logger based on execution config (Phase EB3.B - ADR-003)
        lightning_logger = False  # Default: no logger
        if execution_config.logger_backend is not None:
            try:
                if execution_config.logger_backend == 'csv':
                    from lightning.pytorch.loggers import CSVLogger
                    lightning_logger = CSVLogger(
                        save_dir=str(output_dir),
                        name='lightning_logs',
                    )
                    logger.info(f"Enabled CSVLogger: metrics saved to {output_dir}/lightning_logs/")
                elif execution_config.logger_backend == 'tensorboard':
                    from lightning.pytorch.loggers import TensorBoardLogger
                    lightning_logger = TensorBoardLogger(
                        save_dir=str(output_dir),
                        name='lightning_logs',
                    )
                    logger.info(f"Enabled TensorBoardLogger: run `tensorboard --logdir={output_dir}/lightning_logs/`")
                elif execution_config.logger_backend == 'mlflow':
                    from lightning.pytorch.loggers import MLFlowLogger
                    lightning_logger = MLFlowLogger(
                        experiment_name=getattr(config, 'experiment_name', 'PtychoPINN'),
                        tracking_uri=str(output_dir / 'mlruns'),
                    )
                    logger.info(f"Enabled MLFlowLogger: tracking URI={output_dir}/mlruns")
                else:
                    logger.warning(
                        f"Unknown logger_backend '{execution_config.logger_backend}'. "
                        f"Falling back to logger=False. Supported: 'csv', 'tensorboard', 'mlflow'."
                    )
            except ImportError as e:
                logger.warning(
                    f"Failed to import Lightning logger '{execution_config.logger_backend}': {e}. "
                    f"Metrics logging disabled. Install the required package to enable logging."
                )
                lightning_logger = False
        else:
            logger.info("Logger disabled (logger_backend=None). Loss metrics will not be saved to disk.")
    
        # EXEC-ACCUM-001: Guard against manual optimization + gradient accumulation
        # Lightning's manual optimization (automatic_optimization=False) is incompatible with
        # Trainer(accumulate_grad_batches>1). The PtychoPINN_Lightning module uses manual optimization
        # for custom physics loss integration, so gradient accumulation must be disabled.
>       if not model.automatic_optimization and execution_config.accum_steps > 1:
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'StubLightningModule' object has no attribute 'automatic_optimization'

ptycho_torch/workflows/components.py:831: AttributeError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (10, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (10,)
xcoords shape: (10,)
ycoords shape: (10,)
xcoords_start shape: (10,)
ycoords_start shape: (10,)
----------------------------- Captured stderr call -----------------------------
INFO: Seed set to 42
------------------------------ Captured log call -------------------------------
INFO     lightning.fabric.utilities.seed:seed.py:57 Seed set to 42
_ TestReassembleCdiImageTorchGreen.test_run_cdi_example_torch_do_stitching_delegates_to_reassemble _

self = <test_workflows_components.TestReassembleCdiImageTorchGreen object at 0x74d7767b09d0>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'silu', 'backend': 'tensorflow', 'batch_size': 2, ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=1, model_type='pinn', amp_activation='silu', object...th('/tmp/pytest-of-ollie/pytest-705/test_run_cdi_example_torch_do_0'), sequential_sampling=False, backend='tensorflow')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x74d768143b10>
stitch_train_results = {'history': {'train_loss': [0.1, 0.05]}, 'models': {'autoencoder': {'_sentinel': 'autoencoder'}, 'diffraction_to_obj': MockLightningModule()}}
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x74d76a4865d0>

    def test_run_cdi_example_torch_do_stitching_delegates_to_reassemble(
        self,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data,
        stitch_train_results,
        monkeypatch
    ):
        """
        GREEN TEST: run_cdi_example_torch(do_stitching=True) delegates to stitching path.
    
        Requirement: Phase D2.C workflow integration  ensure orchestration calls reassembly.
    
        TensorFlow baseline (ptycho/workflows/components.py:676-732):
        - run_cdi_example(..., do_stitching=True) invokes reassemble_cdi_image
        - Stitching runs AFTER training completes
        - Returns (recon_amp, recon_phase, results) when stitching enabled
        - Returns (None, None, results) when do_stitching=False
    
        Expected behavior:
        - Runs training (mocked), then calls _reassemble_cdi_image_torch with test_data
        - Stitching results populate amplitude/phase return values
        - Returns (recon_amp, recon_phase, results) when do_stitching=True
    
        Test mechanism:
        - Monkeypatch training path to return stitch_train_results fixture (avoid GPU)
        - Call run_cdi_example_torch with do_stitching=True
        - Assert amplitude/phase are returned (not None)
        - Validate outputs are numpy arrays
    
        Validation coverage:
        - Confirms orchestration wiring exists
        - Ensures stitching path is reachable from public API
        - Documents return value contract for downstream consumers (e.g., ptychodus)
        """
        from ptycho_torch.workflows import components as torch_components
        from ptycho.config.config import update_legacy_dict
        from ptycho import params
    
        # Bridge config (CONFIG-001)
        update_legacy_dict(params.cfg, minimal_training_config)
    
        # Monkeypatch _train_with_lightning to return mock results with Lightning module
        def mock_train_with_lightning(train_container, test_container, config):
            """Stub that returns train_results with mock Lightning module."""
            # Return the stitch_train_results fixture enriched with containers
            results = stitch_train_results.copy()
            results["containers"] = {"train": train_container, "test": test_container}
            return results
    
        monkeypatch.setattr(
            torch_components,
            "_train_with_lightning",
            mock_train_with_lightning
        )
    
        # GREEN PHASE VALIDATION: expect successful stitching
>       recon_amp, recon_phase, results = torch_components.run_cdi_example_torch(
            train_data=dummy_raw_data,
            test_data=dummy_raw_data,  # Use same data for test (deterministic)
            config=minimal_training_config,
            flip_x=False,
            flip_y=False,
            transpose=False,
            M=128,
            do_stitching=True,  # CRITICAL: enable stitching path
        )

tests/torch/test_workflows_components.py:1897: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ptycho_torch/workflows/components.py:161: in run_cdi_example_torch
    train_results = train_cdi_model_torch(train_data, test_data, config, execution_config=execution_config)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

train_data = <ptycho.raw_data.RawData object at 0x74d768143b10>
test_data = <ptycho.raw_data.RawData object at 0x74d768143b10>
config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=1, model_type='pinn', amp_activation='silu', object...th('/tmp/pytest-of-ollie/pytest-705/test_run_cdi_example_torch_do_0'), sequential_sampling=False, backend='tensorflow')
execution_config = None

    def train_cdi_model_torch(
        train_data: Union[RawData, 'RawDataTorch', 'PtychoDataContainerTorch'],
        test_data: Optional[Union[RawData, 'RawDataTorch', 'PtychoDataContainerTorch']],
        config: TrainingConfig,
        execution_config: Optional[Any] = None
    ) -> Dict[str, Any]:
        """
        Train the CDI model using PyTorch Lightning backend.
    
        This function provides API parity with ptycho.workflows.components.train_cdi_model,
        orchestrating data preparation, probe initialization, and Lightning trainer execution.
    
        Args:
            train_data: Training data (RawData, RawDataTorch, or PtychoDataContainerTorch)
            test_data: Optional test data for validation
            config: TrainingConfig instance (TensorFlow dataclass)
            execution_config: Optional PyTorchExecutionConfig for runtime control
    
        Returns:
            Dict[str, Any]: Results dictionary containing:
            - 'history': Training history (losses, metrics)
            - 'train_container': PtychoDataContainerTorch for training data
            - 'test_container': Optional PtychoDataContainerTorch for test data
            - Additional outputs from Lightning trainer
    
        Raises:
            ImportError: If Phase C adapters not available
            TypeError: If input data types are invalid
    
        Phase D2.B Status:
            - Entry signature:  COMPLETE (matches TensorFlow)
            - _ensure_container helper:  COMPLETE (normalizes inputs via Phase C adapters)
            - Lightning orchestration:  STUB (returns minimal dict, full impl pending)
            - Torch-optional:  COMPLETE (importable without torch)
    
        Example:
            >>> config = TrainingConfig(model=ModelConfig(N=64), nepochs=10, ...)
            >>> results = train_cdi_model_torch(train_data, test_data, config)
            >>> print(results['history']['train_loss'][-1])
        """
        # Step 1: Normalize train_data to PtychoDataContainerTorch
        logger.info("Normalizing training data via _ensure_container")
        train_container = _ensure_container(train_data, config)
    
        # Step 2: Normalize test_data if provided
        test_container = None
        if test_data is not None:
            logger.info("Normalizing test data via _ensure_container")
            test_container = _ensure_container(test_data, config)
    
        # Step 3: Initialize probe (TODO: implement probe handling for PyTorch)
        # TensorFlow baseline: probe.set_probe_guess(None, train_container.probe)
        # For Phase D2.B stub, skip probe initialization
        logger.debug("Probe initialization deferred to full Lightning implementation")
    
        # Step 4: Delegate to Lightning trainer
        logger.info("Delegating to Lightning trainer via _train_with_lightning")
>       results = _train_with_lightning(train_container, test_container, config, execution_config=execution_config)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: TestReassembleCdiImageTorchGreen.test_run_cdi_example_torch_do_stitching_delegates_to_reassemble.<locals>.mock_train_with_lightning() got an unexpected keyword argument 'execution_config'

ptycho_torch/workflows/components.py:1114: TypeError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (20, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (20,)
objectGuess shape: (128, 128)
xcoords shape: (20,)
ycoords shape: (20,)
xcoords_start shape: (20,)
ycoords_start shape: (20,)
----------------------------- Captured stdout call -----------------------------
diff3d shape: (20, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (20,)
objectGuess shape: (128, 128)
xcoords shape: (20,)
ycoords shape: (20,)
xcoords_start shape: (20,)
ycoords_start shape: (20,)
DEBUG: nsamples: 10, gridsize: 2 (using efficient random sample-then-group strategy)
INFO: 'Y' array not found. Generating ground truth patches from 'objectGuess' as a fallback.
neighbor-sampled diffraction shape (10, 64, 64, 4)
diff3d shape: (20, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (20,)
objectGuess shape: (128, 128)
xcoords shape: (20,)
ycoords shape: (20,)
xcoords_start shape: (20,)
ycoords_start shape: (20,)
DEBUG: nsamples: 10, gridsize: 2 (using efficient random sample-then-group strategy)
INFO: 'Y' array not found. Generating ground truth patches from 'objectGuess' as a fallback.
neighbor-sampled diffraction shape (10, 64, 64, 4)
_____ TestTrainWithLightningGreen.test_execution_config_overrides_trainer ______

self = <test_workflows_components.TestTrainWithLightningGreen object at 0x74d7767b3950>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x74d76a020ed0>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'sigmoid', 'backend': 'pytorch', 'batch_size': 16, ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj...ensity_scale_trainable=True, output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x74d7680d7e10>

    def test_execution_config_overrides_trainer(
        self,
        monkeypatch,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data
    ):
        """
        RED TEST: _train_with_lightning MUST pass execution config knobs to Trainer.
    
        Requirement: ADR-003 Phase C3.A3  thread trainer kwargs from execution config.
    
        Expected behavior (after wiring):
        - When execution_config supplied, Trainer receives accelerator/deterministic/gradient_clip_val
        - Values override defaults (e.g., accelerator='gpu', deterministic=False, gradient_clip_val=1.0)
        - When execution_config=None, Trainer uses CPU-safe defaults
    
        Test mechanism:
        - Spy on Trainer.__init__ to capture kwargs
        - Supply non-default PyTorchExecutionConfig (accelerator='gpu', deterministic=False)
        - Assert Trainer received those exact values
        - Expect FAILURE because _train_with_lightning currently ignores execution_config
        """
        from ptycho_torch.workflows import components as torch_components
        from ptycho.config.config import PyTorchExecutionConfig
    
        # Spy to track Trainer.__init__ kwargs
        trainer_init_kwargs = {"called": False, "kwargs": None}
    
        class MockTrainer:
            """Stub Trainer that records __init__ kwargs."""
            def __init__(self, **kwargs):
                trainer_init_kwargs["called"] = True
                trainer_init_kwargs["kwargs"] = kwargs
    
            def fit(self, module, train_dataloaders=None, val_dataloaders=None, **kwargs):
                pass
    
        # Monkeypatch Lightning Trainer
        monkeypatch.setattr(
            "lightning.pytorch.Trainer",
            MockTrainer
        )
    
        # Monkeypatch Lightning module to prevent errors
        class StubLightningModule:
            def save_hyperparameters(self):
                pass
    
        monkeypatch.setattr(
            "ptycho_torch.model.PtychoPINN_Lightning",
            lambda *args, **kwargs: StubLightningModule()
        )
    
        # Create execution config with non-default values
        exec_config = PyTorchExecutionConfig(
            accelerator='gpu',  # Override default 'cpu'
            deterministic=False,  # Override default True
            gradient_clip_val=1.0,  # Override default None
            num_workers=4,  # Override default 0
        )
    
        # Create minimal containers
        train_container = {"X": np.ones((10, 64, 64))}
    
        # Call _train_with_lightning with execution_config
>       results = torch_components._train_with_lightning(
            train_container=train_container,
            test_container=None,
            config=minimal_training_config,
            execution_config=exec_config  # CRITICAL: new parameter
        )

tests/torch/test_workflows_components.py:2535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

train_container = {'X': array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., ...        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.]]])}
test_container = None
config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj...ensity_scale_trainable=True, output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow')
execution_config = PyTorchExecutionConfig(accelerator='gpu', strategy='auto', deterministic=False, gradient_clip_val=1.0, accum_steps=1, ...nt_mode='min', early_stop_patience=100, logger_backend='csv', inference_batch_size=None, middle_trim=0, pad_eval=False)

    def _train_with_lightning(
        train_container: 'PtychoDataContainerTorch',
        test_container: Optional['PtychoDataContainerTorch'],
        config: TrainingConfig,
        execution_config: Optional['PyTorchExecutionConfig'] = None
    ) -> Dict[str, Any]:
        """
        Orchestrate Lightning trainer execution for PyTorch model training.
    
        This function implements the Lightning training workflow per Phase D2.B blueprint:
        1. Derives PyTorch config objects from TensorFlow TrainingConfig
        2. Instantiates PtychoPINN_Lightning module with all four config dependencies
        3. Builds train/val dataloaders via _build_lightning_dataloaders helper
        4. Configures Lightning Trainer with checkpoint/logging settings (ADR-003 Phase C3)
        5. Executes training via trainer.fit()
        6. Returns structured results dict with history, containers, and module handle
    
        Args:
            train_container: Normalized training data container
            test_container: Optional normalized test data container
            config: TrainingConfig with training hyperparameters
            execution_config: Optional PyTorchExecutionConfig with runtime knobs (Phase C3.A2)
    
        Returns:
            Dict[str, Any]: Training results including:
                - history: Dict with train_loss and optional val_loss trajectories
                - train_container: Original training container
                - test_container: Original test container
                - models: Dict with 'diffraction_to_obj' (Lightning module) and 'autoencoder' (sentinel)
                          for dual-model bundle persistence per spec 4.6
    
        Raises:
            RuntimeError: If torch or lightning packages are not installed (POLICY-001)
    
        References:
            - Blueprint: plans/active/INTEGRATE-PYTORCH-001/reports/2025-10-18T020940Z/phase_d2_completion/phase_b2_implementation.md
            - Spec: specs/ptychodus_api_spec.md:187 (reconstructor lifecycle contract)
            - Findings: POLICY-001 (PyTorch mandatory), CONFIG-001 (params.cfg already populated by caller)
            - ADR-003 Phase C3: execution_config controls Trainer kwargs (accelerator, deterministic, gradient_clip_val)
        """
        # B2.2: torch-optional imports with POLICY-001 compliant error messaging
        try:
            import torch
            import lightning.pytorch as L
            from ptycho_torch.model import PtychoPINN_Lightning
            from ptycho_torch.config_params import (
                DataConfig as PTDataConfig,
                ModelConfig as PTModelConfig,
                TrainingConfig as PTTrainingConfig,
                InferenceConfig as PTInferenceConfig
            )
        except ImportError as e:
            raise RuntimeError(
                "PyTorch backend requires torch>=2.2 and lightning. "
                "Install with: pip install -e .[torch]\n"
                "See docs/findings.md#policy-001 for PyTorch requirement policy."
            ) from e
    
        logger.info("_train_with_lightning orchestrating Lightning training")
        logger.info(f"Training config: nepochs={config.nepochs}, n_groups={config.n_groups}")
    
        # B2.1: Use config_factory to derive PyTorch configs with correct channel propagation
        # CRITICAL (Phase C4.D B2): Factory ensures C = gridsize**2 is propagated to
        # pt_model_config.C_model and pt_model_config.C_forward, preventing channel mismatch
        # when gridsize > 1 (see docs/findings.md#BUG-TF-001).
        from ptycho_torch.config_factory import create_training_payload
    
        # Build factory overrides from TrainingConfig fields
        # Factory requires n_groups in overrides dict; train_data_file and output_dir as positional
        # Note: Factory expects model_type in PyTorch naming ('Unsupervised'/'Supervised')
        #       but TrainingConfig uses TensorFlow naming ('pinn'/'supervised')
        mode_map = {'pinn': 'Unsupervised', 'supervised': 'Supervised'}
        factory_overrides = {
            'n_groups': config.n_groups,  # Required by factory validation
            'gridsize': config.model.gridsize,
            'model_type': mode_map.get(config.model.model_type, 'Unsupervised'),
            'amp_activation': config.model.amp_activation,
            'n_filters_scale': config.model.n_filters_scale,
            'nphotons': config.nphotons,
            'neighbor_count': config.neighbor_count,
            'max_epochs': config.nepochs,
            'batch_size': getattr(config, 'batch_size', 16),
        }
    
        # Create payload with factory-derived PyTorch configs
        payload = create_training_payload(
            train_data_file=Path(config.train_data_file),
            output_dir=Path(getattr(config, 'output_dir', './outputs')),
            execution_config=execution_config,  # Pass through from caller
            overrides=factory_overrides
        )
    
        # Extract PyTorch configs from payload (gridsize  C propagation already applied)
        pt_data_config = payload.pt_data_config
        pt_model_config = payload.pt_model_config
        pt_training_config = payload.pt_training_config
    
        # CRITICAL: Supervised mode REQUIRES a compatible loss function (MAE)
        # The Lightning module expects loss_name to be defined, which only happens when:
        #   1. mode='Unsupervised' AND loss_function='Poisson'  sets loss_name='poisson_train'
        #   2. mode='Unsupervised' AND loss_function='MAE'  sets loss_name='mae_train'
        #   3. mode='Supervised' AND loss_function='MAE'  sets loss_name='mae_train'
        # Without this override, supervised mode with default loss_function='Poisson' causes
        # AttributeError: 'PtychoPINN_Lightning' object has no attribute 'loss_name'
        # See: ptycho_torch/model.py:1052-1066
        if pt_model_config.mode == 'Supervised' and pt_model_config.loss_function != 'MAE':
            logger.info(
                f"Backend override: supervised mode requires MAE loss "
                f"(was {pt_model_config.loss_function}), forcing loss_function='MAE'"
            )
            # Create new ModelConfig with corrected loss_function
            from dataclasses import replace
            pt_model_config = replace(pt_model_config, loss_function='MAE')
    
        # Create minimal InferenceConfig for Lightning module (training payload doesn't include it)
        pt_inference_config = PTInferenceConfig()
    
        # B2.4: Instantiate PtychoPINN_Lightning with factory-derived config objects
        model = PtychoPINN_Lightning(
            model_config=pt_model_config,
            data_config=pt_data_config,
            training_config=pt_training_config,
            inference_config=pt_inference_config
        )
    
        # Save hyperparameters so checkpoint can reconstruct module without external state
        model.save_hyperparameters()
    
        # B2.3: Build dataloaders via helper
        train_loader, val_loader = _build_lightning_dataloaders(
            train_container, test_container, config
        )
    
        # DATA-SUP-001: Supervised mode requires labeled data
        # Check if supervised mode is requested but training data lacks required labels
        if pt_model_config.mode == 'Supervised':
            # Inspect first batch to verify label keys exist
            try:
                first_batch = next(iter(train_loader))
                batch_dict = first_batch[0]  # Extract tensor dict from batch tuple
                if 'label_amp' not in batch_dict or 'label_phase' not in batch_dict:
                    raise RuntimeError(
                        f"Supervised mode (model_type='supervised') requires labeled datasets with "
                        f"'label_amp' and 'label_phase' keys, but training data lacks these fields. "
                        f"Either: (1) Use a labeled NPZ dataset (see ptycho_torch/notebooks/create_supervised_datasets.ipynb), "
                        f"or (2) Switch to PINN mode (--model_type pinn) for self-supervised physics-based training. "
                        f"See DATA-SUP-001 in docs/findings.md for details."
                    )
            except StopIteration:
                raise RuntimeError(
                    f"Training dataloader is empty. Check dataset path and n_groups configuration."
                )
    
        # B2.5: Configure Trainer with settings from config
        # C3.A3: Thread execution config values to Trainer kwargs
        output_dir = Path(getattr(config, 'output_dir', './outputs'))
        debug_mode = getattr(config, 'debug', False)
    
        # Import execution config defaults if not provided
        if execution_config is None:
            from ptycho.config.config import PyTorchExecutionConfig
            execution_config = PyTorchExecutionConfig()
            logger.info(f"PyTorchExecutionConfig auto-instantiated for Lightning training (accelerator resolved to '{execution_config.accelerator}')")
    
        # EB1.D: Configure checkpoint/early-stop callbacks (ADR-003 Phase EB1)
        callbacks = []
        if execution_config.enable_checkpointing:
            from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping
    
            # Determine if we have validation data to use val metrics
            has_validation = test_container is not None
    
            # EB2.B: Derive monitor metric from model.val_loss_name (ADR-003 Phase EB2)
            # The model's val_loss_name is dynamically constructed based on model_type and loss configuration
            # (e.g., 'poisson_val_Amp_loss' for PINN with amplitude loss, 'mae_val_Phase_loss' for supervised)
            # This ensures checkpoint/early-stop callbacks watch the correct logged metric
            if has_validation and hasattr(model, 'val_loss_name'):
                # Use the model's dynamic validation loss name
                monitor_metric = model.val_loss_name
            else:
                # Fall back to execution config default or train loss
                monitor_metric = execution_config.checkpoint_monitor_metric
                if 'val_' in monitor_metric and not has_validation:
                    # Fall back to train_loss if val metric requested but no validation data
                    monitor_metric = monitor_metric.replace('val_', 'train_')
    
            # Build checkpoint filename template using dynamic metric name
            # Format: epoch={epoch:02d}-<metric_short_name>={<full_metric_name>:.4f}
            if has_validation:
                # Extract short name for filename (remove '_loss' suffix if present)
                metric_short_name = monitor_metric.replace('_loss', '')
                filename_template = f'epoch={{epoch:02d}}-{metric_short_name}={{{monitor_metric}:.4f}}'
            else:
                filename_template = 'epoch={epoch:02d}'
    
            checkpoint_callback = ModelCheckpoint(
                dirpath=str(output_dir / "checkpoints"),
                filename=filename_template,
                monitor=monitor_metric,
                mode=execution_config.checkpoint_mode,
                save_top_k=execution_config.checkpoint_save_top_k,
                save_last=True,  # Always keep last checkpoint for recovery
                verbose=False,
            )
            callbacks.append(checkpoint_callback)
    
            # EarlyStopping callback (ADR-003 Phase EB1.D)
            # Only add early stopping if validation data is available (otherwise no metric to monitor)
            if has_validation:
                early_stop_callback = EarlyStopping(
                    monitor=monitor_metric,
                    mode=execution_config.checkpoint_mode,
                    patience=execution_config.early_stop_patience,
                    verbose=False,
                )
                callbacks.append(early_stop_callback)
    
        # Instantiate logger based on execution config (Phase EB3.B - ADR-003)
        lightning_logger = False  # Default: no logger
        if execution_config.logger_backend is not None:
            try:
                if execution_config.logger_backend == 'csv':
                    from lightning.pytorch.loggers import CSVLogger
                    lightning_logger = CSVLogger(
                        save_dir=str(output_dir),
                        name='lightning_logs',
                    )
                    logger.info(f"Enabled CSVLogger: metrics saved to {output_dir}/lightning_logs/")
                elif execution_config.logger_backend == 'tensorboard':
                    from lightning.pytorch.loggers import TensorBoardLogger
                    lightning_logger = TensorBoardLogger(
                        save_dir=str(output_dir),
                        name='lightning_logs',
                    )
                    logger.info(f"Enabled TensorBoardLogger: run `tensorboard --logdir={output_dir}/lightning_logs/`")
                elif execution_config.logger_backend == 'mlflow':
                    from lightning.pytorch.loggers import MLFlowLogger
                    lightning_logger = MLFlowLogger(
                        experiment_name=getattr(config, 'experiment_name', 'PtychoPINN'),
                        tracking_uri=str(output_dir / 'mlruns'),
                    )
                    logger.info(f"Enabled MLFlowLogger: tracking URI={output_dir}/mlruns")
                else:
                    logger.warning(
                        f"Unknown logger_backend '{execution_config.logger_backend}'. "
                        f"Falling back to logger=False. Supported: 'csv', 'tensorboard', 'mlflow'."
                    )
            except ImportError as e:
                logger.warning(
                    f"Failed to import Lightning logger '{execution_config.logger_backend}': {e}. "
                    f"Metrics logging disabled. Install the required package to enable logging."
                )
                lightning_logger = False
        else:
            logger.info("Logger disabled (logger_backend=None). Loss metrics will not be saved to disk.")
    
        # EXEC-ACCUM-001: Guard against manual optimization + gradient accumulation
        # Lightning's manual optimization (automatic_optimization=False) is incompatible with
        # Trainer(accumulate_grad_batches>1). The PtychoPINN_Lightning module uses manual optimization
        # for custom physics loss integration, so gradient accumulation must be disabled.
>       if not model.automatic_optimization and execution_config.accum_steps > 1:
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'StubLightningModule' object has no attribute 'automatic_optimization'

ptycho_torch/workflows/components.py:831: AttributeError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (10, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (10,)
xcoords shape: (10,)
ycoords shape: (10,)
xcoords_start shape: (10,)
ycoords_start shape: (10,)
----------------------------- Captured stderr call -----------------------------
INFO: Seed set to 42
------------------------------ Captured log call -------------------------------
INFO     lightning.fabric.utilities.seed:seed.py:57 Seed set to 42
____ TestTrainWithLightningGreen.test_execution_config_controls_determinism ____

self = <test_workflows_components.TestTrainWithLightningGreen object at 0x74d7767b4090>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x74d76a231ad0>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'sigmoid', 'backend': 'pytorch', 'batch_size': 16, ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj...ensity_scale_trainable=True, output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x74d76a489550>

    def test_execution_config_controls_determinism(
        self,
        monkeypatch,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data
    ):
        """
        RED TEST: execution_config.deterministic MUST trigger Lightning deterministic mode.
    
        Requirement: ADR-003 Phase C3.C2  validate deterministic behaviour.
    
        Expected behavior:
        - When deterministic=True (default), Trainer receives deterministic=True
        - This triggers torch.use_deterministic_algorithms(True) and seeds
        - When deterministic=False, Trainer allows non-deterministic ops
    
        Test mechanism:
        - Supply execution_config with deterministic=True
        - Assert Trainer.__init__ received deterministic=True kwarg
        - Expect FAILURE because current stub doesn't wire deterministic flag
        """
        from ptycho_torch.workflows import components as torch_components
        from ptycho.config.config import PyTorchExecutionConfig
    
        # Spy to track Trainer.__init__ kwargs
        trainer_init_kwargs = {"called": False, "kwargs": None}
    
        class MockTrainer:
            def __init__(self, **kwargs):
                trainer_init_kwargs["called"] = True
                trainer_init_kwargs["kwargs"] = kwargs
    
            def fit(self, module, train_dataloaders=None, val_dataloaders=None, **kwargs):
                pass
    
        monkeypatch.setattr(
            "lightning.pytorch.Trainer",
            MockTrainer
        )
    
        # Monkeypatch Lightning module
        class StubLightningModule:
            def save_hyperparameters(self):
                pass
    
        monkeypatch.setattr(
            "ptycho_torch.model.PtychoPINN_Lightning",
            lambda *args, **kwargs: StubLightningModule()
        )
    
        # Create execution config with deterministic=True (default)
        exec_config = PyTorchExecutionConfig(
            deterministic=True,
            accelerator='cpu'
        )
    
        # Create minimal containers
        train_container = {"X": np.ones((10, 64, 64))}
    
        # Call _train_with_lightning
>       results = torch_components._train_with_lightning(
            train_container=train_container,
            test_container=None,
            config=minimal_training_config,
            execution_config=exec_config
        )

tests/torch/test_workflows_components.py:2629: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

train_container = {'X': array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., ...        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.]]])}
test_container = None
config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj...ensity_scale_trainable=True, output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow')
execution_config = PyTorchExecutionConfig(accelerator='cpu', strategy='auto', deterministic=True, gradient_clip_val=None, accum_steps=1, ...nt_mode='min', early_stop_patience=100, logger_backend='csv', inference_batch_size=None, middle_trim=0, pad_eval=False)

    def _train_with_lightning(
        train_container: 'PtychoDataContainerTorch',
        test_container: Optional['PtychoDataContainerTorch'],
        config: TrainingConfig,
        execution_config: Optional['PyTorchExecutionConfig'] = None
    ) -> Dict[str, Any]:
        """
        Orchestrate Lightning trainer execution for PyTorch model training.
    
        This function implements the Lightning training workflow per Phase D2.B blueprint:
        1. Derives PyTorch config objects from TensorFlow TrainingConfig
        2. Instantiates PtychoPINN_Lightning module with all four config dependencies
        3. Builds train/val dataloaders via _build_lightning_dataloaders helper
        4. Configures Lightning Trainer with checkpoint/logging settings (ADR-003 Phase C3)
        5. Executes training via trainer.fit()
        6. Returns structured results dict with history, containers, and module handle
    
        Args:
            train_container: Normalized training data container
            test_container: Optional normalized test data container
            config: TrainingConfig with training hyperparameters
            execution_config: Optional PyTorchExecutionConfig with runtime knobs (Phase C3.A2)
    
        Returns:
            Dict[str, Any]: Training results including:
                - history: Dict with train_loss and optional val_loss trajectories
                - train_container: Original training container
                - test_container: Original test container
                - models: Dict with 'diffraction_to_obj' (Lightning module) and 'autoencoder' (sentinel)
                          for dual-model bundle persistence per spec 4.6
    
        Raises:
            RuntimeError: If torch or lightning packages are not installed (POLICY-001)
    
        References:
            - Blueprint: plans/active/INTEGRATE-PYTORCH-001/reports/2025-10-18T020940Z/phase_d2_completion/phase_b2_implementation.md
            - Spec: specs/ptychodus_api_spec.md:187 (reconstructor lifecycle contract)
            - Findings: POLICY-001 (PyTorch mandatory), CONFIG-001 (params.cfg already populated by caller)
            - ADR-003 Phase C3: execution_config controls Trainer kwargs (accelerator, deterministic, gradient_clip_val)
        """
        # B2.2: torch-optional imports with POLICY-001 compliant error messaging
        try:
            import torch
            import lightning.pytorch as L
            from ptycho_torch.model import PtychoPINN_Lightning
            from ptycho_torch.config_params import (
                DataConfig as PTDataConfig,
                ModelConfig as PTModelConfig,
                TrainingConfig as PTTrainingConfig,
                InferenceConfig as PTInferenceConfig
            )
        except ImportError as e:
            raise RuntimeError(
                "PyTorch backend requires torch>=2.2 and lightning. "
                "Install with: pip install -e .[torch]\n"
                "See docs/findings.md#policy-001 for PyTorch requirement policy."
            ) from e
    
        logger.info("_train_with_lightning orchestrating Lightning training")
        logger.info(f"Training config: nepochs={config.nepochs}, n_groups={config.n_groups}")
    
        # B2.1: Use config_factory to derive PyTorch configs with correct channel propagation
        # CRITICAL (Phase C4.D B2): Factory ensures C = gridsize**2 is propagated to
        # pt_model_config.C_model and pt_model_config.C_forward, preventing channel mismatch
        # when gridsize > 1 (see docs/findings.md#BUG-TF-001).
        from ptycho_torch.config_factory import create_training_payload
    
        # Build factory overrides from TrainingConfig fields
        # Factory requires n_groups in overrides dict; train_data_file and output_dir as positional
        # Note: Factory expects model_type in PyTorch naming ('Unsupervised'/'Supervised')
        #       but TrainingConfig uses TensorFlow naming ('pinn'/'supervised')
        mode_map = {'pinn': 'Unsupervised', 'supervised': 'Supervised'}
        factory_overrides = {
            'n_groups': config.n_groups,  # Required by factory validation
            'gridsize': config.model.gridsize,
            'model_type': mode_map.get(config.model.model_type, 'Unsupervised'),
            'amp_activation': config.model.amp_activation,
            'n_filters_scale': config.model.n_filters_scale,
            'nphotons': config.nphotons,
            'neighbor_count': config.neighbor_count,
            'max_epochs': config.nepochs,
            'batch_size': getattr(config, 'batch_size', 16),
        }
    
        # Create payload with factory-derived PyTorch configs
        payload = create_training_payload(
            train_data_file=Path(config.train_data_file),
            output_dir=Path(getattr(config, 'output_dir', './outputs')),
            execution_config=execution_config,  # Pass through from caller
            overrides=factory_overrides
        )
    
        # Extract PyTorch configs from payload (gridsize  C propagation already applied)
        pt_data_config = payload.pt_data_config
        pt_model_config = payload.pt_model_config
        pt_training_config = payload.pt_training_config
    
        # CRITICAL: Supervised mode REQUIRES a compatible loss function (MAE)
        # The Lightning module expects loss_name to be defined, which only happens when:
        #   1. mode='Unsupervised' AND loss_function='Poisson'  sets loss_name='poisson_train'
        #   2. mode='Unsupervised' AND loss_function='MAE'  sets loss_name='mae_train'
        #   3. mode='Supervised' AND loss_function='MAE'  sets loss_name='mae_train'
        # Without this override, supervised mode with default loss_function='Poisson' causes
        # AttributeError: 'PtychoPINN_Lightning' object has no attribute 'loss_name'
        # See: ptycho_torch/model.py:1052-1066
        if pt_model_config.mode == 'Supervised' and pt_model_config.loss_function != 'MAE':
            logger.info(
                f"Backend override: supervised mode requires MAE loss "
                f"(was {pt_model_config.loss_function}), forcing loss_function='MAE'"
            )
            # Create new ModelConfig with corrected loss_function
            from dataclasses import replace
            pt_model_config = replace(pt_model_config, loss_function='MAE')
    
        # Create minimal InferenceConfig for Lightning module (training payload doesn't include it)
        pt_inference_config = PTInferenceConfig()
    
        # B2.4: Instantiate PtychoPINN_Lightning with factory-derived config objects
        model = PtychoPINN_Lightning(
            model_config=pt_model_config,
            data_config=pt_data_config,
            training_config=pt_training_config,
            inference_config=pt_inference_config
        )
    
        # Save hyperparameters so checkpoint can reconstruct module without external state
        model.save_hyperparameters()
    
        # B2.3: Build dataloaders via helper
        train_loader, val_loader = _build_lightning_dataloaders(
            train_container, test_container, config
        )
    
        # DATA-SUP-001: Supervised mode requires labeled data
        # Check if supervised mode is requested but training data lacks required labels
        if pt_model_config.mode == 'Supervised':
            # Inspect first batch to verify label keys exist
            try:
                first_batch = next(iter(train_loader))
                batch_dict = first_batch[0]  # Extract tensor dict from batch tuple
                if 'label_amp' not in batch_dict or 'label_phase' not in batch_dict:
                    raise RuntimeError(
                        f"Supervised mode (model_type='supervised') requires labeled datasets with "
                        f"'label_amp' and 'label_phase' keys, but training data lacks these fields. "
                        f"Either: (1) Use a labeled NPZ dataset (see ptycho_torch/notebooks/create_supervised_datasets.ipynb), "
                        f"or (2) Switch to PINN mode (--model_type pinn) for self-supervised physics-based training. "
                        f"See DATA-SUP-001 in docs/findings.md for details."
                    )
            except StopIteration:
                raise RuntimeError(
                    f"Training dataloader is empty. Check dataset path and n_groups configuration."
                )
    
        # B2.5: Configure Trainer with settings from config
        # C3.A3: Thread execution config values to Trainer kwargs
        output_dir = Path(getattr(config, 'output_dir', './outputs'))
        debug_mode = getattr(config, 'debug', False)
    
        # Import execution config defaults if not provided
        if execution_config is None:
            from ptycho.config.config import PyTorchExecutionConfig
            execution_config = PyTorchExecutionConfig()
            logger.info(f"PyTorchExecutionConfig auto-instantiated for Lightning training (accelerator resolved to '{execution_config.accelerator}')")
    
        # EB1.D: Configure checkpoint/early-stop callbacks (ADR-003 Phase EB1)
        callbacks = []
        if execution_config.enable_checkpointing:
            from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping
    
            # Determine if we have validation data to use val metrics
            has_validation = test_container is not None
    
            # EB2.B: Derive monitor metric from model.val_loss_name (ADR-003 Phase EB2)
            # The model's val_loss_name is dynamically constructed based on model_type and loss configuration
            # (e.g., 'poisson_val_Amp_loss' for PINN with amplitude loss, 'mae_val_Phase_loss' for supervised)
            # This ensures checkpoint/early-stop callbacks watch the correct logged metric
            if has_validation and hasattr(model, 'val_loss_name'):
                # Use the model's dynamic validation loss name
                monitor_metric = model.val_loss_name
            else:
                # Fall back to execution config default or train loss
                monitor_metric = execution_config.checkpoint_monitor_metric
                if 'val_' in monitor_metric and not has_validation:
                    # Fall back to train_loss if val metric requested but no validation data
                    monitor_metric = monitor_metric.replace('val_', 'train_')
    
            # Build checkpoint filename template using dynamic metric name
            # Format: epoch={epoch:02d}-<metric_short_name>={<full_metric_name>:.4f}
            if has_validation:
                # Extract short name for filename (remove '_loss' suffix if present)
                metric_short_name = monitor_metric.replace('_loss', '')
                filename_template = f'epoch={{epoch:02d}}-{metric_short_name}={{{monitor_metric}:.4f}}'
            else:
                filename_template = 'epoch={epoch:02d}'
    
            checkpoint_callback = ModelCheckpoint(
                dirpath=str(output_dir / "checkpoints"),
                filename=filename_template,
                monitor=monitor_metric,
                mode=execution_config.checkpoint_mode,
                save_top_k=execution_config.checkpoint_save_top_k,
                save_last=True,  # Always keep last checkpoint for recovery
                verbose=False,
            )
            callbacks.append(checkpoint_callback)
    
            # EarlyStopping callback (ADR-003 Phase EB1.D)
            # Only add early stopping if validation data is available (otherwise no metric to monitor)
            if has_validation:
                early_stop_callback = EarlyStopping(
                    monitor=monitor_metric,
                    mode=execution_config.checkpoint_mode,
                    patience=execution_config.early_stop_patience,
                    verbose=False,
                )
                callbacks.append(early_stop_callback)
    
        # Instantiate logger based on execution config (Phase EB3.B - ADR-003)
        lightning_logger = False  # Default: no logger
        if execution_config.logger_backend is not None:
            try:
                if execution_config.logger_backend == 'csv':
                    from lightning.pytorch.loggers import CSVLogger
                    lightning_logger = CSVLogger(
                        save_dir=str(output_dir),
                        name='lightning_logs',
                    )
                    logger.info(f"Enabled CSVLogger: metrics saved to {output_dir}/lightning_logs/")
                elif execution_config.logger_backend == 'tensorboard':
                    from lightning.pytorch.loggers import TensorBoardLogger
                    lightning_logger = TensorBoardLogger(
                        save_dir=str(output_dir),
                        name='lightning_logs',
                    )
                    logger.info(f"Enabled TensorBoardLogger: run `tensorboard --logdir={output_dir}/lightning_logs/`")
                elif execution_config.logger_backend == 'mlflow':
                    from lightning.pytorch.loggers import MLFlowLogger
                    lightning_logger = MLFlowLogger(
                        experiment_name=getattr(config, 'experiment_name', 'PtychoPINN'),
                        tracking_uri=str(output_dir / 'mlruns'),
                    )
                    logger.info(f"Enabled MLFlowLogger: tracking URI={output_dir}/mlruns")
                else:
                    logger.warning(
                        f"Unknown logger_backend '{execution_config.logger_backend}'. "
                        f"Falling back to logger=False. Supported: 'csv', 'tensorboard', 'mlflow'."
                    )
            except ImportError as e:
                logger.warning(
                    f"Failed to import Lightning logger '{execution_config.logger_backend}': {e}. "
                    f"Metrics logging disabled. Install the required package to enable logging."
                )
                lightning_logger = False
        else:
            logger.info("Logger disabled (logger_backend=None). Loss metrics will not be saved to disk.")
    
        # EXEC-ACCUM-001: Guard against manual optimization + gradient accumulation
        # Lightning's manual optimization (automatic_optimization=False) is incompatible with
        # Trainer(accumulate_grad_batches>1). The PtychoPINN_Lightning module uses manual optimization
        # for custom physics loss integration, so gradient accumulation must be disabled.
>       if not model.automatic_optimization and execution_config.accum_steps > 1:
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'StubLightningModule' object has no attribute 'automatic_optimization'

ptycho_torch/workflows/components.py:831: AttributeError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (10, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (10,)
xcoords shape: (10,)
ycoords shape: (10,)
xcoords_start shape: (10,)
ycoords_start shape: (10,)
----------------------------- Captured stderr call -----------------------------
INFO: Seed set to 42
------------------------------ Captured log call -------------------------------
INFO     lightning.fabric.utilities.seed:seed.py:57 Seed set to 42
_______ TestLightningExecutionConfig.test_trainer_receives_accumulation ________

self = <test_workflows_components.TestLightningExecutionConfig object at 0x74d7767b6ad0>
minimal_training_config_with_val = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='silu', object.../pytest-of-ollie/pytest-705/test_trainer_receives_accumula0/outputs'), sequential_sampling=False, backend='tensorflow')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x74d76815aed0>

    def test_trainer_receives_accumulation(self, minimal_training_config_with_val, monkeypatch):
        """
        RED Test: Verify Lightning Trainer receives accumulate_grad_batches from execution config.
    
        Expected RED Failure:
        - Trainer not receiving accum_steps from execution config
        OR
        - accumulate_grad_batches not passed to Trainer kwargs
    
        Resolution (GREEN):
        - _train_with_lightning should pass execution_config.accum_steps to Trainer(accumulate_grad_batches=...)
        """
        from ptycho.config.config import PyTorchExecutionConfig
        from unittest.mock import patch, MagicMock
    
        try:
            import lightning.pytorch as L
        except ImportError:
            pytest.skip("Lightning not available")
    
        # Create execution config with custom accumulation
        exec_config = PyTorchExecutionConfig(
            accum_steps=4,  # Override default (1)
            accelerator='cpu',
            deterministic=True,
            num_workers=0,
            enable_checkpointing=False,  # Disable callbacks for simpler mocking
        )
    
        # Mock Trainer to spy on kwargs
        mock_trainer_cls = MagicMock(spec=L.Trainer)
        mock_trainer_instance = MagicMock()
        mock_trainer_cls.return_value = mock_trainer_instance
    
        # Mock data containers
        mock_train_container = MagicMock()
        mock_test_container = MagicMock()
    
        from ptycho_torch.workflows.components import _train_with_lightning
    
        # Patch Trainer at import site
        with patch('lightning.pytorch.Trainer', mock_trainer_cls):
            try:
                _train_with_lightning(
                    train_container=mock_train_container,
                    test_container=mock_test_container,
                    config=minimal_training_config_with_val,
                    execution_config=exec_config,
                )
            except Exception:
                pass  # May fail during training; we only care about Trainer instantiation
    
        # GREEN Phase Assertion:
        # Trainer should receive accumulate_grad_batches from execution_config.accum_steps
>       assert mock_trainer_cls.called, "Trainer not instantiated"
E       AssertionError: Trainer not instantiated
E       assert False
E        +  where False = <MagicMock spec='Trainer' id='128468513030224'>.called

tests/torch/test_workflows_components.py:3137: AssertionError
----------------------------- Captured stdout call -----------------------------
Decoder Block 1: No attention module added.
Decoder Block 2: No attention module added.
Decoder Block 1: No attention module added.
Decoder Block 2: No attention module added.
----------------------------- Captured stderr call -----------------------------
INFO: Seed set to 42
------------------------------ Captured log call -------------------------------
INFO     lightning.fabric.utilities.seed:seed.py:57 Seed set to 42
=============================== warnings summary ===============================
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_num_workers_flag_roundtrip
  /home/ollie/Documents/PtychoPINN/ptycho_torch/train.py:652: UserWarning: Deterministic mode with num_workers=4 may cause performance degradation. Consider setting --num-workers 0 for reproducibility.
    execution_config = build_execution_config_from_args(args, mode='training')

tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_bundle_persistence
  /home/ollie/Documents/PtychoPINN/ptycho_torch/config_factory.py:562: UserWarning: Error reading probeGuess from /tmp/pytest-of-ollie/pytest-705/test_bundle_persistence0/train.npz: No data left in file. Using fallback N=64.
    warnings.warn(

tests/torch/test_cli_train_torch.py: 1 warning
tests/torch/test_config_factory.py: 22 warnings
tests/torch/test_workflows_components.py: 12 warnings
  /home/ollie/Documents/PtychoPINN/ptycho_torch/config_factory.py:264: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    tf_training_config = to_training_config(

tests/torch/test_cli_train_torch.py: 1 warning
tests/torch/test_config_factory.py: 25 warnings
tests/torch/test_workflows_components.py: 12 warnings
  /home/ollie/Documents/PtychoPINN/ptycho_torch/config_factory.py:623: UserWarning: params.cfg already populated. Set force=True to overwrite existing values.
    warnings.warn(

tests/torch/test_config_bridge.py::TestConfigBridgeMVP::test_mvp_config_bridge_populates_params_cfg
  /home/ollie/Documents/PtychoPINN/tests/torch/test_config_bridge.py:100: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    spec_train = config_bridge.to_training_config(

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_direct_fields[batch_size-direct]
  /home/ollie/Documents/PtychoPINN/tests/torch/test_config_bridge.py:235: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    tf_train = config_bridge.to_training_config(

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_transform_fields[nepochs-rename]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_transform_fields[nll_weight-true]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_transform_fields[nll_weight-false]
  /home/ollie/Documents/PtychoPINN/tests/torch/test_config_bridge.py:298: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    tf_train = config_bridge.to_training_config(

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[mae_weight-default]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[mae_weight-override]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[realspace_mae_weight-default]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[realspace_weight-default]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[positions_provided-default]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[probe_trainable-default]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[sequential_sampling-default]
  /home/ollie/Documents/PtychoPINN/tests/torch/test_config_bridge.py:367: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    tf_train = config_bridge.to_training_config(

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_default_divergence_detection[nphotons-divergence]
  /home/ollie/Documents/PtychoPINN/tests/torch/test_config_bridge.py:494: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    tf_train = config_bridge.to_training_config(

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_train_data_file_required_error
  /home/ollie/Documents/PtychoPINN/tests/torch/test_config_bridge.py:576: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    config_bridge.to_training_config(

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_nphotons_override_passes_validation
  /home/ollie/Documents/PtychoPINN/tests/torch/test_config_bridge.py:667: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    tf_train = config_bridge.to_training_config(

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_n_subsample_missing_override_uses_none
  /home/ollie/Documents/PtychoPINN/tests/torch/test_config_bridge.py:708: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    tf_train = config_bridge.to_training_config(

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_n_subsample_explicit_override
  /home/ollie/Documents/PtychoPINN/tests/torch/test_config_bridge.py:743: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    tf_train = config_bridge.to_training_config(

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_subsample_seed_from_dataconfig
  /home/ollie/Documents/PtychoPINN/tests/torch/test_config_bridge.py:854: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    tf_train = config_bridge.to_training_config(

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_subsample_seed_override
  /home/ollie/Documents/PtychoPINN/tests/torch/test_config_bridge.py:883: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    tf_train = config_bridge.to_training_config(

tests/torch/test_config_factory.py::TestProbeSizeInference::test_infer_probe_size_missing_file_fallback
  /home/ollie/Documents/PtychoPINN/ptycho_torch/config_factory.py:556: UserWarning: Data file /nonexistent/data.npz not found. Using fallback N=64.
    warnings.warn(

tests/torch/test_lightning_checkpoint.py::TestLightningCheckpointSerialization::test_checkpoint_contains_hyperparameters
tests/torch/test_lightning_checkpoint.py::TestLightningCheckpointSerialization::test_load_from_checkpoint_without_kwargs
tests/torch/test_lightning_checkpoint.py::TestLightningCheckpointSerialization::test_checkpoint_configs_are_serializable
  /home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.

tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_lightning_poisson_count_contract
  /home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/core/module.py:449: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`

tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_lightning_training_respects_gridsize
tests/torch/test_workflows_components.py::TestTrainWithLightningRed::test_train_with_lightning_instantiates_module
  /home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:751: Checkpoint directory /home/ollie/Documents/PtychoPINN/training_outputs/checkpoints exists and is not empty.

tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_lightning_training_respects_gridsize
tests/torch/test_workflows_components.py::TestTrainWithLightningRed::test_train_with_lightning_instantiates_module
  /home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.

tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_lightning_training_respects_gridsize
  /home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:467: `ModelCheckpoint(monitor='train_loss')` could not find the monitored key in the returned metrics: ['epoch', 'step']. HINT: Did you call `log('train_loss', value)` in the `LightningModule`?

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
SKIPPED [1] tests/test_benchmark_throughput.py:11: scripts/benchmark_inference_throughput.py not found. This is a pre-existing broken test dependency. Add to fix_plan.md for future resolution.
SKIPPED [1] tests/test_run_baseline.py:4: tests/test_utilities.py not found. This is a pre-existing broken test dependency. Add to fix_plan.md for future resolution.
SKIPPED [1] tests/io/test_ptychodus_interop_h5.py:50: ptychodus plugins not available; see specs/ptychodus_api_spec.md
SKIPPED [1] tests/test_generic_loader.py:46: Data loading failed:
SKIPPED [1] ../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/unittest.py:385: Deprecated: generate_simulated_data API changed from (obj,probe,nimages) to (config,obj,probe) and memoization disabled
SKIPPED [1] tests/test_tf_helper.py:199: TensorFlow Addons removed in TF 2.19 migration
SKIPPED [1] tests/test_tf_helper_edge_aware.py:47: tensorflow_addons not available
SKIPPED [1] tests/test_tf_helper_edge_aware.py:86: tensorflow_addons not available
SKIPPED [1] tests/test_tf_helper_edge_aware.py:198: tensorflow_addons not available
SKIPPED [1] tests/test_tf_helper_edge_aware.py:169: tensorflow_addons not available
SKIPPED [1] tests/test_tf_helper_edge_aware.py:257: tensorflow_addons not available
SKIPPED [1] tests/test_tf_helper_edge_aware.py:227: tensorflow_addons not available
SKIPPED [1] tests/torch/test_execution_config_defaults.py:162: Test requires CPU-only host to verify fallback warning
SKIPPED [2] tests/torch/test_integration_workflow_torch.py: Migrated to pytest-native test_run_pytorch_train_save_load_infer
SKIPPED [1] tests/torch/test_tf_helper.py:64: torch tf_helper module not available
SKIPPED [1] tests/torch/test_tf_helper.py:57: torch tf_helper module not available
SKIPPED [1] tests/torch/test_tf_helper.py:74: torch tf_helper module not available - tests would fail
FAILED tests/study/test_phase_g_dense_orchestrator.py::test_run_phase_g_dense_collect_only_generates_commands
FAILED tests/study/test_phase_g_dense_orchestrator.py::test_run_phase_g_dense_post_verify_hooks
FAILED tests/study/test_phase_g_dense_orchestrator.py::test_run_phase_g_dense_exec_invokes_reporting_helper
FAILED tests/study/test_phase_g_dense_orchestrator.py::test_run_phase_g_dense_exec_prints_highlights_preview
FAILED tests/study/test_phase_g_dense_orchestrator.py::test_run_phase_g_dense_exec_runs_analyze_digest
FAILED tests/test_workflow_components.py::TestLoadInferenceBundle::test_load_valid_model_directory
FAILED tests/torch/test_workflows_components.py::TestWorkflowsComponentsScaffold::test_run_cdi_example_calls_update_legacy_dict
FAILED tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_train_cdi_model_torch_invokes_lightning
FAILED tests/torch/test_workflows_components.py::TestWorkflowsComponentsRun::test_run_cdi_example_invokes_training
FAILED tests/torch/test_workflows_components.py::TestWorkflowsComponentsRun::test_run_cdi_example_persists_models
FAILED tests/torch/test_workflows_components.py::TestTrainWithLightningRed::test_train_with_lightning_runs_trainer_fit
FAILED tests/torch/test_workflows_components.py::TestTrainWithLightningRed::test_train_with_lightning_returns_models_dict
FAILED tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_run_cdi_example_torch_do_stitching_delegates_to_reassemble
FAILED tests/torch/test_workflows_components.py::TestTrainWithLightningGreen::test_execution_config_overrides_trainer
FAILED tests/torch/test_workflows_components.py::TestTrainWithLightningGreen::test_execution_config_controls_determinism
FAILED tests/torch/test_workflows_components.py::TestLightningExecutionConfig::test_trainer_receives_accumulation
===== 16 failed, 481 passed, 18 skipped, 104 warnings in 277.83s (0:04:37) =====
