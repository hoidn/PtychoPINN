============================= test session starts ==============================
platform linux -- Python 3.11.13, pytest-8.4.1, pluggy-1.6.0 -- /home/ollie/miniconda3/envs/ptycho311/bin/python3.11
cachedir: .pytest_cache
PyTorch: available
PyTorch version: 2.8.0+cu128
rootdir: /home/ollie/Documents/PtychoPINN
configfile: pyproject.toml
plugins: anyio-4.9.0
collecting ... collected 518 items / 2 skipped

tests/image/test_cropping.py::TestCroppingAlignment::test_align_for_evaluation_bounding_box PASSED [  0%]
tests/image/test_cropping.py::TestCroppingAlignment::test_align_for_evaluation_coordinates_format PASSED [  0%]
tests/image/test_cropping.py::TestCroppingAlignment::test_align_for_evaluation_shapes PASSED [  0%]
tests/image/test_cropping.py::TestCroppingAlignment::test_align_for_evaluation_with_squeeze PASSED [  0%]
tests/image/test_cropping.py::TestCroppingAlignment::test_center_crop_exact_size PASSED [  0%]
tests/image/test_registration.py::TestRegistration::test_apply_shift_and_crop_basic PASSED [  1%]
tests/image/test_registration.py::TestRegistration::test_apply_shift_and_crop_zero_offset PASSED [  1%]
tests/image/test_registration.py::TestRegistration::test_different_image_content PASSED [  1%]
tests/image/test_registration.py::TestRegistration::test_edge_case_maximum_shift PASSED [  1%]
tests/image/test_registration.py::TestRegistration::test_edge_case_single_pixel_shift PASSED [  1%]
tests/image/test_registration.py::TestRegistration::test_find_offset_known_shift_complex PASSED [  2%]
tests/image/test_registration.py::TestRegistration::test_find_offset_known_shift_real PASSED [  2%]
tests/image/test_registration.py::TestRegistration::test_input_validation_2d_requirement PASSED [  2%]
tests/image/test_registration.py::TestRegistration::test_input_validation_excessive_offset PASSED [  2%]
tests/image/test_registration.py::TestRegistration::test_input_validation_shape_matching PASSED [  2%]
tests/image/test_registration.py::TestRegistration::test_noise_robustness PASSED [  3%]
tests/image/test_registration.py::TestRegistration::test_register_and_align_convenience PASSED [  3%]
tests/image/test_registration.py::TestRegistration::test_registration_sign_verification PASSED [  3%]
tests/image/test_registration.py::TestRegistration::test_round_trip_registration PASSED [  3%]
tests/image/test_registration.py::TestRegistration::test_shift_and_crop_preserves_data_type PASSED [  3%]
tests/io/test_ptychodus_interop_h5.py::test_interop_h5_reader SKIPPED    [  4%]
tests/io/test_ptychodus_product_io.py::test_export_writes_minimal_hdf5 PASSED [  4%]
tests/io/test_ptychodus_product_io.py::test_import_reads_hdf5_to_rawdata PASSED [  4%]
tests/io/test_ptychodus_product_io.py::test_cli_convert_run1084_smoke PASSED [  4%]
tests/scripts/test_inference_backend_selector.py::TestInferenceCliBackendDispatch::test_pytorch_backend_dispatch PASSED [  4%]
tests/scripts/test_inference_backend_selector.py::TestInferenceCliBackendDispatch::test_tensorflow_backend_dispatch PASSED [  5%]
tests/scripts/test_inference_backend_selector.py::TestInferenceCliBackendDispatch::test_backend_selector_preserves_config_001_compliance PASSED [  5%]
tests/scripts/test_inference_backend_selector.py::TestInferenceCliBackendDispatch::test_cli_backend_argument_parsing PASSED [  5%]
tests/scripts/test_inference_backend_selector.py::TestInferenceCliBackendDispatch::test_setup_inference_configuration_uses_backend PASSED [  5%]
tests/scripts/test_inference_backend_selector.py::TestInferenceCliBackendDispatch::test_pytorch_inference_execution_path PASSED [  5%]
tests/scripts/test_inference_backend_selector.py::TestInferenceCliBackendDispatch::test_pytorch_execution_config_flags PASSED [  5%]
tests/scripts/test_inference_backend_selector.py::TestInferenceCliBackendDispatch::test_pytorch_backend_moves_model_to_execution_device PASSED [  6%]
tests/scripts/test_inference_backend_selector.py::TestInferenceCliBackendDispatch::test_pytorch_backend_defaults_auto_execution_config PASSED [  6%]
tests/scripts/test_ptychi_reconstruct_tike.py::test_main_uses_cli_arguments PASSED [  6%]
tests/scripts/test_training_backend_selector.py::TestTrainingCliBackendDispatch::test_pytorch_backend_dispatch PASSED [  6%]
tests/scripts/test_training_backend_selector.py::TestTrainingCliBackendDispatch::test_tensorflow_backend_persistence PASSED [  6%]
tests/scripts/test_training_backend_selector.py::TestTrainingCliBackendDispatch::test_pytorch_execution_config_flags PASSED [  7%]
tests/scripts/test_training_backend_selector.py::TestTrainingCliBackendDispatch::test_supervised_mode_enforces_mae_loss PASSED [  7%]
tests/scripts/test_training_backend_selector.py::TestTrainingCliBackendDispatch::test_manual_accumulation_guard PASSED [  7%]
tests/scripts/test_training_backend_selector.py::TestTrainingCliBackendDispatch::test_pytorch_backend_defaults_auto_execution_config PASSED [  7%]
tests/study/test_check_dense_highlights_match.py::test_summary_mismatch_fails PASSED [  7%]
tests/study/test_check_dense_highlights_match.py::test_summary_matches_json PASSED [  8%]
tests/study/test_check_dense_highlights_match.py::test_missing_phase_only_metadata_fails PASSED [  8%]
tests/study/test_dose_overlap_comparison.py::test_build_comparison_jobs_creates_all_conditions PASSED [  8%]
tests/study/test_dose_overlap_comparison.py::test_execute_comparison_jobs_invokes_compare_models PASSED [  8%]
tests/study/test_dose_overlap_comparison.py::test_execute_comparison_jobs_records_summary PASSED [  8%]
tests/study/test_dose_overlap_comparison.py::test_build_comparison_jobs_uses_dose_specific_phase_e_paths PASSED [  9%]
tests/study/test_dose_overlap_comparison.py::test_execute_comparison_jobs_appends_tike_recon_path PASSED [  9%]
tests/study/test_dose_overlap_comparison.py::test_prepare_baseline_inference_data_grouped_flatten_helper PASSED [  9%]
tests/study/test_dose_overlap_comparison.py::test_baseline_model_predict_receives_both_inputs PASSED [  9%]
tests/study/test_dose_overlap_comparison.py::test_baseline_complex_output_converts_to_amplitude_phase PASSED [  9%]
tests/study/test_dose_overlap_comparison.py::test_pinn_reconstruction_reassembles_batched_predictions PASSED [ 10%]
tests/study/test_dose_overlap_comparison.py::test_pinn_reconstruction_reassembles_full_train_split PASSED [ 10%]
tests/study/test_dose_overlap_dataset_contract.py::test_validate_dataset_contract_happy_path PASSED [ 10%]
tests/study/test_dose_overlap_dataset_contract.py::test_validate_dataset_contract_missing_key PASSED [ 10%]
tests/study/test_dose_overlap_dataset_contract.py::test_validate_dataset_contract_wrong_dtype_diffraction PASSED [ 10%]
tests/study/test_dose_overlap_dataset_contract.py::test_validate_dataset_contract_wrong_dtype_object PASSED [ 11%]
tests/study/test_dose_overlap_dataset_contract.py::test_validate_dataset_contract_shape_mismatch PASSED [ 11%]
tests/study/test_dose_overlap_dataset_contract.py::test_validate_dataset_contract_spacing_dense PASSED [ 11%]
tests/study/test_dose_overlap_dataset_contract.py::test_validate_dataset_contract_spacing_violation PASSED [ 11%]
tests/study/test_dose_overlap_dataset_contract.py::test_validate_dataset_contract_oversampling_precondition_pass PASSED [ 11%]
tests/study/test_dose_overlap_dataset_contract.py::test_validate_dataset_contract_oversampling_precondition_fail PASSED [ 11%]
tests/study/test_dose_overlap_dataset_contract.py::test_validate_dataset_contract_oversampling_missing_neighbor_count PASSED [ 12%]
tests/study/test_dose_overlap_dataset_contract.py::test_validate_dataset_contract_unknown_view PASSED [ 12%]
tests/study/test_dose_overlap_design.py::test_study_design_constants PASSED [ 12%]
tests/study/test_dose_overlap_design.py::test_study_design_validation PASSED [ 12%]
tests/study/test_dose_overlap_design.py::test_study_design_to_dict PASSED [ 12%]
tests/study/test_dose_overlap_generation.py::test_build_simulation_plan PASSED [ 13%]
tests/study/test_dose_overlap_generation.py::test_generate_dataset_pipeline_orchestration[1000.0] PASSED [ 13%]
tests/study/test_dose_overlap_generation.py::test_generate_dataset_pipeline_orchestration[10000.0] PASSED [ 13%]
tests/study/test_dose_overlap_generation.py::test_generate_dataset_pipeline_orchestration[100000.0] PASSED [ 13%]
tests/study/test_dose_overlap_generation.py::test_generate_dataset_config_construction PASSED [ 13%]
tests/study/test_dose_overlap_generation.py::test_generate_dataset_validates_with_real_contract PASSED [ 14%]
tests/study/test_dose_overlap_generation.py::test_build_simulation_plan_handles_metadata_pickle_guard PASSED [ 14%]
tests/study/test_dose_overlap_generation.py::test_load_data_for_sim_handles_metadata_pickle_guard PASSED [ 14%]
tests/study/test_dose_overlap_generation.py::test_generate_dataset_for_dose_handles_metadata_splits PASSED [ 14%]
tests/study/test_dose_overlap_overlap.py::test_disc_overlap_fraction_perfect_overlap PASSED [ 14%]
tests/study/test_dose_overlap_overlap.py::test_disc_overlap_fraction_half_diameter PASSED [ 15%]
tests/study/test_dose_overlap_overlap.py::test_disc_overlap_fraction_no_overlap PASSED [ 15%]
tests/study/test_dose_overlap_overlap.py::test_disc_overlap_area_symmetry PASSED [ 15%]
tests/study/test_dose_overlap_overlap.py::test_subsample_images_deterministic PASSED [ 15%]
tests/study/test_dose_overlap_overlap.py::test_filter_dataset_by_mask_handles_scalar_metadata PASSED [ 15%]
tests/study/test_dose_overlap_overlap.py::test_form_groups_gs1 PASSED    [ 16%]
tests/study/test_dose_overlap_overlap.py::test_form_groups_gs2 PASSED    [ 16%]
tests/study/test_dose_overlap_overlap.py::test_metric_1_group_based_synthetic PASSED [ 16%]
tests/study/test_dose_overlap_overlap.py::test_metric_2_image_based_deduplication PASSED [ 16%]
tests/study/test_dose_overlap_overlap.py::test_metric_2_image_based_single_image PASSED [ 16%]
tests/study/test_dose_overlap_overlap.py::test_metric_3_group_to_group_overlapping PASSED [ 16%]
tests/study/test_dose_overlap_overlap.py::test_metric_3_group_to_group_no_overlap PASSED [ 17%]
tests/study/test_dose_overlap_overlap.py::test_compute_overlap_metrics_gs1 PASSED [ 17%]
tests/study/test_dose_overlap_overlap.py::test_compute_overlap_metrics_gs2 PASSED [ 17%]
tests/study/test_dose_overlap_overlap.py::test_compute_overlap_metrics_degenerate_s_img PASSED [ 17%]
tests/study/test_dose_overlap_overlap.py::test_compute_overlap_metrics_invalid_gridsize PASSED [ 17%]
tests/study/test_dose_overlap_overlap.py::test_generate_overlap_views_basic PASSED [ 18%]
tests/study/test_dose_overlap_overlap.py::test_overlap_metrics_bundle PASSED [ 18%]
tests/study/test_dose_overlap_overlap.py::test_generate_overlap_views_dense_acceptance_floor PASSED [ 18%]
tests/study/test_dose_overlap_reconstruction.py::test_build_ptychi_jobs_manifest PASSED [ 18%]
tests/study/test_dose_overlap_reconstruction.py::test_run_ptychi_job_invokes_script PASSED [ 18%]
tests/study/test_dose_overlap_reconstruction.py::test_cli_filters_dry_run PASSED [ 19%]
tests/study/test_dose_overlap_reconstruction.py::test_cli_executes_selected_jobs PASSED [ 19%]
tests/study/test_dose_overlap_reconstruction.py::test_cli_skips_missing_phase_d PASSED [ 19%]
tests/study/test_dose_overlap_training.py::test_train_cdi_model_normalizes_history PASSED [ 19%]
tests/study/test_dose_overlap_training.py::test_build_training_jobs_matrix PASSED [ 19%]
tests/study/test_dose_overlap_training.py::test_run_training_job_invokes_runner PASSED [ 20%]
tests/study/test_dose_overlap_training.py::test_run_training_job_dry_run PASSED [ 20%]
tests/study/test_dose_overlap_training.py::test_execute_training_job_dispatches_tensorflow_by_default PASSED [ 20%]
tests/study/test_dose_overlap_training.py::test_execute_training_job_dispatches_pytorch_when_requested PASSED [ 20%]
tests/study/test_dose_overlap_training.py::test_execute_training_job_tensorflow_persists_bundle PASSED [ 20%]
tests/study/test_dose_overlap_training.py::test_training_cli_filters_jobs PASSED [ 21%]
tests/study/test_dose_overlap_training.py::test_training_cli_manifest_and_bridging PASSED [ 21%]
tests/study/test_dose_overlap_training.py::test_execute_training_job_delegates_to_pytorch_trainer PASSED [ 21%]
tests/study/test_dose_overlap_training.py::test_execute_training_job_persists_bundle PASSED [ 21%]
tests/study/test_dose_overlap_training.py::test_training_cli_invokes_real_runner PASSED [ 21%]
tests/study/test_dose_overlap_training.py::test_build_training_jobs_skips_missing_view PASSED [ 22%]
tests/study/test_dose_overlap_training.py::test_training_cli_records_bundle_path PASSED [ 22%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_artifact_inventory_blocks_missing_entries PASSED [ 22%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_artifact_inventory_passes_with_complete_bundle PASSED [ 22%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_cli_logs_missing PASSED [ 22%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_cli_logs_complete PASSED [ 22%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_cli_phase_logs_missing PASSED [ 23%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_cli_phase_logs_wrong_pattern PASSED [ 23%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_cli_phase_logs_incomplete PASSED [ 23%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_cli_phase_logs_complete PASSED [ 23%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_highlights_missing_model PASSED [ 23%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_highlights_mismatched_value PASSED [ 24%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_highlights_complete PASSED [ 24%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_highlights_missing_preview PASSED [ 24%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_highlights_preview_contains_amplitude PASSED [ 24%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_highlights_preview_mismatch PASSED [ 24%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_highlights_delta_mismatch PASSED [ 25%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_cli_logs_require_ssim_grid_log PASSED [ 25%]
tests/study/test_phase_g_dense_artifacts_verifier.py::test_verify_dense_pipeline_requires_ssim_grid_summary PASSED [ 25%]
tests/study/test_phase_g_dense_metrics_report.py::test_report_phase_g_dense_metrics PASSED [ 25%]
tests/study/test_phase_g_dense_metrics_report.py::test_report_phase_g_dense_metrics_missing_model_fails PASSED [ 25%]
tests/study/test_phase_g_dense_metrics_report.py::test_analyze_dense_metrics_flags_failures PASSED [ 26%]
tests/study/test_phase_g_dense_metrics_report.py::test_analyze_dense_metrics_success_digest PASSED [ 26%]
tests/study/test_phase_g_dense_orchestrator.py::test_run_phase_g_dense_collect_only_generates_commands FAILED [ 26%]
tests/study/test_phase_g_dense_orchestrator.py::test_run_phase_g_dense_post_verify_hooks FAILED [ 26%]
tests/study/test_phase_g_dense_orchestrator.py::test_summarize_phase_g_outputs PASSED [ 26%]
tests/study/test_phase_g_dense_orchestrator.py::test_summarize_phase_g_outputs_fails_on_missing_manifest PASSED [ 27%]
tests/study/test_phase_g_dense_orchestrator.py::test_summarize_phase_g_outputs_fails_on_execution_failures PASSED [ 27%]
tests/study/test_phase_g_dense_orchestrator.py::test_summarize_phase_g_outputs_fails_on_missing_csv PASSED [ 27%]
tests/study/test_phase_g_dense_orchestrator.py::test_validate_phase_c_metadata_requires_metadata PASSED [ 27%]
tests/study/test_phase_g_dense_orchestrator.py::test_validate_phase_c_metadata_requires_canonical_transform PASSED [ 27%]
tests/study/test_phase_g_dense_orchestrator.py::test_validate_phase_c_metadata_accepts_valid_metadata PASSED [ 27%]
tests/study/test_phase_g_dense_orchestrator.py::test_validate_phase_c_metadata_handles_patched_layout PASSED [ 28%]
tests/study/test_phase_g_dense_orchestrator.py::test_prepare_hub_detects_stale_outputs PASSED [ 28%]
tests/study/test_phase_g_dense_orchestrator.py::test_prepare_hub_clobbers_previous_outputs PASSED [ 28%]
tests/study/test_phase_g_dense_orchestrator.py::test_run_phase_g_dense_exec_invokes_reporting_helper FAILED [ 28%]
tests/study/test_phase_g_dense_orchestrator.py::test_run_phase_g_dense_exec_prints_highlights_preview FAILED [ 28%]
tests/study/test_phase_g_dense_orchestrator.py::test_run_phase_g_dense_exec_runs_analyze_digest FAILED [ 29%]
tests/study/test_phase_g_dense_orchestrator.py::test_persist_delta_highlights_creates_preview PASSED [ 29%]
tests/study/test_phase_g_dense_orchestrator.py::test_run_phase_g_dense_collect_only_post_verify_only PASSED [ 29%]
tests/study/test_phase_g_dense_orchestrator.py::test_run_phase_g_dense_post_verify_only_executes_chain PASSED [ 29%]
tests/study/test_ssim_grid.py::test_smoke_ssim_grid PASSED               [ 29%]
tests/test_baselines.py::TestBaselines::test_build_model_always_creates_single_channel_output PASSED [ 30%]
tests/test_cli_args.py::TestCliArgs::test_add_logging_arguments PASSED   [ 30%]
tests/test_cli_args.py::TestCliArgs::test_console_level_choices PASSED   [ 30%]
tests/test_cli_args.py::TestCliArgs::test_get_logging_config_custom_level PASSED [ 30%]
tests/test_cli_args.py::TestCliArgs::test_get_logging_config_defaults PASSED [ 30%]
tests/test_cli_args.py::TestCliArgs::test_get_logging_config_quiet PASSED [ 31%]
tests/test_cli_args.py::TestCliArgs::test_get_logging_config_verbose PASSED [ 31%]
tests/test_cli_args.py::TestCliArgs::test_quiet_verbose_mutually_exclusive PASSED [ 31%]
tests/test_coordinate_grouping.py::TestCoordinateGrouping::test_backward_compatibility PASSED [ 31%]
tests/test_coordinate_grouping.py::TestCoordinateGrouping::test_different_seeds_produce_different_results PASSED [ 31%]
tests/test_coordinate_grouping.py::TestCoordinateGrouping::test_edge_case_k_less_than_c PASSED [ 32%]
tests/test_coordinate_grouping.py::TestCoordinateGrouping::test_edge_case_more_samples_than_points PASSED [ 32%]
tests/test_coordinate_grouping.py::TestCoordinateGrouping::test_edge_case_small_dataset PASSED [ 32%]
tests/test_coordinate_grouping.py::TestCoordinateGrouping::test_efficient_grouping_output_shape PASSED [ 32%]
tests/test_coordinate_grouping.py::TestCoordinateGrouping::test_efficient_grouping_spatial_coherence PASSED [ 32%]
tests/test_coordinate_grouping.py::TestCoordinateGrouping::test_efficient_grouping_valid_indices PASSED [ 33%]
tests/test_coordinate_grouping.py::TestCoordinateGrouping::test_generate_grouped_data_gridsize_1 PASSED [ 33%]
tests/test_coordinate_grouping.py::TestCoordinateGrouping::test_generate_grouped_data_integration PASSED [ 33%]
tests/test_coordinate_grouping.py::TestCoordinateGrouping::test_memory_efficiency PASSED [ 33%]
tests/test_coordinate_grouping.py::TestCoordinateGrouping::test_no_cache_files_created PASSED [ 33%]
tests/test_coordinate_grouping.py::TestCoordinateGrouping::test_performance_improvement PASSED [ 33%]
tests/test_coordinate_grouping.py::TestCoordinateGrouping::test_reproducibility_with_seed PASSED [ 34%]
tests/test_coordinate_grouping.py::TestIntegrationWithExistingCode::test_existing_tests_still_pass PASSED [ 34%]
tests/test_generic_loader.py::TestGenericLoader::test_generic_loader_roundtrip SKIPPED [ 34%]
tests/test_generic_loader.py::test_generic_loader PASSED                 [ 34%]
tests/test_integration_baseline_gs2.py::TestBaselineGridsize2Integration::test_baseline_gridsize2_end_to_end FAILED [ 34%]
tests/test_integration_workflow.py::TestFullWorkflow::test_train_save_load_infer_cycle FAILED [ 35%]
tests/test_log_config.py::TestLogConfig::test_backward_compatibility PASSED [ 35%]
tests/test_log_config.py::TestLogConfig::test_conflicting_flags_verbose_overrides PASSED [ 35%]
tests/test_log_config.py::TestLogConfig::test_custom_console_level PASSED [ 35%]
tests/test_log_config.py::TestLogConfig::test_default_setup_logging_creates_log_directory_and_file PASSED [ 35%]
tests/test_log_config.py::TestLogConfig::test_quiet_flag_overrides_console_level PASSED [ 36%]
tests/test_log_config.py::TestLogConfig::test_quiet_mode_disables_console PASSED [ 36%]
tests/test_log_config.py::TestLogConfig::test_setup_logging_clears_existing_handlers PASSED [ 36%]
tests/test_log_config.py::TestLogConfig::test_string_path_support PASSED [ 36%]
tests/test_log_config.py::TestLogConfig::test_verbose_mode_enables_debug_console PASSED [ 36%]
tests/test_misc.py::test_memoize_simulated_data SKIPPED (Deprecated:...) [ 37%]
tests/test_model_channel_consistency.py::test_amp_head_matches_patch_channels PASSED [ 37%]
tests/test_model_channel_consistency.py::test_diffraction_to_obj_accepts_grouped_inputs PASSED [ 37%]
tests/test_nphotons_metadata_integration.py::TestNphotonsMetadataIntegration::test_configuration_mismatch_warnings PASSED [ 37%]
tests/test_nphotons_metadata_integration.py::TestNphotonsMetadataIntegration::test_end_to_end_workflow_consistency FAILED [ 37%]
tests/test_nphotons_metadata_integration.py::TestNphotonsMetadataIntegration::test_metadata_backward_compatibility PASSED [ 38%]
tests/test_nphotons_metadata_integration.py::TestNphotonsMetadataIntegration::test_metadata_persistence_single_nphotons FAILED [ 38%]
tests/test_nphotons_metadata_integration.py::TestNphotonsMetadataIntegration::test_multiple_nphotons_metadata_consistency PASSED [ 38%]
tests/test_nphotons_metadata_integration.py::TestNphotonsMetadataIntegration::test_training_with_mismatched_config_warns_but_continues FAILED [ 38%]
tests/test_oversampling.py::TestAutomaticOversampling::test_automatic_oversampling_triggers PASSED [ 38%]
tests/test_oversampling.py::TestAutomaticOversampling::test_enable_oversampling_flag_required PASSED [ 38%]
tests/test_oversampling.py::TestAutomaticOversampling::test_gridsize_1_no_oversampling PASSED [ 39%]
tests/test_oversampling.py::TestAutomaticOversampling::test_neighbor_pool_size_guard PASSED [ 39%]
tests/test_oversampling.py::TestAutomaticOversampling::test_oversampling_with_different_k_values PASSED [ 39%]
tests/test_oversampling.py::TestAutomaticOversampling::test_reproducibility_with_seed PASSED [ 39%]
tests/test_oversampling.py::TestAutomaticOversampling::test_standard_sampling_no_oversampling PASSED [ 39%]
tests/test_projective_warp_xla.py::TestProjectiveWarpXLA::test_batch_processing PASSED [ 40%]
tests/test_projective_warp_xla.py::TestProjectiveWarpXLA::test_complex128_dtype PASSED [ 40%]
tests/test_projective_warp_xla.py::TestProjectiveWarpXLA::test_complex64_dtype PASSED [ 40%]
tests/test_projective_warp_xla.py::TestProjectiveWarpXLA::test_fill_modes PASSED [ 40%]
tests/test_projective_warp_xla.py::TestProjectiveWarpXLA::test_float32_dtype PASSED [ 40%]
tests/test_projective_warp_xla.py::TestProjectiveWarpXLA::test_float64_dtype PASSED [ 41%]
tests/test_projective_warp_xla.py::TestProjectiveWarpXLA::test_float64_with_translation PASSED [ 41%]
tests/test_projective_warp_xla.py::TestProjectiveWarpXLA::test_interpolation_modes PASSED [ 41%]
tests/test_projective_warp_xla.py::TestProjectiveWarpXLA::test_jit_compilation_float32 PASSED [ 41%]
tests/test_projective_warp_xla.py::TestProjectiveWarpXLA::test_jit_compilation_float64 PASSED [ 41%]
tests/test_projective_warp_xla.py::TestProjectiveWarpXLA::test_mixed_precision_translation PASSED [ 42%]
tests/test_projective_warp_xla.py::TestProjectiveWarpXLA::test_tfa_params_conversion PASSED [ 42%]
tests/test_raw_data_grouping.py::TestRawDataGrouping::test_content_validity PASSED [ 42%]
tests/test_raw_data_grouping.py::TestRawDataGrouping::test_edge_case_k_less_than_c PASSED [ 42%]
tests/test_raw_data_grouping.py::TestRawDataGrouping::test_edge_case_more_samples_than_points PASSED [ 42%]
tests/test_raw_data_grouping.py::TestRawDataGrouping::test_edge_case_small_dataset PASSED [ 43%]
tests/test_raw_data_grouping.py::TestRawDataGrouping::test_memory_efficiency PASSED [ 43%]
tests/test_raw_data_grouping.py::TestRawDataGrouping::test_output_shape PASSED [ 43%]
tests/test_raw_data_grouping.py::TestRawDataGrouping::test_performance_improvement PASSED [ 43%]
tests/test_raw_data_grouping.py::TestRawDataGrouping::test_reproducibility PASSED [ 43%]
tests/test_raw_data_grouping.py::TestRawDataGrouping::test_uniform_sampling PASSED [ 44%]
tests/test_scaling_regression.py::TestScalingRegression::test_both_arrays_scaled_identically PASSED [ 44%]
tests/test_scaling_regression.py::TestScalingRegression::test_different_nphotons_produce_proportional_scaling PASSED [ 44%]
tests/test_scaling_regression.py::TestScalingRegression::test_intensity_scale_is_valid PASSED [ 44%]
tests/test_scaling_regression.py::TestScalingRegression::test_phase_is_not_scaled PASSED [ 44%]
tests/test_scaling_regression.py::TestScalingRegression::test_scaling_is_reversible PASSED [ 44%]
tests/test_scaling_regression.py::TestScalingRegression::test_scaling_preserves_physics PASSED [ 45%]
tests/test_scaling_regression.py::TestScalingAssertions::test_assertions_catch_invalid_intensity_scale PASSED [ 45%]
tests/test_sequential_sampling.py::TestSequentialSampling::test_default_behavior_is_random PASSED [ 45%]
tests/test_sequential_sampling.py::TestSequentialSampling::test_sequential_sampling_handles_edge_cases PASSED [ 45%]
tests/test_sequential_sampling.py::TestSequentialSampling::test_sequential_sampling_is_deterministic PASSED [ 45%]
tests/test_sequential_sampling.py::TestSequentialSampling::test_sequential_sampling_order PASSED [ 46%]
tests/test_sequential_sampling.py::TestSequentialSampling::test_sequential_sampling_uses_first_n_points PASSED [ 46%]
tests/test_sequential_sampling.py::TestSequentialSampling::test_sequential_sampling_with_gridsize_greater_than_1 PASSED [ 46%]
tests/test_sequential_sampling.py::TestSequentialSampling::test_sequential_sampling_with_seed_parameter PASSED [ 46%]
tests/test_sequential_sampling.py::TestSequentialSampling::test_sequential_vs_random_coverage PASSED [ 46%]
tests/test_sequential_sampling.py::TestIntegrationWithWorkflow::test_cli_argument_parsing PASSED [ 47%]
tests/test_sequential_sampling.py::TestIntegrationWithWorkflow::test_config_flag_exists PASSED [ 47%]
tests/test_subsampling.py::TestSubsampling::test_different_seeds_produce_different_results PASSED [ 47%]
tests/test_subsampling.py::TestSubsampling::test_interaction_with_config_dataclass PASSED [ 47%]
tests/test_subsampling.py::TestSubsampling::test_legacy_n_images_behavior PASSED [ 47%]
tests/test_subsampling.py::TestSubsampling::test_n_subsample_overrides_n_images PASSED [ 48%]
tests/test_subsampling.py::TestSubsampling::test_no_subsample_uses_full_dataset PASSED [ 48%]
tests/test_subsampling.py::TestSubsampling::test_reproducible_subsampling_with_seed PASSED [ 48%]
tests/test_subsampling.py::TestSubsampling::test_sorted_indices_for_consistency PASSED [ 48%]
tests/test_subsampling.py::TestSubsampling::test_subsample_larger_than_dataset PASSED [ 48%]
tests/test_subsampling.py::TestSubsampling::test_subsample_with_n_subsample PASSED [ 49%]
tests/test_subsampling.py::TestSubsampling::test_subsample_zero_edge_case PASSED [ 49%]
tests/test_subsampling.py::TestSubsampling::test_y_patches_subsampled_consistently PASSED [ 49%]
tests/test_tf_helper.py::TestReassemblePosition::test_basic_functionality PASSED [ 49%]
tests/test_tf_helper.py::TestReassemblePosition::test_different_patch_values_blend PASSED [ 49%]
tests/test_tf_helper.py::TestReassemblePosition::test_identical_patches_single_vs_double PASSED [ 50%]
tests/test_tf_helper.py::TestReassemblePosition::test_perfect_overlap_averages_to_identity PASSED [ 50%]
tests/test_tf_helper.py::TestTranslateFunction::test_batch_translation PASSED [ 50%]
tests/test_tf_helper.py::TestTranslateFunction::test_complex_tensor_translation PASSED [ 50%]
tests/test_tf_helper.py::TestTranslateFunction::test_edge_cases PASSED   [ 50%]
tests/test_tf_helper.py::TestTranslateFunction::test_integer_translation PASSED [ 50%]
tests/test_tf_helper.py::TestTranslateFunction::test_subpixel_translation PASSED [ 51%]
tests/test_tf_helper.py::TestTranslateFunction::test_translate_core_matches_addons SKIPPED [ 51%]
tests/test_tf_helper.py::TestTranslateFunction::test_zero_translation PASSED [ 51%]
tests/test_tf_helper_edge_aware.py::TestTranslateSmoothPatterns::test_complex_smooth_translation PASSED [ 51%]
tests/test_tf_helper_edge_aware.py::TestTranslateSmoothPatterns::test_gaussian_probe_translation SKIPPED [ 51%]
tests/test_tf_helper_edge_aware.py::TestTranslateSmoothPatterns::test_smooth_object_translation SKIPPED [ 52%]
tests/test_tf_helper_edge_aware.py::TestTranslateEdgeCases::test_boundary_behavior SKIPPED [ 52%]
tests/test_tf_helper_edge_aware.py::TestTranslateEdgeCases::test_document_edge_differences SKIPPED [ 52%]
tests/test_tf_helper_edge_aware.py::TestPtychoPINNRelevantCases::test_batch_smooth_patterns SKIPPED [ 52%]
tests/test_tf_helper_edge_aware.py::TestPtychoPINNRelevantCases::test_typical_probe_sizes SKIPPED [ 52%]
tests/test_workflow_components.py::TestLoadInferenceBundle::test_exception_propagation PASSED [ 53%]
tests/test_workflow_components.py::TestLoadInferenceBundle::test_load_valid_model_directory FAILED [ 53%]
tests/test_workflow_components.py::TestLoadInferenceBundle::test_missing_diffraction_model PASSED [ 53%]
tests/test_workflow_components.py::TestLoadInferenceBundle::test_missing_model_archive PASSED [ 53%]
tests/test_workflow_components.py::TestLoadInferenceBundle::test_nonexistent_directory PASSED [ 53%]
tests/test_workflow_components.py::TestLoadInferenceBundle::test_not_a_directory PASSED [ 54%]
tests/test_workflow_components.py::TestLoadInferenceBundle::test_path_conversion PASSED [ 54%]
tests/tools/test_generate_patches_tool.py::test_generate_patches_preserves_metadata PASSED [ 54%]
tests/tools/test_generate_patches_tool.py::test_generate_patches_without_metadata PASSED [ 54%]
tests/tools/test_generate_test_index.py::GenerateTestIndexTests::test_get_module_docstring_handles_missing_docstring PASSED [ 54%]
tests/tools/test_generate_test_index.py::GenerateTestIndexTests::test_get_module_docstring_reads_existing_docstring PASSED [ 55%]
tests/tools/test_generate_test_index.py::GenerateTestIndexTests::test_get_test_functions_lists_key_tests PASSED [ 55%]
tests/tools/test_tail_interleave_logs.py::test_interleave_summaries PASSED [ 55%]
tests/tools/test_transpose_rename_convert_tool.py::test_canonicalize_preserves_metadata PASSED [ 55%]
tests/tools/test_transpose_rename_convert_tool.py::test_canonicalize_without_metadata PASSED [ 55%]
tests/tools/test_update_tool.py::TestUpdateTool::test_update_function PASSED [ 55%]
tests/tools/test_update_tool.py::test_update_function PASSED             [ 56%]
tests/torch/test_api_deprecation.py::TestLegacyAPIDeprecation::test_example_train_import_emits_deprecation_warning PASSED [ 56%]
tests/torch/test_api_deprecation.py::TestLegacyAPIDeprecation::test_api_package_import_is_idempotent PASSED [ 56%]
tests/torch/test_backend_selection.py::TestBackendSelection::test_defaults_to_tensorflow_backend PASSED [ 56%]
tests/torch/test_backend_selection.py::TestBackendSelection::test_selects_pytorch_backend PASSED [ 56%]
tests/torch/test_backend_selection.py::TestBackendSelection::test_pytorch_backend_calls_update_legacy_dict PASSED [ 57%]
tests/torch/test_backend_selection.py::TestBackendSelection::test_pytorch_unavailable_raises_error PASSED [ 57%]
tests/torch/test_backend_selection.py::TestBackendSelection::test_inference_config_supports_backend_selection PASSED [ 57%]
tests/torch/test_backend_selection.py::TestBackendSelection::test_backend_selection_preserves_api_parity PASSED [ 57%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLI::test_accelerator_flag_roundtrip FAILED [ 57%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLI::test_num_workers_flag_roundtrip PASSED [ 58%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLI::test_inference_batch_size_flag_roundtrip PASSED [ 58%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLI::test_multiple_execution_config_flags PASSED [ 58%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLIThinWrapper::test_cli_delegates_to_validate_paths PASSED [ 58%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLIThinWrapper::test_cli_delegates_to_helper_for_data_loading PASSED [ 58%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLIThinWrapper::test_cli_delegates_to_inference_helper PASSED [ 59%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLIThinWrapper::test_cli_calls_save_individual_reconstructions PASSED [ 59%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLIThinWrapper::test_quiet_flag_suppresses_progress_output PASSED [ 59%]
tests/torch/test_cli_shared.py::TestResolveAccelerator::test_default_no_device PASSED [ 59%]
tests/torch/test_cli_shared.py::TestResolveAccelerator::test_legacy_device_cpu PASSED [ 59%]
tests/torch/test_cli_shared.py::TestResolveAccelerator::test_legacy_device_cuda_maps_to_gpu PASSED [ 60%]
tests/torch/test_cli_shared.py::TestResolveAccelerator::test_conflict_accelerator_wins PASSED [ 60%]
tests/torch/test_cli_shared.py::TestResolveAccelerator::test_all_accelerator_values_passthrough PASSED [ 60%]
tests/torch/test_cli_shared.py::TestResolveAccelerator::test_resolve_accelerator_auto_defaults PASSED [ 60%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfig::test_training_mode_defaults PASSED [ 60%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfig::test_training_mode_custom_values PASSED [ 61%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfig::test_inference_mode PASSED [ 61%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfig::test_emits_deterministic_warning PASSED [ 61%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfig::test_handles_quiet_flag PASSED [ 61%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfig::test_handles_disable_mlflow_flag PASSED [ 61%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfig::test_quiet_or_disable_mlflow_both_true PASSED [ 61%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfig::test_invalid_mode_raises_value_error PASSED [ 62%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfig::test_resolves_accelerator_from_device_flag PASSED [ 62%]
tests/torch/test_cli_shared.py::TestValidatePaths::test_creates_output_dir PASSED [ 62%]
tests/torch/test_cli_shared.py::TestValidatePaths::test_raises_if_train_file_missing PASSED [ 62%]
tests/torch/test_cli_shared.py::TestValidatePaths::test_raises_if_test_file_missing PASSED [ 62%]
tests/torch/test_cli_shared.py::TestValidatePaths::test_accepts_none_test_file PASSED [ 63%]
tests/torch/test_cli_shared.py::TestValidatePaths::test_works_with_pathlib_path_objects PASSED [ 63%]
tests/torch/test_cli_shared.py::TestValidatePaths::test_accepts_none_train_file_for_inference_mode PASSED [ 63%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfigInferenceMode::test_inference_mode_defaults PASSED [ 63%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfigInferenceMode::test_inference_mode_custom_batch_size PASSED [ 63%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfigInferenceMode::test_inference_mode_respects_quiet PASSED [ 64%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_accelerator_flag_roundtrip PASSED [ 64%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_deterministic_flag_roundtrip PASSED [ 64%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_no_deterministic_flag_roundtrip PASSED [ 64%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_num_workers_flag_roundtrip PASSED [ 64%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_learning_rate_flag_roundtrip PASSED [ 65%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_multiple_execution_config_flags PASSED [ 65%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_bundle_persistence PASSED [ 65%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_enable_checkpointing_flag PASSED [ 65%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_checkpoint_save_top_k_flag PASSED [ 65%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_checkpoint_monitor_flag PASSED [ 66%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_checkpoint_mode_flag PASSED [ 66%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_early_stop_patience_flag PASSED [ 66%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_scheduler_flag_roundtrip PASSED [ 66%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_accumulate_grad_batches_roundtrip PASSED [ 66%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_logger_backend_csv_default PASSED [ 66%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_logger_backend_tensorboard PASSED [ 67%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_logger_backend_none PASSED [ 67%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_disable_mlflow_deprecation_warning PASSED [ 67%]
tests/torch/test_config_bridge.py::TestConfigBridgeMVP::test_mvp_config_bridge_populates_params_cfg PASSED [ 67%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_direct_fields[N-direct] PASSED [ 67%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_direct_fields[n_filters_scale-direct] PASSED [ 68%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_direct_fields[object_big-direct] PASSED [ 68%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_direct_fields[probe_big-direct] PASSED [ 68%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_direct_fields[batch_size-direct] PASSED [ 68%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_transform_fields[gridsize-tuple-to-int] PASSED [ 68%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_transform_fields[model_type-unsupervised] PASSED [ 69%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_transform_fields[model_type-supervised] PASSED [ 69%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_transform_fields[amp_activation-silu] PASSED [ 69%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_transform_fields[amp_activation-SiLU] PASSED [ 69%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_transform_fields[amp_activation-passthrough] PASSED [ 69%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_transform_fields[nepochs-rename] PASSED [ 70%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_transform_fields[nll_weight-true] PASSED [ 70%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_transform_fields[nll_weight-false] PASSED [ 70%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_override_fields[pad_object-default] PASSED [ 70%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_override_fields[pad_object-override] PASSED [ 70%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_override_fields[gaussian_smoothing_sigma-default] PASSED [ 71%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_override_fields[gaussian_smoothing_sigma-override] PASSED [ 71%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[mae_weight-default] PASSED [ 71%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[mae_weight-override] PASSED [ 71%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[realspace_mae_weight-default] PASSED [ 71%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[realspace_weight-default] PASSED [ 72%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[positions_provided-default] PASSED [ 72%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[probe_trainable-default] PASSED [ 72%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[sequential_sampling-default] PASSED [ 72%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_inference_config_override_fields[debug-default] PASSED [ 72%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_inference_config_override_fields[debug-override] PASSED [ 72%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_probe_mask_translation[probe_mask-default] PASSED [ 73%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_probe_mask_override PASSED [ 73%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_default_divergence_detection[nphotons-divergence] PASSED [ 73%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_default_divergence_detection[probe_scale-divergence] PASSED [ 73%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_gridsize_error_handling[gridsize-non-square] PASSED [ 73%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_type_error_handling[model_type-invalid-enum] PASSED [ 74%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_activation_error_handling[amp_activation-unknown] PASSED [ 74%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_train_data_file_required_error PASSED [ 74%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_path_required_error PASSED [ 74%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_nphotons_default_divergence_error PASSED [ 74%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_nphotons_override_passes_validation PASSED [ 75%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_n_subsample_missing_override_uses_none PASSED [ 75%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_n_subsample_explicit_override PASSED [ 75%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_inference_config_n_subsample_missing_override_uses_none PASSED [ 75%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_inference_config_n_subsample_explicit_override PASSED [ 75%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_subsample_seed_from_dataconfig PASSED [ 76%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_subsample_seed_override PASSED [ 76%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_n_groups_missing_override_warning PASSED [ 76%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_test_data_file_training_missing_warning PASSED [ 76%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_params_cfg_matches_baseline PASSED [ 76%]
tests/torch/test_config_factory.py::TestTrainingPayloadStructure::test_training_payload_returns_dataclass PASSED [ 77%]
tests/torch/test_config_factory.py::TestTrainingPayloadStructure::test_training_payload_contains_tf_config PASSED [ 77%]
tests/torch/test_config_factory.py::TestTrainingPayloadStructure::test_training_payload_contains_pytorch_configs PASSED [ 77%]
tests/torch/test_config_factory.py::TestTrainingPayloadStructure::test_training_payload_contains_overrides_dict PASSED [ 77%]
tests/torch/test_config_factory.py::TestTrainingPayloadStructure::test_gridsize_sets_channel_count PASSED [ 77%]
tests/torch/test_config_factory.py::TestInferencePayloadStructure::test_inference_payload_returns_dataclass PASSED [ 77%]
tests/torch/test_config_factory.py::TestInferencePayloadStructure::test_inference_payload_contains_tf_config PASSED [ 78%]
tests/torch/test_config_factory.py::TestInferencePayloadStructure::test_inference_payload_contains_pytorch_configs PASSED [ 78%]
tests/torch/test_config_factory.py::TestConfigBridgeTranslation::test_grid_size_tuple_to_gridsize_int PASSED [ 78%]
tests/torch/test_config_factory.py::TestConfigBridgeTranslation::test_epochs_to_nepochs_conversion PASSED [ 78%]
tests/torch/test_config_factory.py::TestConfigBridgeTranslation::test_k_to_neighbor_count_conversion PASSED [ 78%]
tests/torch/test_config_factory.py::TestLegacyParamsPopulation::test_factory_populates_params_cfg PASSED [ 79%]
tests/torch/test_config_factory.py::TestLegacyParamsPopulation::test_populate_legacy_params_helper PASSED [ 79%]
tests/torch/test_config_factory.py::TestOverridePrecedence::test_override_dict_wins_over_defaults PASSED [ 79%]
tests/torch/test_config_factory.py::TestOverridePrecedence::test_probe_size_override_wins_over_inference PASSED [ 79%]
tests/torch/test_config_factory.py::TestFactoryValidation::test_missing_n_groups_raises_error PASSED [ 79%]
tests/torch/test_config_factory.py::TestFactoryValidation::test_nonexistent_train_data_file_raises_error PASSED [ 80%]
tests/torch/test_config_factory.py::TestFactoryValidation::test_missing_checkpoint_raises_error PASSED [ 80%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_training_payload_execution_config_not_none PASSED [ 80%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_inference_payload_execution_config_not_none PASSED [ 80%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_execution_config_defaults_applied PASSED [ 80%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_execution_config_explicit_instance_propagates PASSED [ 81%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_execution_config_fields_accessible PASSED [ 81%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_overrides_applied_records_execution_knobs PASSED [ 81%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_checkpoint_knobs_propagate_through_factory PASSED [ 81%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_checkpoint_defaults_respected PASSED [ 81%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_scheduler_override_applied PASSED [ 82%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_accum_steps_override_applied PASSED [ 82%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_logger_backend_csv_default PASSED [ 82%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_logger_backend_tensorboard PASSED [ 82%]
tests/torch/test_config_factory.py::TestProbeSizeInference::test_infer_probe_size_from_npz PASSED [ 82%]
tests/torch/test_config_factory.py::TestProbeSizeInference::test_infer_probe_size_missing_file_fallback PASSED [ 83%]
tests/torch/test_data_pipeline.py::TestRawDataTorchAdapter::test_raw_data_torch_matches_tensorflow PASSED [ 83%]
tests/torch/test_data_pipeline.py::TestDataContainerParity::test_data_container_shapes_and_dtypes PASSED [ 83%]
tests/torch/test_data_pipeline.py::TestGroundTruthLoading::test_y_patches_are_complex64 PASSED [ 83%]
tests/torch/test_data_pipeline.py::TestMemmapBridgeParity::test_memmap_loader_matches_raw_data_torch PASSED [ 83%]
tests/torch/test_data_pipeline.py::TestMemmapBridgeParity::test_deterministic_generation_validation PASSED [ 83%]
tests/torch/test_data_pipeline.py::TestMemmapBridgeParity::test_memmap_bridge_accepts_diffraction_legacy PASSED [ 84%]
tests/torch/test_dataloader.py::TestDataloaderCanonicalKeySupport::test_backward_compat_legacy_diff3d PASSED [ 84%]
tests/torch/test_dataloader.py::TestDataloaderCanonicalKeySupport::test_error_when_no_diffraction_key PASSED [ 84%]
tests/torch/test_dataloader.py::TestDataloaderCanonicalKeySupport::test_loads_canonical_diffraction PASSED [ 84%]
tests/torch/test_dataloader.py::TestDataloaderFormatAutoTranspose::test_auto_transposes_legacy_hwn_format PASSED [ 84%]
tests/torch/test_dataloader.py::TestDataloaderFormatAutoTranspose::test_handles_edge_case_square_dataset PASSED [ 85%]
tests/torch/test_dataloader.py::TestDataloaderFormatAutoTranspose::test_npz_headers_also_transposes_shape PASSED [ 85%]
tests/torch/test_dataloader.py::TestDataloaderFormatAutoTranspose::test_preserves_canonical_nwh_format PASSED [ 85%]
tests/torch/test_dataloader.py::TestDataloaderFormatAutoTranspose::test_real_dataset_dimensions PASSED [ 85%]
tests/torch/test_dataloader.py::TestDataloaderFormatAutoTranspose::test_works_with_diff3d_legacy_key PASSED [ 85%]
tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_auto_prefers_cuda PASSED [ 86%]
tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_auto_warns_and_falls_back_to_cpu PASSED [ 86%]
tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_explicit_cpu_bypasses_auto_resolution PASSED [ 86%]
tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_explicit_cuda_bypasses_auto_resolution PASSED [ 86%]
tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_workflow_auto_instantiates_with_hardware_detection PASSED [ 86%]
tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_backend_selector_warns_on_cpu_only_hosts SKIPPED [ 87%]
tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_backend_selector_inherits_gpu_first_defaults PASSED [ 87%]
tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_backend_selector_cpu_fallback_with_warning PASSED [ 87%]
tests/torch/test_fixture_pytorch_integration.py::TestFixtureContract::test_fixture_file_exists PASSED [ 87%]
tests/torch/test_fixture_pytorch_integration.py::TestFixtureContract::test_fixture_outputs_match_contract PASSED [ 87%]
tests/torch/test_fixture_pytorch_integration.py::TestFixtureContract::test_metadata_sidecar_exists PASSED [ 88%]
tests/torch/test_fixture_pytorch_integration.py::TestFixtureContract::test_metadata_content_valid PASSED [ 88%]
tests/torch/test_fixture_pytorch_integration.py::TestFixtureContract::test_coordinate_coverage PASSED [ 88%]
tests/torch/test_fixture_pytorch_integration.py::TestFixtureIntegrationSmoke::test_fixture_loads_with_rawdata PASSED [ 88%]
tests/torch/test_fixture_pytorch_integration.py::TestFixtureIntegrationSmoke::test_fixture_compatible_with_pytorch_dataloader PASSED [ 88%]
tests/torch/test_inference_reassembly_parity.py::test_inference_helper_uses_reassembly PASSED [ 88%]
tests/torch/test_inference_reassembly_parity.py::test_reassembly_canvas_padding_invariants PASSED [ 89%]
tests/torch/test_integration_workflow_torch.py::test_bundle_loader_returns_modules PASSED [ 89%]
tests/torch/test_integration_workflow_torch.py::test_run_pytorch_train_save_load_infer PASSED [ 89%]
tests/torch/test_integration_workflow_torch.py::TestPyTorchIntegrationWorkflow::test_pytorch_train_save_load_infer_cycle_legacy SKIPPED [ 89%]
tests/torch/test_integration_workflow_torch.py::TestPyTorchIntegrationWorkflow::test_pytorch_tf_output_parity SKIPPED [ 89%]
tests/torch/test_lightning_checkpoint.py::TestLightningCheckpointSerialization::test_checkpoint_contains_hyperparameters PASSED [ 90%]
tests/torch/test_lightning_checkpoint.py::TestLightningCheckpointSerialization::test_load_from_checkpoint_without_kwargs PASSED [ 90%]
tests/torch/test_lightning_checkpoint.py::TestLightningCheckpointSerialization::test_checkpoint_configs_are_serializable PASSED [ 90%]
tests/torch/test_loss_modes.py::test_poisson_loss_mode_logs_poisson_metrics PASSED [ 90%]
tests/torch/test_loss_modes.py::test_mae_loss_mode_logs_mae_metrics_only PASSED [ 90%]
tests/torch/test_model_manager.py::TestSaveTorchBundle::test_archive_structure PASSED [ 91%]
tests/torch/test_model_manager.py::TestSaveTorchBundle::test_params_snapshot PASSED [ 91%]
tests/torch/test_model_manager.py::TestLoadTorchBundle::test_load_round_trip_updates_params_cfg PASSED [ 91%]
tests/torch/test_model_manager.py::TestLoadTorchBundle::test_missing_params_raises_value_error PASSED [ 91%]
tests/torch/test_model_manager.py::TestLoadTorchBundle::test_load_round_trip_returns_model_stub PASSED [ 91%]
tests/torch/test_model_manager.py::TestLoadTorchBundle::test_reconstructs_models_from_bundle PASSED [ 92%]
tests/torch/test_tf_helper.py::TestTorchTFHelper::test_combine_complex SKIPPED [ 92%]
tests/torch/test_tf_helper.py::TestTorchTFHelper::test_get_mask SKIPPED  [ 92%]
tests/torch/test_tf_helper.py::TestTorchTFHelper::test_placeholder_torch_functions SKIPPED [ 92%]
tests/torch/test_train_probe_size.py::TestNPZProbeSizeExtraction::test_infer_probe_size_128 PASSED [ 92%]
tests/torch/test_train_probe_size.py::TestNPZProbeSizeExtraction::test_infer_probe_size_from_npz PASSED [ 93%]
tests/torch/test_train_probe_size.py::TestNPZProbeSizeExtraction::test_infer_probe_size_missing_probe PASSED [ 93%]
tests/torch/test_train_probe_size.py::TestNPZProbeSizeExtraction::test_infer_probe_size_real_dataset PASSED [ 93%]
tests/torch/test_train_probe_size.py::TestNPZProbeSizeExtraction::test_infer_probe_size_rectangular PASSED [ 93%]
tests/torch/test_workflows_components.py::TestWorkflowsComponentsScaffold::test_run_cdi_example_calls_update_legacy_dict FAILED [ 93%]
tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_train_cdi_model_torch_invokes_lightning FAILED [ 94%]
tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_lightning_dataloader_tensor_dict_structure PASSED [ 94%]
tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_lightning_poisson_count_contract PASSED [ 94%]
tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_lightning_training_respects_gridsize PASSED [ 94%]
tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_coords_relative_layout PASSED [ 94%]
tests/torch/test_workflows_components.py::TestWorkflowsComponentsRun::test_run_cdi_example_invokes_training FAILED [ 94%]
tests/torch/test_workflows_components.py::TestWorkflowsComponentsRun::test_run_cdi_example_persists_models FAILED [ 95%]
tests/torch/test_workflows_components.py::TestWorkflowsComponentsRun::test_load_inference_bundle_handles_bundle PASSED [ 95%]
tests/torch/test_workflows_components.py::TestTrainWithLightningRed::test_train_with_lightning_instantiates_module PASSED [ 95%]
tests/torch/test_workflows_components.py::TestTrainWithLightningRed::test_train_with_lightning_runs_trainer_fit FAILED [ 95%]
tests/torch/test_workflows_components.py::TestTrainWithLightningRed::test_train_with_lightning_returns_models_dict FAILED [ 95%]
tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_reassemble_cdi_image_torch_guard_without_train_results PASSED [ 96%]
tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_reassemble_cdi_image_torch_flip_transpose_contract[False-False-False] PASSED [ 96%]
tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_reassemble_cdi_image_torch_flip_transpose_contract[True-False-False] PASSED [ 96%]
tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_reassemble_cdi_image_torch_flip_transpose_contract[False-True-False] PASSED [ 96%]
tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_reassemble_cdi_image_torch_flip_transpose_contract[False-False-True] PASSED [ 96%]
tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_reassemble_cdi_image_torch_flip_transpose_contract[True-True-True] PASSED [ 97%]
tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_run_cdi_example_torch_do_stitching_delegates_to_reassemble FAILED [ 97%]
tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_reassemble_cdi_image_torch_return_contract PASSED [ 97%]
tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchFloat32::test_batches_remain_float32 PASSED [ 97%]
tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchFloat32::test_dataloader_casts_float64_to_float32 PASSED [ 97%]
tests/torch/test_workflows_components.py::TestDecoderLastShapeParity::test_probe_big_shape_alignment PASSED [ 98%]
tests/torch/test_workflows_components.py::TestDecoderLastShapeParity::test_probe_big_false_no_mismatch PASSED [ 98%]
tests/torch/test_workflows_components.py::TestTrainWithLightningGreen::test_execution_config_overrides_trainer FAILED [ 98%]
tests/torch/test_workflows_components.py::TestTrainWithLightningGreen::test_execution_config_controls_determinism FAILED [ 98%]
tests/torch/test_workflows_components.py::TestInferenceExecutionConfig::test_inference_uses_execution_batch_size PASSED [ 98%]
tests/torch/test_workflows_components.py::TestLightningCheckpointCallbacks::test_model_checkpoint_callback_configured PASSED [ 99%]
tests/torch/test_workflows_components.py::TestLightningCheckpointCallbacks::test_early_stopping_callback_configured PASSED [ 99%]
tests/torch/test_workflows_components.py::TestLightningCheckpointCallbacks::test_disable_checkpointing_skips_callbacks PASSED [ 99%]
tests/torch/test_workflows_components.py::TestLightningExecutionConfig::test_trainer_receives_accumulation FAILED [ 99%]
tests/torch/test_workflows_components.py::TestLightningExecutionConfig::test_monitor_uses_val_loss_name PASSED [ 99%]
tests/torch/test_workflows_components.py::TestLightningExecutionConfig::test_trainer_receives_logger PASSED [100%]

=================================== FAILURES ===================================
____________ test_run_phase_g_dense_collect_only_generates_commands ____________

tmp_path = PosixPath('/tmp/pytest-of-ollie/pytest-716/test_run_phase_g_dense_collect0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x71a8a8b02550>
capsys = <_pytest.capture.CaptureFixture object at 0x71a8a8b02e50>

    def test_run_phase_g_dense_collect_only_generates_commands(tmp_path: Path, monkeypatch: pytest.MonkeyPatch, capsys) -> None:
        """
        Test that main() with --collect-only prints planned commands without executing them.
    
        Acceptance:
        - Loads main() from orchestrator script via importlib
        - Runs with --collect-only into a tmp hub directory
        - Asserts stdout contains expected command substrings (Phase C/D/E/F/G markers)
        - Verifies no Phase C outputs are created (dry-run mode, no filesystem side effects)
        - Ensures AUTHORITATIVE_CMDS_DOC environment variable is respected
        - Returns 0 exit code on success
    
        Follows TYPE-PATH-001 (Path normalization).
        """
        # Import main() from orchestrator
        module = _import_orchestrator_module()
        main = module.main
    
        # Setup: Create tmp hub directory
        hub = tmp_path / "collect_only_hub"
        hub.mkdir(parents=True)
    
        # Set AUTHORITATIVE_CMDS_DOC to satisfy orchestrator env check
        monkeypatch.setenv("AUTHORITATIVE_CMDS_DOC", "./docs/TESTING_GUIDE.md")
    
        # Prepare sys.argv for argparse
        monkeypatch.setattr(
            sys,
            "argv",
            [
                "run_phase_g_dense.py",
                "--hub", str(hub),
                "--dose", "1000",
                "--view", "dense",
                "--splits", "train", "test",
                "--collect-only",
            ],
        )
    
        # Execute: Call main() (should print commands and return 0)
        exit_code = main()
    
        # Assert: Exit code should be 0
        assert exit_code == 0, f"Expected exit code 0 from --collect-only mode, got {exit_code}"
    
        # Assert: Capture stdout and verify expected command substrings
        captured = capsys.readouterr()
        stdout = captured.out
    
        # Check for phase markers in stdout
        assert "Phase C: Dataset Generation" in stdout, "Missing Phase C command in --collect-only output"
        assert "Phase D: Overlap View Generation" in stdout, "Missing Phase D command in --collect-only output"
        assert "Phase E: Training Baseline (gs1)" in stdout, "Missing Phase E baseline command in --collect-only output"
        assert "Phase E: Training Dense (gs2)" in stdout, "Missing Phase E dense command in --collect-only output"
        assert "Phase F: Reconstruction" in stdout, "Missing Phase F command in --collect-only output"
        assert "Phase G: Comparison" in stdout, "Missing Phase G command in --collect-only output"
    
        # Check for specific command keywords
        assert "studies.fly64_dose_overlap.generation" in stdout, "Missing generation module in command output"
>       assert "studies.fly64_dose_overlap.overlap" in stdout, "Missing overlap module in command output"
E       AssertionError: Missing overlap module in command output
E       assert 'studies.fly64_dose_overlap.overlap' in '[run_phase_g_dense] Collect-only mode: planned commands:\n\n1. Phase C: Dataset Generation\n   Command: /home/ollie/miniconda3/envs/ptycho311/bin/python3.11 -m studies.fly64_dose_overlap.generation --base-npz tike_outputs/fly001_reconstructed_final_prepared/fly001_reconstructed_interp_smooth_both.npz --output-root /tmp/pytest-of-ollie/pytest-716/test_run_phase_g_dense_collect0/collect_only_hub/data/phase_c --dose 1000\n   Log: /tmp/pytest-of-ollie/pytest-716/test_run_phase_g_dense_collect0/collect_only_hub/cli/phase_c_generation.log\n\n2. Phase D: Overlap View Generation\n   Command: __PHASE_D_PROGRAMMATIC__\n   Log: /tmp/pytest-of-ollie/pytest-716/test_run_phase_g_dense_collect0/collect_only_hub/cli/phase_d_dense.log\n\n3. Phase E: Training Baseline (gs1)\n   Command: /home/ollie/miniconda3/envs/ptycho311/bin/python3.11 -m studies.fly64_dose_overlap.training --phase-c-root /tmp/pytest-of-ollie/pytest-716/test_run_phase_g_dense_collect0/collect_only_hub/data/phase_c --phase-d-root /tmp/pytest-of-ollie/pytest-716/test_run_phase_g_dense_collect0/collect_only_hub/data/phase_d --artifact-root /tmp/pytest-of-ollie/pytest-716/test_run_phase_g_dense_collect0/collect_only_hub/data/phase...t-716/test_run_phase_g_dense_collect0/collect_only_hub\n   Log: /tmp/pytest-of-ollie/pytest-716/test_run_phase_g_dense_collect0/collect_only_hub/cli/ssim_grid_cli.log\n\n12. Post-Verify: Verify pipeline artifacts\n   Command: /home/ollie/miniconda3/envs/ptycho311/bin/python3.11 /home/ollie/Documents/PtychoPINN/plans/active/STUDY-SYNTH-FLY64-DOSE-OVERLAP-001/bin/verify_dense_pipeline_artifacts.py --hub /tmp/pytest-of-ollie/pytest-716/test_run_phase_g_dense_collect0/collect_only_hub --report /tmp/pytest-of-ollie/pytest-716/test_run_phase_g_dense_collect0/collect_only_hub/analysis/verification_report.json --dose 1000 --view dense\n   Log: /tmp/pytest-of-ollie/pytest-716/test_run_phase_g_dense_collect0/collect_only_hub/analysis/verify_dense_stdout.log\n\n13. Post-Verify: Check highlights match\n   Command: /home/ollie/miniconda3/envs/ptycho311/bin/python3.11 /home/ollie/Documents/PtychoPINN/plans/active/STUDY-SYNTH-FLY64-DOSE-OVERLAP-001/bin/check_dense_highlights_match.py --hub /tmp/pytest-of-ollie/pytest-716/test_run_phase_g_dense_collect0/collect_only_hub\n   Log: /tmp/pytest-of-ollie/pytest-716/test_run_phase_g_dense_collect0/collect_only_hub/analysis/check_dense_highlights.log\n'

tests/study/test_phase_g_dense_orchestrator.py:88: AssertionError
___________________ test_run_phase_g_dense_post_verify_hooks ___________________

tmp_path = PosixPath('/tmp/pytest-of-ollie/pytest-716/test_run_phase_g_dense_post_ve0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x71a8a8f1ecd0>

    def test_run_phase_g_dense_post_verify_hooks(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
        """
        Test that main() invokes post-verify commands (verify_dense_pipeline_artifacts.py + check_dense_highlights_match.py) with correct paths.
    
        Acceptance:
        - Loads main() from orchestrator script via importlib
        - Stubs prepare_hub, validate_phase_c_metadata, summarize_phase_g_outputs (no-op for speed)
        - Monkeypatches run_command to record invocations
        - Runs main() without --skip-post-verify (default: post-verify enabled)
        - Asserts post-verify commands are invoked AFTER ssim_grid
        - Validates verify command includes --hub, --report, --dose, --view flags
        - Validates check command includes --hub flag
        - Validates log paths point to analysis/verify_dense_stdout.log and analysis/check_dense_highlights.log
        - Ensures AUTHORITATIVE_CMDS_DOC environment variable is respected
        - Returns 0 exit code on success
    
        Follows TYPE-PATH-001 (Path normalization), DATA-001, TEST-CLI-001.
        """
        # Import main() and helper functions from orchestrator
        module = _import_orchestrator_module()
        main = module.main
    
        # Setup: Create tmp hub directory
        hub = tmp_path / "post_verify_hub"
        hub.mkdir(parents=True)
    
        # Create expected directory structure for Phase CG
        phase_c_root = hub / "data" / "phase_c"
        phase_c_root.mkdir(parents=True)
        cli_log_dir = hub / "cli"
        cli_log_dir.mkdir(parents=True)
        phase_g_root = hub / "analysis"
        phase_g_root.mkdir(parents=True)
    
        # Set AUTHORITATIVE_CMDS_DOC to satisfy orchestrator env check
        monkeypatch.setenv("AUTHORITATIVE_CMDS_DOC", "./docs/TESTING_GUIDE.md")
    
        # Prepare sys.argv for argparse (NO --skip-post-verify, so post-verify enabled)
        monkeypatch.setattr(
            sys,
            "argv",
            [
                "run_phase_g_dense.py",
                "--hub", str(hub),
                "--dose", "1000",
                "--view", "dense",
                "--splits", "train", "test",
                "--clobber",  # Required to pass prepare_hub check
            ],
        )
    
        # Stub heavy helpers to no-op (we only care about run_command invocations)
        def stub_prepare_hub(hub_path, clobber):
            """No-op stub for prepare_hub."""
            pass
    
        def stub_validate_phase_c_metadata(hub_path):
            """No-op stub for validate_phase_c_metadata."""
            pass
    
        def stub_summarize_phase_g_outputs(hub_path):
            """Create metrics_summary.json with test data for delta computation."""
            analysis = Path(hub_path) / "analysis"
            analysis.mkdir(parents=True, exist_ok=True)
    
            # Create metrics_summary.json with aggregate_metrics for delta computation
            summary_data = {
                "n_jobs": 2,
                "n_success": 2,
                "n_failed": 0,
                "jobs": [],
                "aggregate_metrics": {
                    "PtychoPINN": {
                        "ms_ssim": {"mean_amplitude": 0.950, "mean_phase": 0.920},
                        "mae": {"mean_amplitude": 0.025, "mean_phase": 0.035}
                    },
                    "Baseline": {
                        "ms_ssim": {"mean_amplitude": 0.930, "mean_phase": 0.900},
                        "mae": {"mean_amplitude": 0.030, "mean_phase": 0.040}
                    },
                    "PtyChi": {
                        "ms_ssim": {"mean_amplitude": 0.940, "mean_phase": 0.910},
                        "mae": {"mean_amplitude": 0.027, "mean_phase": 0.037}
                    }
                }
            }
    
            import json
            metrics_summary_path = analysis / "metrics_summary.json"
            with metrics_summary_path.open("w", encoding="utf-8") as f:
                json.dump(summary_data, f, indent=2)
    
        def stub_generate_artifact_inventory(hub_path):
            """Create artifact_inventory.txt to satisfy orchestrator validation."""
            analysis = Path(hub_path) / "analysis"
            analysis.mkdir(parents=True, exist_ok=True)
            inventory_path = analysis / "artifact_inventory.txt"
            inventory_path.write_text("# Stub artifact inventory\n", encoding="utf-8")
    
        monkeypatch.setattr(module, "prepare_hub", stub_prepare_hub)
        monkeypatch.setattr(module, "validate_phase_c_metadata", stub_validate_phase_c_metadata)
        monkeypatch.setattr(module, "summarize_phase_g_outputs", stub_summarize_phase_g_outputs)
        monkeypatch.setattr(module, "generate_artifact_inventory", stub_generate_artifact_inventory)
    
        # Record run_command invocations
        run_command_calls = []
    
        def stub_run_command(cmd, log_path):
            """Record cmd and log_path, create required files for orchestrator progression."""
            run_command_calls.append((cmd, log_path))
            cmd_str = " ".join(str(c) for c in cmd)
    
            # When reporting helper is invoked, create highlights file
            if "report_phase_g_dense_metrics.py" in cmd_str and "--highlights" in cmd_str:
                for i, part in enumerate(cmd):
                    if str(part) == "--highlights" and i + 1 < len(cmd):
                        highlights_path = Path(cmd[i + 1])
                        highlights_path.parent.mkdir(parents=True, exist_ok=True)
                        highlights_path.write_text("MS-SSIM Deltas\n", encoding="utf-8")
                        break
    
            # When analyze digest is invoked, create digest file
            if "analyze_dense_metrics.py" in cmd_str and "--output" in cmd_str:
                for i, part in enumerate(cmd):
                    if str(part) == "--output" and i + 1 < len(cmd):
                        digest_path = Path(cmd[i + 1])
                        digest_path.parent.mkdir(parents=True, exist_ok=True)
                        digest_path.write_text("# Digest\n", encoding="utf-8")
                        break
    
            # When ssim_grid is invoked, create summary file
            if "ssim_grid.py" in cmd_str and "--hub" in cmd_str:
                for i, part in enumerate(cmd):
                    if str(part) == "--hub" and i + 1 < len(cmd):
                        hub_path = Path(cmd[i + 1])
                        ssim_grid_summary_path = hub_path / "analysis" / "ssim_grid_summary.md"
                        ssim_grid_summary_path.parent.mkdir(parents=True, exist_ok=True)
                        ssim_grid_summary_path.write_text("# SSIM Grid\n", encoding="utf-8")
                        break
    
            # When verify_dense_pipeline_artifacts is invoked, create report file
            if "verify_dense_pipeline_artifacts.py" in cmd_str and "--report" in cmd_str:
                for i, part in enumerate(cmd):
                    if str(part) == "--report" and i + 1 < len(cmd):
                        report_path = Path(cmd[i + 1])
                        report_path.parent.mkdir(parents=True, exist_ok=True)
                        report_path.write_text('{"valid": true}\n', encoding="utf-8")
                        break
    
            # When check_dense_highlights_match is invoked, no file creation needed (just stdout)
            # (check_dense_highlights_match.py outputs to stdout, which is captured by run_command log_path)
    
        monkeypatch.setattr(module, "run_command", stub_run_command)
    
        # Execute: Call main() (should execute Phase CG pipeline + reporting helper + analyze digest + ssim_grid + post-verify)
        exit_code = main()
    
        # Assert: Exit code should be 0
>       assert exit_code == 0, f"Expected exit code 0 from real execution mode, got {exit_code}"
E       AssertionError: Expected exit code 0 from real execution mode, got 1
E       assert 1 == 0

tests/study/test_phase_g_dense_orchestrator.py:278: AssertionError
----------------------------- Captured stdout call -----------------------------

================================================================================
[run_phase_g_dense] Preparing hub...
================================================================================


[run_phase_g_dense] Starting Phase CG pipeline for dose=1000, view=dense, splits=['train', 'test']
[run_phase_g_dense] Hub: /tmp/pytest-of-ollie/pytest-716/test_run_phase_g_dense_post_ve0/post_verify_hub
[run_phase_g_dense] Total commands: 8


================================================================================
[1/8] Phase C: Dataset Generation
================================================================================


================================================================================
[run_phase_g_dense] Cleaning up unwanted dose directories (keeping dose_1000 only)...
================================================================================


================================================================================
[run_phase_g_dense] Validating Phase C metadata...
================================================================================


================================================================================
[2/8] Phase D: Overlap View Generation
================================================================================

_____________ test_run_phase_g_dense_exec_invokes_reporting_helper _____________

tmp_path = PosixPath('/tmp/pytest-of-ollie/pytest-716/test_run_phase_g_dense_exec_in0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x71a8a8f1fe10>

    def test_run_phase_g_dense_exec_invokes_reporting_helper(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
        """
        Test that main() in real execution mode invokes the reporting helper after Phase CG pipeline.
    
        Acceptance:
        - Loads main() from orchestrator script via importlib
        - Stubs prepare_hub, validate_phase_c_metadata, summarize_phase_g_outputs (no-op for speed)
        - Monkeypatches run_command to record invocations
        - Runs main() without --collect-only to trigger real execution path
        - Asserts final run_command call targets report_phase_g_dense_metrics.py script
        - Validates command includes --metrics metrics_summary.json and --output aggregate_report.md
        - Validates log_path points to cli/aggregate_report_cli.log
        - Ensures AUTHORITATIVE_CMDS_DOC environment variable is respected
        - Returns 0 exit code on success
    
        Follows TYPE-PATH-001 (Path normalization).
        """
        # Import main() and helper functions from orchestrator
        module = _import_orchestrator_module()
        main = module.main
    
        # Setup: Create tmp hub directory
        hub = tmp_path / "exec_hub"
        hub.mkdir(parents=True)
    
        # Create expected directory structure for Phase CG
        phase_c_root = hub / "data" / "phase_c"
        phase_c_root.mkdir(parents=True)
        cli_log_dir = hub / "cli"
        cli_log_dir.mkdir(parents=True)
        phase_g_root = hub / "analysis"
        phase_g_root.mkdir(parents=True)
    
        # Set AUTHORITATIVE_CMDS_DOC to satisfy orchestrator env check
        monkeypatch.setenv("AUTHORITATIVE_CMDS_DOC", "./docs/TESTING_GUIDE.md")
    
        # Prepare sys.argv for argparse (NO --collect-only, so real execution)
        monkeypatch.setattr(
            sys,
            "argv",
            [
                "run_phase_g_dense.py",
                "--hub", str(hub),
                "--dose", "1000",
                "--view", "dense",
                "--splits", "train", "test",
                "--clobber",  # Required to pass prepare_hub check
            ],
        )
    
        # Stub heavy helpers to no-op (we only care about run_command invocations)
        def stub_prepare_hub(hub_path, clobber):
            """No-op stub for prepare_hub."""
            pass
    
        def stub_validate_phase_c_metadata(hub_path):
            """No-op stub for validate_phase_c_metadata."""
            pass
    
        def stub_summarize_phase_g_outputs(hub_path):
            """No-op stub for summarize_phase_g_outputs."""
            pass
    
        monkeypatch.setattr(module, "prepare_hub", stub_prepare_hub)
        monkeypatch.setattr(module, "validate_phase_c_metadata", stub_validate_phase_c_metadata)
        monkeypatch.setattr(module, "summarize_phase_g_outputs", stub_summarize_phase_g_outputs)
    
        # Record run_command invocations
        run_command_calls = []
    
        def stub_run_command(cmd, log_path):
            """Record cmd and log_path for assertions, and create highlights file when reporting helper is invoked."""
            run_command_calls.append((cmd, log_path))
            # When reporting helper is invoked, create the highlights file to satisfy orchestrator expectations
            cmd_str = " ".join(str(c) for c in cmd)
            if "report_phase_g_dense_metrics.py" in cmd_str and "--highlights" in cmd_str:
                # Extract highlights path from command
                for i, part in enumerate(cmd):
                    if str(part) == "--highlights" and i + 1 < len(cmd):
                        highlights_path = Path(cmd[i + 1])
                        highlights_path.parent.mkdir(parents=True, exist_ok=True)
                        # Write minimal highlights content
                        highlights_path.write_text("Minimal highlights for test\n", encoding="utf-8")
                        break
    
        monkeypatch.setattr(module, "run_command", stub_run_command)
    
        # Execute: Call main() (should execute Phase CG pipeline + reporting helper)
        exit_code = main()
    
        # Assert: Exit code should be 0
>       assert exit_code == 0, f"Expected exit code 0 from real execution mode, got {exit_code}"
E       AssertionError: Expected exit code 0 from real execution mode, got 1
E       assert 1 == 0

tests/study/test_phase_g_dense_orchestrator.py:1055: AssertionError
----------------------------- Captured stdout call -----------------------------

================================================================================
[run_phase_g_dense] Preparing hub...
================================================================================


[run_phase_g_dense] Starting Phase CG pipeline for dose=1000, view=dense, splits=['train', 'test']
[run_phase_g_dense] Hub: /tmp/pytest-of-ollie/pytest-716/test_run_phase_g_dense_exec_in0/exec_hub
[run_phase_g_dense] Total commands: 8


================================================================================
[1/8] Phase C: Dataset Generation
================================================================================


================================================================================
[run_phase_g_dense] Cleaning up unwanted dose directories (keeping dose_1000 only)...
================================================================================


================================================================================
[run_phase_g_dense] Validating Phase C metadata...
================================================================================


================================================================================
[2/8] Phase D: Overlap View Generation
================================================================================

____________ test_run_phase_g_dense_exec_prints_highlights_preview _____________

tmp_path = PosixPath('/tmp/pytest-of-ollie/pytest-716/test_run_phase_g_dense_exec_pr0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x71a8a8f1f550>
capsys = <_pytest.capture.CaptureFixture object at 0x71a9e629f350>

    def test_run_phase_g_dense_exec_prints_highlights_preview(tmp_path: Path, monkeypatch: pytest.MonkeyPatch, capsys) -> None:
        """
        Test that main() in real execution mode prints an "Aggregate highlights" preview after reporting helper.
    
        Acceptance:
        - Loads main() from orchestrator script via importlib
        - Stubs prepare_hub, validate_phase_c_metadata, summarize_phase_g_outputs (no-op for speed)
        - Stubs run_command to write deterministic highlights text when reporting helper is invoked
        - Runs main() without --collect-only to trigger real execution path
        - Captures stdout via capsys
        - Asserts stdout contains "Aggregate highlights preview" banner
        - Asserts stdout contains sample highlights content (MS-SSIM/MAE deltas)
        - Returns 0 exit code on success
    
        Follows TYPE-PATH-001 (Path normalization).
        """
        # Import main() and helper functions from orchestrator
        module = _import_orchestrator_module()
        main = module.main
    
        # Setup: Create tmp hub directory
        hub = tmp_path / "exec_hub"
        hub.mkdir(parents=True)
    
        # Create expected directory structure for Phase CG
        phase_c_root = hub / "data" / "phase_c"
        phase_c_root.mkdir(parents=True)
        cli_log_dir = hub / "cli"
        cli_log_dir.mkdir(parents=True)
        phase_g_root = hub / "analysis"
        phase_g_root.mkdir(parents=True)
    
        # Set AUTHORITATIVE_CMDS_DOC to satisfy orchestrator env check
        monkeypatch.setenv("AUTHORITATIVE_CMDS_DOC", "./docs/TESTING_GUIDE.md")
    
        # Prepare sys.argv for argparse (NO --collect-only, so real execution)
        monkeypatch.setattr(
            sys,
            "argv",
            [
                "run_phase_g_dense.py",
                "--hub", str(hub),
                "--dose", "1000",
                "--view", "dense",
                "--splits", "train", "test",
                "--clobber",  # Required to pass prepare_hub check
            ],
        )
    
        # Stub heavy helpers to no-op (we only care about run_command invocations)
        def stub_prepare_hub(hub_path, clobber):
            """No-op stub for prepare_hub."""
            pass
    
        def stub_validate_phase_c_metadata(hub_path):
            """No-op stub for validate_phase_c_metadata."""
            pass
    
        def stub_summarize_phase_g_outputs(hub_path):
            """No-op stub for summarize_phase_g_outputs."""
            pass
    
        monkeypatch.setattr(module, "prepare_hub", stub_prepare_hub)
        monkeypatch.setattr(module, "validate_phase_c_metadata", stub_validate_phase_c_metadata)
        monkeypatch.setattr(module, "summarize_phase_g_outputs", stub_summarize_phase_g_outputs)
    
        # Create deterministic highlights file when reporting helper is invoked
        def stub_run_command(cmd, log_path):
            """Stub that writes highlights file when reporting helper is invoked."""
            cmd_str = " ".join(str(c) for c in cmd)
            if "report_phase_g_dense_metrics.py" in cmd_str and "--highlights" in cmd_str:
                # Extract highlights path from command
                # Command format: [..., '--highlights', 'path/to/aggregate_highlights.txt', ...]
                for i, part in enumerate(cmd):
                    if str(part) == "--highlights" and i + 1 < len(cmd):
                        highlights_path = Path(cmd[i + 1])
                        highlights_path.parent.mkdir(parents=True, exist_ok=True)
                        # Write deterministic highlights content
                        highlights_path.write_text(
                            "Phase G Dense Metrics  Highlights\n"
                            "==================================================\n"
                            "\n"
                            "MS-SSIM Deltas (PtychoPINN - Baseline):\n"
                            "  Amplitude (mean): +0.123\n"
                            "  Phase (mean):     +0.045\n"
                            "\n"
                            "MS-SSIM Deltas (PtychoPINN - PtyChi):\n"
                            "  Amplitude (mean): +0.067\n"
                            "  Phase (mean):     +0.012\n"
                            "\n"
                            "MAE Deltas (PtychoPINN - Baseline):\n"
                            "  [Note: Negative = PtychoPINN better (lower error)]\n"
                            "  Amplitude (mean): -0.008\n"
                            "  Phase (mean):     -0.003\n"
                            "\n"
                            "MAE Deltas (PtychoPINN - PtyChi):\n"
                            "  [Note: Negative = PtychoPINN better (lower error)]\n"
                            "  Amplitude (mean): -0.005\n"
                            "  Phase (mean):     -0.001\n",
                            encoding="utf-8"
                        )
                        break
    
        monkeypatch.setattr(module, "run_command", stub_run_command)
    
        # Execute: Call main() (should execute Phase CG pipeline + reporting helper + highlights preview)
        exit_code = main()
    
        # Assert: Exit code should be 0
>       assert exit_code == 0, f"Expected exit code 0 from real execution mode, got {exit_code}"
E       AssertionError: Expected exit code 0 from real execution mode, got 1
E       assert 1 == 0

tests/study/test_phase_g_dense_orchestrator.py:1201: AssertionError
----------------------------- Captured stdout call -----------------------------

================================================================================
[run_phase_g_dense] Preparing hub...
================================================================================


[run_phase_g_dense] Starting Phase CG pipeline for dose=1000, view=dense, splits=['train', 'test']
[run_phase_g_dense] Hub: /tmp/pytest-of-ollie/pytest-716/test_run_phase_g_dense_exec_pr0/exec_hub
[run_phase_g_dense] Total commands: 8


================================================================================
[1/8] Phase C: Dataset Generation
================================================================================


================================================================================
[run_phase_g_dense] Cleaning up unwanted dose directories (keeping dose_1000 only)...
================================================================================


================================================================================
[run_phase_g_dense] Validating Phase C metadata...
================================================================================


================================================================================
[2/8] Phase D: Overlap View Generation
================================================================================

_______________ test_run_phase_g_dense_exec_runs_analyze_digest ________________

tmp_path = PosixPath('/tmp/pytest-of-ollie/pytest-716/test_run_phase_g_dense_exec_ru0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x71a8a8b11910>

    def test_run_phase_g_dense_exec_runs_analyze_digest(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
        """
        Test that main() in real execution mode invokes analyze_dense_metrics.py after reporting helper
        and emits MS-SSIM/MAE delta summary to stdout.
    
        Acceptance:
        - Loads main() from orchestrator script via importlib
        - Stubs prepare_hub, validate_phase_c_metadata, summarize_phase_g_outputs (seeds metrics_summary.json)
        - Monkeypatches run_command to record invocations and create required files
        - Runs main() without --collect-only to trigger real execution path
        - Asserts analyze_dense_metrics.py is invoked after report_phase_g_dense_metrics.py
        - Validates analyze command includes --metrics, --highlights, --output flags
        - Validates log_path points to cli/metrics_digest_cli.log
        - Validates stdout contains delta block with four lines (MS-SSIM vs Baseline/PtyChi, MAE vs Baseline/PtyChi)
        - Ensures AUTHORITATIVE_CMDS_DOC environment variable is respected
        - Returns 0 exit code on success
    
        Follows TYPE-PATH-001 (Path normalization).
        """
        # Import main() and helper functions from orchestrator
        module = _import_orchestrator_module()
        main = module.main
    
        # Setup: Create tmp hub directory
        hub = tmp_path / "exec_hub"
        hub.mkdir(parents=True)
    
        # Create expected directory structure for Phase CG
        phase_c_root = hub / "data" / "phase_c"
        phase_c_root.mkdir(parents=True)
        cli_log_dir = hub / "cli"
        cli_log_dir.mkdir(parents=True)
        phase_g_root = hub / "analysis"
        phase_g_root.mkdir(parents=True)
    
        # Set AUTHORITATIVE_CMDS_DOC to satisfy orchestrator env check
        monkeypatch.setenv("AUTHORITATIVE_CMDS_DOC", "./docs/TESTING_GUIDE.md")
    
        # Prepare sys.argv for argparse (NO --collect-only, so real execution)
        monkeypatch.setattr(
            sys,
            "argv",
            [
                "run_phase_g_dense.py",
                "--hub", str(hub),
                "--dose", "1000",
                "--view", "dense",
                "--splits", "train", "test",
                "--clobber",  # Required to pass prepare_hub check
            ],
        )
    
        # Stub heavy helpers to no-op (we only care about run_command invocations)
        def stub_prepare_hub(hub_path, clobber):
            """No-op stub for prepare_hub."""
            pass
    
        def stub_validate_phase_c_metadata(hub_path):
            """No-op stub for validate_phase_c_metadata."""
            pass
    
        def stub_summarize_phase_g_outputs(hub_path):
            """Create metrics_summary.json with test data for delta computation."""
            analysis = Path(hub_path) / "analysis"
            analysis.mkdir(parents=True, exist_ok=True)
    
            # Create metrics_summary.json with aggregate_metrics for delta computation
            summary_data = {
                "n_jobs": 2,
                "n_success": 2,
                "n_failed": 0,
                "jobs": [],
                "aggregate_metrics": {
                    "PtychoPINN": {
                        "ms_ssim": {
                            "mean_amplitude": 0.950,
                            "best_amplitude": 0.955,
                            "mean_phase": 0.920,
                            "best_phase": 0.925
                        },
                        "mae": {
                            "mean_amplitude": 0.025,
                            "mean_phase": 0.035
                        }
                    },
                    "Baseline": {
                        "ms_ssim": {
                            "mean_amplitude": 0.930,
                            "best_amplitude": 0.935,
                            "mean_phase": 0.900,
                            "best_phase": 0.905
                        },
                        "mae": {
                            "mean_amplitude": 0.030,
                            "mean_phase": 0.040
                        }
                    },
                    "PtyChi": {
                        "ms_ssim": {
                            "mean_amplitude": 0.940,
                            "best_amplitude": 0.945,
                            "mean_phase": 0.910,
                            "best_phase": 0.915
                        },
                        "mae": {
                            "mean_amplitude": 0.027,
                            "mean_phase": 0.037
                        }
                    }
                }
            }
    
            import json
            metrics_summary_path = analysis / "metrics_summary.json"
            with metrics_summary_path.open("w", encoding="utf-8") as f:
                json.dump(summary_data, f, indent=2)
    
        def stub_generate_artifact_inventory(hub_path):
            """Create artifact_inventory.txt with test data listing key artifacts."""
            analysis = Path(hub_path) / "analysis"
            analysis.mkdir(parents=True, exist_ok=True)
    
            inventory_path = analysis / "artifact_inventory.txt"
    
            # List key artifacts that should be present after pipeline execution
            artifacts = [
                "analysis/aggregate_highlights.txt",
                "analysis/aggregate_report.md",
                "analysis/comparison_manifest.json",
                "analysis/metrics_delta_highlights.txt",
                "analysis/metrics_delta_summary.json",
                "analysis/metrics_digest.md",
                "analysis/metrics_summary.json",
                "analysis/metrics_summary.md",
                "cli/metrics_digest_cli.log",
                "cli/aggregate_report_cli.log",
            ]
    
            # Write sorted artifact list (deterministic)
            with inventory_path.open("w", encoding="utf-8") as f:
                for artifact in sorted(artifacts):
                    f.write(f"{artifact}\n")
    
        monkeypatch.setattr(module, "prepare_hub", stub_prepare_hub)
        monkeypatch.setattr(module, "validate_phase_c_metadata", stub_validate_phase_c_metadata)
        monkeypatch.setattr(module, "summarize_phase_g_outputs", stub_summarize_phase_g_outputs)
        monkeypatch.setattr(module, "generate_artifact_inventory", stub_generate_artifact_inventory)
    
        # Record run_command invocations
        run_command_calls = []
    
        def stub_run_command(cmd, log_path):
            """Record cmd and log_path, create required files for orchestrator progression."""
            run_command_calls.append((cmd, log_path))
            cmd_str = " ".join(str(c) for c in cmd)
    
            # Create log file for every invocation (TEST-CLI-001)
            log_path.parent.mkdir(parents=True, exist_ok=True)
            log_path.write_text(f"Stub log for: {cmd_str}\n", encoding="utf-8")
    
            # When reporting helper is invoked, create highlights file
            if "report_phase_g_dense_metrics.py" in cmd_str and "--highlights" in cmd_str:
                for i, part in enumerate(cmd):
                    if str(part) == "--highlights" and i + 1 < len(cmd):
                        highlights_path = Path(cmd[i + 1])
                        highlights_path.parent.mkdir(parents=True, exist_ok=True)
                        highlights_path.write_text("MS-SSIM Deltas (PtychoPINN - Baseline):\n  Amplitude (mean): +0.050\n", encoding="utf-8")
                        break
    
            # When analyze digest is invoked, create digest file
            if "analyze_dense_metrics.py" in cmd_str and "--output" in cmd_str:
                for i, part in enumerate(cmd):
                    if str(part) == "--output" and i + 1 < len(cmd):
                        digest_path = Path(cmd[i + 1])
                        digest_path.parent.mkdir(parents=True, exist_ok=True)
                        digest_path.write_text("# Phase G Dense Metrics Digest\n", encoding="utf-8")
                        break
    
            # When ssim_grid is invoked, create summary file and log
            if "ssim_grid.py" in cmd_str and "--hub" in cmd_str:
                # ssim_grid.py creates analysis/ssim_grid_summary.md
                for i, part in enumerate(cmd):
                    if str(part) == "--hub" and i + 1 < len(cmd):
                        hub_path = Path(cmd[i + 1])
                        ssim_grid_summary_path = hub_path / "analysis" / "ssim_grid_summary.md"
                        ssim_grid_summary_path.parent.mkdir(parents=True, exist_ok=True)
                        ssim_grid_summary_path.write_text("# SSIM Grid Summary (Phase-Only)\n", encoding="utf-8")
                        # Create ssim_grid CLI log
                        ssim_grid_log_path = hub_path / "cli" / "ssim_grid_cli.log"
                        ssim_grid_log_path.parent.mkdir(parents=True, exist_ok=True)
                        ssim_grid_log_path.write_text("SSIM grid generation complete\n", encoding="utf-8")
                        break
    
            # When verify_dense_pipeline_artifacts is invoked, create report and log files
            if "verify_dense_pipeline_artifacts.py" in cmd_str and "--report" in cmd_str:
                for i, part in enumerate(cmd):
                    if str(part) == "--report" and i + 1 < len(cmd):
                        report_path = Path(cmd[i + 1])
                        report_path.parent.mkdir(parents=True, exist_ok=True)
                        report_path.write_text('{"valid": true}\n', encoding="utf-8")
                        # Create verification log in analysis/
                        hub_path = report_path.parent.parent  # analysis -> hub
                        verify_log_path = hub_path / "analysis" / "verify_dense_stdout.log"
                        verify_log_path.parent.mkdir(parents=True, exist_ok=True)
                        verify_log_path.write_text("Verification complete\n", encoding="utf-8")
                        break
    
            # When check_dense_highlights_match is invoked, create log file
            if "check_dense_highlights_match.py" in cmd_str:
                # Extract hub path from command to create log in analysis/
                for i, part in enumerate(cmd):
                    if str(part) == "--hub" and i + 1 < len(cmd):
                        hub_path = Path(cmd[i + 1])
                        check_log_path = hub_path / "analysis" / "check_dense_highlights.log"
                        check_log_path.parent.mkdir(parents=True, exist_ok=True)
                        check_log_path.write_text("Highlights check complete\n", encoding="utf-8")
                        break
    
        monkeypatch.setattr(module, "run_command", stub_run_command)
    
        # Execute: Call main() with stdout capture (should execute Phase CG pipeline + reporting helper + analyze digest)
        import io
        import contextlib
    
        stdout_buffer = io.StringIO()
        with contextlib.redirect_stdout(stdout_buffer):
            exit_code = main()
    
        stdout = stdout_buffer.getvalue()
    
        # Assert: Exit code should be 0
>       assert exit_code == 0, f"Expected exit code 0 from real execution mode, got {exit_code}"
E       AssertionError: Expected exit code 0 from real execution mode, got 1
E       assert 1 == 0

tests/study/test_phase_g_dense_orchestrator.py:1459: AssertionError
_____ TestBaselineGridsize2Integration.test_baseline_gridsize2_end_to_end ______

self = <tests.test_integration_baseline_gs2.TestBaselineGridsize2Integration testMethod=test_baseline_gridsize2_end_to_end>

    def test_baseline_gridsize2_end_to_end(self):
        """Test that baseline model runs successfully with gridsize=2."""
    
        # Check if test data exists
        test_data_path = project_root / "datasets" / "fly" / "fly001_transposed.npz"
        if not test_data_path.exists():
            self.skipTest(f"Test data not found at {test_data_path}")
    
        # Build the command
        cmd = [
            sys.executable,
            str(project_root / "scripts" / "run_baseline.py"),
            "--train_data_file", str(test_data_path),
            "--test_data_file", str(test_data_path),
            "--gridsize", "2",
            "--n_groups", "128",  # Use small number for quick test
            "--nepochs", "2",  # Quick test with few epochs
            "--output_dir", str(self.output_dir),
            "--quiet"  # Suppress verbose output
        ]
    
        # Run the command
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            cwd=str(project_root)
        )
    
        # Check for successful completion
>       self.assertEqual(result.returncode, 0,
                        f"Baseline script failed with return code {result.returncode}\n"
                        f"stdout: {result.stdout}\n"
                        f"stderr: {result.stderr}")
E       AssertionError: 2 != 0 : Baseline script failed with return code 2
E       stdout: 
E       stderr: 2025-11-13 15:40:52.527934: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
E       WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E       E0000 00:00:1763077252.538717 2348236 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E       E0000 00:00:1763077252.542317 2348236 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E       W0000 00:00:1763077252.551629 2348236 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E       W0000 00:00:1763077252.551639 2348236 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E       W0000 00:00:1763077252.551640 2348236 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E       W0000 00:00:1763077252.551642 2348236 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E       usage: run_baseline.py [-h] [--config CONFIG] [--do_stitching]
E                              [--quiet | --verbose]
E                              [--console-level {DEBUG,INFO,WARNING,ERROR,CRITICAL}]
E                              [--N {64,128,256}] [--gridsize GRIDSIZE]
E                              [--n_filters_scale N_FILTERS_SCALE]
E                              [--model_type {pinn,supervised}]
E                              [--amp_activation {sigmoid,swish,softplus,relu}]
E                              [--object_big OBJECT_BIG] [--probe_big PROBE_BIG]
E                              [--probe_mask PROBE_MASK] [--pad_object PAD_OBJECT]
E                              [--probe_scale PROBE_SCALE]
E                              [--gaussian_smoothing_sigma GAUSSIAN_SMOOTHING_SIGMA]
E                              [--train_data_file TRAIN_DATA_FILE]
E                              [--test_data_file TEST_DATA_FILE]
E                              [--batch_size BATCH_SIZE] [--nepochs NEPOCHS]
E                              [--mae_weight MAE_WEIGHT] [--nll_weight NLL_WEIGHT]
E                              [--realspace_mae_weight REALSPACE_MAE_WEIGHT]
E                              [--realspace_weight REALSPACE_WEIGHT]
E                              [--nphotons NPHOTONS] [--n_groups N_GROUPS]
E                              [--n_images N_IMAGES] [--n_subsample N_SUBSAMPLE]
E                              [--subsample_seed SUBSAMPLE_SEED]
E                              [--neighbor_count NEIGHBOR_COUNT]
E                              [--enable_oversampling ENABLE_OVERSAMPLING]
E                              [--neighbor_pool_size NEIGHBOR_POOL_SIZE]
E                              [--positions_provided POSITIONS_PROVIDED]
E                              [--probe_trainable PROBE_TRAINABLE]
E                              [--intensity_scale_trainable INTENSITY_SCALE_TRAINABLE]
E                              [--output_dir OUTPUT_DIR]
E                              [--sequential_sampling SEQUENTIAL_SAMPLING]
E                              [--backend {tensorflow,pytorch}]
E                              [--torch_loss_mode TORCH_LOSS_MODE]
E       run_baseline.py: error: argument --torch_loss_mode: invalid Literal value: 'poisson'

tests/test_integration_baseline_gs2.py:57: AssertionError
______________ TestFullWorkflow.test_train_save_load_infer_cycle _______________

self = <tests.test_integration_workflow.TestFullWorkflow testMethod=test_train_save_load_infer_cycle>

    def test_train_save_load_infer_cycle(self):
        """
        Tests the complete train -> save -> load -> infer workflow.
        This is the ultimate validation of the model save/restore cycle as it
        simulates a real user workflow across separate processes.
        """
        # --- 1. Define Paths ---
        data_file = project_root / "ptycho" / "datasets" / "Run1084_recon3_postPC_shrunk_3.npz"
        training_output_dir = self.output_path / "training_outputs"
        inference_output_dir = self.output_path / "lcls_output"
    
        # --- 2. Training Step ---
        print("--- Running Training Step (subprocess) ---")
        train_command = [
            sys.executable, str(project_root / "scripts" / "training" / "train.py"),
            "--train_data_file", str(data_file),
            "--test_data_file", str(data_file),
            "--output_dir", str(training_output_dir),
            "--nepochs", "2",
            "--n_images", "64",
            "--gridsize", "1", # Explicitly set for clarity
            "--quiet"
        ]
    
        train_result = subprocess.run(train_command, capture_output=True, text=True)
    
>       self.assertEqual(train_result.returncode, 0,
                         f"Training script failed with stdout:\n{train_result.stdout}\nstderr:\n{train_result.stderr}")
E       AssertionError: 1 != 0 : Training script failed with stdout:
E       2025-11-13 15:40:58,669 - INFO - Configuration setup complete
E       2025-11-13 15:40:58,670 - INFO - Final configuration: TrainingConfig(model=ModelConfig(N=64, gridsize=1, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', object_big=True, probe_big=True, probe_mask=False, pad_object=True, probe_scale=4.0, gaussian_smoothing_sigma=0.0), train_data_file=PosixPath('/home/ollie/Documents/PtychoPINN/ptycho/datasets/Run1084_recon3_postPC_shrunk_3.npz'), test_data_file=PosixPath('/home/ollie/Documents/PtychoPINN/ptycho/datasets/Run1084_recon3_postPC_shrunk_3.npz'), batch_size=16, nepochs=2, mae_weight=0.0, nll_weight=1.0, realspace_mae_weight=0.0, realspace_weight=0.0, nphotons=1000000000.0, n_groups=64, n_images=64, n_subsample=None, subsample_seed=None, neighbor_count=4, enable_oversampling=False, neighbor_pool_size=None, positions_provided=True, probe_trainable=False, intensity_scale_trainable=True, output_dir=PosixPath('/tmp/tmpzav7aycu/training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
E       2025-11-13 15:40:58,670 - INFO - Legacy mode: using 64 groups (gridsize=1)
E       2025-11-13 15:40:58,670 - INFO - Starting training with n_subsample=64, n_groups=64, stitching=disabled
E       2025-11-13 15:40:58,670 - INFO - Loading data from /home/ollie/Documents/PtychoPINN/ptycho/datasets/Run1084_recon3_postPC_shrunk_3.npz with n_images=64, n_subsample=64
E       2025-11-13 15:40:58,686 - INFO - Independent sampling: subsampling 64 images from 1087 total
E       2025-11-13 15:40:58,686 - INFO - Randomly subsampled 64 images
E       diff3d shape: (64, 64, 64)
E       probeGuess shape: (64, 64)
E       scan_index shape: (64,)
E       objectGuess shape: (227, 226)
E       xcoords shape: (64,)
E       ycoords shape: (64,)
E       xcoords_start shape: (64,)
E       ycoords_start shape: (64,)
E       2025-11-13 15:40:58,703 - INFO - Loading data from /home/ollie/Documents/PtychoPINN/ptycho/datasets/Run1084_recon3_postPC_shrunk_3.npz with n_images=None, n_subsample=None
E       2025-11-13 15:40:58,719 - INFO - Using full dataset of 1087 images
E       diff3d shape: (1087, 64, 64)
E       probeGuess shape: (64, 64)
E       scan_index shape: (1087,)
E       objectGuess shape: (227, 226)
E       xcoords shape: (1087,)
E       ycoords shape: (1087,)
E       xcoords_start shape: (1087,)
E       ycoords_start shape: (1087,)
E       2025-11-13 15:40:58,733 - INFO - Loaded test data from /home/ollie/Documents/PtychoPINN/ptycho/datasets/Run1084_recon3_postPC_shrunk_3.npz
E       2025-11-13 15:40:58,734 - INFO - Backend dispatcher: params.cfg synchronized with TrainingConfig (backend=tensorflow)
E       2025-11-13 15:40:58,734 - INFO - Backend dispatcher: routing to TensorFlow workflow (ptycho.workflows.components)
E       2025-11-13 15:40:58,734 - INFO - [OVERSAMPLING DEBUG] generate_grouped_data called with: nsamples=64, n_points=64, C=1, K=4
E       2025-11-13 15:40:58,734 - INFO - [OVERSAMPLING DEBUG] Parameters: gridsize=1, N=64, sequential_sampling=False
E       2025-11-13 15:40:58,734 - INFO - [OVERSAMPLING DEBUG] Oversampling flags: enable_oversampling=False, neighbor_pool_size=None, effective_K=4
E       2025-11-13 15:40:58,734 - INFO - [OVERSAMPLING DEBUG] Oversampling check: nsamples > n_points = 64 > 64 = False
E       2025-11-13 15:40:58,734 - INFO - [OVERSAMPLING DEBUG] Oversampling check: C > 1 = 1 > 1 = False
E       2025-11-13 15:40:58,734 - INFO - [OVERSAMPLING DEBUG] needs_oversampling = False
E       DEBUG: nsamples: 64, gridsize: 1 (using efficient random sample-then-group strategy)
E       2025-11-13 15:40:58,734 - INFO - Using efficient random sampling strategy for gridsize=1
E       2025-11-13 15:40:58,734 - INFO - [OVERSAMPLING DEBUG] Taking efficient branch: standard sample-then-group
E       2025-11-13 15:40:58,734 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently called with: nsamples=64, K=4, C=1
E       2025-11-13 15:40:58,734 - INFO - Generating 64 groups efficiently from 64 points (K=4, C=1)
E       2025-11-13 15:40:58,734 - INFO - [OVERSAMPLING DEBUG] Standard case: using 64 groups from 64 points
E       2025-11-13 15:40:58,734 - INFO - [OVERSAMPLING DEBUG] Using all 64 points as seeds (no sampling needed)
E       2025-11-13 15:40:58,734 - INFO - Using all 64 points as seeds
E       2025-11-13 15:40:58,734 - INFO - Using seed indices directly for C=1 (gridsize=1) - no neighbor search needed
E       2025-11-13 15:40:58,734 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently completed: generated 64 groups
E       2025-11-13 15:40:58,734 - INFO - Successfully generated 64 groups with shape (64, 1)
E       2025-11-13 15:40:58,734 - INFO - [OVERSAMPLING DEBUG] Generated 64 groups in total
E       2025-11-13 15:40:58,734 - INFO - Generated 64 groups efficiently
E       INFO: 'Y' array not found. Generating ground truth patches from 'objectGuess' as a fallback.
E       neighbor-sampled diffraction shape (64, 64, 64, 1)
E       loader: using provided ground truth patches.
E       INFO: None
E       <PtychoDataContainer X=(64, 64, 64, 1) Y_I=(64, 64, 64, 1) Y_phi=(64, 64, 64, 1) norm_Y_I=() coords_nominal=(64, 1, 2, 1) coords_true=(64, 1, 2, 1) nn_indices=(64, 1) mean=31.500 global_offsets=(64, 1, 2, 1) mean=58.203 local_offsets=(64, 1, 2, 1) mean=0.000 probe=(64, 64) mean_amplitude=0.322>
E       2025-11-13 15:41:00,339 - INFO - [OVERSAMPLING DEBUG] generate_grouped_data called with: nsamples=64, n_points=1087, C=1, K=4
E       2025-11-13 15:41:00,339 - INFO - [OVERSAMPLING DEBUG] Parameters: gridsize=1, N=64, sequential_sampling=False
E       2025-11-13 15:41:00,339 - INFO - [OVERSAMPLING DEBUG] Oversampling flags: enable_oversampling=False, neighbor_pool_size=None, effective_K=4
E       2025-11-13 15:41:00,339 - INFO - [OVERSAMPLING DEBUG] Oversampling check: nsamples > n_points = 64 > 1087 = False
E       2025-11-13 15:41:00,339 - INFO - [OVERSAMPLING DEBUG] Oversampling check: C > 1 = 1 > 1 = False
E       2025-11-13 15:41:00,339 - INFO - [OVERSAMPLING DEBUG] needs_oversampling = False
E       DEBUG: nsamples: 64, gridsize: 1 (using efficient random sample-then-group strategy)
E       2025-11-13 15:41:00,339 - INFO - Using efficient random sampling strategy for gridsize=1
E       2025-11-13 15:41:00,339 - INFO - [OVERSAMPLING DEBUG] Taking efficient branch: standard sample-then-group
E       2025-11-13 15:41:00,339 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently called with: nsamples=64, K=4, C=1
E       2025-11-13 15:41:00,339 - INFO - Generating 64 groups efficiently from 1087 points (K=4, C=1)
E       2025-11-13 15:41:00,339 - INFO - [OVERSAMPLING DEBUG] Standard case: using 64 groups from 1087 points
E       2025-11-13 15:41:00,339 - INFO - [OVERSAMPLING DEBUG] Randomly sampled 64 seed points
E       2025-11-13 15:41:00,339 - INFO - Sampled 64 seed points from 1087 total points
E       2025-11-13 15:41:00,339 - INFO - Using seed indices directly for C=1 (gridsize=1) - no neighbor search needed
E       2025-11-13 15:41:00,339 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently completed: generated 64 groups
E       2025-11-13 15:41:00,339 - INFO - Successfully generated 64 groups with shape (64, 1)
E       2025-11-13 15:41:00,339 - INFO - [OVERSAMPLING DEBUG] Generated 64 groups in total
E       2025-11-13 15:41:00,339 - INFO - Generated 64 groups efficiently
E       INFO: 'Y' array not found. Generating ground truth patches from 'objectGuess' as a fallback.
E       neighbor-sampled diffraction shape (64, 64, 64, 1)
E       loader: using provided ground truth patches.
E       INFO: None
E       <PtychoDataContainer X=(64, 64, 64, 1) Y_I=(64, 64, 64, 1) Y_phi=(64, 64, 64, 1) norm_Y_I=() coords_nominal=(64, 1, 2, 1) coords_true=(64, 1, 2, 1) nn_indices=(64, 1) mean=541.281 global_offsets=(64, 1, 2, 1) mean=57.950 local_offsets=(64, 1, 2, 1) mean=0.000 probe=(64, 64) mean_amplitude=0.322>
E       DEBUG: Setting probe to tf.Tensor(
E       [[[-0.00399459+8.81725922e-03j]
E         [ 0.00787074+1.04518672e-02j]
E         [-0.01909852+2.85197329e-03j]
E         ...
E         [-0.00430549-1.44718047e-02j]
E         [ 0.00670903+2.63100304e-02j]
E         [-0.01938361-6.67245360e-03j]]
E       
E        [[ 0.00921909+5.32836141e-03j]
E         [ 0.00422684-1.61591489e-02j]
E         [-0.00324812+1.62756350e-02j]
E         ...
E         [-0.00462818-2.68168072e-03j]
E         [ 0.01194528-6.69307355e-03j]
E         [ 0.0090712 -3.79238278e-03j]]
E       
E        [[ 0.0029482 -1.10371104e-02j]
E         [-0.01489174+5.26464824e-03j]
E         [-0.00083129+1.31990444e-02j]
E         ...
E         [ 0.00208769+1.68677475e-02j]
E         [ 0.00556216-3.03630810e-02j]
E         [ 0.01604221+1.45274084e-02j]]
E       
E        ...
E       
E        [[ 0.01654117+3.83263193e-02j]
E         [-0.01336122+7.02320365e-04j]
E         [-0.00380601-8.65837932e-03j]
E         ...
E         [ 0.01785212+2.66705058e-03j]
E         [-0.01791506+7.16461660e-03j]
E         [ 0.01052329-2.80616116e-02j]]
E       
E        [[-0.02582748-2.17778888e-05j]
E         [ 0.0098637 -3.15875164e-03j]
E         [-0.0156169 +2.47947779e-02j]
E         ...
E         [-0.01378679-1.89308310e-03j]
E         [ 0.00439025-1.43125653e-02j]
E         [ 0.02400579-1.19637549e-02j]]
E       
E        [[-0.00013256-1.32157737e-02j]
E         [ 0.03951043+1.84629709e-02j]
E         [-0.00848375+3.90608120e-03j]
E         ...
E         [-0.00414446+2.40438167e-04j]
E         [ 0.01858319+1.01260375e-02j]
E         [ 0.01299333+1.25071155e-02j]]], shape=(64, 64, 1), dtype=complex64) in params
E       DEBUG: Setting intensity_scale to 988.21173 in params
E       DEBUG: Setting probe to tf.Tensor(
E       [[[-0.00399459+8.81725922e-03j]
E         [ 0.00787074+1.04518672e-02j]
E         [-0.01909852+2.85197329e-03j]
E         ...
E         [-0.00430549-1.44718047e-02j]
E         [ 0.00670903+2.63100304e-02j]
E         [-0.01938361-6.67245360e-03j]]
E       
E        [[ 0.00921909+5.32836141e-03j]
E         [ 0.00422684-1.61591489e-02j]
E         [-0.00324812+1.62756350e-02j]
E         ...
E         [-0.00462818-2.68168072e-03j]
E         [ 0.01194528-6.69307355e-03j]
E         [ 0.0090712 -3.79238278e-03j]]
E       
E        [[ 0.0029482 -1.10371104e-02j]
E         [-0.01489174+5.26464824e-03j]
E         [-0.00083129+1.31990444e-02j]
E         ...
E         [ 0.00208769+1.68677475e-02j]
E         [ 0.00556216-3.03630810e-02j]
E         [ 0.01604221+1.45274084e-02j]]
E       
E        ...
E       
E        [[ 0.01654117+3.83263193e-02j]
E         [-0.01336122+7.02320365e-04j]
E         [-0.00380601-8.65837932e-03j]
E         ...
E         [ 0.01785212+2.66705058e-03j]
E         [-0.01791506+7.16461660e-03j]
E         [ 0.01052329-2.80616116e-02j]]
E       
E        [[-0.02582748-2.17778888e-05j]
E         [ 0.0098637 -3.15875164e-03j]
E         [-0.0156169 +2.47947779e-02j]
E         ...
E         [-0.01378679-1.89308310e-03j]
E         [ 0.00439025-1.43125653e-02j]
E         [ 0.02400579-1.19637549e-02j]]
E       
E        [[-0.00013256-1.32157737e-02j]
E         [ 0.03951043+1.84629709e-02j]
E         [-0.00848375+3.90608120e-03j]
E         ...
E         [-0.00414446+2.40438167e-04j]
E         [ 0.01858319+1.01260375e-02j]
E         [ 0.01299333+1.25071155e-02j]]], shape=(64, 64, 1), dtype=complex64) in params
E       Model: "functional"
E       
E        Layer (type)         Output Shape          Param #  Connected to      
E       
E        input (InputLayer)   (None, 64, 64, 1)           0  -                 
E       
E        intensity_scaler     (None, 64, 64, 1)           0  input[0][0]       
E        (IntensityScaler)                                                     
E       
E        conv2d (Conv2D)      (None, 64, 64,            640  intensity_scaler 
E                             64)                                              
E       
E        conv2d_1 (Conv2D)    (None, 64, 64,         36,928  conv2d[0][0]      
E                             64)                                              
E       
E        max_pooling2d        (None, 32, 32,              0  conv2d_1[0][0]    
E        (MaxPooling2D)       64)                                              
E       
E        conv2d_2 (Conv2D)    (None, 32, 32,         73,856  max_pooling2d[0] 
E                             128)                                             
E       
E        conv2d_3 (Conv2D)    (None, 32, 32,        147,584  conv2d_2[0][0]    
E                             128)                                             
E       
E        max_pooling2d_1      (None, 16, 16,              0  conv2d_3[0][0]    
E        (MaxPooling2D)       128)                                             
E       
E        conv2d_4 (Conv2D)    (None, 16, 16,        295,168  max_pooling2d_1[ 
E                             256)                                             
E       
E        conv2d_5 (Conv2D)    (None, 16, 16,        590,080  conv2d_4[0][0]    
E                             256)                                             
E       
E        max_pooling2d_2      (None, 8, 8, 256)           0  conv2d_5[0][0]    
E        (MaxPooling2D)                                                        
E       
E        conv2d_8 (Conv2D)    (None, 8, 8, 128)     295,040  max_pooling2d_2[ 
E       
E        conv2d_16 (Conv2D)   (None, 8, 8, 128)     295,040  max_pooling2d_2[ 
E       
E        conv2d_9 (Conv2D)    (None, 8, 8, 128)     147,584  conv2d_8[0][0]    
E       
E        conv2d_17 (Conv2D)   (None, 8, 8, 128)     147,584  conv2d_16[0][0]   
E       
E        up_sampling2d        (None, 16, 16,              0  conv2d_9[0][0]    
E        (UpSampling2D)       128)                                             
E       
E        up_sampling2d_3      (None, 16, 16,              0  conv2d_17[0][0]   
E        (UpSampling2D)       128)                                             
E       
E        conv2d_10 (Conv2D)   (None, 16, 16,         73,792  up_sampling2d[0] 
E                             64)                                              
E       
E        conv2d_18 (Conv2D)   (None, 16, 16,         73,792  up_sampling2d_3[ 
E                             64)                                              
E       
E        conv2d_11 (Conv2D)   (None, 16, 16,         36,928  conv2d_10[0][0]   
E                             64)                                              
E       
E        conv2d_19 (Conv2D)   (None, 16, 16,         36,928  conv2d_18[0][0]   
E                             64)                                              
E       
E        up_sampling2d_1      (None, 32, 32,              0  conv2d_11[0][0]   
E        (UpSampling2D)       64)                                              
E       
E        up_sampling2d_4      (None, 32, 32,              0  conv2d_19[0][0]   
E        (UpSampling2D)       64)                                              
E       
E        get_item_1           (None, 32, 32, 4)           0  up_sampling2d_1[ 
E        (GetItem)                                                             
E       
E        get_item_3           (None, 32, 32, 4)           0  up_sampling2d_4[ 
E        (GetItem)                                                             
E       
E        conv2d_12 (Conv2D)   (None, 32, 32,          2,368  get_item_1[0][0]  
E                             64)                                              
E       
E        conv2d_20 (Conv2D)   (None, 32, 32,          2,368  get_item_3[0][0]  
E                             64)                                              
E       
E        conv2d_13 (Conv2D)   (None, 32, 32,         36,928  conv2d_12[0][0]   
E                             64)                                              
E       
E        conv2d_21 (Conv2D)   (None, 32, 32,         36,928  conv2d_20[0][0]   
E                             64)                                              
E       
E        up_sampling2d_2      (None, 64, 64,              0  conv2d_13[0][0]   
E        (UpSampling2D)       64)                                              
E       
E        up_sampling2d_5      (None, 64, 64,              0  conv2d_21[0][0]   
E        (UpSampling2D)       64)                                              
E       
E        get_item (GetItem)   (None, 32, 32,              0  up_sampling2d_1[ 
E                             60)                                              
E       
E        conv2d_7 (Conv2D)    (None, 64, 64, 1)         577  up_sampling2d_2[ 
E       
E        get_item_2           (None, 32, 32,              0  up_sampling2d_4[ 
E        (GetItem)            60)                                              
E       
E        conv2d_15 (Conv2D)   (None, 64, 64, 1)         577  up_sampling2d_5[ 
E       
E        conv2d_6 (Conv2D)    (None, 32, 32, 1)         541  get_item[0][0]    
E       
E        silu (Silu)          (None, 64, 64, 1)           0  conv2d_7[0][0]    
E       
E        conv2d_14 (Conv2D)   (None, 32, 32, 1)         541  get_item_2[0][0]  
E       
E        silu_1 (Silu)        (None, 64, 64, 1)           0  conv2d_15[0][0]   
E       
E        amp                  (None, 32, 32, 1)           0  conv2d_6[0][0]    
E        (ActivationLayer)                                                     
E       
E        center_mask_layer    (None, 64, 64, 1)           0  silu[0][0]        
E        (CenterMaskLayer)                                                     
E       
E        phi                  (None, 32, 32, 1)           0  conv2d_14[0][0]   
E        (ActivationLayer)                                                     
E       
E        center_mask_layer_1  (None, 64, 64, 1)           0  silu_1[0][0]      
E        (CenterMaskLayer)                                                     
E       
E        amp_padded           (None, 64, 64, 1)           0  amp[0][0]         
E        (ZeroPadding2D)                                                       
E       
E        multiply (Multiply)  (None, 64, 64, 1)           0  silu[0][0],       
E                                                            center_mask_laye 
E       
E        phase_padded         (None, 64, 64, 1)           0  phi[0][0]         
E        (ZeroPadding2D)                                                       
E       
E        multiply_1           (None, 64, 64, 1)           0  silu_1[0][0],     
E        (Multiply)                                          center_mask_laye 
E       
E        add (Add)            (None, 64, 64, 1)           0  amp_padded[0][0], 
E                                                            multiply[0][0]    
E       
E        add_1 (Add)          (None, 64, 64, 1)           0  phase_padded[0][ 
E                                                            multiply_1[0][0]  
E       
E        obj                  (None, 64, 64, 1)           0  add[0][0],        
E        (CombineComplexLay                                 add_1[0][0]       
E       
E        input_positions      (None, 1, 2, 1)             0  -                 
E        (InputLayer)                                                          
E       
E        padded_obj_2         (None, 74, 74, 1)           0  obj[0][0],        
E        (ReassemblePatches                                 input_positions[ 
E       
E        padded_objs_with_o  (None, 64, 64, 1)           0  padded_obj_2[0][ 
E        (ExtractPatchesPos                                 input_positions[ 
E       
E        probe_illumination   [(None, 64, 64,             0  padded_objs_with 
E        (ProbeIllumination)  1), (None, 64,                                   
E                             64, 1)]                                          
E       
E        pred_amplitude       [(None, 64, 64,             0  probe_illuminati 
E        (PadAndDiffractLay  1), (None, 64,                                   
E                             64, 1)]                                          
E       
E        pred_diff_channels   (None, 64, 64, 1)           0  pred_amplitude[0 
E        (FlatToChannelLaye                                                   
E       
E        intensity_scaler_i  (None, 64, 64, 1)           0  pred_diff_channe 
E        (IntensityScaler_i                                                   
E       
E        trimmed_obj          (None, 64, 64, 1)           0  padded_obj_2[0][ 
E        (TrimReconstructio                                                   
E       
E        pred_intensity       (None, 64, 64, 1)           0  intensity_scaler 
E        (SquareLayer)                                                         
E       
E        Total params: 2,331,772 (8.90 MB)
E        Trainable params: 2,331,772 (8.90 MB)
E        Non-trainable params: 0 (0.00 B)
E       None
E       Current Parameters:
E       --------------------
E       N: 64
E       amp_activation: sigmoid
E       backend: tensorflow
E       batch_size: 16
E       bigN: 64
E       big_gridsize: 10
E       data_source: generic
E       debug: True
E       default_probe_scale: 0.7
E       enable_oversampling: False
E       gaussian_smoothing_sigma: 0.0
E       gridsize: 1
E       h5_path: wts.h5
E       intensity_scale: 988.2117309570312
E       intensity_scale.trainable: True
E       label: 
E       mae_weight: 0.0
E       max_position_jitter: 10
E       model_type: pinn
E       n_filters_scale: 2
E       n_groups: 64
E       n_images: 64
E       neighbor_count: 4
E       nepochs: 2
E       nimgs_test: 3
E       nimgs_train: 9
E       nll_weight: 1.0
E       nphotons: 1000000000.0
E       npseed: 42
E       object.big: True
E       offset: 4
E       outer_offset_test: None
E       outer_offset_train: None
E       output_prefix: /tmp/tmpzav7aycu/training_outputs
E       pad_object: True
E       positions.provided: True
E       probe:
E         shape: (64, 64, 1)
E         mean: -0.016-0.002j
E         std: 0.666
E         min: -3.769-0.236j
E         max: 2.823-0.116j
E       probe.big: True
E       probe.mask: False
E       probe.trainable: False
E       probe_scale: 4.0
E       realspace_mae_weight: 0.0
E       realspace_weight: 0.0
E       sequential_sampling: False
E       set_phi: False
E       sim_jitter_scale: 0.0
E       size: 392
E       test_data_file_path: /home/ollie/Documents/PtychoPINN/ptycho/datasets/Run1084_recon3_postPC_shrunk_3.npz
E       torch_loss_mode: poisson
E       train_data_file_path: /home/ollie/Documents/PtychoPINN/ptycho/datasets/Run1084_recon3_postPC_shrunk_3.npz
E       tv_weight: 0.0
E       use_xla_translate: True
E       Epoch 1/2
E       2025-11-13 15:41:01,779 - ERROR - An error occurred during execution: Exception encountered when calling ReassemblePatchesLayer.call().
E       
E       [1mUsing a symbolic `tf.Tensor` as a Python `bool` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.[0m
E       
E       Arguments received by ReassemblePatchesLayer.call():
E          inputs=['tf.Tensor(shape=(None, 64, 64, 1), dtype=complex64)', 'tf.Tensor(shape=(None, 1, 2, 1), dtype=complex64)']
E       
E       stderr:
E       2025-11-13 15:40:55.963362: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
E       WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E       E0000 00:00:1763077255.974325 2348306 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E       E0000 00:00:1763077255.977862 2348306 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E       W0000 00:00:1763077255.987149 2348306 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E       W0000 00:00:1763077255.987159 2348306 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E       W0000 00:00:1763077255.987161 2348306 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E       W0000 00:00:1763077255.987162 2348306 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E       I0000 00:00:1763077258.843114 2348306 gpu_process_state.cc:208] Using CUDA malloc Async allocator for GPU: 0
E       I0000 00:00:1763077258.844438 2348306 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21737 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:04:00.0, compute capability: 8.6
E       WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E       I0000 00:00:1763077259.585533 2348306 service.cc:152] XLA service 0x2afcf240 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
E       I0000 00:00:1763077259.585553 2348306 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6
E       I0000 00:00:1763077259.622265 2348306 cuda_dnn.cc:529] Loaded cuDNN version 91002
E       I0000 00:00:1763077259.756066 2348306 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
E       I0000 00:00:1763077261.227213 2348306 cupti_tracer.cc:1026] Profiler found 1 GPUs
E       W0000 00:00:1763077261.243608 2348306 cupti_tracer.cc:1213] Fail to use per-thread activity buffer, cupti trace overhead may be big. CUPTI ERROR CODE:1
E       I0000 00:00:1763077261.244442 2348306 cupti_tracer.cc:1249] CUPTI activity buffer flushed
E       Traceback (most recent call last):
E         File "/home/ollie/Documents/PtychoPINN/scripts/training/train.py", line 440, in <module>
E           main()
E         File "/home/ollie/Documents/PtychoPINN/scripts/training/train.py", line 419, in main
E           recon_amp, recon_phase, results = run_cdi_example_with_backend(
E                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E         File "/home/ollie/Documents/PtychoPINN/ptycho/workflows/backend_selector.py", line 141, in run_cdi_example_with_backend
E           recon_amp, recon_phase, results = tf_components.run_cdi_example(
E                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E         File "/home/ollie/Documents/PtychoPINN/ptycho/workflows/components.py", line 846, in run_cdi_example
E           train_results = train_cdi_model(train_data, test_data, config)
E                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E         File "/home/ollie/Documents/PtychoPINN/ptycho/workflows/components.py", line 726, in train_cdi_model
E           results = train_pinn.train_eval(PtychoDataset(train_container, test_container))
E                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E         File "/home/ollie/Documents/PtychoPINN/ptycho/train_pinn.py", line 90, in train_eval
E           model_instance, history = train(ptycho_dataset.train_data)
E                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E         File "/home/ollie/Documents/PtychoPINN/ptycho/train_pinn.py", line 86, in train
E           return model_instance, model.train(nepochs, train_data)
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E         File "/home/ollie/Documents/PtychoPINN/ptycho/model.py", line 593, in train
E           history=autoencoder.fit(
E                   ^^^^^^^^^^^^^^^^
E         File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
E           raise e.with_traceback(filtered_tb) from None
E         File "/home/ollie/Documents/PtychoPINN/ptycho/custom_layers.py", line 159, in call
E           if total_patches > batch_threshold:
E              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: Exception encountered when calling ReassemblePatchesLayer.call().
E       
E       [1mUsing a symbolic `tf.Tensor` as a Python `bool` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.[0m
E       
E       Arguments received by ReassemblePatchesLayer.call():
E          inputs=['tf.Tensor(shape=(None, 64, 64, 1), dtype=complex64)', 'tf.Tensor(shape=(None, 1, 2, 1), dtype=complex64)']

tests/test_integration_workflow.py:56: AssertionError
----------------------------- Captured stdout call -----------------------------

Created temporary directory for test run: /tmp/tmpzav7aycu
--- Running Training Step (subprocess) ---
Cleaned up temporary directory: /tmp/tmpzav7aycu
_____ TestNphotonsMetadataIntegration.test_end_to_end_workflow_consistency _____

self = <tests.test_nphotons_metadata_integration.TestNphotonsMetadataIntegration testMethod=test_end_to_end_workflow_consistency>

    def test_end_to_end_workflow_consistency(self):
        """Test complete end-to-end workflow maintains nphotons consistency."""
        print("--- Testing end-to-end nphotons consistency ---")
    
        test_nphotons = 5e5  # Use unusual value to ensure it's preserved
    
        # 1. Simulate  Train  Infer complete workflow
        sim_file = self._simulate_with_nphotons(test_nphotons, "_e2e")
>       model_dir = self._train_model(sim_file, test_nphotons, "_e2e")
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/test_nphotons_metadata_integration.py:343: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_nphotons_metadata_integration.py:149: in _train_model
    self.assertEqual(result.returncode, 0,
E   AssertionError: 1 != 0 : Training failed for nphotons=500000.0
E   stdout: 2025-11-13 15:41:18,083 - INFO - Configuration setup complete
E   2025-11-13 15:41:18,084 - INFO - Final configuration: TrainingConfig(model=ModelConfig(N=64, gridsize=1, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', object_big=True, probe_big=True, probe_mask=False, pad_object=True, probe_scale=4.0, gaussian_smoothing_sigma=0.0), train_data_file=PosixPath('/tmp/tmp1i23z19e/simulation/simulated_data_nphotons_500000.0_e2e.npz'), test_data_file=PosixPath('/tmp/tmp1i23z19e/simulation/simulated_data_nphotons_500000.0_e2e.npz'), batch_size=16, nepochs=2, mae_weight=0.0, nll_weight=1.0, realspace_mae_weight=0.0, realspace_weight=0.0, nphotons=500000.0, n_groups=50, n_images=50, n_subsample=None, subsample_seed=None, neighbor_count=4, enable_oversampling=False, neighbor_pool_size=None, positions_provided=True, probe_trainable=False, intensity_scale_trainable=True, output_dir=PosixPath('/tmp/tmp1i23z19e/training/training_nphotons_500000.0_e2e'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
E   2025-11-13 15:41:18,084 - INFO - Legacy mode: using 50 groups (gridsize=1)
E   2025-11-13 15:41:18,084 - INFO - Starting training with n_subsample=50, n_groups=50, stitching=disabled
E   2025-11-13 15:41:18,084 - INFO - Loading data from /tmp/tmp1i23z19e/simulation/simulated_data_nphotons_500000.0_e2e.npz with n_images=50, n_subsample=50
E   2025-11-13 15:41:18,090 - INFO - Independent sampling: subsampling 50 images from 100 total
E   2025-11-13 15:41:18,090 - INFO - Randomly subsampled 50 images
E   diff3d shape: (50, 64, 64)
E   probeGuess shape: (64, 64)
E   scan_index shape: (50,)
E   objectGuess shape: (227, 226)
E   xcoords shape: (50,)
E   ycoords shape: (50,)
E   xcoords_start shape: (50,)
E   ycoords_start shape: (50,)
E   2025-11-13 15:41:18,102 - INFO - Overriding nphotons from config (5.0e+05) with value from dataset metadata: 5.0e+05
E   2025-11-13 15:41:18,102 - INFO - Loading data from /tmp/tmp1i23z19e/simulation/simulated_data_nphotons_500000.0_e2e.npz with n_images=None, n_subsample=None
E   2025-11-13 15:41:18,107 - INFO - Using full dataset of 100 images
E   diff3d shape: (100, 64, 64)
E   probeGuess shape: (64, 64)
E   scan_index shape: (100,)
E   objectGuess shape: (227, 226)
E   xcoords shape: (100,)
E   ycoords shape: (100,)
E   xcoords_start shape: (100,)
E   ycoords_start shape: (100,)
E   2025-11-13 15:41:18,108 - INFO - Loaded test data from /tmp/tmp1i23z19e/simulation/simulated_data_nphotons_500000.0_e2e.npz
E   2025-11-13 15:41:18,108 - INFO - Backend dispatcher: params.cfg synchronized with TrainingConfig (backend=tensorflow)
E   2025-11-13 15:41:18,108 - INFO - Backend dispatcher: routing to TensorFlow workflow (ptycho.workflows.components)
E   2025-11-13 15:41:18,108 - INFO - [OVERSAMPLING DEBUG] generate_grouped_data called with: nsamples=50, n_points=50, C=1, K=4
E   2025-11-13 15:41:18,108 - INFO - [OVERSAMPLING DEBUG] Parameters: gridsize=1, N=64, sequential_sampling=False
E   2025-11-13 15:41:18,108 - INFO - [OVERSAMPLING DEBUG] Oversampling flags: enable_oversampling=False, neighbor_pool_size=None, effective_K=4
E   2025-11-13 15:41:18,109 - INFO - [OVERSAMPLING DEBUG] Oversampling check: nsamples > n_points = 50 > 50 = False
E   2025-11-13 15:41:18,109 - INFO - [OVERSAMPLING DEBUG] Oversampling check: C > 1 = 1 > 1 = False
E   2025-11-13 15:41:18,109 - INFO - [OVERSAMPLING DEBUG] needs_oversampling = False
E   DEBUG: nsamples: 50, gridsize: 1 (using efficient random sample-then-group strategy)
E   2025-11-13 15:41:18,109 - INFO - Using efficient random sampling strategy for gridsize=1
E   2025-11-13 15:41:18,109 - INFO - [OVERSAMPLING DEBUG] Taking efficient branch: standard sample-then-group
E   2025-11-13 15:41:18,109 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently called with: nsamples=50, K=4, C=1
E   2025-11-13 15:41:18,109 - INFO - Generating 50 groups efficiently from 50 points (K=4, C=1)
E   2025-11-13 15:41:18,109 - INFO - [OVERSAMPLING DEBUG] Standard case: using 50 groups from 50 points
E   2025-11-13 15:41:18,109 - INFO - [OVERSAMPLING DEBUG] Using all 50 points as seeds (no sampling needed)
E   2025-11-13 15:41:18,109 - INFO - Using all 50 points as seeds
E   2025-11-13 15:41:18,109 - INFO - Using seed indices directly for C=1 (gridsize=1) - no neighbor search needed
E   2025-11-13 15:41:18,109 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently completed: generated 50 groups
E   2025-11-13 15:41:18,109 - INFO - Successfully generated 50 groups with shape (50, 1)
E   2025-11-13 15:41:18,109 - INFO - [OVERSAMPLING DEBUG] Generated 50 groups in total
E   2025-11-13 15:41:18,109 - INFO - Generated 50 groups efficiently
E   INFO: 'Y' array not found. Generating ground truth patches from 'objectGuess' as a fallback.
E   neighbor-sampled diffraction shape (50, 64, 64, 1)
E   loader: using provided ground truth patches.
E   INFO: None
E   <PtychoDataContainer X=(50, 64, 64, 1) Y_I=(50, 64, 64, 1) Y_phi=(50, 64, 64, 1) norm_Y_I=() coords_nominal=(50, 1, 2, 1) coords_true=(50, 1, 2, 1) nn_indices=(50, 1) mean=24.500 global_offsets=(50, 1, 2, 1) mean=112.180 local_offsets=(50, 1, 2, 1) mean=0.000 probe=(64, 64) mean_amplitude=0.322>
E   2025-11-13 15:41:19,689 - INFO - [OVERSAMPLING DEBUG] generate_grouped_data called with: nsamples=50, n_points=100, C=1, K=4
E   2025-11-13 15:41:19,689 - INFO - [OVERSAMPLING DEBUG] Parameters: gridsize=1, N=64, sequential_sampling=False
E   2025-11-13 15:41:19,689 - INFO - [OVERSAMPLING DEBUG] Oversampling flags: enable_oversampling=False, neighbor_pool_size=None, effective_K=4
E   2025-11-13 15:41:19,689 - INFO - [OVERSAMPLING DEBUG] Oversampling check: nsamples > n_points = 50 > 100 = False
E   2025-11-13 15:41:19,689 - INFO - [OVERSAMPLING DEBUG] Oversampling check: C > 1 = 1 > 1 = False
E   2025-11-13 15:41:19,689 - INFO - [OVERSAMPLING DEBUG] needs_oversampling = False
E   DEBUG: nsamples: 50, gridsize: 1 (using efficient random sample-then-group strategy)
E   2025-11-13 15:41:19,689 - INFO - Using efficient random sampling strategy for gridsize=1
E   2025-11-13 15:41:19,689 - INFO - [OVERSAMPLING DEBUG] Taking efficient branch: standard sample-then-group
E   2025-11-13 15:41:19,689 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently called with: nsamples=50, K=4, C=1
E   2025-11-13 15:41:19,689 - INFO - Generating 50 groups efficiently from 100 points (K=4, C=1)
E   2025-11-13 15:41:19,689 - INFO - [OVERSAMPLING DEBUG] Standard case: using 50 groups from 100 points
E   2025-11-13 15:41:19,689 - INFO - [OVERSAMPLING DEBUG] Randomly sampled 50 seed points
E   2025-11-13 15:41:19,689 - INFO - Sampled 50 seed points from 100 total points
E   2025-11-13 15:41:19,689 - INFO - Using seed indices directly for C=1 (gridsize=1) - no neighbor search needed
E   2025-11-13 15:41:19,689 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently completed: generated 50 groups
E   2025-11-13 15:41:19,689 - INFO - Successfully generated 50 groups with shape (50, 1)
E   2025-11-13 15:41:19,689 - INFO - [OVERSAMPLING DEBUG] Generated 50 groups in total
E   2025-11-13 15:41:19,689 - INFO - Generated 50 groups efficiently
E   INFO: 'Y' array not found. Generating ground truth patches from 'objectGuess' as a fallback.
E   neighbor-sampled diffraction shape (50, 64, 64, 1)
E   loader: using provided ground truth patches.
E   INFO: None
E   <PtychoDataContainer X=(50, 64, 64, 1) Y_I=(50, 64, 64, 1) Y_phi=(50, 64, 64, 1) norm_Y_I=() coords_nominal=(50, 1, 2, 1) coords_true=(50, 1, 2, 1) nn_indices=(50, 1) mean=51.160 global_offsets=(50, 1, 2, 1) mean=110.459 local_offsets=(50, 1, 2, 1) mean=0.000 probe=(64, 64) mean_amplitude=0.322>
E   DEBUG: Setting probe to tf.Tensor(
E   [[[-0.00399459+8.81725922e-03j]
E     [ 0.00787074+1.04518672e-02j]
E     [-0.01909852+2.85197329e-03j]
E     ...
E     [-0.00430549-1.44718047e-02j]
E     [ 0.00670903+2.63100304e-02j]
E     [-0.01938361-6.67245360e-03j]]
E   
E    [[ 0.00921909+5.32836141e-03j]
E     [ 0.00422684-1.61591489e-02j]
E     [-0.00324812+1.62756350e-02j]
E     ...
E     [-0.00462818-2.68168072e-03j]
E     [ 0.01194528-6.69307355e-03j]
E     [ 0.0090712 -3.79238278e-03j]]
E   
E    [[ 0.0029482 -1.10371104e-02j]
E     [-0.01489174+5.26464824e-03j]
E     [-0.00083129+1.31990444e-02j]
E     ...
E     [ 0.00208769+1.68677475e-02j]
E     [ 0.00556216-3.03630810e-02j]
E     [ 0.01604221+1.45274084e-02j]]
E   
E    ...
E   
E    [[ 0.01654117+3.83263193e-02j]
E     [-0.01336122+7.02320365e-04j]
E     [-0.00380601-8.65837932e-03j]
E     ...
E     [ 0.01785212+2.66705058e-03j]
E     [-0.01791506+7.16461660e-03j]
E     [ 0.01052329-2.80616116e-02j]]
E   
E    [[-0.02582748-2.17778888e-05j]
E     [ 0.0098637 -3.15875164e-03j]
E     [-0.0156169 +2.47947779e-02j]
E     ...
E     [-0.01378679-1.89308310e-03j]
E     [ 0.00439025-1.43125653e-02j]
E     [ 0.02400579-1.19637549e-02j]]
E   
E    [[-0.00013256-1.32157737e-02j]
E     [ 0.03951043+1.84629709e-02j]
E     [-0.00848375+3.90608120e-03j]
E     ...
E     [-0.00414446+2.40438167e-04j]
E     [ 0.01858319+1.01260375e-02j]
E     [ 0.01299333+1.25071155e-02j]]], shape=(64, 64, 1), dtype=complex64) in params
E   DEBUG: Setting intensity_scale to 22.097086 in params
E   DEBUG: Setting probe to tf.Tensor(
E   [[[-0.00399459+8.81725922e-03j]
E     [ 0.00787074+1.04518672e-02j]
E     [-0.01909852+2.85197329e-03j]
E     ...
E     [-0.00430549-1.44718047e-02j]
E     [ 0.00670903+2.63100304e-02j]
E     [-0.01938361-6.67245360e-03j]]
E   
E    [[ 0.00921909+5.32836141e-03j]
E     [ 0.00422684-1.61591489e-02j]
E     [-0.00324812+1.62756350e-02j]
E     ...
E     [-0.00462818-2.68168072e-03j]
E     [ 0.01194528-6.69307355e-03j]
E     [ 0.0090712 -3.79238278e-03j]]
E   
E    [[ 0.0029482 -1.10371104e-02j]
E     [-0.01489174+5.26464824e-03j]
E     [-0.00083129+1.31990444e-02j]
E     ...
E     [ 0.00208769+1.68677475e-02j]
E     [ 0.00556216-3.03630810e-02j]
E     [ 0.01604221+1.45274084e-02j]]
E   
E    ...
E   
E    [[ 0.01654117+3.83263193e-02j]
E     [-0.01336122+7.02320365e-04j]
E     [-0.00380601-8.65837932e-03j]
E     ...
E     [ 0.01785212+2.66705058e-03j]
E     [-0.01791506+7.16461660e-03j]
E     [ 0.01052329-2.80616116e-02j]]
E   
E    [[-0.02582748-2.17778888e-05j]
E     [ 0.0098637 -3.15875164e-03j]
E     [-0.0156169 +2.47947779e-02j]
E     ...
E     [-0.01378679-1.89308310e-03j]
E     [ 0.00439025-1.43125653e-02j]
E     [ 0.02400579-1.19637549e-02j]]
E   
E    [[-0.00013256-1.32157737e-02j]
E     [ 0.03951043+1.84629709e-02j]
E     [-0.00848375+3.90608120e-03j]
E     ...
E     [-0.00414446+2.40438167e-04j]
E     [ 0.01858319+1.01260375e-02j]
E     [ 0.01299333+1.25071155e-02j]]], shape=(64, 64, 1), dtype=complex64) in params
E   Model: "functional"
E   
E    Layer (type)         Output Shape          Param #  Connected to      
E   
E    input (InputLayer)   (None, 64, 64, 1)           0  -                 
E   
E    intensity_scaler     (None, 64, 64, 1)           0  input[0][0]       
E    (IntensityScaler)                                                     
E   
E    conv2d (Conv2D)      (None, 64, 64,            640  intensity_scaler 
E                         64)                                              
E   
E    conv2d_1 (Conv2D)    (None, 64, 64,         36,928  conv2d[0][0]      
E                         64)                                              
E   
E    max_pooling2d        (None, 32, 32,              0  conv2d_1[0][0]    
E    (MaxPooling2D)       64)                                              
E   
E    conv2d_2 (Conv2D)    (None, 32, 32,         73,856  max_pooling2d[0] 
E                         128)                                             
E   
E    conv2d_3 (Conv2D)    (None, 32, 32,        147,584  conv2d_2[0][0]    
E                         128)                                             
E   
E    max_pooling2d_1      (None, 16, 16,              0  conv2d_3[0][0]    
E    (MaxPooling2D)       128)                                             
E   
E    conv2d_4 (Conv2D)    (None, 16, 16,        295,168  max_pooling2d_1[ 
E                         256)                                             
E   
E    conv2d_5 (Conv2D)    (None, 16, 16,        590,080  conv2d_4[0][0]    
E                         256)                                             
E   
E    max_pooling2d_2      (None, 8, 8, 256)           0  conv2d_5[0][0]    
E    (MaxPooling2D)                                                        
E   
E    conv2d_8 (Conv2D)    (None, 8, 8, 128)     295,040  max_pooling2d_2[ 
E   
E    conv2d_16 (Conv2D)   (None, 8, 8, 128)     295,040  max_pooling2d_2[ 
E   
E    conv2d_9 (Conv2D)    (None, 8, 8, 128)     147,584  conv2d_8[0][0]    
E   
E    conv2d_17 (Conv2D)   (None, 8, 8, 128)     147,584  conv2d_16[0][0]   
E   
E    up_sampling2d        (None, 16, 16,              0  conv2d_9[0][0]    
E    (UpSampling2D)       128)                                             
E   
E    up_sampling2d_3      (None, 16, 16,              0  conv2d_17[0][0]   
E    (UpSampling2D)       128)                                             
E   
E    conv2d_10 (Conv2D)   (None, 16, 16,         73,792  up_sampling2d[0] 
E                         64)                                              
E   
E    conv2d_18 (Conv2D)   (None, 16, 16,         73,792  up_sampling2d_3[ 
E                         64)                                              
E   
E    conv2d_11 (Conv2D)   (None, 16, 16,         36,928  conv2d_10[0][0]   
E                         64)                                              
E   
E    conv2d_19 (Conv2D)   (None, 16, 16,         36,928  conv2d_18[0][0]   
E                         64)                                              
E   
E    up_sampling2d_1      (None, 32, 32,              0  conv2d_11[0][0]   
E    (UpSampling2D)       64)                                              
E   
E    up_sampling2d_4      (None, 32, 32,              0  conv2d_19[0][0]   
E    (UpSampling2D)       64)                                              
E   
E    get_item_1           (None, 32, 32, 4)           0  up_sampling2d_1[ 
E    (GetItem)                                                             
E   
E    get_item_3           (None, 32, 32, 4)           0  up_sampling2d_4[ 
E    (GetItem)                                                             
E   
E    conv2d_12 (Conv2D)   (None, 32, 32,          2,368  get_item_1[0][0]  
E                         64)                                              
E   
E    conv2d_20 (Conv2D)   (None, 32, 32,          2,368  get_item_3[0][0]  
E                         64)                                              
E   
E    conv2d_13 (Conv2D)   (None, 32, 32,         36,928  conv2d_12[0][0]   
E                         64)                                              
E   
E    conv2d_21 (Conv2D)   (None, 32, 32,         36,928  conv2d_20[0][0]   
E                         64)                                              
E   
E    up_sampling2d_2      (None, 64, 64,              0  conv2d_13[0][0]   
E    (UpSampling2D)       64)                                              
E   
E    up_sampling2d_5      (None, 64, 64,              0  conv2d_21[0][0]   
E    (UpSampling2D)       64)                                              
E   
E    get_item (GetItem)   (None, 32, 32,              0  up_sampling2d_1[ 
E                         60)                                              
E   
E    conv2d_7 (Conv2D)    (None, 64, 64, 1)         577  up_sampling2d_2[ 
E   
E    get_item_2           (None, 32, 32,              0  up_sampling2d_4[ 
E    (GetItem)            60)                                              
E   
E    conv2d_15 (Conv2D)   (None, 64, 64, 1)         577  up_sampling2d_5[ 
E   
E    conv2d_6 (Conv2D)    (None, 32, 32, 1)         541  get_item[0][0]    
E   
E    silu (Silu)          (None, 64, 64, 1)           0  conv2d_7[0][0]    
E   
E    conv2d_14 (Conv2D)   (None, 32, 32, 1)         541  get_item_2[0][0]  
E   
E    silu_1 (Silu)        (None, 64, 64, 1)           0  conv2d_15[0][0]   
E   
E    amp                  (None, 32, 32, 1)           0  conv2d_6[0][0]    
E    (ActivationLayer)                                                     
E   
E    center_mask_layer    (None, 64, 64, 1)           0  silu[0][0]        
E    (CenterMaskLayer)                                                     
E   
E    phi                  (None, 32, 32, 1)           0  conv2d_14[0][0]   
E    (ActivationLayer)                                                     
E   
E    center_mask_layer_1  (None, 64, 64, 1)           0  silu_1[0][0]      
E    (CenterMaskLayer)                                                     
E   
E    amp_padded           (None, 64, 64, 1)           0  amp[0][0]         
E    (ZeroPadding2D)                                                       
E   
E    multiply (Multiply)  (None, 64, 64, 1)           0  silu[0][0],       
E                                                        center_mask_laye 
E   
E    phase_padded         (None, 64, 64, 1)           0  phi[0][0]         
E    (ZeroPadding2D)                                                       
E   
E    multiply_1           (None, 64, 64, 1)           0  silu_1[0][0],     
E    (Multiply)                                          center_mask_laye 
E   
E    add (Add)            (None, 64, 64, 1)           0  amp_padded[0][0], 
E                                                        multiply[0][0]    
E   
E    add_1 (Add)          (None, 64, 64, 1)           0  phase_padded[0][ 
E                                                        multiply_1[0][0]  
E   
E    obj                  (None, 64, 64, 1)           0  add[0][0],        
E    (CombineComplexLay                                 add_1[0][0]       
E   
E    input_positions      (None, 1, 2, 1)             0  -                 
E    (InputLayer)                                                          
E   
E    padded_obj_2         (None, 74, 74, 1)           0  obj[0][0],        
E    (ReassemblePatches                                 input_positions[ 
E   
E    padded_objs_with_o  (None, 64, 64, 1)           0  padded_obj_2[0][ 
E    (ExtractPatchesPos                                 input_positions[ 
E   
E    probe_illumination   [(None, 64, 64,             0  padded_objs_with 
E    (ProbeIllumination)  1), (None, 64,                                   
E                         64, 1)]                                          
E   
E    pred_amplitude       [(None, 64, 64,             0  probe_illuminati 
E    (PadAndDiffractLay  1), (None, 64,                                   
E                         64, 1)]                                          
E   
E    pred_diff_channels   (None, 64, 64, 1)           0  pred_amplitude[0 
E    (FlatToChannelLaye                                                   
E   
E    intensity_scaler_i  (None, 64, 64, 1)           0  pred_diff_channe 
E    (IntensityScaler_i                                                   
E   
E    trimmed_obj          (None, 64, 64, 1)           0  padded_obj_2[0][ 
E    (TrimReconstructio                                                   
E   
E    pred_intensity       (None, 64, 64, 1)           0  intensity_scaler 
E    (SquareLayer)                                                         
E   
E    Total params: 2,331,772 (8.90 MB)
E    Trainable params: 2,331,772 (8.90 MB)
E    Non-trainable params: 0 (0.00 B)
E   None
E   Current Parameters:
E   --------------------
E   N: 64
E   amp_activation: sigmoid
E   backend: tensorflow
E   batch_size: 16
E   bigN: 64
E   big_gridsize: 10
E   data_source: generic
E   debug: True
E   default_probe_scale: 0.7
E   enable_oversampling: False
E   gaussian_smoothing_sigma: 0.0
E   gridsize: 1
E   h5_path: wts.h5
E   intensity_scale: 22.09708595275879
E   intensity_scale.trainable: True
E   label: 
E   mae_weight: 0.0
E   max_position_jitter: 10
E   model_type: pinn
E   n_filters_scale: 2
E   n_groups: 50
E   n_images: 50
E   neighbor_count: 4
E   nepochs: 2
E   nimgs_test: 3
E   nimgs_train: 9
E   nll_weight: 1.0
E   nphotons: 500000.0
E   npseed: 42
E   object.big: True
E   offset: 4
E   outer_offset_test: None
E   outer_offset_train: None
E   output_prefix: /tmp/tmp1i23z19e/training/training_nphotons_500000.0_e2e
E   pad_object: True
E   positions.provided: True
E   probe:
E     shape: (64, 64, 1)
E     mean: -0.016-0.002j
E     std: 0.666
E     min: -3.769-0.236j
E     max: 2.823-0.116j
E   probe.big: True
E   probe.mask: False
E   probe.trainable: False
E   probe_scale: 4.0
E   realspace_mae_weight: 0.0
E   realspace_weight: 0.0
E   sequential_sampling: False
E   set_phi: False
E   sim_jitter_scale: 0.0
E   size: 392
E   test_data_file_path: /tmp/tmp1i23z19e/simulation/simulated_data_nphotons_500000.0_e2e.npz
E   torch_loss_mode: poisson
E   train_data_file_path: /tmp/tmp1i23z19e/simulation/simulated_data_nphotons_500000.0_e2e.npz
E   tv_weight: 0.0
E   use_xla_translate: True
E   Epoch 1/2
E   2025-11-13 15:41:21,082 - ERROR - An error occurred during execution: Exception encountered when calling ReassemblePatchesLayer.call().
E   
E   [1mUsing a symbolic `tf.Tensor` as a Python `bool` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.[0m
E   
E   Arguments received by ReassemblePatchesLayer.call():
E      inputs=['tf.Tensor(shape=(None, 64, 64, 1), dtype=complex64)', 'tf.Tensor(shape=(None, 1, 2, 1), dtype=complex64)']
E   
E   stderr: 2025-11-13 15:41:15.441378: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
E   WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E   E0000 00:00:1763077275.452497 2349273 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E   E0000 00:00:1763077275.456089 2349273 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E   W0000 00:00:1763077275.465702 2349273 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E   W0000 00:00:1763077275.465711 2349273 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E   W0000 00:00:1763077275.465712 2349273 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E   W0000 00:00:1763077275.465714 2349273 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E   I0000 00:00:1763077278.218231 2349273 gpu_process_state.cc:208] Using CUDA malloc Async allocator for GPU: 0
E   I0000 00:00:1763077278.219425 2349273 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21277 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:04:00.0, compute capability: 8.6
E   WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E   I0000 00:00:1763077278.969432 2349273 service.cc:152] XLA service 0x2d667870 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
E   I0000 00:00:1763077278.969452 2349273 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6
E   I0000 00:00:1763077279.006016 2349273 cuda_dnn.cc:529] Loaded cuDNN version 91002
E   I0000 00:00:1763077279.141744 2349273 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
E   I0000 00:00:1763077280.531343 2349273 cupti_tracer.cc:1026] Profiler found 1 GPUs
E   W0000 00:00:1763077280.547425 2349273 cupti_tracer.cc:1213] Fail to use per-thread activity buffer, cupti trace overhead may be big. CUPTI ERROR CODE:1
E   I0000 00:00:1763077280.548247 2349273 cupti_tracer.cc:1249] CUPTI activity buffer flushed
E   Traceback (most recent call last):
E     File "/home/ollie/Documents/PtychoPINN/scripts/training/train.py", line 440, in <module>
E       main()
E     File "/home/ollie/Documents/PtychoPINN/scripts/training/train.py", line 419, in main
E       recon_amp, recon_phase, results = run_cdi_example_with_backend(
E                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E     File "/home/ollie/Documents/PtychoPINN/ptycho/workflows/backend_selector.py", line 141, in run_cdi_example_with_backend
E       recon_amp, recon_phase, results = tf_components.run_cdi_example(
E                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E     File "/home/ollie/Documents/PtychoPINN/ptycho/workflows/components.py", line 846, in run_cdi_example
E       train_results = train_cdi_model(train_data, test_data, config)
E                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E     File "/home/ollie/Documents/PtychoPINN/ptycho/workflows/components.py", line 726, in train_cdi_model
E       results = train_pinn.train_eval(PtychoDataset(train_container, test_container))
E                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E     File "/home/ollie/Documents/PtychoPINN/ptycho/train_pinn.py", line 90, in train_eval
E       model_instance, history = train(ptycho_dataset.train_data)
E                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E     File "/home/ollie/Documents/PtychoPINN/ptycho/train_pinn.py", line 86, in train
E       return model_instance, model.train(nepochs, train_data)
E                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E     File "/home/ollie/Documents/PtychoPINN/ptycho/model.py", line 593, in train
E       history=autoencoder.fit(
E               ^^^^^^^^^^^^^^^^
E     File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
E       raise e.with_traceback(filtered_tb) from None
E     File "/home/ollie/Documents/PtychoPINN/ptycho/custom_layers.py", line 159, in call
E       if total_patches > batch_threshold:
E          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: Exception encountered when calling ReassemblePatchesLayer.call().
E   
E   [1mUsing a symbolic `tf.Tensor` as a Python `bool` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.[0m
E   
E   Arguments received by ReassemblePatchesLayer.call():
E      inputs=['tf.Tensor(shape=(None, 64, 64, 1), dtype=complex64)', 'tf.Tensor(shape=(None, 1, 2, 1), dtype=complex64)']
----------------------------- Captured stdout call -----------------------------

Created test directories under: /tmp/tmp1i23z19e
--- Testing end-to-end nphotons consistency ---
--- Simulating data with nphotons=500000.0 ---
--- Training model with data from nphotons=500000.0 ---
Cleaned up test directory: /tmp/tmp1i23z19e
__ TestNphotonsMetadataIntegration.test_metadata_persistence_single_nphotons ___

self = <tests.test_nphotons_metadata_integration.TestNphotonsMetadataIntegration testMethod=test_metadata_persistence_single_nphotons>

    def test_metadata_persistence_single_nphotons(self):
        """Test metadata persistence through complete workflow for single nphotons value."""
        nphotons = 1e6
    
        # 1. Simulate data with specific nphotons
        sim_file = self._simulate_with_nphotons(nphotons, "_single")
    
        # 2. Verify simulation metadata
        sim_metadata = self._verify_metadata(sim_file, nphotons)
    
        # 3. Train model with simulated data
>       model_dir = self._train_model(sim_file, nphotons, "_single")
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/test_nphotons_metadata_integration.py:217: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_nphotons_metadata_integration.py:149: in _train_model
    self.assertEqual(result.returncode, 0,
E   AssertionError: 1 != 0 : Training failed for nphotons=1000000.0
E   stdout: 2025-11-13 15:41:30,820 - INFO - Configuration setup complete
E   2025-11-13 15:41:30,821 - INFO - Final configuration: TrainingConfig(model=ModelConfig(N=64, gridsize=1, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', object_big=True, probe_big=True, probe_mask=False, pad_object=True, probe_scale=4.0, gaussian_smoothing_sigma=0.0), train_data_file=PosixPath('/tmp/tmpgfgp7m3l/simulation/simulated_data_nphotons_1000000.0_single.npz'), test_data_file=PosixPath('/tmp/tmpgfgp7m3l/simulation/simulated_data_nphotons_1000000.0_single.npz'), batch_size=16, nepochs=2, mae_weight=0.0, nll_weight=1.0, realspace_mae_weight=0.0, realspace_weight=0.0, nphotons=1000000.0, n_groups=50, n_images=50, n_subsample=None, subsample_seed=None, neighbor_count=4, enable_oversampling=False, neighbor_pool_size=None, positions_provided=True, probe_trainable=False, intensity_scale_trainable=True, output_dir=PosixPath('/tmp/tmpgfgp7m3l/training/training_nphotons_1000000.0_single'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
E   2025-11-13 15:41:30,821 - INFO - Legacy mode: using 50 groups (gridsize=1)
E   2025-11-13 15:41:30,821 - INFO - Starting training with n_subsample=50, n_groups=50, stitching=disabled
E   2025-11-13 15:41:30,821 - INFO - Loading data from /tmp/tmpgfgp7m3l/simulation/simulated_data_nphotons_1000000.0_single.npz with n_images=50, n_subsample=50
E   2025-11-13 15:41:30,827 - INFO - Independent sampling: subsampling 50 images from 100 total
E   2025-11-13 15:41:30,827 - INFO - Randomly subsampled 50 images
E   diff3d shape: (50, 64, 64)
E   probeGuess shape: (64, 64)
E   scan_index shape: (50,)
E   objectGuess shape: (227, 226)
E   xcoords shape: (50,)
E   ycoords shape: (50,)
E   xcoords_start shape: (50,)
E   ycoords_start shape: (50,)
E   2025-11-13 15:41:30,840 - INFO - Overriding nphotons from config (1.0e+06) with value from dataset metadata: 1.0e+06
E   2025-11-13 15:41:30,840 - INFO - Loading data from /tmp/tmpgfgp7m3l/simulation/simulated_data_nphotons_1000000.0_single.npz with n_images=None, n_subsample=None
E   2025-11-13 15:41:30,846 - INFO - Using full dataset of 100 images
E   diff3d shape: (100, 64, 64)
E   probeGuess shape: (64, 64)
E   scan_index shape: (100,)
E   objectGuess shape: (227, 226)
E   xcoords shape: (100,)
E   ycoords shape: (100,)
E   xcoords_start shape: (100,)
E   ycoords_start shape: (100,)
E   2025-11-13 15:41:30,847 - INFO - Loaded test data from /tmp/tmpgfgp7m3l/simulation/simulated_data_nphotons_1000000.0_single.npz
E   2025-11-13 15:41:30,847 - INFO - Backend dispatcher: params.cfg synchronized with TrainingConfig (backend=tensorflow)
E   2025-11-13 15:41:30,847 - INFO - Backend dispatcher: routing to TensorFlow workflow (ptycho.workflows.components)
E   2025-11-13 15:41:30,847 - INFO - [OVERSAMPLING DEBUG] generate_grouped_data called with: nsamples=50, n_points=50, C=1, K=4
E   2025-11-13 15:41:30,847 - INFO - [OVERSAMPLING DEBUG] Parameters: gridsize=1, N=64, sequential_sampling=False
E   2025-11-13 15:41:30,847 - INFO - [OVERSAMPLING DEBUG] Oversampling flags: enable_oversampling=False, neighbor_pool_size=None, effective_K=4
E   2025-11-13 15:41:30,847 - INFO - [OVERSAMPLING DEBUG] Oversampling check: nsamples > n_points = 50 > 50 = False
E   2025-11-13 15:41:30,847 - INFO - [OVERSAMPLING DEBUG] Oversampling check: C > 1 = 1 > 1 = False
E   2025-11-13 15:41:30,847 - INFO - [OVERSAMPLING DEBUG] needs_oversampling = False
E   DEBUG: nsamples: 50, gridsize: 1 (using efficient random sample-then-group strategy)
E   2025-11-13 15:41:30,847 - INFO - Using efficient random sampling strategy for gridsize=1
E   2025-11-13 15:41:30,847 - INFO - [OVERSAMPLING DEBUG] Taking efficient branch: standard sample-then-group
E   2025-11-13 15:41:30,847 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently called with: nsamples=50, K=4, C=1
E   2025-11-13 15:41:30,847 - INFO - Generating 50 groups efficiently from 50 points (K=4, C=1)
E   2025-11-13 15:41:30,847 - INFO - [OVERSAMPLING DEBUG] Standard case: using 50 groups from 50 points
E   2025-11-13 15:41:30,847 - INFO - [OVERSAMPLING DEBUG] Using all 50 points as seeds (no sampling needed)
E   2025-11-13 15:41:30,848 - INFO - Using all 50 points as seeds
E   2025-11-13 15:41:30,848 - INFO - Using seed indices directly for C=1 (gridsize=1) - no neighbor search needed
E   2025-11-13 15:41:30,848 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently completed: generated 50 groups
E   2025-11-13 15:41:30,848 - INFO - Successfully generated 50 groups with shape (50, 1)
E   2025-11-13 15:41:30,848 - INFO - [OVERSAMPLING DEBUG] Generated 50 groups in total
E   2025-11-13 15:41:30,848 - INFO - Generated 50 groups efficiently
E   INFO: 'Y' array not found. Generating ground truth patches from 'objectGuess' as a fallback.
E   neighbor-sampled diffraction shape (50, 64, 64, 1)
E   loader: using provided ground truth patches.
E   INFO: None
E   <PtychoDataContainer X=(50, 64, 64, 1) Y_I=(50, 64, 64, 1) Y_phi=(50, 64, 64, 1) norm_Y_I=() coords_nominal=(50, 1, 2, 1) coords_true=(50, 1, 2, 1) nn_indices=(50, 1) mean=24.500 global_offsets=(50, 1, 2, 1) mean=111.249 local_offsets=(50, 1, 2, 1) mean=0.000 probe=(64, 64) mean_amplitude=0.322>
E   2025-11-13 15:41:32,447 - INFO - [OVERSAMPLING DEBUG] generate_grouped_data called with: nsamples=50, n_points=100, C=1, K=4
E   2025-11-13 15:41:32,447 - INFO - [OVERSAMPLING DEBUG] Parameters: gridsize=1, N=64, sequential_sampling=False
E   2025-11-13 15:41:32,447 - INFO - [OVERSAMPLING DEBUG] Oversampling flags: enable_oversampling=False, neighbor_pool_size=None, effective_K=4
E   2025-11-13 15:41:32,447 - INFO - [OVERSAMPLING DEBUG] Oversampling check: nsamples > n_points = 50 > 100 = False
E   2025-11-13 15:41:32,447 - INFO - [OVERSAMPLING DEBUG] Oversampling check: C > 1 = 1 > 1 = False
E   2025-11-13 15:41:32,447 - INFO - [OVERSAMPLING DEBUG] needs_oversampling = False
E   DEBUG: nsamples: 50, gridsize: 1 (using efficient random sample-then-group strategy)
E   2025-11-13 15:41:32,447 - INFO - Using efficient random sampling strategy for gridsize=1
E   2025-11-13 15:41:32,447 - INFO - [OVERSAMPLING DEBUG] Taking efficient branch: standard sample-then-group
E   2025-11-13 15:41:32,447 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently called with: nsamples=50, K=4, C=1
E   2025-11-13 15:41:32,447 - INFO - Generating 50 groups efficiently from 100 points (K=4, C=1)
E   2025-11-13 15:41:32,447 - INFO - [OVERSAMPLING DEBUG] Standard case: using 50 groups from 100 points
E   2025-11-13 15:41:32,447 - INFO - [OVERSAMPLING DEBUG] Randomly sampled 50 seed points
E   2025-11-13 15:41:32,447 - INFO - Sampled 50 seed points from 100 total points
E   2025-11-13 15:41:32,447 - INFO - Using seed indices directly for C=1 (gridsize=1) - no neighbor search needed
E   2025-11-13 15:41:32,447 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently completed: generated 50 groups
E   2025-11-13 15:41:32,448 - INFO - Successfully generated 50 groups with shape (50, 1)
E   2025-11-13 15:41:32,448 - INFO - [OVERSAMPLING DEBUG] Generated 50 groups in total
E   2025-11-13 15:41:32,448 - INFO - Generated 50 groups efficiently
E   INFO: 'Y' array not found. Generating ground truth patches from 'objectGuess' as a fallback.
E   neighbor-sampled diffraction shape (50, 64, 64, 1)
E   loader: using provided ground truth patches.
E   INFO: None
E   <PtychoDataContainer X=(50, 64, 64, 1) Y_I=(50, 64, 64, 1) Y_phi=(50, 64, 64, 1) norm_Y_I=() coords_nominal=(50, 1, 2, 1) coords_true=(50, 1, 2, 1) nn_indices=(50, 1) mean=48.760 global_offsets=(50, 1, 2, 1) mean=108.717 local_offsets=(50, 1, 2, 1) mean=0.000 probe=(64, 64) mean_amplitude=0.322>
E   DEBUG: Setting probe to tf.Tensor(
E   [[[-0.00399459+8.81725922e-03j]
E     [ 0.00787074+1.04518672e-02j]
E     [-0.01909852+2.85197329e-03j]
E     ...
E     [-0.00430549-1.44718047e-02j]
E     [ 0.00670903+2.63100304e-02j]
E     [-0.01938361-6.67245360e-03j]]
E   
E    [[ 0.00921909+5.32836141e-03j]
E     [ 0.00422684-1.61591489e-02j]
E     [-0.00324812+1.62756350e-02j]
E     ...
E     [-0.00462818-2.68168072e-03j]
E     [ 0.01194528-6.69307355e-03j]
E     [ 0.0090712 -3.79238278e-03j]]
E   
E    [[ 0.0029482 -1.10371104e-02j]
E     [-0.01489174+5.26464824e-03j]
E     [-0.00083129+1.31990444e-02j]
E     ...
E     [ 0.00208769+1.68677475e-02j]
E     [ 0.00556216-3.03630810e-02j]
E     [ 0.01604221+1.45274084e-02j]]
E   
E    ...
E   
E    [[ 0.01654117+3.83263193e-02j]
E     [-0.01336122+7.02320365e-04j]
E     [-0.00380601-8.65837932e-03j]
E     ...
E     [ 0.01785212+2.66705058e-03j]
E     [-0.01791506+7.16461660e-03j]
E     [ 0.01052329-2.80616116e-02j]]
E   
E    [[-0.02582748-2.17778888e-05j]
E     [ 0.0098637 -3.15875164e-03j]
E     [-0.0156169 +2.47947779e-02j]
E     ...
E     [-0.01378679-1.89308310e-03j]
E     [ 0.00439025-1.43125653e-02j]
E     [ 0.02400579-1.19637549e-02j]]
E   
E    [[-0.00013256-1.32157737e-02j]
E     [ 0.03951043+1.84629709e-02j]
E     [-0.00848375+3.90608120e-03j]
E     ...
E     [-0.00414446+2.40438167e-04j]
E     [ 0.01858319+1.01260375e-02j]
E     [ 0.01299333+1.25071155e-02j]]], shape=(64, 64, 1), dtype=complex64) in params
E   DEBUG: Setting intensity_scale to 31.25 in params
E   DEBUG: Setting probe to tf.Tensor(
E   [[[-0.00399459+8.81725922e-03j]
E     [ 0.00787074+1.04518672e-02j]
E     [-0.01909852+2.85197329e-03j]
E     ...
E     [-0.00430549-1.44718047e-02j]
E     [ 0.00670903+2.63100304e-02j]
E     [-0.01938361-6.67245360e-03j]]
E   
E    [[ 0.00921909+5.32836141e-03j]
E     [ 0.00422684-1.61591489e-02j]
E     [-0.00324812+1.62756350e-02j]
E     ...
E     [-0.00462818-2.68168072e-03j]
E     [ 0.01194528-6.69307355e-03j]
E     [ 0.0090712 -3.79238278e-03j]]
E   
E    [[ 0.0029482 -1.10371104e-02j]
E     [-0.01489174+5.26464824e-03j]
E     [-0.00083129+1.31990444e-02j]
E     ...
E     [ 0.00208769+1.68677475e-02j]
E     [ 0.00556216-3.03630810e-02j]
E     [ 0.01604221+1.45274084e-02j]]
E   
E    ...
E   
E    [[ 0.01654117+3.83263193e-02j]
E     [-0.01336122+7.02320365e-04j]
E     [-0.00380601-8.65837932e-03j]
E     ...
E     [ 0.01785212+2.66705058e-03j]
E     [-0.01791506+7.16461660e-03j]
E     [ 0.01052329-2.80616116e-02j]]
E   
E    [[-0.02582748-2.17778888e-05j]
E     [ 0.0098637 -3.15875164e-03j]
E     [-0.0156169 +2.47947779e-02j]
E     ...
E     [-0.01378679-1.89308310e-03j]
E     [ 0.00439025-1.43125653e-02j]
E     [ 0.02400579-1.19637549e-02j]]
E   
E    [[-0.00013256-1.32157737e-02j]
E     [ 0.03951043+1.84629709e-02j]
E     [-0.00848375+3.90608120e-03j]
E     ...
E     [-0.00414446+2.40438167e-04j]
E     [ 0.01858319+1.01260375e-02j]
E     [ 0.01299333+1.25071155e-02j]]], shape=(64, 64, 1), dtype=complex64) in params
E   Model: "functional"
E   
E    Layer (type)         Output Shape          Param #  Connected to      
E   
E    input (InputLayer)   (None, 64, 64, 1)           0  -                 
E   
E    intensity_scaler     (None, 64, 64, 1)           0  input[0][0]       
E    (IntensityScaler)                                                     
E   
E    conv2d (Conv2D)      (None, 64, 64,            640  intensity_scaler 
E                         64)                                              
E   
E    conv2d_1 (Conv2D)    (None, 64, 64,         36,928  conv2d[0][0]      
E                         64)                                              
E   
E    max_pooling2d        (None, 32, 32,              0  conv2d_1[0][0]    
E    (MaxPooling2D)       64)                                              
E   
E    conv2d_2 (Conv2D)    (None, 32, 32,         73,856  max_pooling2d[0] 
E                         128)                                             
E   
E    conv2d_3 (Conv2D)    (None, 32, 32,        147,584  conv2d_2[0][0]    
E                         128)                                             
E   
E    max_pooling2d_1      (None, 16, 16,              0  conv2d_3[0][0]    
E    (MaxPooling2D)       128)                                             
E   
E    conv2d_4 (Conv2D)    (None, 16, 16,        295,168  max_pooling2d_1[ 
E                         256)                                             
E   
E    conv2d_5 (Conv2D)    (None, 16, 16,        590,080  conv2d_4[0][0]    
E                         256)                                             
E   
E    max_pooling2d_2      (None, 8, 8, 256)           0  conv2d_5[0][0]    
E    (MaxPooling2D)                                                        
E   
E    conv2d_8 (Conv2D)    (None, 8, 8, 128)     295,040  max_pooling2d_2[ 
E   
E    conv2d_16 (Conv2D)   (None, 8, 8, 128)     295,040  max_pooling2d_2[ 
E   
E    conv2d_9 (Conv2D)    (None, 8, 8, 128)     147,584  conv2d_8[0][0]    
E   
E    conv2d_17 (Conv2D)   (None, 8, 8, 128)     147,584  conv2d_16[0][0]   
E   
E    up_sampling2d        (None, 16, 16,              0  conv2d_9[0][0]    
E    (UpSampling2D)       128)                                             
E   
E    up_sampling2d_3      (None, 16, 16,              0  conv2d_17[0][0]   
E    (UpSampling2D)       128)                                             
E   
E    conv2d_10 (Conv2D)   (None, 16, 16,         73,792  up_sampling2d[0] 
E                         64)                                              
E   
E    conv2d_18 (Conv2D)   (None, 16, 16,         73,792  up_sampling2d_3[ 
E                         64)                                              
E   
E    conv2d_11 (Conv2D)   (None, 16, 16,         36,928  conv2d_10[0][0]   
E                         64)                                              
E   
E    conv2d_19 (Conv2D)   (None, 16, 16,         36,928  conv2d_18[0][0]   
E                         64)                                              
E   
E    up_sampling2d_1      (None, 32, 32,              0  conv2d_11[0][0]   
E    (UpSampling2D)       64)                                              
E   
E    up_sampling2d_4      (None, 32, 32,              0  conv2d_19[0][0]   
E    (UpSampling2D)       64)                                              
E   
E    get_item_1           (None, 32, 32, 4)           0  up_sampling2d_1[ 
E    (GetItem)                                                             
E   
E    get_item_3           (None, 32, 32, 4)           0  up_sampling2d_4[ 
E    (GetItem)                                                             
E   
E    conv2d_12 (Conv2D)   (None, 32, 32,          2,368  get_item_1[0][0]  
E                         64)                                              
E   
E    conv2d_20 (Conv2D)   (None, 32, 32,          2,368  get_item_3[0][0]  
E                         64)                                              
E   
E    conv2d_13 (Conv2D)   (None, 32, 32,         36,928  conv2d_12[0][0]   
E                         64)                                              
E   
E    conv2d_21 (Conv2D)   (None, 32, 32,         36,928  conv2d_20[0][0]   
E                         64)                                              
E   
E    up_sampling2d_2      (None, 64, 64,              0  conv2d_13[0][0]   
E    (UpSampling2D)       64)                                              
E   
E    up_sampling2d_5      (None, 64, 64,              0  conv2d_21[0][0]   
E    (UpSampling2D)       64)                                              
E   
E    get_item (GetItem)   (None, 32, 32,              0  up_sampling2d_1[ 
E                         60)                                              
E   
E    conv2d_7 (Conv2D)    (None, 64, 64, 1)         577  up_sampling2d_2[ 
E   
E    get_item_2           (None, 32, 32,              0  up_sampling2d_4[ 
E    (GetItem)            60)                                              
E   
E    conv2d_15 (Conv2D)   (None, 64, 64, 1)         577  up_sampling2d_5[ 
E   
E    conv2d_6 (Conv2D)    (None, 32, 32, 1)         541  get_item[0][0]    
E   
E    silu (Silu)          (None, 64, 64, 1)           0  conv2d_7[0][0]    
E   
E    conv2d_14 (Conv2D)   (None, 32, 32, 1)         541  get_item_2[0][0]  
E   
E    silu_1 (Silu)        (None, 64, 64, 1)           0  conv2d_15[0][0]   
E   
E    amp                  (None, 32, 32, 1)           0  conv2d_6[0][0]    
E    (ActivationLayer)                                                     
E   
E    center_mask_layer    (None, 64, 64, 1)           0  silu[0][0]        
E    (CenterMaskLayer)                                                     
E   
E    phi                  (None, 32, 32, 1)           0  conv2d_14[0][0]   
E    (ActivationLayer)                                                     
E   
E    center_mask_layer_1  (None, 64, 64, 1)           0  silu_1[0][0]      
E    (CenterMaskLayer)                                                     
E   
E    amp_padded           (None, 64, 64, 1)           0  amp[0][0]         
E    (ZeroPadding2D)                                                       
E   
E    multiply (Multiply)  (None, 64, 64, 1)           0  silu[0][0],       
E                                                        center_mask_laye 
E   
E    phase_padded         (None, 64, 64, 1)           0  phi[0][0]         
E    (ZeroPadding2D)                                                       
E   
E    multiply_1           (None, 64, 64, 1)           0  silu_1[0][0],     
E    (Multiply)                                          center_mask_laye 
E   
E    add (Add)            (None, 64, 64, 1)           0  amp_padded[0][0], 
E                                                        multiply[0][0]    
E   
E    add_1 (Add)          (None, 64, 64, 1)           0  phase_padded[0][ 
E                                                        multiply_1[0][0]  
E   
E    obj                  (None, 64, 64, 1)           0  add[0][0],        
E    (CombineComplexLay                                 add_1[0][0]       
E   
E    input_positions      (None, 1, 2, 1)             0  -                 
E    (InputLayer)                                                          
E   
E    padded_obj_2         (None, 74, 74, 1)           0  obj[0][0],        
E    (ReassemblePatches                                 input_positions[ 
E   
E    padded_objs_with_o  (None, 64, 64, 1)           0  padded_obj_2[0][ 
E    (ExtractPatchesPos                                 input_positions[ 
E   
E    probe_illumination   [(None, 64, 64,             0  padded_objs_with 
E    (ProbeIllumination)  1), (None, 64,                                   
E                         64, 1)]                                          
E   
E    pred_amplitude       [(None, 64, 64,             0  probe_illuminati 
E    (PadAndDiffractLay  1), (None, 64,                                   
E                         64, 1)]                                          
E   
E    pred_diff_channels   (None, 64, 64, 1)           0  pred_amplitude[0 
E    (FlatToChannelLaye                                                   
E   
E    intensity_scaler_i  (None, 64, 64, 1)           0  pred_diff_channe 
E    (IntensityScaler_i                                                   
E   
E    trimmed_obj          (None, 64, 64, 1)           0  padded_obj_2[0][ 
E    (TrimReconstructio                                                   
E   
E    pred_intensity       (None, 64, 64, 1)           0  intensity_scaler 
E    (SquareLayer)                                                         
E   
E    Total params: 2,331,772 (8.90 MB)
E    Trainable params: 2,331,772 (8.90 MB)
E    Non-trainable params: 0 (0.00 B)
E   None
E   Current Parameters:
E   --------------------
E   N: 64
E   amp_activation: sigmoid
E   backend: tensorflow
E   batch_size: 16
E   bigN: 64
E   big_gridsize: 10
E   data_source: generic
E   debug: True
E   default_probe_scale: 0.7
E   enable_oversampling: False
E   gaussian_smoothing_sigma: 0.0
E   gridsize: 1
E   h5_path: wts.h5
E   intensity_scale: 31.25
E   intensity_scale.trainable: True
E   label: 
E   mae_weight: 0.0
E   max_position_jitter: 10
E   model_type: pinn
E   n_filters_scale: 2
E   n_groups: 50
E   n_images: 50
E   neighbor_count: 4
E   nepochs: 2
E   nimgs_test: 3
E   nimgs_train: 9
E   nll_weight: 1.0
E   nphotons: 1000000.0
E   npseed: 42
E   object.big: True
E   offset: 4
E   outer_offset_test: None
E   outer_offset_train: None
E   output_prefix: /tmp/tmpgfgp7m3l/training/training_nphotons_1000000.0_single
E   pad_object: True
E   positions.provided: True
E   probe:
E     shape: (64, 64, 1)
E     mean: -0.016-0.002j
E     std: 0.666
E     min: -3.769-0.236j
E     max: 2.823-0.116j
E   probe.big: True
E   probe.mask: False
E   probe.trainable: False
E   probe_scale: 4.0
E   realspace_mae_weight: 0.0
E   realspace_weight: 0.0
E   sequential_sampling: False
E   set_phi: False
E   sim_jitter_scale: 0.0
E   size: 392
E   test_data_file_path: /tmp/tmpgfgp7m3l/simulation/simulated_data_nphotons_1000000.0_single.npz
E   torch_loss_mode: poisson
E   train_data_file_path: /tmp/tmpgfgp7m3l/simulation/simulated_data_nphotons_1000000.0_single.npz
E   tv_weight: 0.0
E   use_xla_translate: True
E   Epoch 1/2
E   2025-11-13 15:41:33,839 - ERROR - An error occurred during execution: Exception encountered when calling ReassemblePatchesLayer.call().
E   
E   [1mUsing a symbolic `tf.Tensor` as a Python `bool` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.[0m
E   
E   Arguments received by ReassemblePatchesLayer.call():
E      inputs=['tf.Tensor(shape=(None, 64, 64, 1), dtype=complex64)', 'tf.Tensor(shape=(None, 1, 2, 1), dtype=complex64)']
E   
E   stderr: 2025-11-13 15:41:28.092318: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
E   WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E   E0000 00:00:1763077288.103285 2349905 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E   E0000 00:00:1763077288.106900 2349905 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E   W0000 00:00:1763077288.116734 2349905 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E   W0000 00:00:1763077288.116743 2349905 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E   W0000 00:00:1763077288.116745 2349905 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E   W0000 00:00:1763077288.116746 2349905 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E   I0000 00:00:1763077290.981443 2349905 gpu_process_state.cc:208] Using CUDA malloc Async allocator for GPU: 0
E   I0000 00:00:1763077290.982768 2349905 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21277 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:04:00.0, compute capability: 8.6
E   WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E   I0000 00:00:1763077291.713783 2349905 service.cc:152] XLA service 0x1c612d30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
E   I0000 00:00:1763077291.713804 2349905 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6
E   I0000 00:00:1763077291.750057 2349905 cuda_dnn.cc:529] Loaded cuDNN version 91002
E   I0000 00:00:1763077291.886365 2349905 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
E   I0000 00:00:1763077293.286824 2349905 cupti_tracer.cc:1026] Profiler found 1 GPUs
E   W0000 00:00:1763077293.303093 2349905 cupti_tracer.cc:1213] Fail to use per-thread activity buffer, cupti trace overhead may be big. CUPTI ERROR CODE:1
E   I0000 00:00:1763077293.303712 2349905 cupti_tracer.cc:1249] CUPTI activity buffer flushed
E   Traceback (most recent call last):
E     File "/home/ollie/Documents/PtychoPINN/scripts/training/train.py", line 440, in <module>
E       main()
E     File "/home/ollie/Documents/PtychoPINN/scripts/training/train.py", line 419, in main
E       recon_amp, recon_phase, results = run_cdi_example_with_backend(
E                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E     File "/home/ollie/Documents/PtychoPINN/ptycho/workflows/backend_selector.py", line 141, in run_cdi_example_with_backend
E       recon_amp, recon_phase, results = tf_components.run_cdi_example(
E                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E     File "/home/ollie/Documents/PtychoPINN/ptycho/workflows/components.py", line 846, in run_cdi_example
E       train_results = train_cdi_model(train_data, test_data, config)
E                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E     File "/home/ollie/Documents/PtychoPINN/ptycho/workflows/components.py", line 726, in train_cdi_model
E       results = train_pinn.train_eval(PtychoDataset(train_container, test_container))
E                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E     File "/home/ollie/Documents/PtychoPINN/ptycho/train_pinn.py", line 90, in train_eval
E       model_instance, history = train(ptycho_dataset.train_data)
E                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E     File "/home/ollie/Documents/PtychoPINN/ptycho/train_pinn.py", line 86, in train
E       return model_instance, model.train(nepochs, train_data)
E                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E     File "/home/ollie/Documents/PtychoPINN/ptycho/model.py", line 593, in train
E       history=autoencoder.fit(
E               ^^^^^^^^^^^^^^^^
E     File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
E       raise e.with_traceback(filtered_tb) from None
E     File "/home/ollie/Documents/PtychoPINN/ptycho/custom_layers.py", line 159, in call
E       if total_patches > batch_threshold:
E          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: Exception encountered when calling ReassemblePatchesLayer.call().
E   
E   [1mUsing a symbolic `tf.Tensor` as a Python `bool` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.[0m
E   
E   Arguments received by ReassemblePatchesLayer.call():
E      inputs=['tf.Tensor(shape=(None, 64, 64, 1), dtype=complex64)', 'tf.Tensor(shape=(None, 1, 2, 1), dtype=complex64)']
----------------------------- Captured stdout call -----------------------------

Created test directories under: /tmp/tmpgfgp7m3l
--- Simulating data with nphotons=1000000.0 ---
Verifying metadata in simulated_data_nphotons_1000000.0_single.npz...
 Metadata verified: nphotons=1000000.0
--- Training model with data from nphotons=1000000.0 ---
Cleaned up test directory: /tmp/tmpgfgp7m3l
_ TestNphotonsMetadataIntegration.test_training_with_mismatched_config_warns_but_continues _

self = <tests.test_nphotons_metadata_integration.TestNphotonsMetadataIntegration testMethod=test_training_with_mismatched_config_warns_but_continues>

    def test_training_with_mismatched_config_warns_but_continues(self):
        """Test that training with mismatched nphotons generates warnings but continues."""
        print("--- Testing training with mismatched nphotons ---")
    
        # 1. Simulate data with one nphotons value
        data_nphotons = 1e4
        sim_file = self._simulate_with_nphotons(data_nphotons, "_train_mismatch")
    
        # 2. Train with different nphotons - should warn but not fail
        config_nphotons = 1e6
        training_output_dir = self.train_dir / "training_mismatch_test"
    
        train_command = [
            sys.executable,
            str(project_root / "scripts" / "training" / "train.py"),
            "--train_data_file", str(sim_file),
            "--test_data_file", str(sim_file),
            "--output_dir", str(training_output_dir),
            "--nepochs", "2",
            "--n_images", "50",
            "--gridsize", "1",
            "--nphotons", str(config_nphotons),  # Different from data
            "--quiet"
        ]
    
        result = subprocess.run(train_command, capture_output=True, text=True)
    
        # Training should succeed despite mismatch
>       self.assertEqual(result.returncode, 0,
                        f"Training should succeed with nphotons mismatch\n"
                        f"stdout: {result.stdout}\nstderr: {result.stderr}")
E       AssertionError: 1 != 0 : Training should succeed with nphotons mismatch
E       stdout: 2025-11-13 15:42:01,329 - INFO - Configuration setup complete
E       2025-11-13 15:42:01,329 - INFO - Final configuration: TrainingConfig(model=ModelConfig(N=64, gridsize=1, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', object_big=True, probe_big=True, probe_mask=False, pad_object=True, probe_scale=4.0, gaussian_smoothing_sigma=0.0), train_data_file=PosixPath('/tmp/tmpgctm8cuo/simulation/simulated_data_nphotons_10000.0_train_mismatch.npz'), test_data_file=PosixPath('/tmp/tmpgctm8cuo/simulation/simulated_data_nphotons_10000.0_train_mismatch.npz'), batch_size=16, nepochs=2, mae_weight=0.0, nll_weight=1.0, realspace_mae_weight=0.0, realspace_weight=0.0, nphotons=1000000.0, n_groups=50, n_images=50, n_subsample=None, subsample_seed=None, neighbor_count=4, enable_oversampling=False, neighbor_pool_size=None, positions_provided=True, probe_trainable=False, intensity_scale_trainable=True, output_dir=PosixPath('/tmp/tmpgctm8cuo/training/training_mismatch_test'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
E       2025-11-13 15:42:01,329 - INFO - Legacy mode: using 50 groups (gridsize=1)
E       2025-11-13 15:42:01,329 - INFO - Starting training with n_subsample=50, n_groups=50, stitching=disabled
E       2025-11-13 15:42:01,329 - INFO - Loading data from /tmp/tmpgctm8cuo/simulation/simulated_data_nphotons_10000.0_train_mismatch.npz with n_images=50, n_subsample=50
E       2025-11-13 15:42:01,334 - INFO - Independent sampling: subsampling 50 images from 100 total
E       2025-11-13 15:42:01,334 - INFO - Randomly subsampled 50 images
E       diff3d shape: (50, 64, 64)
E       probeGuess shape: (64, 64)
E       scan_index shape: (50,)
E       objectGuess shape: (227, 226)
E       xcoords shape: (50,)
E       ycoords shape: (50,)
E       xcoords_start shape: (50,)
E       ycoords_start shape: (50,)
E       2025-11-13 15:42:01,345 - INFO - Overriding nphotons from config (1.0e+06) with value from dataset metadata: 1.0e+04
E       2025-11-13 15:42:01,345 - INFO - Loading data from /tmp/tmpgctm8cuo/simulation/simulated_data_nphotons_10000.0_train_mismatch.npz with n_images=None, n_subsample=None
E       2025-11-13 15:42:01,349 - INFO - Using full dataset of 100 images
E       diff3d shape: (100, 64, 64)
E       probeGuess shape: (64, 64)
E       scan_index shape: (100,)
E       objectGuess shape: (227, 226)
E       xcoords shape: (100,)
E       ycoords shape: (100,)
E       xcoords_start shape: (100,)
E       ycoords_start shape: (100,)
E       2025-11-13 15:42:01,350 - INFO - Loaded test data from /tmp/tmpgctm8cuo/simulation/simulated_data_nphotons_10000.0_train_mismatch.npz
E       2025-11-13 15:42:01,350 - INFO - Backend dispatcher: params.cfg synchronized with TrainingConfig (backend=tensorflow)
E       2025-11-13 15:42:01,350 - INFO - Backend dispatcher: routing to TensorFlow workflow (ptycho.workflows.components)
E       2025-11-13 15:42:01,350 - INFO - [OVERSAMPLING DEBUG] generate_grouped_data called with: nsamples=50, n_points=50, C=1, K=4
E       2025-11-13 15:42:01,350 - INFO - [OVERSAMPLING DEBUG] Parameters: gridsize=1, N=64, sequential_sampling=False
E       2025-11-13 15:42:01,350 - INFO - [OVERSAMPLING DEBUG] Oversampling flags: enable_oversampling=False, neighbor_pool_size=None, effective_K=4
E       2025-11-13 15:42:01,350 - INFO - [OVERSAMPLING DEBUG] Oversampling check: nsamples > n_points = 50 > 50 = False
E       2025-11-13 15:42:01,350 - INFO - [OVERSAMPLING DEBUG] Oversampling check: C > 1 = 1 > 1 = False
E       2025-11-13 15:42:01,350 - INFO - [OVERSAMPLING DEBUG] needs_oversampling = False
E       DEBUG: nsamples: 50, gridsize: 1 (using efficient random sample-then-group strategy)
E       2025-11-13 15:42:01,350 - INFO - Using efficient random sampling strategy for gridsize=1
E       2025-11-13 15:42:01,350 - INFO - [OVERSAMPLING DEBUG] Taking efficient branch: standard sample-then-group
E       2025-11-13 15:42:01,350 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently called with: nsamples=50, K=4, C=1
E       2025-11-13 15:42:01,350 - INFO - Generating 50 groups efficiently from 50 points (K=4, C=1)
E       2025-11-13 15:42:01,350 - INFO - [OVERSAMPLING DEBUG] Standard case: using 50 groups from 50 points
E       2025-11-13 15:42:01,350 - INFO - [OVERSAMPLING DEBUG] Using all 50 points as seeds (no sampling needed)
E       2025-11-13 15:42:01,350 - INFO - Using all 50 points as seeds
E       2025-11-13 15:42:01,350 - INFO - Using seed indices directly for C=1 (gridsize=1) - no neighbor search needed
E       2025-11-13 15:42:01,350 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently completed: generated 50 groups
E       2025-11-13 15:42:01,350 - INFO - Successfully generated 50 groups with shape (50, 1)
E       2025-11-13 15:42:01,350 - INFO - [OVERSAMPLING DEBUG] Generated 50 groups in total
E       2025-11-13 15:42:01,350 - INFO - Generated 50 groups efficiently
E       INFO: 'Y' array not found. Generating ground truth patches from 'objectGuess' as a fallback.
E       neighbor-sampled diffraction shape (50, 64, 64, 1)
E       loader: using provided ground truth patches.
E       INFO: None
E       <PtychoDataContainer X=(50, 64, 64, 1) Y_I=(50, 64, 64, 1) Y_phi=(50, 64, 64, 1) norm_Y_I=() coords_nominal=(50, 1, 2, 1) coords_true=(50, 1, 2, 1) nn_indices=(50, 1) mean=24.500 global_offsets=(50, 1, 2, 1) mean=111.225 local_offsets=(50, 1, 2, 1) mean=0.000 probe=(64, 64) mean_amplitude=0.322>
E       2025-11-13 15:42:02,934 - INFO - [OVERSAMPLING DEBUG] generate_grouped_data called with: nsamples=50, n_points=100, C=1, K=4
E       2025-11-13 15:42:02,934 - INFO - [OVERSAMPLING DEBUG] Parameters: gridsize=1, N=64, sequential_sampling=False
E       2025-11-13 15:42:02,934 - INFO - [OVERSAMPLING DEBUG] Oversampling flags: enable_oversampling=False, neighbor_pool_size=None, effective_K=4
E       2025-11-13 15:42:02,934 - INFO - [OVERSAMPLING DEBUG] Oversampling check: nsamples > n_points = 50 > 100 = False
E       2025-11-13 15:42:02,934 - INFO - [OVERSAMPLING DEBUG] Oversampling check: C > 1 = 1 > 1 = False
E       2025-11-13 15:42:02,934 - INFO - [OVERSAMPLING DEBUG] needs_oversampling = False
E       DEBUG: nsamples: 50, gridsize: 1 (using efficient random sample-then-group strategy)
E       2025-11-13 15:42:02,934 - INFO - Using efficient random sampling strategy for gridsize=1
E       2025-11-13 15:42:02,934 - INFO - [OVERSAMPLING DEBUG] Taking efficient branch: standard sample-then-group
E       2025-11-13 15:42:02,934 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently called with: nsamples=50, K=4, C=1
E       2025-11-13 15:42:02,934 - INFO - Generating 50 groups efficiently from 100 points (K=4, C=1)
E       2025-11-13 15:42:02,934 - INFO - [OVERSAMPLING DEBUG] Standard case: using 50 groups from 100 points
E       2025-11-13 15:42:02,934 - INFO - [OVERSAMPLING DEBUG] Randomly sampled 50 seed points
E       2025-11-13 15:42:02,934 - INFO - Sampled 50 seed points from 100 total points
E       2025-11-13 15:42:02,934 - INFO - Using seed indices directly for C=1 (gridsize=1) - no neighbor search needed
E       2025-11-13 15:42:02,934 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently completed: generated 50 groups
E       2025-11-13 15:42:02,934 - INFO - Successfully generated 50 groups with shape (50, 1)
E       2025-11-13 15:42:02,934 - INFO - [OVERSAMPLING DEBUG] Generated 50 groups in total
E       2025-11-13 15:42:02,934 - INFO - Generated 50 groups efficiently
E       INFO: 'Y' array not found. Generating ground truth patches from 'objectGuess' as a fallback.
E       neighbor-sampled diffraction shape (50, 64, 64, 1)
E       loader: using provided ground truth patches.
E       INFO: None
E       <PtychoDataContainer X=(50, 64, 64, 1) Y_I=(50, 64, 64, 1) Y_phi=(50, 64, 64, 1) norm_Y_I=() coords_nominal=(50, 1, 2, 1) coords_true=(50, 1, 2, 1) nn_indices=(50, 1) mean=51.800 global_offsets=(50, 1, 2, 1) mean=107.510 local_offsets=(50, 1, 2, 1) mean=0.000 probe=(64, 64) mean_amplitude=0.322>
E       DEBUG: Setting probe to tf.Tensor(
E       [[[-0.00399459+8.81725922e-03j]
E         [ 0.00787074+1.04518672e-02j]
E         [-0.01909852+2.85197329e-03j]
E         ...
E         [-0.00430549-1.44718047e-02j]
E         [ 0.00670903+2.63100304e-02j]
E         [-0.01938361-6.67245360e-03j]]
E       
E        [[ 0.00921909+5.32836141e-03j]
E         [ 0.00422684-1.61591489e-02j]
E         [-0.00324812+1.62756350e-02j]
E         ...
E         [-0.00462818-2.68168072e-03j]
E         [ 0.01194528-6.69307355e-03j]
E         [ 0.0090712 -3.79238278e-03j]]
E       
E        [[ 0.0029482 -1.10371104e-02j]
E         [-0.01489174+5.26464824e-03j]
E         [-0.00083129+1.31990444e-02j]
E         ...
E         [ 0.00208769+1.68677475e-02j]
E         [ 0.00556216-3.03630810e-02j]
E         [ 0.01604221+1.45274084e-02j]]
E       
E        ...
E       
E        [[ 0.01654117+3.83263193e-02j]
E         [-0.01336122+7.02320365e-04j]
E         [-0.00380601-8.65837932e-03j]
E         ...
E         [ 0.01785212+2.66705058e-03j]
E         [-0.01791506+7.16461660e-03j]
E         [ 0.01052329-2.80616116e-02j]]
E       
E        [[-0.02582748-2.17778888e-05j]
E         [ 0.0098637 -3.15875164e-03j]
E         [-0.0156169 +2.47947779e-02j]
E         ...
E         [-0.01378679-1.89308310e-03j]
E         [ 0.00439025-1.43125653e-02j]
E         [ 0.02400579-1.19637549e-02j]]
E       
E        [[-0.00013256-1.32157737e-02j]
E         [ 0.03951043+1.84629709e-02j]
E         [-0.00848375+3.90608120e-03j]
E         ...
E         [-0.00414446+2.40438167e-04j]
E         [ 0.01858319+1.01260375e-02j]
E         [ 0.01299333+1.25071155e-02j]]], shape=(64, 64, 1), dtype=complex64) in params
E       DEBUG: Setting intensity_scale to 3.125 in params
E       DEBUG: Setting probe to tf.Tensor(
E       [[[-0.00399459+8.81725922e-03j]
E         [ 0.00787074+1.04518672e-02j]
E         [-0.01909852+2.85197329e-03j]
E         ...
E         [-0.00430549-1.44718047e-02j]
E         [ 0.00670903+2.63100304e-02j]
E         [-0.01938361-6.67245360e-03j]]
E       
E        [[ 0.00921909+5.32836141e-03j]
E         [ 0.00422684-1.61591489e-02j]
E         [-0.00324812+1.62756350e-02j]
E         ...
E         [-0.00462818-2.68168072e-03j]
E         [ 0.01194528-6.69307355e-03j]
E         [ 0.0090712 -3.79238278e-03j]]
E       
E        [[ 0.0029482 -1.10371104e-02j]
E         [-0.01489174+5.26464824e-03j]
E         [-0.00083129+1.31990444e-02j]
E         ...
E         [ 0.00208769+1.68677475e-02j]
E         [ 0.00556216-3.03630810e-02j]
E         [ 0.01604221+1.45274084e-02j]]
E       
E        ...
E       
E        [[ 0.01654117+3.83263193e-02j]
E         [-0.01336122+7.02320365e-04j]
E         [-0.00380601-8.65837932e-03j]
E         ...
E         [ 0.01785212+2.66705058e-03j]
E         [-0.01791506+7.16461660e-03j]
E         [ 0.01052329-2.80616116e-02j]]
E       
E        [[-0.02582748-2.17778888e-05j]
E         [ 0.0098637 -3.15875164e-03j]
E         [-0.0156169 +2.47947779e-02j]
E         ...
E         [-0.01378679-1.89308310e-03j]
E         [ 0.00439025-1.43125653e-02j]
E         [ 0.02400579-1.19637549e-02j]]
E       
E        [[-0.00013256-1.32157737e-02j]
E         [ 0.03951043+1.84629709e-02j]
E         [-0.00848375+3.90608120e-03j]
E         ...
E         [-0.00414446+2.40438167e-04j]
E         [ 0.01858319+1.01260375e-02j]
E         [ 0.01299333+1.25071155e-02j]]], shape=(64, 64, 1), dtype=complex64) in params
E       Model: "functional"
E       
E        Layer (type)         Output Shape          Param #  Connected to      
E       
E        input (InputLayer)   (None, 64, 64, 1)           0  -                 
E       
E        intensity_scaler     (None, 64, 64, 1)           0  input[0][0]       
E        (IntensityScaler)                                                     
E       
E        conv2d (Conv2D)      (None, 64, 64,            640  intensity_scaler 
E                             64)                                              
E       
E        conv2d_1 (Conv2D)    (None, 64, 64,         36,928  conv2d[0][0]      
E                             64)                                              
E       
E        max_pooling2d        (None, 32, 32,              0  conv2d_1[0][0]    
E        (MaxPooling2D)       64)                                              
E       
E        conv2d_2 (Conv2D)    (None, 32, 32,         73,856  max_pooling2d[0] 
E                             128)                                             
E       
E        conv2d_3 (Conv2D)    (None, 32, 32,        147,584  conv2d_2[0][0]    
E                             128)                                             
E       
E        max_pooling2d_1      (None, 16, 16,              0  conv2d_3[0][0]    
E        (MaxPooling2D)       128)                                             
E       
E        conv2d_4 (Conv2D)    (None, 16, 16,        295,168  max_pooling2d_1[ 
E                             256)                                             
E       
E        conv2d_5 (Conv2D)    (None, 16, 16,        590,080  conv2d_4[0][0]    
E                             256)                                             
E       
E        max_pooling2d_2      (None, 8, 8, 256)           0  conv2d_5[0][0]    
E        (MaxPooling2D)                                                        
E       
E        conv2d_8 (Conv2D)    (None, 8, 8, 128)     295,040  max_pooling2d_2[ 
E       
E        conv2d_16 (Conv2D)   (None, 8, 8, 128)     295,040  max_pooling2d_2[ 
E       
E        conv2d_9 (Conv2D)    (None, 8, 8, 128)     147,584  conv2d_8[0][0]    
E       
E        conv2d_17 (Conv2D)   (None, 8, 8, 128)     147,584  conv2d_16[0][0]   
E       
E        up_sampling2d        (None, 16, 16,              0  conv2d_9[0][0]    
E        (UpSampling2D)       128)                                             
E       
E        up_sampling2d_3      (None, 16, 16,              0  conv2d_17[0][0]   
E        (UpSampling2D)       128)                                             
E       
E        conv2d_10 (Conv2D)   (None, 16, 16,         73,792  up_sampling2d[0] 
E                             64)                                              
E       
E        conv2d_18 (Conv2D)   (None, 16, 16,         73,792  up_sampling2d_3[ 
E                             64)                                              
E       
E        conv2d_11 (Conv2D)   (None, 16, 16,         36,928  conv2d_10[0][0]   
E                             64)                                              
E       
E        conv2d_19 (Conv2D)   (None, 16, 16,         36,928  conv2d_18[0][0]   
E                             64)                                              
E       
E        up_sampling2d_1      (None, 32, 32,              0  conv2d_11[0][0]   
E        (UpSampling2D)       64)                                              
E       
E        up_sampling2d_4      (None, 32, 32,              0  conv2d_19[0][0]   
E        (UpSampling2D)       64)                                              
E       
E        get_item_1           (None, 32, 32, 4)           0  up_sampling2d_1[ 
E        (GetItem)                                                             
E       
E        get_item_3           (None, 32, 32, 4)           0  up_sampling2d_4[ 
E        (GetItem)                                                             
E       
E        conv2d_12 (Conv2D)   (None, 32, 32,          2,368  get_item_1[0][0]  
E                             64)                                              
E       
E        conv2d_20 (Conv2D)   (None, 32, 32,          2,368  get_item_3[0][0]  
E                             64)                                              
E       
E        conv2d_13 (Conv2D)   (None, 32, 32,         36,928  conv2d_12[0][0]   
E                             64)                                              
E       
E        conv2d_21 (Conv2D)   (None, 32, 32,         36,928  conv2d_20[0][0]   
E                             64)                                              
E       
E        up_sampling2d_2      (None, 64, 64,              0  conv2d_13[0][0]   
E        (UpSampling2D)       64)                                              
E       
E        up_sampling2d_5      (None, 64, 64,              0  conv2d_21[0][0]   
E        (UpSampling2D)       64)                                              
E       
E        get_item (GetItem)   (None, 32, 32,              0  up_sampling2d_1[ 
E                             60)                                              
E       
E        conv2d_7 (Conv2D)    (None, 64, 64, 1)         577  up_sampling2d_2[ 
E       
E        get_item_2           (None, 32, 32,              0  up_sampling2d_4[ 
E        (GetItem)            60)                                              
E       
E        conv2d_15 (Conv2D)   (None, 64, 64, 1)         577  up_sampling2d_5[ 
E       
E        conv2d_6 (Conv2D)    (None, 32, 32, 1)         541  get_item[0][0]    
E       
E        silu (Silu)          (None, 64, 64, 1)           0  conv2d_7[0][0]    
E       
E        conv2d_14 (Conv2D)   (None, 32, 32, 1)         541  get_item_2[0][0]  
E       
E        silu_1 (Silu)        (None, 64, 64, 1)           0  conv2d_15[0][0]   
E       
E        amp                  (None, 32, 32, 1)           0  conv2d_6[0][0]    
E        (ActivationLayer)                                                     
E       
E        center_mask_layer    (None, 64, 64, 1)           0  silu[0][0]        
E        (CenterMaskLayer)                                                     
E       
E        phi                  (None, 32, 32, 1)           0  conv2d_14[0][0]   
E        (ActivationLayer)                                                     
E       
E        center_mask_layer_1  (None, 64, 64, 1)           0  silu_1[0][0]      
E        (CenterMaskLayer)                                                     
E       
E        amp_padded           (None, 64, 64, 1)           0  amp[0][0]         
E        (ZeroPadding2D)                                                       
E       
E        multiply (Multiply)  (None, 64, 64, 1)           0  silu[0][0],       
E                                                            center_mask_laye 
E       
E        phase_padded         (None, 64, 64, 1)           0  phi[0][0]         
E        (ZeroPadding2D)                                                       
E       
E        multiply_1           (None, 64, 64, 1)           0  silu_1[0][0],     
E        (Multiply)                                          center_mask_laye 
E       
E        add (Add)            (None, 64, 64, 1)           0  amp_padded[0][0], 
E                                                            multiply[0][0]    
E       
E        add_1 (Add)          (None, 64, 64, 1)           0  phase_padded[0][ 
E                                                            multiply_1[0][0]  
E       
E        obj                  (None, 64, 64, 1)           0  add[0][0],        
E        (CombineComplexLay                                 add_1[0][0]       
E       
E        input_positions      (None, 1, 2, 1)             0  -                 
E        (InputLayer)                                                          
E       
E        padded_obj_2         (None, 74, 74, 1)           0  obj[0][0],        
E        (ReassemblePatches                                 input_positions[ 
E       
E        padded_objs_with_o  (None, 64, 64, 1)           0  padded_obj_2[0][ 
E        (ExtractPatchesPos                                 input_positions[ 
E       
E        probe_illumination   [(None, 64, 64,             0  padded_objs_with 
E        (ProbeIllumination)  1), (None, 64,                                   
E                             64, 1)]                                          
E       
E        pred_amplitude       [(None, 64, 64,             0  probe_illuminati 
E        (PadAndDiffractLay  1), (None, 64,                                   
E                             64, 1)]                                          
E       
E        pred_diff_channels   (None, 64, 64, 1)           0  pred_amplitude[0 
E        (FlatToChannelLaye                                                   
E       
E        intensity_scaler_i  (None, 64, 64, 1)           0  pred_diff_channe 
E        (IntensityScaler_i                                                   
E       
E        trimmed_obj          (None, 64, 64, 1)           0  padded_obj_2[0][ 
E        (TrimReconstructio                                                   
E       
E        pred_intensity       (None, 64, 64, 1)           0  intensity_scaler 
E        (SquareLayer)                                                         
E       
E        Total params: 2,331,772 (8.90 MB)
E        Trainable params: 2,331,772 (8.90 MB)
E        Non-trainable params: 0 (0.00 B)
E       None
E       Current Parameters:
E       --------------------
E       N: 64
E       amp_activation: sigmoid
E       backend: tensorflow
E       batch_size: 16
E       bigN: 64
E       big_gridsize: 10
E       data_source: generic
E       debug: True
E       default_probe_scale: 0.7
E       enable_oversampling: False
E       gaussian_smoothing_sigma: 0.0
E       gridsize: 1
E       h5_path: wts.h5
E       intensity_scale: 3.125
E       intensity_scale.trainable: True
E       label: 
E       mae_weight: 0.0
E       max_position_jitter: 10
E       model_type: pinn
E       n_filters_scale: 2
E       n_groups: 50
E       n_images: 50
E       neighbor_count: 4
E       nepochs: 2
E       nimgs_test: 3
E       nimgs_train: 9
E       nll_weight: 1.0
E       nphotons: 10000.0
E       npseed: 42
E       object.big: True
E       offset: 4
E       outer_offset_test: None
E       outer_offset_train: None
E       output_prefix: /tmp/tmpgctm8cuo/training/training_mismatch_test
E       pad_object: True
E       positions.provided: True
E       probe:
E         shape: (64, 64, 1)
E         mean: -0.016-0.002j
E         std: 0.666
E         min: -3.769-0.236j
E         max: 2.823-0.116j
E       probe.big: True
E       probe.mask: False
E       probe.trainable: False
E       probe_scale: 4.0
E       realspace_mae_weight: 0.0
E       realspace_weight: 0.0
E       sequential_sampling: False
E       set_phi: False
E       sim_jitter_scale: 0.0
E       size: 392
E       test_data_file_path: /tmp/tmpgctm8cuo/simulation/simulated_data_nphotons_10000.0_train_mismatch.npz
E       torch_loss_mode: poisson
E       train_data_file_path: /tmp/tmpgctm8cuo/simulation/simulated_data_nphotons_10000.0_train_mismatch.npz
E       tv_weight: 0.0
E       use_xla_translate: True
E       Epoch 1/2
E       2025-11-13 15:42:04,325 - ERROR - An error occurred during execution: Exception encountered when calling ReassemblePatchesLayer.call().
E       
E       [1mUsing a symbolic `tf.Tensor` as a Python `bool` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.[0m
E       
E       Arguments received by ReassemblePatchesLayer.call():
E          inputs=['tf.Tensor(shape=(None, 64, 64, 1), dtype=complex64)', 'tf.Tensor(shape=(None, 1, 2, 1), dtype=complex64)']
E       
E       stderr: 2025-11-13 15:41:58.573629: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
E       WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E       E0000 00:00:1763077318.585190 2351395 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E       E0000 00:00:1763077318.588844 2351395 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E       W0000 00:00:1763077318.598490 2351395 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E       W0000 00:00:1763077318.598500 2351395 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E       W0000 00:00:1763077318.598501 2351395 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E       W0000 00:00:1763077318.598503 2351395 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E       I0000 00:00:1763077321.472716 2351395 gpu_process_state.cc:208] Using CUDA malloc Async allocator for GPU: 0
E       I0000 00:00:1763077321.473929 2351395 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21277 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:04:00.0, compute capability: 8.6
E       WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E       I0000 00:00:1763077322.206347 2351395 service.cc:152] XLA service 0x31639970 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
E       I0000 00:00:1763077322.206367 2351395 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6
E       I0000 00:00:1763077322.242620 2351395 cuda_dnn.cc:529] Loaded cuDNN version 91002
E       I0000 00:00:1763077322.377754 2351395 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
E       I0000 00:00:1763077323.775323 2351395 cupti_tracer.cc:1026] Profiler found 1 GPUs
E       W0000 00:00:1763077323.791660 2351395 cupti_tracer.cc:1213] Fail to use per-thread activity buffer, cupti trace overhead may be big. CUPTI ERROR CODE:1
E       I0000 00:00:1763077323.792364 2351395 cupti_tracer.cc:1249] CUPTI activity buffer flushed
E       Traceback (most recent call last):
E         File "/home/ollie/Documents/PtychoPINN/scripts/training/train.py", line 440, in <module>
E           main()
E         File "/home/ollie/Documents/PtychoPINN/scripts/training/train.py", line 419, in main
E           recon_amp, recon_phase, results = run_cdi_example_with_backend(
E                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E         File "/home/ollie/Documents/PtychoPINN/ptycho/workflows/backend_selector.py", line 141, in run_cdi_example_with_backend
E           recon_amp, recon_phase, results = tf_components.run_cdi_example(
E                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E         File "/home/ollie/Documents/PtychoPINN/ptycho/workflows/components.py", line 846, in run_cdi_example
E           train_results = train_cdi_model(train_data, test_data, config)
E                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E         File "/home/ollie/Documents/PtychoPINN/ptycho/workflows/components.py", line 726, in train_cdi_model
E           results = train_pinn.train_eval(PtychoDataset(train_container, test_container))
E                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E         File "/home/ollie/Documents/PtychoPINN/ptycho/train_pinn.py", line 90, in train_eval
E           model_instance, history = train(ptycho_dataset.train_data)
E                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E         File "/home/ollie/Documents/PtychoPINN/ptycho/train_pinn.py", line 86, in train
E           return model_instance, model.train(nepochs, train_data)
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E         File "/home/ollie/Documents/PtychoPINN/ptycho/model.py", line 593, in train
E           history=autoencoder.fit(
E                   ^^^^^^^^^^^^^^^^
E         File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
E           raise e.with_traceback(filtered_tb) from None
E         File "/home/ollie/Documents/PtychoPINN/ptycho/custom_layers.py", line 159, in call
E           if total_patches > batch_threshold:
E              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: Exception encountered when calling ReassemblePatchesLayer.call().
E       
E       [1mUsing a symbolic `tf.Tensor` as a Python `bool` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.[0m
E       
E       Arguments received by ReassemblePatchesLayer.call():
E          inputs=['tf.Tensor(shape=(None, 64, 64, 1), dtype=complex64)', 'tf.Tensor(shape=(None, 1, 2, 1), dtype=complex64)']

tests/test_nphotons_metadata_integration.py:325: AssertionError
----------------------------- Captured stdout call -----------------------------

Created test directories under: /tmp/tmpgctm8cuo
--- Testing training with mismatched nphotons ---
--- Simulating data with nphotons=10000.0 ---
Cleaned up test directory: /tmp/tmpgctm8cuo
___________ TestLoadInferenceBundle.test_load_valid_model_directory ____________

self = <tests.test_workflow_components.TestLoadInferenceBundle testMethod=test_load_valid_model_directory>

    def test_load_valid_model_directory(self):
        """Test loading from a valid model directory."""
        # Create mock model archive
        self.create_mock_model_archive(include_diffraction_model=True)
    
        # Mock ModelManager.load_multiple_models
        with patch('ptycho.workflows.components.ModelManager.load_multiple_models') as mock_load:
            # Create a mock model
            mock_model = MagicMock(spec=tf.keras.Model)
            mock_load.return_value = {'diffraction_to_obj': mock_model}
    
            # Test loading
            model, config = load_inference_bundle(self.model_dir)
    
            # Verify results
            self.assertIsNotNone(model)
>           self.assertEqual(model, mock_model)
E           AssertionError: <DiffractionToObjectAdapter name=diffraction_to_obj, built=False> != <MagicMock spec='Model' id='124969198032272'>

tests/test_workflow_components.py:82: AssertionError
------------------------------ Captured log call -------------------------------
WARNING  absl:saving_api.py:83 You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`.
_______________ TestInferenceCLI.test_accelerator_flag_roundtrip _______________

self = <test_cli_inference_torch.TestInferenceCLI object at 0x71a9dc64ba10>
minimal_inference_args = ['--model_path', '/tmp/pytest-of-ollie/pytest-716/test_accelerator_flag_roundtri0/model', '--test_data', '/tmp/pytest-...i0/test.npz', '--output_dir', '/tmp/pytest-of-ollie/pytest-716/test_accelerator_flag_roundtri0/inference_outputs', ...]
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x71a8a87d9710>

    def test_accelerator_flag_roundtrip(self, minimal_inference_args, monkeypatch):
        """
        Test that accelerator flag is properly handled by _run_inference_and_reconstruct (DEVICE-MISMATCH-001).
    
        Expected behavior:
        - CLI parses --accelerator flag and builds execution_config
        - execution_config accelerator maps to device string ('cuda', 'mps', 'cpu')
        - _run_inference_and_reconstruct receives device and moves model to it
        - Model.to(device) and model.eval() are called inside helper
    
        Phase: R (DEVICE-MISMATCH-001 fix)
        Reference: input.md Do Now step 3, DEVICE-MISMATCH-001 finding
        """
        import numpy as np
        from unittest.mock import MagicMock, patch
    
        # Mock RawData with minimal required fields
        mock_raw_data = MagicMock()
        mock_raw_data.diff3d = np.random.rand(10, 64, 64).astype(np.float32)
        mock_raw_data.probeGuess = np.random.rand(64, 64).astype(np.complex64)
        mock_raw_data.xcoords = np.random.rand(10)
        mock_raw_data.ycoords = np.random.rand(10)
    
        # Mock model with .to() and .eval() tracking
        mock_model = MagicMock()
        device_calls = []
        eval_calls = []
    
        def track_to_call(device):
            device_calls.append(device)
            return mock_model
    
        def track_eval_call():
            eval_calls.append(True)
            return mock_model
    
        mock_model.to = MagicMock(side_effect=track_to_call)
        mock_model.eval = MagicMock(side_effect=track_eval_call)
        mock_model.forward_predict = MagicMock(
            return_value=MagicMock(
                cpu=MagicMock(
                    return_value=MagicMock(
                        numpy=MagicMock(return_value=np.random.rand(1, 1, 64, 64).astype(np.complex64))
                    )
                )
            )
        )
    
        # Import and call _run_inference_and_reconstruct directly
        from ptycho_torch.inference import _run_inference_and_reconstruct
        from ptycho.config.config import InferenceConfig, ModelConfig, PyTorchExecutionConfig
    
        config = InferenceConfig(
            model=ModelConfig(N=64, gridsize=1),
            model_path=Path('outputs/test/bundle.zip'),
            test_data_file=Path('test.npz'),
            backend='pytorch',
            output_dir=Path('outputs/inference'),
            n_groups=10
        )
    
        execution_config = PyTorchExecutionConfig(
            accelerator='cuda',  # Request CUDA device
            num_workers=0,
            inference_batch_size=None
        )
    
        # Call helper with 'cuda' device
>       _run_inference_and_reconstruct(
            mock_model, mock_raw_data, config, execution_config, 'cuda', quiet=True
        )

tests/torch/test_cli_inference_torch.py:270: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

model = <MagicMock id='124969202723664'>
raw_data = <MagicMock id='124969195242448'>
config = InferenceConfig(model=ModelConfig(N=64, gridsize=1, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', ob...oversampling=False, neighbor_pool_size=None, debug=False, output_dir=PosixPath('outputs/inference'), backend='pytorch')
execution_config = PyTorchExecutionConfig(accelerator='cuda', strategy='auto', deterministic=True, gradient_clip_val=None, accum_steps=1,...nt_mode='min', early_stop_patience=100, logger_backend='csv', inference_batch_size=None, middle_trim=0, pad_eval=False)
device = 'cuda', quiet = True, debug_dump_dir = None, debug_patch_limit = 16
intensity_scale_override = None

    def _run_inference_and_reconstruct(
        model,
        raw_data,
        config,
        execution_config,
        device,
        quiet=False,
        debug_dump_dir=None,
        debug_patch_limit=16,
        intensity_scale_override: Optional[float] = None,
    ):
        """
        Extract inference logic into testable helper function (Phase D.C C3).
    
        Args:
            model: Loaded Lightning module (should be in eval mode)
            raw_data: RawData instance with test data
            config: TFInferenceConfig with n_groups, etc.
            execution_config: PyTorchExecutionConfig with device, batch size, etc.
            device: Torch device string ('cpu', 'cuda', 'mps')
            quiet: Suppress progress output (default: False)
    
        Returns:
            Tuple of (amplitude, phase) numpy arrays
    
        Notes:
            - Wraps existing simplified inference logic (lines 563-641)
            - Enforces DTYPE-001 (float32 for diffraction, complex64 for probe)
            - Handles shape permutations (H,W,N  N,H,W)
            - Averages across batch for single reconstruction
            - DEVICE-MISMATCH-001: Ensures model is on the correct device
            - Optional debug instrumentation dumps patch/canvas stats when debug_dump_dir is set
        """
        import torch
        import numpy as np
    
        # DEVICE-MISMATCH-001 fix: Ensure model is on the requested device and in eval mode
        model.to(device)
        model.eval()
    
        # DTYPE ENFORCEMENT (Phase D1d): Cast to float32 per DATA-001
        diffraction = torch.from_numpy(raw_data.diff3d).to(device, dtype=torch.float32)
        probe = torch.from_numpy(raw_data.probeGuess).to(device, dtype=torch.complex64)
    
        # Handle different diffraction shapes (H, W, n) vs (n, H, W)
        # Auto-detect legacy (H, W, n) format where the last dim (n) is the largest
        if diffraction.ndim == 3 and diffraction.shape[-1] > max(diffraction.shape[0], diffraction.shape[1]):
            # Transpose from (H, W, n) to (n, H, W)
            diffraction = diffraction.permute(2, 0, 1)
    
        # Limit to n_groups
        diffraction = diffraction[:config.n_groups]
    
        # Add channel dimension if needed: (n, H, W) -> (n, 1, H, W)
        if diffraction.ndim == 3:
            diffraction = diffraction.unsqueeze(1)
    
        # Ensure probe is complex64
        if not torch.is_complex(probe):
            probe = probe.to(torch.complex64)
    
        # Add batch dimension to probe if needed
        if probe.ndim == 2:
            probe = probe.unsqueeze(0).unsqueeze(0).unsqueeze(0)  # (1, 1, 1, H, W)
    
        # Prepare positions (API requires it), real offsets computed for reassembly below
        batch_size = diffraction.shape[0]
        N = diffraction.shape[-1]
        positions = torch.zeros((batch_size, 1, 1, 2), device=device)
    
        # Prepare scaling factors (match training normalization)
        from ptycho_torch import helper as hh
        from ptycho_torch.config_params import DataConfig as PTDataConfig
    
        data_cfg_norm = PTDataConfig(N=int(N), grid_size=(1, 1))
        rms_scale = hh.get_rms_scaling_factor(diffraction.squeeze(1), data_cfg_norm)
        physics_scale = hh.get_physics_scaling_factor(diffraction.squeeze(1), data_cfg_norm)
        if intensity_scale_override is not None:
            override_value = float(intensity_scale_override)
            logger.info("Using bundle intensity_scale override: %.6f", override_value)
            rms_scale = torch.ones_like(rms_scale) * override_value
            physics_scale = torch.ones_like(physics_scale) * override_value
        if not isinstance(rms_scale, torch.Tensor):
            rms_scale = torch.from_numpy(rms_scale)
        if not isinstance(physics_scale, torch.Tensor):
            physics_scale = torch.from_numpy(physics_scale)
        rms_scale = rms_scale.to(device=device, dtype=torch.float32)
        physics_scale = physics_scale.to(device=device, dtype=torch.float32)
        if rms_scale.ndim == 1:
            rms_scale = rms_scale.view(-1, 1, 1, 1)
        if physics_scale.ndim == 1:
            physics_scale = physics_scale.view(-1, 1, 1, 1)
    
        physics_weight = 1.0 if getattr(model, 'torch_loss_mode', 'poisson') == 'poisson' else 0.0
        input_scale_factor = rms_scale
        output_scale_factor = (1.0 - physics_weight) * rms_scale + physics_weight * physics_scale
    
        if not quiet:
            print(f"Running inference on {batch_size} images...")
    
        scaling_debug = bool(debug_dump_dir)
        if scaling_debug:
            def _mean_abs(tensor):
                return float(torch.mean(torch.abs(tensor.detach())).cpu())
    
            def _mean_val(tensor):
                return float(torch.mean(tensor.detach()).cpu())
    
            msg = (
                "Torch scaling debug (inference pre-forward): mean|input|="
                f"{_mean_abs(diffraction):.6f} mean_input_scale={_mean_val(input_scale_factor):.6f} "
                f"mean_physics_scale={_mean_val(physics_scale):.6f} physics_weight={float(physics_weight):.3f}"
            )
            logger.info(msg)
            print(msg)
    
        # Forward pass through model to get per-patch complex predictions
        with torch.no_grad():
            patch_complex = model.forward_predict(
                diffraction,
                positions,
                probe,
                input_scale_factor
            )
    
        if scaling_debug:
            def _mean_abs(tensor):
                return float(torch.mean(torch.abs(tensor.detach())).cpu())
    
            def _mean_val(tensor):
                return float(torch.mean(tensor.detach()).cpu())
    
            msg = (
                "Torch scaling debug (inference post-forward): mean|patch|="
                f"{_mean_abs(patch_complex):.6f} mean_output_scale={_mean_val(output_scale_factor):.6f}"
            )
            logger.info(msg)
            print(msg)
    
        # Compute pixel offsets relative to center-of-mass (B, 1, 1, 2)
        x = torch.from_numpy(raw_data.xcoords[:batch_size]).to(device=device, dtype=torch.float32)
        y = torch.from_numpy(raw_data.ycoords[:batch_size]).to(device=device, dtype=torch.float32)
        dx = x - torch.mean(x)
        dy = y - torch.mean(y)
        offsets = torch.stack([dx, dy], dim=-1).view(batch_size, 1, 1, 2)
    
        # Position-aware reassembly using torch helper to produce stitched canvas
        from ptycho_torch.config_params import DataConfig, ModelConfig
        from ptycho_torch import helper as hh
    
        # Minimal configs required for padding and translation
        N = patch_complex.shape[-1]
        data_cfg = DataConfig(N=int(N), grid_size=(1, 1))
        model_cfg = ModelConfig()
        # Ensure channel consistency for reassembly (C_forward must match predicted channels)
        model_cfg.C_forward = int(patch_complex.shape[1])
    
        # Compute dynamic canvas size to avoid clipping: M >= N + 2*max(|dx|, |dy|)
        max_shift = torch.max(torch.stack([dx.abs(), dy.abs()], dim=0)).item()
>       M = int(np.ceil(N + 2 * max_shift))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: only length-1 arrays can be converted to Python scalars

ptycho_torch/inference.py:585: TypeError
_ TestWorkflowsComponentsScaffold.test_run_cdi_example_calls_update_legacy_dict _

self = <test_workflows_components.TestWorkflowsComponentsScaffold object at 0x71a8a9180390>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x71a86413c310>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'sigmoid', 'backend': 'tensorflow', 'batch_size': 16, ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')

    def test_run_cdi_example_calls_update_legacy_dict(
        self,
        monkeypatch,
        params_cfg_snapshot,
        minimal_training_config
    ):
        """
        CRITICAL PARITY TEST: run_cdi_example_torch must call update_legacy_dict.
    
        Requirement: specs/ptychodus_api_spec.md:187 mandates that PyTorch entry
        points must synchronize params.cfg via update_legacy_dict() to prevent
        silent CONFIG-001 violations (shape mismatch errors from empty params.cfg).
    
        Red-phase contract:
        - Entry signature: run_cdi_example_torch(train_data, test_data, config, ...)
        - MUST call ptycho.config.config.update_legacy_dict(params.cfg, config)
        - Stub implementation may raise NotImplementedError for paths Phase D2.B/C fill
    
        Test mechanism:
        - Use monkeypatch to spy on update_legacy_dict calls
        - Pass minimal dummy data (no actual training execution required)
        - Assert update_legacy_dict was invoked with correct params.cfg + config args
        """
        # Import the module under test
        # This import must succeed even when torch unavailable (torch-optional)
        from ptycho_torch.workflows import components as torch_components
        from ptycho.config.config import update_legacy_dict
        from ptycho.raw_data import RawData
    
        # Spy flag to track update_legacy_dict invocation
        update_legacy_dict_called = {"called": False, "args": None}
    
        def mock_update_legacy_dict(cfg_dict, config_obj):
            """Spy that records invocation and delegates to real function."""
            update_legacy_dict_called["called"] = True
            update_legacy_dict_called["args"] = (cfg_dict, config_obj)
            # Call the real function to populate params.cfg for validation
            update_legacy_dict(cfg_dict, config_obj)
    
        # Patch update_legacy_dict with spy
        monkeypatch.setattr(
            "ptycho.config.config.update_legacy_dict",
            mock_update_legacy_dict
        )
    
        # Create minimal dummy train_data (RawData-compatible stub)
        # For scaffold test, we don't need valid NPZ data  just RawData structure
        dummy_coords = np.array([0.0, 1.0, 2.0])
        dummy_diff = np.random.rand(3, 64, 64).astype(np.float32)
        dummy_probe = np.ones((64, 64), dtype=np.complex64)
        dummy_scan_index = np.array([0, 1, 2], dtype=int)
    
        train_data = RawData(
            xcoords=dummy_coords,
            ycoords=dummy_coords,
            xcoords_start=dummy_coords,
            ycoords_start=dummy_coords,
            diff3d=dummy_diff,
            probeGuess=dummy_probe,
            scan_index=dummy_scan_index,
        )
    
        # Attempt to call run_cdi_example_torch
        # Phase D2.A: expects NotImplementedError (scaffold only)  COMPLETED
        # Phase D2.B/C: IMPLEMENTED  now returns results tuple
        # Test validates update_legacy_dict was called, but doesn't fully exercise training
        # (training path tested separately in TestWorkflowsComponentsTraining)
    
        # Monkeypatch train_cdi_model_torch to prevent full training execution in this test
        def mock_train_cdi_model_torch(train_data, test_data, config):
            """Minimal stub to prevent full training in scaffold test."""
            return {"history": {"train_loss": [0.5]}, "train_container": None, "test_container": None}
    
        monkeypatch.setattr(
            "ptycho_torch.workflows.components.train_cdi_model_torch",
            mock_train_cdi_model_torch
        )
    
        # Call should now succeed (Phase D2.C implemented)
>       recon_amp, recon_phase, results = torch_components.run_cdi_example_torch(
            train_data=train_data,
            test_data=None,  # Optional
            config=minimal_training_config,
            flip_x=False,
            flip_y=False,
            transpose=False,
            M=20,
            do_stitching=False,
        )

tests/torch/test_workflows_components.py:156: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

train_data = <ptycho.raw_data.RawData object at 0x71a8a9144ed0>
test_data = None
config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
flip_x = False, flip_y = False, transpose = False, M = 20, do_stitching = False
execution_config = None

    def run_cdi_example_torch(
        train_data: Union[RawData, 'RawDataTorch', 'PtychoDataContainerTorch'],
        test_data: Optional[Union[RawData, 'RawDataTorch', 'PtychoDataContainerTorch']],
        config: TrainingConfig,
        flip_x: bool = False,
        flip_y: bool = False,
        transpose: bool = False,
        M: int = 20,
        do_stitching: bool = False,
        execution_config: Optional[Any] = None
    ) -> Tuple[Optional[Any], Optional[Any], Dict[str, Any]]:
        """
        Run the main CDI example execution flow using PyTorch backend.
    
        This function provides API parity with ptycho.workflows.components.run_cdi_example,
        enabling transparent backend selection from Ptychodus per specs/ptychodus_api_spec.md 4.5.
    
        CRITICAL: This function MUST call update_legacy_dict(params.cfg, config) before
        delegating to core modules to prevent CONFIG-001 violations (empty params.cfg
        causing silent shape mismatches downstream).
    
        Args:
            train_data: Training data (RawData, RawDataTorch, or PtychoDataContainerTorch)
            test_data: Optional test data (same type constraints as train_data)
            config: TrainingConfig instance (TensorFlow dataclass, translated via config_bridge)
            flip_x: Whether to flip the x coordinates during reconstruction
            flip_y: Whether to flip the y coordinates during reconstruction
            transpose: Whether to transpose the image by swapping dimensions
            M: Parameter for reassemble_position function (default: 20)
            do_stitching: Whether to perform image stitching after training
            execution_config: Optional PyTorchExecutionConfig for runtime control (accelerator,
                             num_workers, learning_rate, scheduler, logger, checkpointing).
                             See CONFIG-002, CONFIG-LOGGER-001.
    
        Returns:
            Tuple containing:
            - reconstructed amplitude (or None if stitching disabled)
            - reconstructed phase (or None if stitching disabled)
            - results dictionary (training history, containers, metrics)
    
        Raises:
            NotImplementedError: Phase D2.B/C not yet implemented (scaffold only)
    
        Phase D2.A Scaffold Status:
            - Entry signature:  COMPLETE (matches TensorFlow)
            - update_legacy_dict call:  COMPLETE (CONFIG-001 compliance)
            - Placeholder logic:  COMPLETE (raises NotImplementedError)
            - Torch-optional:  COMPLETE (importable without torch)
    
        Phase D2.B/C TODO:
            - Implement train_cdi_model_torch delegation (Lightning trainer orchestration)
            - Implement reassemble_cdi_image_torch (optional stitching path)
            - Add MLflow disable flag handling
            - Validate deterministic seeds from config
    
        Example (Post D2.B/C):
            >>> from ptycho_torch.workflows.components import run_cdi_example_torch
            >>> from ptycho.config.config import TrainingConfig, ModelConfig
            >>> from ptycho.raw_data import RawData
            >>>
            >>> # Load data
            >>> train_data = RawData.from_file("train.npz")
            >>> config = TrainingConfig(model=ModelConfig(N=64), ...)
            >>>
            >>> # Execute PyTorch pipeline
            >>> amp, phase, results = run_cdi_example_torch(
            ...     train_data, None, config, do_stitching=False
            ... )
        """
        # CRITICAL: Update params.cfg before delegating (CONFIG-001 compliance)
        # This ensures legacy modules invoked downstream observe correct configuration state
        ptycho_config.update_legacy_dict(params.cfg, config)
        logger.info("PyTorch workflow: params.cfg synchronized with TrainingConfig")
    
        # Step 1: Train the model (Phase D2.B  delegates to Lightning trainer stub)
        logger.info("Invoking PyTorch training orchestration via train_cdi_model_torch")
        # Note: train_cdi_model_torch will need to be updated to accept execution_config
        # For now, we pass it as a keyword argument for forward compatibility
>       train_results = train_cdi_model_torch(train_data, test_data, config, execution_config=execution_config)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: TestWorkflowsComponentsScaffold.test_run_cdi_example_calls_update_legacy_dict.<locals>.mock_train_cdi_model_torch() got an unexpected keyword argument 'execution_config'

ptycho_torch/workflows/components.py:161: TypeError
----------------------------- Captured stdout call -----------------------------
diff3d shape: (3, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (3,)
xcoords shape: (3,)
ycoords shape: (3,)
xcoords_start shape: (3,)
ycoords_start shape: (3,)
_ TestWorkflowsComponentsTraining.test_train_cdi_model_torch_invokes_lightning _

self = <test_workflows_components.TestWorkflowsComponentsTraining object at 0x71a8a9180d50>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x71a8a87fd090>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'sigmoid', 'backend': 'tensorflow', 'batch_size': 16, ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=1, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x71a8642c1b10>

    def test_train_cdi_model_torch_invokes_lightning(
        self,
        monkeypatch,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data
    ):
        """
        CRITICAL TRAINING PATH TEST: train_cdi_model_torch must delegate to Lightning.
    
        Requirement: Phase D2.B must implement training orchestration following the
        pattern in plans/active/INTEGRATE-PYTORCH-001/reports/2025-10-17T093500Z/
        phase_d2_training_analysis.md.
    
        Red-phase contract:
        - Entry signature: train_cdi_model_torch(train_data, test_data, config)
        - MUST call _ensure_container(data, config) for train/test inputs
        - MUST delegate to Lightning trainer with normalized config
        - MUST return dict with keys: history, train_container, test_container
        - Stub implementation may raise NotImplementedError initially
    
        Test mechanism:
        - Use monkeypatch to spy on _ensure_container and Lightning orchestration calls
        - Pass minimal RawData (no actual training execution required)
        - Assert expected orchestration order without running full training
        """
        # Import the module under test
        from ptycho_torch.workflows import components as torch_components
    
        # Spy flags to track internal calls
        ensure_container_calls = []
        lightning_trainer_called = {"called": False, "config": None}
    
        def mock_ensure_container(data, config):
            """Spy that records _ensure_container invocations."""
            ensure_container_calls.append({
                "data": data,
                "config": config
            })
            # Return a sentinel PtychoDataContainerTorch-like object
            # In Phase D2.B implementation, this would be a real container
            return {"X": np.ones((2, 64, 64)), "Y": np.ones((2, 64, 64), dtype=np.complex64)}
    
        def mock_lightning_orchestrator(train_container, test_container, config):
            """Spy that records Lightning trainer invocation."""
            lightning_trainer_called["called"] = True
            lightning_trainer_called["config"] = config
            # Return minimal training results dict
            return {
                "history": {"train_loss": [0.5, 0.3], "val_loss": [0.6, 0.4]},
                "train_container": train_container,
                "test_container": test_container,
            }
    
        # Patch internal helpers (Phase D2.B implemented)
        monkeypatch.setattr(
            "ptycho_torch.workflows.components._ensure_container",
            mock_ensure_container
        )
        monkeypatch.setattr(
            "ptycho_torch.workflows.components._train_with_lightning",
            mock_lightning_orchestrator
        )
    
        # Call train_cdi_model_torch (Phase D2.B green phase)
>       results = torch_components.train_cdi_model_torch(
            train_data=dummy_raw_data,
            test_data=None,  # Optional
            config=minimal_training_config
        )

tests/torch/test_workflows_components.py:318: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

train_data = <ptycho.raw_data.RawData object at 0x71a8642c1b10>
test_data = None
config = TrainingConfig(model=ModelConfig(N=64, gridsize=1, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
execution_config = None

    def train_cdi_model_torch(
        train_data: Union[RawData, 'RawDataTorch', 'PtychoDataContainerTorch'],
        test_data: Optional[Union[RawData, 'RawDataTorch', 'PtychoDataContainerTorch']],
        config: TrainingConfig,
        execution_config: Optional[Any] = None
    ) -> Dict[str, Any]:
        """
        Train the CDI model using PyTorch Lightning backend.
    
        This function provides API parity with ptycho.workflows.components.train_cdi_model,
        orchestrating data preparation, probe initialization, and Lightning trainer execution.
    
        Args:
            train_data: Training data (RawData, RawDataTorch, or PtychoDataContainerTorch)
            test_data: Optional test data for validation
            config: TrainingConfig instance (TensorFlow dataclass)
            execution_config: Optional PyTorchExecutionConfig for runtime control
    
        Returns:
            Dict[str, Any]: Results dictionary containing:
            - 'history': Training history (losses, metrics)
            - 'train_container': PtychoDataContainerTorch for training data
            - 'test_container': Optional PtychoDataContainerTorch for test data
            - Additional outputs from Lightning trainer
    
        Raises:
            ImportError: If Phase C adapters not available
            TypeError: If input data types are invalid
    
        Phase D2.B Status:
            - Entry signature:  COMPLETE (matches TensorFlow)
            - _ensure_container helper:  COMPLETE (normalizes inputs via Phase C adapters)
            - Lightning orchestration:  STUB (returns minimal dict, full impl pending)
            - Torch-optional:  COMPLETE (importable without torch)
    
        Example:
            >>> config = TrainingConfig(model=ModelConfig(N=64), nepochs=10, ...)
            >>> results = train_cdi_model_torch(train_data, test_data, config)
            >>> print(results['history']['train_loss'][-1])
        """
        # Step 1: Normalize train_data to PtychoDataContainerTorch
        logger.info("Normalizing training data via _ensure_container")
        train_container = _ensure_container(train_data, config)
    
        # Step 2: Normalize test_data if provided
        test_container = None
        if test_data is not None:
            logger.info("Normalizing test data via _ensure_container")
            test_container = _ensure_container(test_data, config)
    
        # Step 3: Initialize probe (TODO: implement probe handling for PyTorch)
        # TensorFlow baseline: probe.set_probe_guess(None, train_container.probe)
        # For Phase D2.B stub, skip probe initialization
        logger.debug("Probe initialization deferred to full Lightning implementation")
    
        # Step 4: Delegate to Lightning trainer
        logger.info("Delegating to Lightning trainer via _train_with_lightning")
>       results = _train_with_lightning(train_container, test_container, config, execution_config=execution_config)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: TestWorkflowsComponentsTraining.test_train_cdi_model_torch_invokes_lightning.<locals>.mock_lightning_orchestrator() got an unexpected keyword argument 'execution_config'

ptycho_torch/workflows/components.py:1137: TypeError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (10, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (10,)
xcoords shape: (10,)
ycoords shape: (10,)
xcoords_start shape: (10,)
ycoords_start shape: (10,)
_______ TestWorkflowsComponentsRun.test_run_cdi_example_invokes_training _______

self = <test_workflows_components.TestWorkflowsComponentsRun object at 0x71a8a9183390>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x71a8642b5cd0>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'sigmoid', 'backend': 'tensorflow', 'batch_size': 16, ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x71a8a8670650>

    def test_run_cdi_example_invokes_training(
        self,
        monkeypatch,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data
    ):
        """
        CRITICAL PARITY TEST: run_cdi_example_torch must invoke training orchestration.
    
        Requirement: Phase D2.C must implement full workflow orchestration following
        TensorFlow baseline ptycho/workflows/components.py:676-723 and mirroring
        the reconstructor lifecycle per specs/ptychodus_api_spec.md 4.5.
    
        Red-phase contract:
        - Entry signature: run_cdi_example_torch(train_data, test_data, config, do_stitching=False, ...)
        - MUST call train_cdi_model_torch(train_data, test_data, config) first
        - When do_stitching=False: return (None, None, results_dict)
        - When do_stitching=True + test_data: invoke reassemble helper, return (amp, phase, results)
        - results dict MUST contain keys from training (history, containers)
    
        Test mechanism:
        - Use monkeypatch to spy on train_cdi_model_torch call
        - Pass minimal RawData + do_stitching=False (no inference path required)
        - Assert train_cdi_model_torch was invoked with correct args
        - Validate return signature matches TensorFlow baseline
        """
        # Import the module under test
        from ptycho_torch.workflows import components as torch_components
    
        # Spy flag to track train_cdi_model_torch invocation
        train_cdi_model_torch_called = {"called": False, "args": None}
    
        def mock_train_cdi_model_torch(train_data, test_data, config):
            """Spy that records train_cdi_model_torch invocation."""
            train_cdi_model_torch_called["called"] = True
            train_cdi_model_torch_called["args"] = (train_data, test_data, config)
            # Return minimal training results dict
            return {
                "history": {"train_loss": [0.5, 0.3]},
                "train_container": {"sentinel": "train"},
                "test_container": None,
            }
    
        # Patch train_cdi_model_torch
        monkeypatch.setattr(
            "ptycho_torch.workflows.components.train_cdi_model_torch",
            mock_train_cdi_model_torch
        )
    
        # Call run_cdi_example_torch with do_stitching=False (Phase D2.C red phase)
>       recon_amp, recon_phase, results = torch_components.run_cdi_example_torch(
            train_data=dummy_raw_data,
            test_data=None,
            config=minimal_training_config,
            flip_x=False,
            flip_y=False,
            transpose=False,
            M=20,
            do_stitching=False,
        )

tests/torch/test_workflows_components.py:925: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

train_data = <ptycho.raw_data.RawData object at 0x71a8a8670650>
test_data = None
config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
flip_x = False, flip_y = False, transpose = False, M = 20, do_stitching = False
execution_config = None

    def run_cdi_example_torch(
        train_data: Union[RawData, 'RawDataTorch', 'PtychoDataContainerTorch'],
        test_data: Optional[Union[RawData, 'RawDataTorch', 'PtychoDataContainerTorch']],
        config: TrainingConfig,
        flip_x: bool = False,
        flip_y: bool = False,
        transpose: bool = False,
        M: int = 20,
        do_stitching: bool = False,
        execution_config: Optional[Any] = None
    ) -> Tuple[Optional[Any], Optional[Any], Dict[str, Any]]:
        """
        Run the main CDI example execution flow using PyTorch backend.
    
        This function provides API parity with ptycho.workflows.components.run_cdi_example,
        enabling transparent backend selection from Ptychodus per specs/ptychodus_api_spec.md 4.5.
    
        CRITICAL: This function MUST call update_legacy_dict(params.cfg, config) before
        delegating to core modules to prevent CONFIG-001 violations (empty params.cfg
        causing silent shape mismatches downstream).
    
        Args:
            train_data: Training data (RawData, RawDataTorch, or PtychoDataContainerTorch)
            test_data: Optional test data (same type constraints as train_data)
            config: TrainingConfig instance (TensorFlow dataclass, translated via config_bridge)
            flip_x: Whether to flip the x coordinates during reconstruction
            flip_y: Whether to flip the y coordinates during reconstruction
            transpose: Whether to transpose the image by swapping dimensions
            M: Parameter for reassemble_position function (default: 20)
            do_stitching: Whether to perform image stitching after training
            execution_config: Optional PyTorchExecutionConfig for runtime control (accelerator,
                             num_workers, learning_rate, scheduler, logger, checkpointing).
                             See CONFIG-002, CONFIG-LOGGER-001.
    
        Returns:
            Tuple containing:
            - reconstructed amplitude (or None if stitching disabled)
            - reconstructed phase (or None if stitching disabled)
            - results dictionary (training history, containers, metrics)
    
        Raises:
            NotImplementedError: Phase D2.B/C not yet implemented (scaffold only)
    
        Phase D2.A Scaffold Status:
            - Entry signature:  COMPLETE (matches TensorFlow)
            - update_legacy_dict call:  COMPLETE (CONFIG-001 compliance)
            - Placeholder logic:  COMPLETE (raises NotImplementedError)
            - Torch-optional:  COMPLETE (importable without torch)
    
        Phase D2.B/C TODO:
            - Implement train_cdi_model_torch delegation (Lightning trainer orchestration)
            - Implement reassemble_cdi_image_torch (optional stitching path)
            - Add MLflow disable flag handling
            - Validate deterministic seeds from config
    
        Example (Post D2.B/C):
            >>> from ptycho_torch.workflows.components import run_cdi_example_torch
            >>> from ptycho.config.config import TrainingConfig, ModelConfig
            >>> from ptycho.raw_data import RawData
            >>>
            >>> # Load data
            >>> train_data = RawData.from_file("train.npz")
            >>> config = TrainingConfig(model=ModelConfig(N=64), ...)
            >>>
            >>> # Execute PyTorch pipeline
            >>> amp, phase, results = run_cdi_example_torch(
            ...     train_data, None, config, do_stitching=False
            ... )
        """
        # CRITICAL: Update params.cfg before delegating (CONFIG-001 compliance)
        # This ensures legacy modules invoked downstream observe correct configuration state
        ptycho_config.update_legacy_dict(params.cfg, config)
        logger.info("PyTorch workflow: params.cfg synchronized with TrainingConfig")
    
        # Step 1: Train the model (Phase D2.B  delegates to Lightning trainer stub)
        logger.info("Invoking PyTorch training orchestration via train_cdi_model_torch")
        # Note: train_cdi_model_torch will need to be updated to accept execution_config
        # For now, we pass it as a keyword argument for forward compatibility
>       train_results = train_cdi_model_torch(train_data, test_data, config, execution_config=execution_config)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: TestWorkflowsComponentsRun.test_run_cdi_example_invokes_training.<locals>.mock_train_cdi_model_torch() got an unexpected keyword argument 'execution_config'

ptycho_torch/workflows/components.py:161: TypeError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (10, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (10,)
xcoords shape: (10,)
ycoords shape: (10,)
xcoords_start shape: (10,)
ycoords_start shape: (10,)
_______ TestWorkflowsComponentsRun.test_run_cdi_example_persists_models ________

self = <test_workflows_components.TestWorkflowsComponentsRun object at 0x71a8a9183a90>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x71a8a858b450>
tmp_path = PosixPath('/tmp/pytest-of-ollie/pytest-716/test_run_cdi_example_persists_0')
params_cfg_snapshot = {'N': 64, 'amp_activation': 'sigmoid', 'backend': 'tensorflow', 'batch_size': 16, ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x71a8a858bb10>

    def test_run_cdi_example_persists_models(
        self,
        monkeypatch,
        tmp_path,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data
    ):
        """
        REGRESSION TEST: run_cdi_example_torch must persist models when config.output_dir set.
    
        Requirement: Phase D4.B2  validate PyTorch orchestration maintains persistence
        parity with TensorFlow baseline per specs/ptychodus_api_spec.md:4.6.
    
        TensorFlow baseline (ptycho/workflows/components.py:709-723):
        - When config.output_dir is provided, calls save_model() or ModelManager.save()
        - Produces wts.h5.zip archive in output_dir with dual-model bundle
        - Persistence happens after training completes successfully
    
        Red-phase expectation:
        - run_cdi_example_torch currently does NOT call save_torch_bundle
        - Once Phase D4.C1 complete, SHOULD invoke save_torch_bundle when output_dir set
        - Test will FAIL until orchestration wiring is complete
    
        Test mechanism:
        - Monkeypatch save_torch_bundle to spy on invocation
        - Set config.output_dir to tmp_path
        - Call run_cdi_example_torch
        - Validate save_torch_bundle was called with correct models dict + base_path
        """
        # Import the module under test
        from ptycho_torch.workflows import components as torch_components
        from ptycho.config.config import TrainingConfig, ModelConfig
    
        # Spy flag to track save_torch_bundle invocation
        save_torch_bundle_called = {"called": False, "args": None, "kwargs": None}
    
        def mock_save_torch_bundle(models_dict, base_path, config, **kwargs):
            """Spy that records save_torch_bundle invocation."""
            save_torch_bundle_called["called"] = True
            save_torch_bundle_called["args"] = (models_dict, base_path, config)
            save_torch_bundle_called["kwargs"] = kwargs
    
        # Monkeypatch save_torch_bundle
        monkeypatch.setattr(
            "ptycho_torch.workflows.components.save_torch_bundle",
            mock_save_torch_bundle
        )
    
        # Monkeypatch train_cdi_model_torch to return minimal results with models
        def mock_train_cdi_model_torch(train_data, test_data, config):
            """Return stub results including models dict for persistence."""
            return {
                "history": {"train_loss": [0.5, 0.3]},
                "train_container": {"sentinel": "train"},
                "test_container": None,
                "models": {
                    'autoencoder': {'_sentinel': 'trained_autoencoder'},
                    'diffraction_to_obj': {'_sentinel': 'trained_diffraction'},
                },
            }
    
        monkeypatch.setattr(
            "ptycho_torch.workflows.components.train_cdi_model_torch",
            mock_train_cdi_model_torch
        )
    
        # Create config with output_dir set
        model_config = ModelConfig(N=64, gridsize=2, model_type='pinn')
        config_with_output = TrainingConfig(
            model=model_config,
            train_data_file=Path("/tmp/dummy_train.npz"),
            test_data_file=Path("/tmp/dummy_test.npz"),
            n_groups=10,
            neighbor_count=4,
            nphotons=1e9,
            output_dir=tmp_path,  # Enable persistence
        )
    
        # Call run_cdi_example_torch
>       recon_amp, recon_phase, results = torch_components.run_cdi_example_torch(
            train_data=dummy_raw_data,
            test_data=None,
            config=config_with_output,
            flip_x=False,
            flip_y=False,
            transpose=False,
            M=20,
            do_stitching=False,
        )

tests/torch/test_workflows_components.py:1036: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

train_data = <ptycho.raw_data.RawData object at 0x71a8a858bb10>
test_data = None
config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj...test-716/test_run_cdi_example_persists_0'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
flip_x = False, flip_y = False, transpose = False, M = 20, do_stitching = False
execution_config = None

    def run_cdi_example_torch(
        train_data: Union[RawData, 'RawDataTorch', 'PtychoDataContainerTorch'],
        test_data: Optional[Union[RawData, 'RawDataTorch', 'PtychoDataContainerTorch']],
        config: TrainingConfig,
        flip_x: bool = False,
        flip_y: bool = False,
        transpose: bool = False,
        M: int = 20,
        do_stitching: bool = False,
        execution_config: Optional[Any] = None
    ) -> Tuple[Optional[Any], Optional[Any], Dict[str, Any]]:
        """
        Run the main CDI example execution flow using PyTorch backend.
    
        This function provides API parity with ptycho.workflows.components.run_cdi_example,
        enabling transparent backend selection from Ptychodus per specs/ptychodus_api_spec.md 4.5.
    
        CRITICAL: This function MUST call update_legacy_dict(params.cfg, config) before
        delegating to core modules to prevent CONFIG-001 violations (empty params.cfg
        causing silent shape mismatches downstream).
    
        Args:
            train_data: Training data (RawData, RawDataTorch, or PtychoDataContainerTorch)
            test_data: Optional test data (same type constraints as train_data)
            config: TrainingConfig instance (TensorFlow dataclass, translated via config_bridge)
            flip_x: Whether to flip the x coordinates during reconstruction
            flip_y: Whether to flip the y coordinates during reconstruction
            transpose: Whether to transpose the image by swapping dimensions
            M: Parameter for reassemble_position function (default: 20)
            do_stitching: Whether to perform image stitching after training
            execution_config: Optional PyTorchExecutionConfig for runtime control (accelerator,
                             num_workers, learning_rate, scheduler, logger, checkpointing).
                             See CONFIG-002, CONFIG-LOGGER-001.
    
        Returns:
            Tuple containing:
            - reconstructed amplitude (or None if stitching disabled)
            - reconstructed phase (or None if stitching disabled)
            - results dictionary (training history, containers, metrics)
    
        Raises:
            NotImplementedError: Phase D2.B/C not yet implemented (scaffold only)
    
        Phase D2.A Scaffold Status:
            - Entry signature:  COMPLETE (matches TensorFlow)
            - update_legacy_dict call:  COMPLETE (CONFIG-001 compliance)
            - Placeholder logic:  COMPLETE (raises NotImplementedError)
            - Torch-optional:  COMPLETE (importable without torch)
    
        Phase D2.B/C TODO:
            - Implement train_cdi_model_torch delegation (Lightning trainer orchestration)
            - Implement reassemble_cdi_image_torch (optional stitching path)
            - Add MLflow disable flag handling
            - Validate deterministic seeds from config
    
        Example (Post D2.B/C):
            >>> from ptycho_torch.workflows.components import run_cdi_example_torch
            >>> from ptycho.config.config import TrainingConfig, ModelConfig
            >>> from ptycho.raw_data import RawData
            >>>
            >>> # Load data
            >>> train_data = RawData.from_file("train.npz")
            >>> config = TrainingConfig(model=ModelConfig(N=64), ...)
            >>>
            >>> # Execute PyTorch pipeline
            >>> amp, phase, results = run_cdi_example_torch(
            ...     train_data, None, config, do_stitching=False
            ... )
        """
        # CRITICAL: Update params.cfg before delegating (CONFIG-001 compliance)
        # This ensures legacy modules invoked downstream observe correct configuration state
        ptycho_config.update_legacy_dict(params.cfg, config)
        logger.info("PyTorch workflow: params.cfg synchronized with TrainingConfig")
    
        # Step 1: Train the model (Phase D2.B  delegates to Lightning trainer stub)
        logger.info("Invoking PyTorch training orchestration via train_cdi_model_torch")
        # Note: train_cdi_model_torch will need to be updated to accept execution_config
        # For now, we pass it as a keyword argument for forward compatibility
>       train_results = train_cdi_model_torch(train_data, test_data, config, execution_config=execution_config)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: TestWorkflowsComponentsRun.test_run_cdi_example_persists_models.<locals>.mock_train_cdi_model_torch() got an unexpected keyword argument 'execution_config'

ptycho_torch/workflows/components.py:161: TypeError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (10, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (10,)
xcoords shape: (10,)
ycoords shape: (10,)
xcoords_start shape: (10,)
ycoords_start shape: (10,)
_____ TestTrainWithLightningRed.test_train_with_lightning_runs_trainer_fit _____

self = <test_workflows_components.TestTrainWithLightningRed object at 0x71a8a9191210>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x71a8642e2550>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'sigmoid', 'backend': 'pytorch', 'batch_size': 16, ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x71a8642e1290>

    def test_train_with_lightning_runs_trainer_fit(
        self,
        monkeypatch,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data
    ):
        """
        RED TEST 2: _train_with_lightning MUST invoke Trainer.fit with dataloaders.
    
        Requirement: docs/workflows/pytorch.md 5 Lightning trainer expectations
        require Trainer.fit orchestration with train/val dataloaders.
    
        Design contract (phase_b_test_design.md 2):
        - _train_with_lightning MUST construct lightning.pytorch.Trainer
        - MUST invoke trainer.fit(module, train_dataloader, val_dataloader)
        - Dataloaders MUST be derived from provided train/test containers
        - Validation dataloader is None when test_container is None
    
        Test mechanism:
        - Monkeypatch Trainer constructor to return stub exposing fit_called flag
        - Monkeypatch dataloader builders (future helpers) with sentinels
        - Invoke _train_with_lightning
        - Assert Trainer.fit was called with correct dataloaders
    
        Expected red-phase failure:
        - Stub never constructs Trainer or calls fit
        - fit_called flag remains False  assertion fails
        """
        from ptycho_torch.workflows import components as torch_components
    
        # Spy to track Trainer.fit invocation
        trainer_fit_called = {"called": False, "args": None, "kwargs": None}
    
        class MockTrainer:
            """Stub Trainer that records fit() calls."""
            def fit(self, module, train_dataloaders=None, val_dataloaders=None, **kwargs):
                trainer_fit_called["called"] = True
                trainer_fit_called["args"] = (module, train_dataloaders, val_dataloaders)
                trainer_fit_called["kwargs"] = kwargs
    
        # Monkeypatch Lightning Trainer
        monkeypatch.setattr(
            "lightning.pytorch.Trainer",
            lambda **kwargs: MockTrainer()
        )
    
        # Monkeypatch Lightning module to prevent import errors
        class StubLightningModule:
            def save_hyperparameters(self):
                pass
    
        monkeypatch.setattr(
            "ptycho_torch.model.PtychoPINN_Lightning",
            lambda *args, **kwargs: StubLightningModule()
        )
    
        # Create sentinel dataloaders (Phase B2 will wire real loader builders)
        sentinel_train_loader = {"_sentinel": "train_dataloader"}
        sentinel_val_loader = None  # test_container is None
    
        # Monkeypatch future dataloader builder helper
        # (Phase B2 will add _build_lightning_dataloaders or similar)
        def mock_build_dataloaders(container, config, shuffle=True):
            """Sentinel that returns mock dataloader."""
            if container is not None:
                return sentinel_train_loader
            return None
    
        # For red phase, assume _train_with_lightning will eventually call helper
        # For now, test just validates fit() invocation pattern
    
        # Create minimal containers
        train_container = {"X": np.ones((10, 64, 64))}
    
        # Call _train_with_lightning
>       results = torch_components._train_with_lightning(
            train_container=train_container,
            test_container=None,
            config=minimal_training_config
        )

tests/torch/test_workflows_components.py:1433: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

train_container = {'X': array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., ...        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.]]])}
test_container = None
config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
execution_config = PyTorchExecutionConfig(accelerator='cuda', strategy='auto', deterministic=True, gradient_clip_val=None, accum_steps=1,...nt_mode='min', early_stop_patience=100, logger_backend='csv', inference_batch_size=None, middle_trim=0, pad_eval=False)

    def _train_with_lightning(
        train_container: 'PtychoDataContainerTorch',
        test_container: Optional['PtychoDataContainerTorch'],
        config: TrainingConfig,
        execution_config: Optional['PyTorchExecutionConfig'] = None
    ) -> Dict[str, Any]:
        """
        Orchestrate Lightning trainer execution for PyTorch model training.
    
        This function implements the Lightning training workflow per Phase D2.B blueprint:
        1. Derives PyTorch config objects from TensorFlow TrainingConfig
        2. Instantiates PtychoPINN_Lightning module with all four config dependencies
        3. Builds train/val dataloaders via _build_lightning_dataloaders helper
        4. Configures Lightning Trainer with checkpoint/logging settings (ADR-003 Phase C3)
        5. Executes training via trainer.fit()
        6. Returns structured results dict with history, containers, and module handle
    
        Args:
            train_container: Normalized training data container
            test_container: Optional normalized test data container
            config: TrainingConfig with training hyperparameters
            execution_config: Optional PyTorchExecutionConfig with runtime knobs (Phase C3.A2)
    
        Returns:
            Dict[str, Any]: Training results including:
                - history: Dict with train_loss and optional val_loss trajectories
                - train_container: Original training container
                - test_container: Original test container
                - models: Dict with 'diffraction_to_obj' (Lightning module) and 'autoencoder' (sentinel)
                          for dual-model bundle persistence per spec 4.6
    
        Raises:
            RuntimeError: If torch or lightning packages are not installed (POLICY-001)
    
        References:
            - Blueprint: plans/active/INTEGRATE-PYTORCH-001/reports/2025-10-18T020940Z/phase_d2_completion/phase_b2_implementation.md
            - Spec: specs/ptychodus_api_spec.md:187 (reconstructor lifecycle contract)
            - Findings: POLICY-001 (PyTorch mandatory), CONFIG-001 (params.cfg already populated by caller)
            - ADR-003 Phase C3: execution_config controls Trainer kwargs (accelerator, deterministic, gradient_clip_val)
        """
        # B2.2: torch-optional imports with POLICY-001 compliant error messaging
        try:
            import torch
            import lightning.pytorch as L
            from ptycho_torch.model import PtychoPINN_Lightning
            from ptycho_torch.config_params import (
                DataConfig as PTDataConfig,
                ModelConfig as PTModelConfig,
                TrainingConfig as PTTrainingConfig,
                InferenceConfig as PTInferenceConfig
            )
        except ImportError as e:
            raise RuntimeError(
                "PyTorch backend requires torch>=2.2 and lightning. "
                "Install with: pip install -e .[torch]\n"
                "See docs/findings.md#policy-001 for PyTorch requirement policy."
            ) from e
    
        logger.info("_train_with_lightning orchestrating Lightning training")
        logger.info(f"Training config: nepochs={config.nepochs}, n_groups={config.n_groups}")
    
        # B2.1: Use config_factory to derive PyTorch configs with correct channel propagation
        # CRITICAL (Phase C4.D B2): Factory ensures C = gridsize**2 is propagated to
        # pt_model_config.C_model and pt_model_config.C_forward, preventing channel mismatch
        # when gridsize > 1 (see docs/findings.md#BUG-TF-001).
        from ptycho_torch.config_factory import create_training_payload
    
        # Build factory overrides from TrainingConfig fields
        # Factory requires n_groups in overrides dict; train_data_file and output_dir as positional
        # Note: Factory expects model_type in PyTorch naming ('Unsupervised'/'Supervised')
        #       but TrainingConfig uses TensorFlow naming ('pinn'/'supervised')
        mode_map = {'pinn': 'Unsupervised', 'supervised': 'Supervised'}
        factory_overrides = {
            'n_groups': config.n_groups,  # Required by factory validation
            'gridsize': config.model.gridsize,
            'model_type': mode_map.get(config.model.model_type, 'Unsupervised'),
            'amp_activation': config.model.amp_activation,
            'n_filters_scale': config.model.n_filters_scale,
            'nphotons': config.nphotons,
            'neighbor_count': config.neighbor_count,
            'max_epochs': config.nepochs,
            'batch_size': getattr(config, 'batch_size', 16),
            'subsample_seed': getattr(config, 'subsample_seed', None),
            'torch_loss_mode': getattr(config, 'torch_loss_mode', 'poisson'),
        }
    
        # Create payload with factory-derived PyTorch configs
        payload = create_training_payload(
            train_data_file=Path(config.train_data_file),
            output_dir=Path(getattr(config, 'output_dir', './outputs')),
            execution_config=execution_config,  # Pass through from caller
            overrides=factory_overrides
        )
    
        # Extract PyTorch configs from payload (gridsize  C propagation already applied)
        pt_data_config = payload.pt_data_config
        pt_model_config = payload.pt_model_config
        pt_training_config = payload.pt_training_config
    
        # CRITICAL: Supervised mode REQUIRES a compatible loss function (MAE)
        # The Lightning module expects loss_name to be defined, which only happens when:
        #   1. mode='Unsupervised' AND loss_function='Poisson'  sets loss_name='poisson_train'
        #   2. mode='Unsupervised' AND loss_function='MAE'  sets loss_name='mae_train'
        #   3. mode='Supervised' AND loss_function='MAE'  sets loss_name='mae_train'
        # Without this override, supervised mode with default loss_function='Poisson' causes
        # AttributeError: 'PtychoPINN_Lightning' object has no attribute 'loss_name'
        # See: ptycho_torch/model.py:1052-1066
        if pt_model_config.mode == 'Supervised' and pt_model_config.loss_function != 'MAE':
            logger.info(
                f"Backend override: supervised mode requires MAE loss "
                f"(was {pt_model_config.loss_function}), forcing loss_function='MAE'"
            )
            # Create new ModelConfig with corrected loss_function
            from dataclasses import replace
            pt_model_config = replace(pt_model_config, loss_function='MAE')
    
        # Create minimal InferenceConfig for Lightning module (training payload doesn't include it)
        pt_inference_config = PTInferenceConfig()
    
        # B2.4: Instantiate PtychoPINN_Lightning with factory-derived config objects
        model = PtychoPINN_Lightning(
            model_config=pt_model_config,
            data_config=pt_data_config,
            training_config=pt_training_config,
            inference_config=pt_inference_config
        )
    
        # Save hyperparameters so checkpoint can reconstruct module without external state
        model.save_hyperparameters()
    
        # B2.3: Build dataloaders via helper
        train_loader, val_loader = _build_lightning_dataloaders(
            train_container, test_container, config
        )
    
        # DATA-SUP-001: Supervised mode requires labeled data
        # Check if supervised mode is requested but training data lacks required labels
        if pt_model_config.mode == 'Supervised':
            # Inspect first batch to verify label keys exist
            try:
                first_batch = next(iter(train_loader))
                batch_dict = first_batch[0]  # Extract tensor dict from batch tuple
                if 'label_amp' not in batch_dict or 'label_phase' not in batch_dict:
                    raise RuntimeError(
                        f"Supervised mode (model_type='supervised') requires labeled datasets with "
                        f"'label_amp' and 'label_phase' keys, but training data lacks these fields. "
                        f"Either: (1) Use a labeled NPZ dataset (see ptycho_torch/notebooks/create_supervised_datasets.ipynb), "
                        f"or (2) Switch to PINN mode (--model_type pinn) for self-supervised physics-based training. "
                        f"See DATA-SUP-001 in docs/findings.md for details."
                    )
            except StopIteration:
                raise RuntimeError(
                    f"Training dataloader is empty. Check dataset path and n_groups configuration."
                )
    
        # B2.5: Configure Trainer with settings from config
        # C3.A3: Thread execution config values to Trainer kwargs
        output_dir = Path(getattr(config, 'output_dir', './outputs'))
        debug_mode = getattr(config, 'debug', False)
    
        # Import execution config defaults if not provided
        if execution_config is None:
            from ptycho.config.config import PyTorchExecutionConfig
            execution_config = PyTorchExecutionConfig()
            logger.info(f"PyTorchExecutionConfig auto-instantiated for Lightning training (accelerator resolved to '{execution_config.accelerator}')")
    
        # EB1.D: Configure checkpoint/early-stop callbacks (ADR-003 Phase EB1)
        callbacks = []
        if execution_config.enable_checkpointing:
            from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping
    
            # Determine if we have validation data to use val metrics
            has_validation = test_container is not None
    
            # EB2.B: Derive monitor metric from model.val_loss_name (ADR-003 Phase EB2)
            # The model's val_loss_name is dynamically constructed based on model_type and loss configuration
            # (e.g., 'poisson_val_Amp_loss' for PINN with amplitude loss, 'mae_val_Phase_loss' for supervised)
            # This ensures checkpoint/early-stop callbacks watch the correct logged metric
            if has_validation and hasattr(model, 'val_loss_name'):
                # Use the model's dynamic validation loss name
                monitor_metric = model.val_loss_name
            else:
                # Fall back to execution config default or train loss
                monitor_metric = execution_config.checkpoint_monitor_metric
                if 'val_' in monitor_metric and not has_validation:
                    # Fall back to train_loss if val metric requested but no validation data
                    monitor_metric = monitor_metric.replace('val_', 'train_')
    
            # Build checkpoint filename template using dynamic metric name
            # Format: epoch={epoch:02d}-<metric_short_name>={<full_metric_name>:.4f}
            if has_validation:
                # Extract short name for filename (remove '_loss' suffix if present)
                metric_short_name = monitor_metric.replace('_loss', '')
                filename_template = f'epoch={{epoch:02d}}-{metric_short_name}={{{monitor_metric}:.4f}}'
            else:
                filename_template = 'epoch={epoch:02d}'
    
            checkpoint_callback = ModelCheckpoint(
                dirpath=str(output_dir / "checkpoints"),
                filename=filename_template,
                monitor=monitor_metric,
                mode=execution_config.checkpoint_mode,
                save_top_k=execution_config.checkpoint_save_top_k,
                save_last=True,  # Always keep last checkpoint for recovery
                verbose=False,
            )
            callbacks.append(checkpoint_callback)
    
            # EarlyStopping callback (ADR-003 Phase EB1.D)
            # Only add early stopping if validation data is available (otherwise no metric to monitor)
            if has_validation:
                early_stop_callback = EarlyStopping(
                    monitor=monitor_metric,
                    mode=execution_config.checkpoint_mode,
                    patience=execution_config.early_stop_patience,
                    verbose=False,
                )
                callbacks.append(early_stop_callback)
    
        # Instantiate logger based on execution config (Phase EB3.B - ADR-003)
        lightning_logger = False  # Default: no logger
        if execution_config.logger_backend is not None:
            try:
                if execution_config.logger_backend == 'csv':
                    from lightning.pytorch.loggers import CSVLogger
                    lightning_logger = CSVLogger(
                        save_dir=str(output_dir),
                        name='lightning_logs',
                    )
                    logger.info(f"Enabled CSVLogger: metrics saved to {output_dir}/lightning_logs/")
                elif execution_config.logger_backend == 'tensorboard':
                    from lightning.pytorch.loggers import TensorBoardLogger
                    lightning_logger = TensorBoardLogger(
                        save_dir=str(output_dir),
                        name='lightning_logs',
                    )
                    logger.info(f"Enabled TensorBoardLogger: run `tensorboard --logdir={output_dir}/lightning_logs/`")
                elif execution_config.logger_backend == 'mlflow':
                    from lightning.pytorch.loggers import MLFlowLogger
                    lightning_logger = MLFlowLogger(
                        experiment_name=getattr(config, 'experiment_name', 'PtychoPINN'),
                        tracking_uri=str(output_dir / 'mlruns'),
                    )
                    logger.info(f"Enabled MLFlowLogger: tracking URI={output_dir}/mlruns")
                else:
                    logger.warning(
                        f"Unknown logger_backend '{execution_config.logger_backend}'. "
                        f"Falling back to logger=False. Supported: 'csv', 'tensorboard', 'mlflow'."
                    )
            except ImportError as e:
                logger.warning(
                    f"Failed to import Lightning logger '{execution_config.logger_backend}': {e}. "
                    f"Metrics logging disabled. Install the required package to enable logging."
                )
                lightning_logger = False
        else:
            logger.info("Logger disabled (logger_backend=None). Loss metrics will not be saved to disk.")
    
        # EXEC-ACCUM-001: Guard against manual optimization + gradient accumulation
        # Lightning's manual optimization (automatic_optimization=False) is incompatible with
        # Trainer(accumulate_grad_batches>1). The PtychoPINN_Lightning module uses manual optimization
        # for custom physics loss integration, so gradient accumulation must be disabled.
>       if not model.automatic_optimization and execution_config.accum_steps > 1:
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'StubLightningModule' object has no attribute 'automatic_optimization'

ptycho_torch/workflows/components.py:854: AttributeError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (10, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (10,)
xcoords shape: (10,)
ycoords shape: (10,)
xcoords_start shape: (10,)
ycoords_start shape: (10,)
----------------------------- Captured stderr call -----------------------------
INFO: Seed set to 42
------------------------------ Captured log call -------------------------------
INFO     lightning.fabric.utilities.seed:seed.py:57 Seed set to 42
___ TestTrainWithLightningRed.test_train_with_lightning_returns_models_dict ____

self = <test_workflows_components.TestTrainWithLightningRed object at 0x71a8a91918d0>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x71a8641ffd10>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'sigmoid', 'backend': 'pytorch', 'batch_size': 16, ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x71a8641ff150>

    def test_train_with_lightning_returns_models_dict(
        self,
        monkeypatch,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data
    ):
        """
        RED TEST 3: _train_with_lightning MUST return results dict with 'models' key.
    
        Requirement: Phase D4 persistence tests require trained module handles
        for save_torch_bundle orchestration (mirrors TensorFlow train_cdi_model).
    
        Design contract (phase_b_test_design.md 3):
        - _train_with_lightning returns Dict[str, Any]
        - Results dict MUST contain 'models' key
        - models['lightning_module'] (or models['diffraction_to_obj']) MUST point to trained module
        - This enables downstream save_torch_bundle to persist checkpoint
    
        Test mechanism:
        - Monkeypatch Lightning components to return stub module
        - Invoke _train_with_lightning
        - Assert results dict contains 'models' key with module handle
    
        Expected red-phase failure:
        - Stub returns only history/containers  missing 'models' key
        - Assertion fails
        """
        from ptycho_torch.workflows import components as torch_components
    
        # Stub Lightning module with sentinel identity
        class StubLightningModule:
            def save_hyperparameters(self):
                pass
    
            _sentinel = "trained_lightning_module"
    
        stub_module = StubLightningModule()
    
        # Monkeypatch Lightning module constructor
        monkeypatch.setattr(
            "ptycho_torch.model.PtychoPINN_Lightning",
            lambda *args, **kwargs: stub_module
        )
    
        # Monkeypatch Trainer to skip actual training
        class MockTrainer:
            def fit(self, module, train_dataloaders=None, val_dataloaders=None, **kwargs):
                pass
    
        monkeypatch.setattr(
            "lightning.pytorch.Trainer",
            lambda **kwargs: MockTrainer()
        )
    
        # Create minimal containers
        train_container = {"X": np.ones((10, 64, 64))}
    
        # Call _train_with_lightning
>       results = torch_components._train_with_lightning(
            train_container=train_container,
            test_container=None,
            config=minimal_training_config
        )

tests/torch/test_workflows_components.py:1513: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

train_container = {'X': array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., ...        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.]]])}
test_container = None
config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
execution_config = PyTorchExecutionConfig(accelerator='cuda', strategy='auto', deterministic=True, gradient_clip_val=None, accum_steps=1,...nt_mode='min', early_stop_patience=100, logger_backend='csv', inference_batch_size=None, middle_trim=0, pad_eval=False)

    def _train_with_lightning(
        train_container: 'PtychoDataContainerTorch',
        test_container: Optional['PtychoDataContainerTorch'],
        config: TrainingConfig,
        execution_config: Optional['PyTorchExecutionConfig'] = None
    ) -> Dict[str, Any]:
        """
        Orchestrate Lightning trainer execution for PyTorch model training.
    
        This function implements the Lightning training workflow per Phase D2.B blueprint:
        1. Derives PyTorch config objects from TensorFlow TrainingConfig
        2. Instantiates PtychoPINN_Lightning module with all four config dependencies
        3. Builds train/val dataloaders via _build_lightning_dataloaders helper
        4. Configures Lightning Trainer with checkpoint/logging settings (ADR-003 Phase C3)
        5. Executes training via trainer.fit()
        6. Returns structured results dict with history, containers, and module handle
    
        Args:
            train_container: Normalized training data container
            test_container: Optional normalized test data container
            config: TrainingConfig with training hyperparameters
            execution_config: Optional PyTorchExecutionConfig with runtime knobs (Phase C3.A2)
    
        Returns:
            Dict[str, Any]: Training results including:
                - history: Dict with train_loss and optional val_loss trajectories
                - train_container: Original training container
                - test_container: Original test container
                - models: Dict with 'diffraction_to_obj' (Lightning module) and 'autoencoder' (sentinel)
                          for dual-model bundle persistence per spec 4.6
    
        Raises:
            RuntimeError: If torch or lightning packages are not installed (POLICY-001)
    
        References:
            - Blueprint: plans/active/INTEGRATE-PYTORCH-001/reports/2025-10-18T020940Z/phase_d2_completion/phase_b2_implementation.md
            - Spec: specs/ptychodus_api_spec.md:187 (reconstructor lifecycle contract)
            - Findings: POLICY-001 (PyTorch mandatory), CONFIG-001 (params.cfg already populated by caller)
            - ADR-003 Phase C3: execution_config controls Trainer kwargs (accelerator, deterministic, gradient_clip_val)
        """
        # B2.2: torch-optional imports with POLICY-001 compliant error messaging
        try:
            import torch
            import lightning.pytorch as L
            from ptycho_torch.model import PtychoPINN_Lightning
            from ptycho_torch.config_params import (
                DataConfig as PTDataConfig,
                ModelConfig as PTModelConfig,
                TrainingConfig as PTTrainingConfig,
                InferenceConfig as PTInferenceConfig
            )
        except ImportError as e:
            raise RuntimeError(
                "PyTorch backend requires torch>=2.2 and lightning. "
                "Install with: pip install -e .[torch]\n"
                "See docs/findings.md#policy-001 for PyTorch requirement policy."
            ) from e
    
        logger.info("_train_with_lightning orchestrating Lightning training")
        logger.info(f"Training config: nepochs={config.nepochs}, n_groups={config.n_groups}")
    
        # B2.1: Use config_factory to derive PyTorch configs with correct channel propagation
        # CRITICAL (Phase C4.D B2): Factory ensures C = gridsize**2 is propagated to
        # pt_model_config.C_model and pt_model_config.C_forward, preventing channel mismatch
        # when gridsize > 1 (see docs/findings.md#BUG-TF-001).
        from ptycho_torch.config_factory import create_training_payload
    
        # Build factory overrides from TrainingConfig fields
        # Factory requires n_groups in overrides dict; train_data_file and output_dir as positional
        # Note: Factory expects model_type in PyTorch naming ('Unsupervised'/'Supervised')
        #       but TrainingConfig uses TensorFlow naming ('pinn'/'supervised')
        mode_map = {'pinn': 'Unsupervised', 'supervised': 'Supervised'}
        factory_overrides = {
            'n_groups': config.n_groups,  # Required by factory validation
            'gridsize': config.model.gridsize,
            'model_type': mode_map.get(config.model.model_type, 'Unsupervised'),
            'amp_activation': config.model.amp_activation,
            'n_filters_scale': config.model.n_filters_scale,
            'nphotons': config.nphotons,
            'neighbor_count': config.neighbor_count,
            'max_epochs': config.nepochs,
            'batch_size': getattr(config, 'batch_size', 16),
            'subsample_seed': getattr(config, 'subsample_seed', None),
            'torch_loss_mode': getattr(config, 'torch_loss_mode', 'poisson'),
        }
    
        # Create payload with factory-derived PyTorch configs
        payload = create_training_payload(
            train_data_file=Path(config.train_data_file),
            output_dir=Path(getattr(config, 'output_dir', './outputs')),
            execution_config=execution_config,  # Pass through from caller
            overrides=factory_overrides
        )
    
        # Extract PyTorch configs from payload (gridsize  C propagation already applied)
        pt_data_config = payload.pt_data_config
        pt_model_config = payload.pt_model_config
        pt_training_config = payload.pt_training_config
    
        # CRITICAL: Supervised mode REQUIRES a compatible loss function (MAE)
        # The Lightning module expects loss_name to be defined, which only happens when:
        #   1. mode='Unsupervised' AND loss_function='Poisson'  sets loss_name='poisson_train'
        #   2. mode='Unsupervised' AND loss_function='MAE'  sets loss_name='mae_train'
        #   3. mode='Supervised' AND loss_function='MAE'  sets loss_name='mae_train'
        # Without this override, supervised mode with default loss_function='Poisson' causes
        # AttributeError: 'PtychoPINN_Lightning' object has no attribute 'loss_name'
        # See: ptycho_torch/model.py:1052-1066
        if pt_model_config.mode == 'Supervised' and pt_model_config.loss_function != 'MAE':
            logger.info(
                f"Backend override: supervised mode requires MAE loss "
                f"(was {pt_model_config.loss_function}), forcing loss_function='MAE'"
            )
            # Create new ModelConfig with corrected loss_function
            from dataclasses import replace
            pt_model_config = replace(pt_model_config, loss_function='MAE')
    
        # Create minimal InferenceConfig for Lightning module (training payload doesn't include it)
        pt_inference_config = PTInferenceConfig()
    
        # B2.4: Instantiate PtychoPINN_Lightning with factory-derived config objects
        model = PtychoPINN_Lightning(
            model_config=pt_model_config,
            data_config=pt_data_config,
            training_config=pt_training_config,
            inference_config=pt_inference_config
        )
    
        # Save hyperparameters so checkpoint can reconstruct module without external state
        model.save_hyperparameters()
    
        # B2.3: Build dataloaders via helper
        train_loader, val_loader = _build_lightning_dataloaders(
            train_container, test_container, config
        )
    
        # DATA-SUP-001: Supervised mode requires labeled data
        # Check if supervised mode is requested but training data lacks required labels
        if pt_model_config.mode == 'Supervised':
            # Inspect first batch to verify label keys exist
            try:
                first_batch = next(iter(train_loader))
                batch_dict = first_batch[0]  # Extract tensor dict from batch tuple
                if 'label_amp' not in batch_dict or 'label_phase' not in batch_dict:
                    raise RuntimeError(
                        f"Supervised mode (model_type='supervised') requires labeled datasets with "
                        f"'label_amp' and 'label_phase' keys, but training data lacks these fields. "
                        f"Either: (1) Use a labeled NPZ dataset (see ptycho_torch/notebooks/create_supervised_datasets.ipynb), "
                        f"or (2) Switch to PINN mode (--model_type pinn) for self-supervised physics-based training. "
                        f"See DATA-SUP-001 in docs/findings.md for details."
                    )
            except StopIteration:
                raise RuntimeError(
                    f"Training dataloader is empty. Check dataset path and n_groups configuration."
                )
    
        # B2.5: Configure Trainer with settings from config
        # C3.A3: Thread execution config values to Trainer kwargs
        output_dir = Path(getattr(config, 'output_dir', './outputs'))
        debug_mode = getattr(config, 'debug', False)
    
        # Import execution config defaults if not provided
        if execution_config is None:
            from ptycho.config.config import PyTorchExecutionConfig
            execution_config = PyTorchExecutionConfig()
            logger.info(f"PyTorchExecutionConfig auto-instantiated for Lightning training (accelerator resolved to '{execution_config.accelerator}')")
    
        # EB1.D: Configure checkpoint/early-stop callbacks (ADR-003 Phase EB1)
        callbacks = []
        if execution_config.enable_checkpointing:
            from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping
    
            # Determine if we have validation data to use val metrics
            has_validation = test_container is not None
    
            # EB2.B: Derive monitor metric from model.val_loss_name (ADR-003 Phase EB2)
            # The model's val_loss_name is dynamically constructed based on model_type and loss configuration
            # (e.g., 'poisson_val_Amp_loss' for PINN with amplitude loss, 'mae_val_Phase_loss' for supervised)
            # This ensures checkpoint/early-stop callbacks watch the correct logged metric
            if has_validation and hasattr(model, 'val_loss_name'):
                # Use the model's dynamic validation loss name
                monitor_metric = model.val_loss_name
            else:
                # Fall back to execution config default or train loss
                monitor_metric = execution_config.checkpoint_monitor_metric
                if 'val_' in monitor_metric and not has_validation:
                    # Fall back to train_loss if val metric requested but no validation data
                    monitor_metric = monitor_metric.replace('val_', 'train_')
    
            # Build checkpoint filename template using dynamic metric name
            # Format: epoch={epoch:02d}-<metric_short_name>={<full_metric_name>:.4f}
            if has_validation:
                # Extract short name for filename (remove '_loss' suffix if present)
                metric_short_name = monitor_metric.replace('_loss', '')
                filename_template = f'epoch={{epoch:02d}}-{metric_short_name}={{{monitor_metric}:.4f}}'
            else:
                filename_template = 'epoch={epoch:02d}'
    
            checkpoint_callback = ModelCheckpoint(
                dirpath=str(output_dir / "checkpoints"),
                filename=filename_template,
                monitor=monitor_metric,
                mode=execution_config.checkpoint_mode,
                save_top_k=execution_config.checkpoint_save_top_k,
                save_last=True,  # Always keep last checkpoint for recovery
                verbose=False,
            )
            callbacks.append(checkpoint_callback)
    
            # EarlyStopping callback (ADR-003 Phase EB1.D)
            # Only add early stopping if validation data is available (otherwise no metric to monitor)
            if has_validation:
                early_stop_callback = EarlyStopping(
                    monitor=monitor_metric,
                    mode=execution_config.checkpoint_mode,
                    patience=execution_config.early_stop_patience,
                    verbose=False,
                )
                callbacks.append(early_stop_callback)
    
        # Instantiate logger based on execution config (Phase EB3.B - ADR-003)
        lightning_logger = False  # Default: no logger
        if execution_config.logger_backend is not None:
            try:
                if execution_config.logger_backend == 'csv':
                    from lightning.pytorch.loggers import CSVLogger
                    lightning_logger = CSVLogger(
                        save_dir=str(output_dir),
                        name='lightning_logs',
                    )
                    logger.info(f"Enabled CSVLogger: metrics saved to {output_dir}/lightning_logs/")
                elif execution_config.logger_backend == 'tensorboard':
                    from lightning.pytorch.loggers import TensorBoardLogger
                    lightning_logger = TensorBoardLogger(
                        save_dir=str(output_dir),
                        name='lightning_logs',
                    )
                    logger.info(f"Enabled TensorBoardLogger: run `tensorboard --logdir={output_dir}/lightning_logs/`")
                elif execution_config.logger_backend == 'mlflow':
                    from lightning.pytorch.loggers import MLFlowLogger
                    lightning_logger = MLFlowLogger(
                        experiment_name=getattr(config, 'experiment_name', 'PtychoPINN'),
                        tracking_uri=str(output_dir / 'mlruns'),
                    )
                    logger.info(f"Enabled MLFlowLogger: tracking URI={output_dir}/mlruns")
                else:
                    logger.warning(
                        f"Unknown logger_backend '{execution_config.logger_backend}'. "
                        f"Falling back to logger=False. Supported: 'csv', 'tensorboard', 'mlflow'."
                    )
            except ImportError as e:
                logger.warning(
                    f"Failed to import Lightning logger '{execution_config.logger_backend}': {e}. "
                    f"Metrics logging disabled. Install the required package to enable logging."
                )
                lightning_logger = False
        else:
            logger.info("Logger disabled (logger_backend=None). Loss metrics will not be saved to disk.")
    
        # EXEC-ACCUM-001: Guard against manual optimization + gradient accumulation
        # Lightning's manual optimization (automatic_optimization=False) is incompatible with
        # Trainer(accumulate_grad_batches>1). The PtychoPINN_Lightning module uses manual optimization
        # for custom physics loss integration, so gradient accumulation must be disabled.
>       if not model.automatic_optimization and execution_config.accum_steps > 1:
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'StubLightningModule' object has no attribute 'automatic_optimization'

ptycho_torch/workflows/components.py:854: AttributeError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (10, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (10,)
xcoords shape: (10,)
ycoords shape: (10,)
xcoords_start shape: (10,)
ycoords_start shape: (10,)
----------------------------- Captured stderr call -----------------------------
INFO: Seed set to 42
------------------------------ Captured log call -------------------------------
INFO     lightning.fabric.utilities.seed:seed.py:57 Seed set to 42
_ TestReassembleCdiImageTorchGreen.test_run_cdi_example_torch_do_stitching_delegates_to_reassemble _

self = <test_workflows_components.TestReassembleCdiImageTorchGreen object at 0x71a8a9190810>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'silu', 'backend': 'tensorflow', 'batch_size': 2, ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=1, model_type='pinn', amp_activation='silu', object...test-716/test_run_cdi_example_torch_do_0'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x71a8642e3790>
stitch_train_results = {'history': {'train_loss': [0.1, 0.05]}, 'models': {'autoencoder': {'_sentinel': 'autoencoder'}, 'diffraction_to_obj': MockLightningModule()}}
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x71a8641044d0>

    def test_run_cdi_example_torch_do_stitching_delegates_to_reassemble(
        self,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data,
        stitch_train_results,
        monkeypatch
    ):
        """
        GREEN TEST: run_cdi_example_torch(do_stitching=True) delegates to stitching path.
    
        Requirement: Phase D2.C workflow integration  ensure orchestration calls reassembly.
    
        TensorFlow baseline (ptycho/workflows/components.py:676-732):
        - run_cdi_example(..., do_stitching=True) invokes reassemble_cdi_image
        - Stitching runs AFTER training completes
        - Returns (recon_amp, recon_phase, results) when stitching enabled
        - Returns (None, None, results) when do_stitching=False
    
        Expected behavior:
        - Runs training (mocked), then calls _reassemble_cdi_image_torch with test_data
        - Stitching results populate amplitude/phase return values
        - Returns (recon_amp, recon_phase, results) when do_stitching=True
    
        Test mechanism:
        - Monkeypatch training path to return stitch_train_results fixture (avoid GPU)
        - Call run_cdi_example_torch with do_stitching=True
        - Assert amplitude/phase are returned (not None)
        - Validate outputs are numpy arrays
    
        Validation coverage:
        - Confirms orchestration wiring exists
        - Ensures stitching path is reachable from public API
        - Documents return value contract for downstream consumers (e.g., ptychodus)
        """
        from ptycho_torch.workflows import components as torch_components
        from ptycho.config.config import update_legacy_dict
        from ptycho import params
    
        # Bridge config (CONFIG-001)
        update_legacy_dict(params.cfg, minimal_training_config)
    
        # Monkeypatch _train_with_lightning to return mock results with Lightning module
        def mock_train_with_lightning(train_container, test_container, config):
            """Stub that returns train_results with mock Lightning module."""
            # Return the stitch_train_results fixture enriched with containers
            results = stitch_train_results.copy()
            results["containers"] = {"train": train_container, "test": test_container}
            return results
    
        monkeypatch.setattr(
            torch_components,
            "_train_with_lightning",
            mock_train_with_lightning
        )
    
        # GREEN PHASE VALIDATION: expect successful stitching
>       recon_amp, recon_phase, results = torch_components.run_cdi_example_torch(
            train_data=dummy_raw_data,
            test_data=dummy_raw_data,  # Use same data for test (deterministic)
            config=minimal_training_config,
            flip_x=False,
            flip_y=False,
            transpose=False,
            M=128,
            do_stitching=True,  # CRITICAL: enable stitching path
        )

tests/torch/test_workflows_components.py:1897: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ptycho_torch/workflows/components.py:161: in run_cdi_example_torch
    train_results = train_cdi_model_torch(train_data, test_data, config, execution_config=execution_config)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

train_data = <ptycho.raw_data.RawData object at 0x71a8642e3790>
test_data = <ptycho.raw_data.RawData object at 0x71a8642e3790>
config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=1, model_type='pinn', amp_activation='silu', object...test-716/test_run_cdi_example_torch_do_0'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
execution_config = None

    def train_cdi_model_torch(
        train_data: Union[RawData, 'RawDataTorch', 'PtychoDataContainerTorch'],
        test_data: Optional[Union[RawData, 'RawDataTorch', 'PtychoDataContainerTorch']],
        config: TrainingConfig,
        execution_config: Optional[Any] = None
    ) -> Dict[str, Any]:
        """
        Train the CDI model using PyTorch Lightning backend.
    
        This function provides API parity with ptycho.workflows.components.train_cdi_model,
        orchestrating data preparation, probe initialization, and Lightning trainer execution.
    
        Args:
            train_data: Training data (RawData, RawDataTorch, or PtychoDataContainerTorch)
            test_data: Optional test data for validation
            config: TrainingConfig instance (TensorFlow dataclass)
            execution_config: Optional PyTorchExecutionConfig for runtime control
    
        Returns:
            Dict[str, Any]: Results dictionary containing:
            - 'history': Training history (losses, metrics)
            - 'train_container': PtychoDataContainerTorch for training data
            - 'test_container': Optional PtychoDataContainerTorch for test data
            - Additional outputs from Lightning trainer
    
        Raises:
            ImportError: If Phase C adapters not available
            TypeError: If input data types are invalid
    
        Phase D2.B Status:
            - Entry signature:  COMPLETE (matches TensorFlow)
            - _ensure_container helper:  COMPLETE (normalizes inputs via Phase C adapters)
            - Lightning orchestration:  STUB (returns minimal dict, full impl pending)
            - Torch-optional:  COMPLETE (importable without torch)
    
        Example:
            >>> config = TrainingConfig(model=ModelConfig(N=64), nepochs=10, ...)
            >>> results = train_cdi_model_torch(train_data, test_data, config)
            >>> print(results['history']['train_loss'][-1])
        """
        # Step 1: Normalize train_data to PtychoDataContainerTorch
        logger.info("Normalizing training data via _ensure_container")
        train_container = _ensure_container(train_data, config)
    
        # Step 2: Normalize test_data if provided
        test_container = None
        if test_data is not None:
            logger.info("Normalizing test data via _ensure_container")
            test_container = _ensure_container(test_data, config)
    
        # Step 3: Initialize probe (TODO: implement probe handling for PyTorch)
        # TensorFlow baseline: probe.set_probe_guess(None, train_container.probe)
        # For Phase D2.B stub, skip probe initialization
        logger.debug("Probe initialization deferred to full Lightning implementation")
    
        # Step 4: Delegate to Lightning trainer
        logger.info("Delegating to Lightning trainer via _train_with_lightning")
>       results = _train_with_lightning(train_container, test_container, config, execution_config=execution_config)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: TestReassembleCdiImageTorchGreen.test_run_cdi_example_torch_do_stitching_delegates_to_reassemble.<locals>.mock_train_with_lightning() got an unexpected keyword argument 'execution_config'

ptycho_torch/workflows/components.py:1137: TypeError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (20, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (20,)
objectGuess shape: (128, 128)
xcoords shape: (20,)
ycoords shape: (20,)
xcoords_start shape: (20,)
ycoords_start shape: (20,)
----------------------------- Captured stdout call -----------------------------
diff3d shape: (20, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (20,)
objectGuess shape: (128, 128)
xcoords shape: (20,)
ycoords shape: (20,)
xcoords_start shape: (20,)
ycoords_start shape: (20,)
DEBUG: nsamples: 10, gridsize: 2 (using efficient random sample-then-group strategy)
INFO: 'Y' array not found. Generating ground truth patches from 'objectGuess' as a fallback.
neighbor-sampled diffraction shape (10, 64, 64, 4)
diff3d shape: (20, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (20,)
objectGuess shape: (128, 128)
xcoords shape: (20,)
ycoords shape: (20,)
xcoords_start shape: (20,)
ycoords_start shape: (20,)
DEBUG: nsamples: 10, gridsize: 2 (using efficient random sample-then-group strategy)
INFO: 'Y' array not found. Generating ground truth patches from 'objectGuess' as a fallback.
neighbor-sampled diffraction shape (10, 64, 64, 4)
_____ TestTrainWithLightningGreen.test_execution_config_overrides_trainer ______

self = <test_workflows_components.TestTrainWithLightningGreen object at 0x71a8a919e310>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x71a864251ad0>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'sigmoid', 'backend': 'pytorch', 'batch_size': 16, ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x71a864272310>

    def test_execution_config_overrides_trainer(
        self,
        monkeypatch,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data
    ):
        """
        RED TEST: _train_with_lightning MUST pass execution config knobs to Trainer.
    
        Requirement: ADR-003 Phase C3.A3  thread trainer kwargs from execution config.
    
        Expected behavior (after wiring):
        - When execution_config supplied, Trainer receives accelerator/deterministic/gradient_clip_val
        - Values override defaults (e.g., accelerator='gpu', deterministic=False, gradient_clip_val=1.0)
        - When execution_config=None, Trainer uses CPU-safe defaults
    
        Test mechanism:
        - Spy on Trainer.__init__ to capture kwargs
        - Supply non-default PyTorchExecutionConfig (accelerator='gpu', deterministic=False)
        - Assert Trainer received those exact values
        - Expect FAILURE because _train_with_lightning currently ignores execution_config
        """
        from ptycho_torch.workflows import components as torch_components
        from ptycho.config.config import PyTorchExecutionConfig
    
        # Spy to track Trainer.__init__ kwargs
        trainer_init_kwargs = {"called": False, "kwargs": None}
    
        class MockTrainer:
            """Stub Trainer that records __init__ kwargs."""
            def __init__(self, **kwargs):
                trainer_init_kwargs["called"] = True
                trainer_init_kwargs["kwargs"] = kwargs
    
            def fit(self, module, train_dataloaders=None, val_dataloaders=None, **kwargs):
                pass
    
        # Monkeypatch Lightning Trainer
        monkeypatch.setattr(
            "lightning.pytorch.Trainer",
            MockTrainer
        )
    
        # Monkeypatch Lightning module to prevent errors
        class StubLightningModule:
            def save_hyperparameters(self):
                pass
    
        monkeypatch.setattr(
            "ptycho_torch.model.PtychoPINN_Lightning",
            lambda *args, **kwargs: StubLightningModule()
        )
    
        # Create execution config with non-default values
        exec_config = PyTorchExecutionConfig(
            accelerator='gpu',  # Override default 'cpu'
            deterministic=False,  # Override default True
            gradient_clip_val=1.0,  # Override default None
            num_workers=4,  # Override default 0
        )
    
        # Create minimal containers
        train_container = {"X": np.ones((10, 64, 64))}
    
        # Call _train_with_lightning with execution_config
>       results = torch_components._train_with_lightning(
            train_container=train_container,
            test_container=None,
            config=minimal_training_config,
            execution_config=exec_config  # CRITICAL: new parameter
        )

tests/torch/test_workflows_components.py:2535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

train_container = {'X': array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., ...        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.]]])}
test_container = None
config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
execution_config = PyTorchExecutionConfig(accelerator='gpu', strategy='auto', deterministic=False, gradient_clip_val=1.0, accum_steps=1, ...nt_mode='min', early_stop_patience=100, logger_backend='csv', inference_batch_size=None, middle_trim=0, pad_eval=False)

    def _train_with_lightning(
        train_container: 'PtychoDataContainerTorch',
        test_container: Optional['PtychoDataContainerTorch'],
        config: TrainingConfig,
        execution_config: Optional['PyTorchExecutionConfig'] = None
    ) -> Dict[str, Any]:
        """
        Orchestrate Lightning trainer execution for PyTorch model training.
    
        This function implements the Lightning training workflow per Phase D2.B blueprint:
        1. Derives PyTorch config objects from TensorFlow TrainingConfig
        2. Instantiates PtychoPINN_Lightning module with all four config dependencies
        3. Builds train/val dataloaders via _build_lightning_dataloaders helper
        4. Configures Lightning Trainer with checkpoint/logging settings (ADR-003 Phase C3)
        5. Executes training via trainer.fit()
        6. Returns structured results dict with history, containers, and module handle
    
        Args:
            train_container: Normalized training data container
            test_container: Optional normalized test data container
            config: TrainingConfig with training hyperparameters
            execution_config: Optional PyTorchExecutionConfig with runtime knobs (Phase C3.A2)
    
        Returns:
            Dict[str, Any]: Training results including:
                - history: Dict with train_loss and optional val_loss trajectories
                - train_container: Original training container
                - test_container: Original test container
                - models: Dict with 'diffraction_to_obj' (Lightning module) and 'autoencoder' (sentinel)
                          for dual-model bundle persistence per spec 4.6
    
        Raises:
            RuntimeError: If torch or lightning packages are not installed (POLICY-001)
    
        References:
            - Blueprint: plans/active/INTEGRATE-PYTORCH-001/reports/2025-10-18T020940Z/phase_d2_completion/phase_b2_implementation.md
            - Spec: specs/ptychodus_api_spec.md:187 (reconstructor lifecycle contract)
            - Findings: POLICY-001 (PyTorch mandatory), CONFIG-001 (params.cfg already populated by caller)
            - ADR-003 Phase C3: execution_config controls Trainer kwargs (accelerator, deterministic, gradient_clip_val)
        """
        # B2.2: torch-optional imports with POLICY-001 compliant error messaging
        try:
            import torch
            import lightning.pytorch as L
            from ptycho_torch.model import PtychoPINN_Lightning
            from ptycho_torch.config_params import (
                DataConfig as PTDataConfig,
                ModelConfig as PTModelConfig,
                TrainingConfig as PTTrainingConfig,
                InferenceConfig as PTInferenceConfig
            )
        except ImportError as e:
            raise RuntimeError(
                "PyTorch backend requires torch>=2.2 and lightning. "
                "Install with: pip install -e .[torch]\n"
                "See docs/findings.md#policy-001 for PyTorch requirement policy."
            ) from e
    
        logger.info("_train_with_lightning orchestrating Lightning training")
        logger.info(f"Training config: nepochs={config.nepochs}, n_groups={config.n_groups}")
    
        # B2.1: Use config_factory to derive PyTorch configs with correct channel propagation
        # CRITICAL (Phase C4.D B2): Factory ensures C = gridsize**2 is propagated to
        # pt_model_config.C_model and pt_model_config.C_forward, preventing channel mismatch
        # when gridsize > 1 (see docs/findings.md#BUG-TF-001).
        from ptycho_torch.config_factory import create_training_payload
    
        # Build factory overrides from TrainingConfig fields
        # Factory requires n_groups in overrides dict; train_data_file and output_dir as positional
        # Note: Factory expects model_type in PyTorch naming ('Unsupervised'/'Supervised')
        #       but TrainingConfig uses TensorFlow naming ('pinn'/'supervised')
        mode_map = {'pinn': 'Unsupervised', 'supervised': 'Supervised'}
        factory_overrides = {
            'n_groups': config.n_groups,  # Required by factory validation
            'gridsize': config.model.gridsize,
            'model_type': mode_map.get(config.model.model_type, 'Unsupervised'),
            'amp_activation': config.model.amp_activation,
            'n_filters_scale': config.model.n_filters_scale,
            'nphotons': config.nphotons,
            'neighbor_count': config.neighbor_count,
            'max_epochs': config.nepochs,
            'batch_size': getattr(config, 'batch_size', 16),
            'subsample_seed': getattr(config, 'subsample_seed', None),
            'torch_loss_mode': getattr(config, 'torch_loss_mode', 'poisson'),
        }
    
        # Create payload with factory-derived PyTorch configs
        payload = create_training_payload(
            train_data_file=Path(config.train_data_file),
            output_dir=Path(getattr(config, 'output_dir', './outputs')),
            execution_config=execution_config,  # Pass through from caller
            overrides=factory_overrides
        )
    
        # Extract PyTorch configs from payload (gridsize  C propagation already applied)
        pt_data_config = payload.pt_data_config
        pt_model_config = payload.pt_model_config
        pt_training_config = payload.pt_training_config
    
        # CRITICAL: Supervised mode REQUIRES a compatible loss function (MAE)
        # The Lightning module expects loss_name to be defined, which only happens when:
        #   1. mode='Unsupervised' AND loss_function='Poisson'  sets loss_name='poisson_train'
        #   2. mode='Unsupervised' AND loss_function='MAE'  sets loss_name='mae_train'
        #   3. mode='Supervised' AND loss_function='MAE'  sets loss_name='mae_train'
        # Without this override, supervised mode with default loss_function='Poisson' causes
        # AttributeError: 'PtychoPINN_Lightning' object has no attribute 'loss_name'
        # See: ptycho_torch/model.py:1052-1066
        if pt_model_config.mode == 'Supervised' and pt_model_config.loss_function != 'MAE':
            logger.info(
                f"Backend override: supervised mode requires MAE loss "
                f"(was {pt_model_config.loss_function}), forcing loss_function='MAE'"
            )
            # Create new ModelConfig with corrected loss_function
            from dataclasses import replace
            pt_model_config = replace(pt_model_config, loss_function='MAE')
    
        # Create minimal InferenceConfig for Lightning module (training payload doesn't include it)
        pt_inference_config = PTInferenceConfig()
    
        # B2.4: Instantiate PtychoPINN_Lightning with factory-derived config objects
        model = PtychoPINN_Lightning(
            model_config=pt_model_config,
            data_config=pt_data_config,
            training_config=pt_training_config,
            inference_config=pt_inference_config
        )
    
        # Save hyperparameters so checkpoint can reconstruct module without external state
        model.save_hyperparameters()
    
        # B2.3: Build dataloaders via helper
        train_loader, val_loader = _build_lightning_dataloaders(
            train_container, test_container, config
        )
    
        # DATA-SUP-001: Supervised mode requires labeled data
        # Check if supervised mode is requested but training data lacks required labels
        if pt_model_config.mode == 'Supervised':
            # Inspect first batch to verify label keys exist
            try:
                first_batch = next(iter(train_loader))
                batch_dict = first_batch[0]  # Extract tensor dict from batch tuple
                if 'label_amp' not in batch_dict or 'label_phase' not in batch_dict:
                    raise RuntimeError(
                        f"Supervised mode (model_type='supervised') requires labeled datasets with "
                        f"'label_amp' and 'label_phase' keys, but training data lacks these fields. "
                        f"Either: (1) Use a labeled NPZ dataset (see ptycho_torch/notebooks/create_supervised_datasets.ipynb), "
                        f"or (2) Switch to PINN mode (--model_type pinn) for self-supervised physics-based training. "
                        f"See DATA-SUP-001 in docs/findings.md for details."
                    )
            except StopIteration:
                raise RuntimeError(
                    f"Training dataloader is empty. Check dataset path and n_groups configuration."
                )
    
        # B2.5: Configure Trainer with settings from config
        # C3.A3: Thread execution config values to Trainer kwargs
        output_dir = Path(getattr(config, 'output_dir', './outputs'))
        debug_mode = getattr(config, 'debug', False)
    
        # Import execution config defaults if not provided
        if execution_config is None:
            from ptycho.config.config import PyTorchExecutionConfig
            execution_config = PyTorchExecutionConfig()
            logger.info(f"PyTorchExecutionConfig auto-instantiated for Lightning training (accelerator resolved to '{execution_config.accelerator}')")
    
        # EB1.D: Configure checkpoint/early-stop callbacks (ADR-003 Phase EB1)
        callbacks = []
        if execution_config.enable_checkpointing:
            from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping
    
            # Determine if we have validation data to use val metrics
            has_validation = test_container is not None
    
            # EB2.B: Derive monitor metric from model.val_loss_name (ADR-003 Phase EB2)
            # The model's val_loss_name is dynamically constructed based on model_type and loss configuration
            # (e.g., 'poisson_val_Amp_loss' for PINN with amplitude loss, 'mae_val_Phase_loss' for supervised)
            # This ensures checkpoint/early-stop callbacks watch the correct logged metric
            if has_validation and hasattr(model, 'val_loss_name'):
                # Use the model's dynamic validation loss name
                monitor_metric = model.val_loss_name
            else:
                # Fall back to execution config default or train loss
                monitor_metric = execution_config.checkpoint_monitor_metric
                if 'val_' in monitor_metric and not has_validation:
                    # Fall back to train_loss if val metric requested but no validation data
                    monitor_metric = monitor_metric.replace('val_', 'train_')
    
            # Build checkpoint filename template using dynamic metric name
            # Format: epoch={epoch:02d}-<metric_short_name>={<full_metric_name>:.4f}
            if has_validation:
                # Extract short name for filename (remove '_loss' suffix if present)
                metric_short_name = monitor_metric.replace('_loss', '')
                filename_template = f'epoch={{epoch:02d}}-{metric_short_name}={{{monitor_metric}:.4f}}'
            else:
                filename_template = 'epoch={epoch:02d}'
    
            checkpoint_callback = ModelCheckpoint(
                dirpath=str(output_dir / "checkpoints"),
                filename=filename_template,
                monitor=monitor_metric,
                mode=execution_config.checkpoint_mode,
                save_top_k=execution_config.checkpoint_save_top_k,
                save_last=True,  # Always keep last checkpoint for recovery
                verbose=False,
            )
            callbacks.append(checkpoint_callback)
    
            # EarlyStopping callback (ADR-003 Phase EB1.D)
            # Only add early stopping if validation data is available (otherwise no metric to monitor)
            if has_validation:
                early_stop_callback = EarlyStopping(
                    monitor=monitor_metric,
                    mode=execution_config.checkpoint_mode,
                    patience=execution_config.early_stop_patience,
                    verbose=False,
                )
                callbacks.append(early_stop_callback)
    
        # Instantiate logger based on execution config (Phase EB3.B - ADR-003)
        lightning_logger = False  # Default: no logger
        if execution_config.logger_backend is not None:
            try:
                if execution_config.logger_backend == 'csv':
                    from lightning.pytorch.loggers import CSVLogger
                    lightning_logger = CSVLogger(
                        save_dir=str(output_dir),
                        name='lightning_logs',
                    )
                    logger.info(f"Enabled CSVLogger: metrics saved to {output_dir}/lightning_logs/")
                elif execution_config.logger_backend == 'tensorboard':
                    from lightning.pytorch.loggers import TensorBoardLogger
                    lightning_logger = TensorBoardLogger(
                        save_dir=str(output_dir),
                        name='lightning_logs',
                    )
                    logger.info(f"Enabled TensorBoardLogger: run `tensorboard --logdir={output_dir}/lightning_logs/`")
                elif execution_config.logger_backend == 'mlflow':
                    from lightning.pytorch.loggers import MLFlowLogger
                    lightning_logger = MLFlowLogger(
                        experiment_name=getattr(config, 'experiment_name', 'PtychoPINN'),
                        tracking_uri=str(output_dir / 'mlruns'),
                    )
                    logger.info(f"Enabled MLFlowLogger: tracking URI={output_dir}/mlruns")
                else:
                    logger.warning(
                        f"Unknown logger_backend '{execution_config.logger_backend}'. "
                        f"Falling back to logger=False. Supported: 'csv', 'tensorboard', 'mlflow'."
                    )
            except ImportError as e:
                logger.warning(
                    f"Failed to import Lightning logger '{execution_config.logger_backend}': {e}. "
                    f"Metrics logging disabled. Install the required package to enable logging."
                )
                lightning_logger = False
        else:
            logger.info("Logger disabled (logger_backend=None). Loss metrics will not be saved to disk.")
    
        # EXEC-ACCUM-001: Guard against manual optimization + gradient accumulation
        # Lightning's manual optimization (automatic_optimization=False) is incompatible with
        # Trainer(accumulate_grad_batches>1). The PtychoPINN_Lightning module uses manual optimization
        # for custom physics loss integration, so gradient accumulation must be disabled.
>       if not model.automatic_optimization and execution_config.accum_steps > 1:
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'StubLightningModule' object has no attribute 'automatic_optimization'

ptycho_torch/workflows/components.py:854: AttributeError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (10, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (10,)
xcoords shape: (10,)
ycoords shape: (10,)
xcoords_start shape: (10,)
ycoords_start shape: (10,)
----------------------------- Captured stderr call -----------------------------
INFO: Seed set to 42
------------------------------ Captured log call -------------------------------
INFO     lightning.fabric.utilities.seed:seed.py:57 Seed set to 42
____ TestTrainWithLightningGreen.test_execution_config_controls_determinism ____

self = <test_workflows_components.TestTrainWithLightningGreen object at 0x71a8a919e910>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x71a8642b4210>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'sigmoid', 'backend': 'pytorch', 'batch_size': 16, ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x71a8a874d710>

    def test_execution_config_controls_determinism(
        self,
        monkeypatch,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data
    ):
        """
        RED TEST: execution_config.deterministic MUST trigger Lightning deterministic mode.
    
        Requirement: ADR-003 Phase C3.C2  validate deterministic behaviour.
    
        Expected behavior:
        - When deterministic=True (default), Trainer receives deterministic=True
        - This triggers torch.use_deterministic_algorithms(True) and seeds
        - When deterministic=False, Trainer allows non-deterministic ops
    
        Test mechanism:
        - Supply execution_config with deterministic=True
        - Assert Trainer.__init__ received deterministic=True kwarg
        - Expect FAILURE because current stub doesn't wire deterministic flag
        """
        from ptycho_torch.workflows import components as torch_components
        from ptycho.config.config import PyTorchExecutionConfig
    
        # Spy to track Trainer.__init__ kwargs
        trainer_init_kwargs = {"called": False, "kwargs": None}
    
        class MockTrainer:
            def __init__(self, **kwargs):
                trainer_init_kwargs["called"] = True
                trainer_init_kwargs["kwargs"] = kwargs
    
            def fit(self, module, train_dataloaders=None, val_dataloaders=None, **kwargs):
                pass
    
        monkeypatch.setattr(
            "lightning.pytorch.Trainer",
            MockTrainer
        )
    
        # Monkeypatch Lightning module
        class StubLightningModule:
            def save_hyperparameters(self):
                pass
    
        monkeypatch.setattr(
            "ptycho_torch.model.PtychoPINN_Lightning",
            lambda *args, **kwargs: StubLightningModule()
        )
    
        # Create execution config with deterministic=True (default)
        exec_config = PyTorchExecutionConfig(
            deterministic=True,
            accelerator='cpu'
        )
    
        # Create minimal containers
        train_container = {"X": np.ones((10, 64, 64))}
    
        # Call _train_with_lightning
>       results = torch_components._train_with_lightning(
            train_container=train_container,
            test_container=None,
            config=minimal_training_config,
            execution_config=exec_config
        )

tests/torch/test_workflows_components.py:2629: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

train_container = {'X': array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., ...        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.]]])}
test_container = None
config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', obj..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
execution_config = PyTorchExecutionConfig(accelerator='cpu', strategy='auto', deterministic=True, gradient_clip_val=None, accum_steps=1, ...nt_mode='min', early_stop_patience=100, logger_backend='csv', inference_batch_size=None, middle_trim=0, pad_eval=False)

    def _train_with_lightning(
        train_container: 'PtychoDataContainerTorch',
        test_container: Optional['PtychoDataContainerTorch'],
        config: TrainingConfig,
        execution_config: Optional['PyTorchExecutionConfig'] = None
    ) -> Dict[str, Any]:
        """
        Orchestrate Lightning trainer execution for PyTorch model training.
    
        This function implements the Lightning training workflow per Phase D2.B blueprint:
        1. Derives PyTorch config objects from TensorFlow TrainingConfig
        2. Instantiates PtychoPINN_Lightning module with all four config dependencies
        3. Builds train/val dataloaders via _build_lightning_dataloaders helper
        4. Configures Lightning Trainer with checkpoint/logging settings (ADR-003 Phase C3)
        5. Executes training via trainer.fit()
        6. Returns structured results dict with history, containers, and module handle
    
        Args:
            train_container: Normalized training data container
            test_container: Optional normalized test data container
            config: TrainingConfig with training hyperparameters
            execution_config: Optional PyTorchExecutionConfig with runtime knobs (Phase C3.A2)
    
        Returns:
            Dict[str, Any]: Training results including:
                - history: Dict with train_loss and optional val_loss trajectories
                - train_container: Original training container
                - test_container: Original test container
                - models: Dict with 'diffraction_to_obj' (Lightning module) and 'autoencoder' (sentinel)
                          for dual-model bundle persistence per spec 4.6
    
        Raises:
            RuntimeError: If torch or lightning packages are not installed (POLICY-001)
    
        References:
            - Blueprint: plans/active/INTEGRATE-PYTORCH-001/reports/2025-10-18T020940Z/phase_d2_completion/phase_b2_implementation.md
            - Spec: specs/ptychodus_api_spec.md:187 (reconstructor lifecycle contract)
            - Findings: POLICY-001 (PyTorch mandatory), CONFIG-001 (params.cfg already populated by caller)
            - ADR-003 Phase C3: execution_config controls Trainer kwargs (accelerator, deterministic, gradient_clip_val)
        """
        # B2.2: torch-optional imports with POLICY-001 compliant error messaging
        try:
            import torch
            import lightning.pytorch as L
            from ptycho_torch.model import PtychoPINN_Lightning
            from ptycho_torch.config_params import (
                DataConfig as PTDataConfig,
                ModelConfig as PTModelConfig,
                TrainingConfig as PTTrainingConfig,
                InferenceConfig as PTInferenceConfig
            )
        except ImportError as e:
            raise RuntimeError(
                "PyTorch backend requires torch>=2.2 and lightning. "
                "Install with: pip install -e .[torch]\n"
                "See docs/findings.md#policy-001 for PyTorch requirement policy."
            ) from e
    
        logger.info("_train_with_lightning orchestrating Lightning training")
        logger.info(f"Training config: nepochs={config.nepochs}, n_groups={config.n_groups}")
    
        # B2.1: Use config_factory to derive PyTorch configs with correct channel propagation
        # CRITICAL (Phase C4.D B2): Factory ensures C = gridsize**2 is propagated to
        # pt_model_config.C_model and pt_model_config.C_forward, preventing channel mismatch
        # when gridsize > 1 (see docs/findings.md#BUG-TF-001).
        from ptycho_torch.config_factory import create_training_payload
    
        # Build factory overrides from TrainingConfig fields
        # Factory requires n_groups in overrides dict; train_data_file and output_dir as positional
        # Note: Factory expects model_type in PyTorch naming ('Unsupervised'/'Supervised')
        #       but TrainingConfig uses TensorFlow naming ('pinn'/'supervised')
        mode_map = {'pinn': 'Unsupervised', 'supervised': 'Supervised'}
        factory_overrides = {
            'n_groups': config.n_groups,  # Required by factory validation
            'gridsize': config.model.gridsize,
            'model_type': mode_map.get(config.model.model_type, 'Unsupervised'),
            'amp_activation': config.model.amp_activation,
            'n_filters_scale': config.model.n_filters_scale,
            'nphotons': config.nphotons,
            'neighbor_count': config.neighbor_count,
            'max_epochs': config.nepochs,
            'batch_size': getattr(config, 'batch_size', 16),
            'subsample_seed': getattr(config, 'subsample_seed', None),
            'torch_loss_mode': getattr(config, 'torch_loss_mode', 'poisson'),
        }
    
        # Create payload with factory-derived PyTorch configs
        payload = create_training_payload(
            train_data_file=Path(config.train_data_file),
            output_dir=Path(getattr(config, 'output_dir', './outputs')),
            execution_config=execution_config,  # Pass through from caller
            overrides=factory_overrides
        )
    
        # Extract PyTorch configs from payload (gridsize  C propagation already applied)
        pt_data_config = payload.pt_data_config
        pt_model_config = payload.pt_model_config
        pt_training_config = payload.pt_training_config
    
        # CRITICAL: Supervised mode REQUIRES a compatible loss function (MAE)
        # The Lightning module expects loss_name to be defined, which only happens when:
        #   1. mode='Unsupervised' AND loss_function='Poisson'  sets loss_name='poisson_train'
        #   2. mode='Unsupervised' AND loss_function='MAE'  sets loss_name='mae_train'
        #   3. mode='Supervised' AND loss_function='MAE'  sets loss_name='mae_train'
        # Without this override, supervised mode with default loss_function='Poisson' causes
        # AttributeError: 'PtychoPINN_Lightning' object has no attribute 'loss_name'
        # See: ptycho_torch/model.py:1052-1066
        if pt_model_config.mode == 'Supervised' and pt_model_config.loss_function != 'MAE':
            logger.info(
                f"Backend override: supervised mode requires MAE loss "
                f"(was {pt_model_config.loss_function}), forcing loss_function='MAE'"
            )
            # Create new ModelConfig with corrected loss_function
            from dataclasses import replace
            pt_model_config = replace(pt_model_config, loss_function='MAE')
    
        # Create minimal InferenceConfig for Lightning module (training payload doesn't include it)
        pt_inference_config = PTInferenceConfig()
    
        # B2.4: Instantiate PtychoPINN_Lightning with factory-derived config objects
        model = PtychoPINN_Lightning(
            model_config=pt_model_config,
            data_config=pt_data_config,
            training_config=pt_training_config,
            inference_config=pt_inference_config
        )
    
        # Save hyperparameters so checkpoint can reconstruct module without external state
        model.save_hyperparameters()
    
        # B2.3: Build dataloaders via helper
        train_loader, val_loader = _build_lightning_dataloaders(
            train_container, test_container, config
        )
    
        # DATA-SUP-001: Supervised mode requires labeled data
        # Check if supervised mode is requested but training data lacks required labels
        if pt_model_config.mode == 'Supervised':
            # Inspect first batch to verify label keys exist
            try:
                first_batch = next(iter(train_loader))
                batch_dict = first_batch[0]  # Extract tensor dict from batch tuple
                if 'label_amp' not in batch_dict or 'label_phase' not in batch_dict:
                    raise RuntimeError(
                        f"Supervised mode (model_type='supervised') requires labeled datasets with "
                        f"'label_amp' and 'label_phase' keys, but training data lacks these fields. "
                        f"Either: (1) Use a labeled NPZ dataset (see ptycho_torch/notebooks/create_supervised_datasets.ipynb), "
                        f"or (2) Switch to PINN mode (--model_type pinn) for self-supervised physics-based training. "
                        f"See DATA-SUP-001 in docs/findings.md for details."
                    )
            except StopIteration:
                raise RuntimeError(
                    f"Training dataloader is empty. Check dataset path and n_groups configuration."
                )
    
        # B2.5: Configure Trainer with settings from config
        # C3.A3: Thread execution config values to Trainer kwargs
        output_dir = Path(getattr(config, 'output_dir', './outputs'))
        debug_mode = getattr(config, 'debug', False)
    
        # Import execution config defaults if not provided
        if execution_config is None:
            from ptycho.config.config import PyTorchExecutionConfig
            execution_config = PyTorchExecutionConfig()
            logger.info(f"PyTorchExecutionConfig auto-instantiated for Lightning training (accelerator resolved to '{execution_config.accelerator}')")
    
        # EB1.D: Configure checkpoint/early-stop callbacks (ADR-003 Phase EB1)
        callbacks = []
        if execution_config.enable_checkpointing:
            from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping
    
            # Determine if we have validation data to use val metrics
            has_validation = test_container is not None
    
            # EB2.B: Derive monitor metric from model.val_loss_name (ADR-003 Phase EB2)
            # The model's val_loss_name is dynamically constructed based on model_type and loss configuration
            # (e.g., 'poisson_val_Amp_loss' for PINN with amplitude loss, 'mae_val_Phase_loss' for supervised)
            # This ensures checkpoint/early-stop callbacks watch the correct logged metric
            if has_validation and hasattr(model, 'val_loss_name'):
                # Use the model's dynamic validation loss name
                monitor_metric = model.val_loss_name
            else:
                # Fall back to execution config default or train loss
                monitor_metric = execution_config.checkpoint_monitor_metric
                if 'val_' in monitor_metric and not has_validation:
                    # Fall back to train_loss if val metric requested but no validation data
                    monitor_metric = monitor_metric.replace('val_', 'train_')
    
            # Build checkpoint filename template using dynamic metric name
            # Format: epoch={epoch:02d}-<metric_short_name>={<full_metric_name>:.4f}
            if has_validation:
                # Extract short name for filename (remove '_loss' suffix if present)
                metric_short_name = monitor_metric.replace('_loss', '')
                filename_template = f'epoch={{epoch:02d}}-{metric_short_name}={{{monitor_metric}:.4f}}'
            else:
                filename_template = 'epoch={epoch:02d}'
    
            checkpoint_callback = ModelCheckpoint(
                dirpath=str(output_dir / "checkpoints"),
                filename=filename_template,
                monitor=monitor_metric,
                mode=execution_config.checkpoint_mode,
                save_top_k=execution_config.checkpoint_save_top_k,
                save_last=True,  # Always keep last checkpoint for recovery
                verbose=False,
            )
            callbacks.append(checkpoint_callback)
    
            # EarlyStopping callback (ADR-003 Phase EB1.D)
            # Only add early stopping if validation data is available (otherwise no metric to monitor)
            if has_validation:
                early_stop_callback = EarlyStopping(
                    monitor=monitor_metric,
                    mode=execution_config.checkpoint_mode,
                    patience=execution_config.early_stop_patience,
                    verbose=False,
                )
                callbacks.append(early_stop_callback)
    
        # Instantiate logger based on execution config (Phase EB3.B - ADR-003)
        lightning_logger = False  # Default: no logger
        if execution_config.logger_backend is not None:
            try:
                if execution_config.logger_backend == 'csv':
                    from lightning.pytorch.loggers import CSVLogger
                    lightning_logger = CSVLogger(
                        save_dir=str(output_dir),
                        name='lightning_logs',
                    )
                    logger.info(f"Enabled CSVLogger: metrics saved to {output_dir}/lightning_logs/")
                elif execution_config.logger_backend == 'tensorboard':
                    from lightning.pytorch.loggers import TensorBoardLogger
                    lightning_logger = TensorBoardLogger(
                        save_dir=str(output_dir),
                        name='lightning_logs',
                    )
                    logger.info(f"Enabled TensorBoardLogger: run `tensorboard --logdir={output_dir}/lightning_logs/`")
                elif execution_config.logger_backend == 'mlflow':
                    from lightning.pytorch.loggers import MLFlowLogger
                    lightning_logger = MLFlowLogger(
                        experiment_name=getattr(config, 'experiment_name', 'PtychoPINN'),
                        tracking_uri=str(output_dir / 'mlruns'),
                    )
                    logger.info(f"Enabled MLFlowLogger: tracking URI={output_dir}/mlruns")
                else:
                    logger.warning(
                        f"Unknown logger_backend '{execution_config.logger_backend}'. "
                        f"Falling back to logger=False. Supported: 'csv', 'tensorboard', 'mlflow'."
                    )
            except ImportError as e:
                logger.warning(
                    f"Failed to import Lightning logger '{execution_config.logger_backend}': {e}. "
                    f"Metrics logging disabled. Install the required package to enable logging."
                )
                lightning_logger = False
        else:
            logger.info("Logger disabled (logger_backend=None). Loss metrics will not be saved to disk.")
    
        # EXEC-ACCUM-001: Guard against manual optimization + gradient accumulation
        # Lightning's manual optimization (automatic_optimization=False) is incompatible with
        # Trainer(accumulate_grad_batches>1). The PtychoPINN_Lightning module uses manual optimization
        # for custom physics loss integration, so gradient accumulation must be disabled.
>       if not model.automatic_optimization and execution_config.accum_steps > 1:
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'StubLightningModule' object has no attribute 'automatic_optimization'

ptycho_torch/workflows/components.py:854: AttributeError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (10, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (10,)
xcoords shape: (10,)
ycoords shape: (10,)
xcoords_start shape: (10,)
ycoords_start shape: (10,)
----------------------------- Captured stderr call -----------------------------
INFO: Seed set to 42
------------------------------ Captured log call -------------------------------
INFO     lightning.fabric.utilities.seed:seed.py:57 Seed set to 42
_______ TestLightningExecutionConfig.test_trainer_receives_accumulation ________

self = <test_workflows_components.TestLightningExecutionConfig object at 0x71a8a91a1210>
minimal_training_config_with_val = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='silu', object.../test_trainer_receives_accumula0/outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x71a8642b4050>

    def test_trainer_receives_accumulation(self, minimal_training_config_with_val, monkeypatch):
        """
        RED Test: Verify Lightning Trainer receives accumulate_grad_batches from execution config.
    
        Expected RED Failure:
        - Trainer not receiving accum_steps from execution config
        OR
        - accumulate_grad_batches not passed to Trainer kwargs
    
        Resolution (GREEN):
        - _train_with_lightning should pass execution_config.accum_steps to Trainer(accumulate_grad_batches=...)
        """
        from ptycho.config.config import PyTorchExecutionConfig
        from unittest.mock import patch, MagicMock
    
        try:
            import lightning.pytorch as L
        except ImportError:
            pytest.skip("Lightning not available")
    
        # Create execution config with custom accumulation
        exec_config = PyTorchExecutionConfig(
            accum_steps=4,  # Override default (1)
            accelerator='cpu',
            deterministic=True,
            num_workers=0,
            enable_checkpointing=False,  # Disable callbacks for simpler mocking
        )
    
        # Mock Trainer to spy on kwargs
        mock_trainer_cls = MagicMock(spec=L.Trainer)
        mock_trainer_instance = MagicMock()
        mock_trainer_cls.return_value = mock_trainer_instance
    
        # Mock data containers
        mock_train_container = MagicMock()
        mock_test_container = MagicMock()
    
        from ptycho_torch.workflows.components import _train_with_lightning
    
        # Patch Trainer at import site
        with patch('lightning.pytorch.Trainer', mock_trainer_cls):
            try:
                _train_with_lightning(
                    train_container=mock_train_container,
                    test_container=mock_test_container,
                    config=minimal_training_config_with_val,
                    execution_config=exec_config,
                )
            except Exception:
                pass  # May fail during training; we only care about Trainer instantiation
    
        # GREEN Phase Assertion:
        # Trainer should receive accumulate_grad_batches from execution_config.accum_steps
>       assert mock_trainer_cls.called, "Trainer not instantiated"
E       AssertionError: Trainer not instantiated
E       assert False
E        +  where False = <MagicMock spec='Trainer' id='124968049004240'>.called

tests/torch/test_workflows_components.py:3137: AssertionError
----------------------------- Captured stdout call -----------------------------
Decoder Block 1: No attention module added.
Decoder Block 2: No attention module added.
Decoder Block 1: No attention module added.
Decoder Block 2: No attention module added.
----------------------------- Captured stderr call -----------------------------
INFO: Seed set to 42
------------------------------ Captured log call -------------------------------
INFO     lightning.fabric.utilities.seed:seed.py:57 Seed set to 42
=============================== warnings summary ===============================
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_num_workers_flag_roundtrip
  /home/ollie/Documents/PtychoPINN/ptycho_torch/train.py:659: UserWarning: Deterministic mode with num_workers=4 may cause performance degradation. Consider setting --num-workers 0 for reproducibility.
    execution_config = build_execution_config_from_args(args, mode='training')

tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_bundle_persistence
  /home/ollie/Documents/PtychoPINN/ptycho_torch/config_factory.py:563: UserWarning: Error reading probeGuess from /tmp/pytest-of-ollie/pytest-716/test_bundle_persistence0/train.npz: No data left in file. Using fallback N=64.
    warnings.warn(

tests/torch/test_cli_train_torch.py: 1 warning
tests/torch/test_config_factory.py: 22 warnings
tests/torch/test_workflows_components.py: 12 warnings
  /home/ollie/Documents/PtychoPINN/ptycho_torch/config_factory.py:265: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    tf_training_config = to_training_config(

tests/torch/test_cli_train_torch.py: 1 warning
tests/torch/test_config_factory.py: 25 warnings
tests/torch/test_workflows_components.py: 12 warnings
  /home/ollie/Documents/PtychoPINN/ptycho_torch/config_factory.py:624: UserWarning: params.cfg already populated. Set force=True to overwrite existing values.
    warnings.warn(

tests/torch/test_config_bridge.py::TestConfigBridgeMVP::test_mvp_config_bridge_populates_params_cfg
  /home/ollie/Documents/PtychoPINN/tests/torch/test_config_bridge.py:100: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    spec_train = config_bridge.to_training_config(

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_direct_fields[batch_size-direct]
  /home/ollie/Documents/PtychoPINN/tests/torch/test_config_bridge.py:235: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    tf_train = config_bridge.to_training_config(

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_transform_fields[nepochs-rename]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_transform_fields[nll_weight-true]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_transform_fields[nll_weight-false]
  /home/ollie/Documents/PtychoPINN/tests/torch/test_config_bridge.py:298: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    tf_train = config_bridge.to_training_config(

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[mae_weight-default]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[mae_weight-override]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[realspace_mae_weight-default]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[realspace_weight-default]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[positions_provided-default]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[probe_trainable-default]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[sequential_sampling-default]
  /home/ollie/Documents/PtychoPINN/tests/torch/test_config_bridge.py:367: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    tf_train = config_bridge.to_training_config(

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_default_divergence_detection[nphotons-divergence]
  /home/ollie/Documents/PtychoPINN/tests/torch/test_config_bridge.py:494: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    tf_train = config_bridge.to_training_config(

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_train_data_file_required_error
  /home/ollie/Documents/PtychoPINN/tests/torch/test_config_bridge.py:576: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    config_bridge.to_training_config(

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_nphotons_override_passes_validation
  /home/ollie/Documents/PtychoPINN/tests/torch/test_config_bridge.py:667: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    tf_train = config_bridge.to_training_config(

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_n_subsample_missing_override_uses_none
  /home/ollie/Documents/PtychoPINN/tests/torch/test_config_bridge.py:708: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    tf_train = config_bridge.to_training_config(

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_n_subsample_explicit_override
  /home/ollie/Documents/PtychoPINN/tests/torch/test_config_bridge.py:743: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    tf_train = config_bridge.to_training_config(

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_subsample_seed_from_dataconfig
  /home/ollie/Documents/PtychoPINN/tests/torch/test_config_bridge.py:854: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    tf_train = config_bridge.to_training_config(

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_subsample_seed_override
  /home/ollie/Documents/PtychoPINN/tests/torch/test_config_bridge.py:883: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    tf_train = config_bridge.to_training_config(

tests/torch/test_config_factory.py::TestProbeSizeInference::test_infer_probe_size_missing_file_fallback
  /home/ollie/Documents/PtychoPINN/ptycho_torch/config_factory.py:557: UserWarning: Data file /nonexistent/data.npz not found. Using fallback N=64.
    warnings.warn(

tests/torch/test_lightning_checkpoint.py::TestLightningCheckpointSerialization::test_checkpoint_contains_hyperparameters
tests/torch/test_lightning_checkpoint.py::TestLightningCheckpointSerialization::test_load_from_checkpoint_without_kwargs
tests/torch/test_lightning_checkpoint.py::TestLightningCheckpointSerialization::test_checkpoint_configs_are_serializable
  /home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.

tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_lightning_poisson_count_contract
  /home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/core/module.py:449: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`

tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_lightning_training_respects_gridsize
tests/torch/test_workflows_components.py::TestTrainWithLightningRed::test_train_with_lightning_instantiates_module
  /home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:751: Checkpoint directory /home/ollie/Documents/PtychoPINN/training_outputs/checkpoints exists and is not empty.

tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_lightning_training_respects_gridsize
tests/torch/test_workflows_components.py::TestTrainWithLightningRed::test_train_with_lightning_instantiates_module
  /home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.

tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_lightning_training_respects_gridsize
  /home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:467: `ModelCheckpoint(monitor='train_loss')` could not find the monitored key in the returned metrics: ['epoch', 'step']. HINT: Did you call `log('train_loss', value)` in the `LightningModule`?

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
SKIPPED [1] tests/test_benchmark_throughput.py:11: scripts/benchmark_inference_throughput.py not found. This is a pre-existing broken test dependency. Add to fix_plan.md for future resolution.
SKIPPED [1] tests/test_run_baseline.py:4: tests/test_utilities.py not found. This is a pre-existing broken test dependency. Add to fix_plan.md for future resolution.
SKIPPED [1] tests/io/test_ptychodus_interop_h5.py:50: ptychodus plugins not available; see specs/ptychodus_api_spec.md
SKIPPED [1] tests/test_generic_loader.py:46: Data loading failed:
SKIPPED [1] ../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/unittest.py:385: Deprecated: generate_simulated_data API changed from (obj,probe,nimages) to (config,obj,probe) and memoization disabled
SKIPPED [1] tests/test_tf_helper.py:199: TensorFlow Addons removed in TF 2.19 migration
SKIPPED [1] tests/test_tf_helper_edge_aware.py:47: tensorflow_addons not available
SKIPPED [1] tests/test_tf_helper_edge_aware.py:86: tensorflow_addons not available
SKIPPED [1] tests/test_tf_helper_edge_aware.py:198: tensorflow_addons not available
SKIPPED [1] tests/test_tf_helper_edge_aware.py:169: tensorflow_addons not available
SKIPPED [1] tests/test_tf_helper_edge_aware.py:257: tensorflow_addons not available
SKIPPED [1] tests/test_tf_helper_edge_aware.py:227: tensorflow_addons not available
SKIPPED [1] tests/torch/test_execution_config_defaults.py:162: Test requires CPU-only host to verify fallback warning
SKIPPED [2] tests/torch/test_integration_workflow_torch.py: Migrated to pytest-native test_run_pytorch_train_save_load_infer
SKIPPED [1] tests/torch/test_tf_helper.py:64: torch tf_helper module not available
SKIPPED [1] tests/torch/test_tf_helper.py:57: torch tf_helper module not available
SKIPPED [1] tests/torch/test_tf_helper.py:74: torch tf_helper module not available - tests would fail
FAILED tests/study/test_phase_g_dense_orchestrator.py::test_run_phase_g_dense_collect_only_generates_commands
FAILED tests/study/test_phase_g_dense_orchestrator.py::test_run_phase_g_dense_post_verify_hooks
FAILED tests/study/test_phase_g_dense_orchestrator.py::test_run_phase_g_dense_exec_invokes_reporting_helper
FAILED tests/study/test_phase_g_dense_orchestrator.py::test_run_phase_g_dense_exec_prints_highlights_preview
FAILED tests/study/test_phase_g_dense_orchestrator.py::test_run_phase_g_dense_exec_runs_analyze_digest
FAILED tests/test_integration_baseline_gs2.py::TestBaselineGridsize2Integration::test_baseline_gridsize2_end_to_end
FAILED tests/test_integration_workflow.py::TestFullWorkflow::test_train_save_load_infer_cycle
FAILED tests/test_nphotons_metadata_integration.py::TestNphotonsMetadataIntegration::test_end_to_end_workflow_consistency
FAILED tests/test_nphotons_metadata_integration.py::TestNphotonsMetadataIntegration::test_metadata_persistence_single_nphotons
FAILED tests/test_nphotons_metadata_integration.py::TestNphotonsMetadataIntegration::test_training_with_mismatched_config_warns_but_continues
FAILED tests/test_workflow_components.py::TestLoadInferenceBundle::test_load_valid_model_directory
FAILED tests/torch/test_cli_inference_torch.py::TestInferenceCLI::test_accelerator_flag_roundtrip
FAILED tests/torch/test_workflows_components.py::TestWorkflowsComponentsScaffold::test_run_cdi_example_calls_update_legacy_dict
FAILED tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_train_cdi_model_torch_invokes_lightning
FAILED tests/torch/test_workflows_components.py::TestWorkflowsComponentsRun::test_run_cdi_example_invokes_training
FAILED tests/torch/test_workflows_components.py::TestWorkflowsComponentsRun::test_run_cdi_example_persists_models
FAILED tests/torch/test_workflows_components.py::TestTrainWithLightningRed::test_train_with_lightning_runs_trainer_fit
FAILED tests/torch/test_workflows_components.py::TestTrainWithLightningRed::test_train_with_lightning_returns_models_dict
FAILED tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_run_cdi_example_torch_do_stitching_delegates_to_reassemble
FAILED tests/torch/test_workflows_components.py::TestTrainWithLightningGreen::test_execution_config_overrides_trainer
FAILED tests/torch/test_workflows_components.py::TestTrainWithLightningGreen::test_execution_config_controls_determinism
FAILED tests/torch/test_workflows_components.py::TestLightningExecutionConfig::test_trainer_receives_accumulation
===== 22 failed, 480 passed, 18 skipped, 104 warnings in 181.72s (0:03:01) =====
