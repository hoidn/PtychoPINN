============================= test session starts ==============================
platform linux -- Python 3.11.13, pytest-8.4.1, pluggy-1.6.0 -- /home/ollie/miniconda3/envs/ptycho311/bin/python3.11
cachedir: .pytest_cache
PyTorch: available
PyTorch version: 2.8.0+cu128
rootdir: /home/ollie/Documents/PtychoPINN
configfile: pyproject.toml
plugins: anyio-4.9.0
collecting ... collected 2 items

tests/study/test_dose_overlap_comparison.py::test_pinn_reconstruction_reassembles_batched_predictions PASSED [ 50%]
tests/study/test_dose_overlap_comparison.py::test_pinn_reconstruction_reassembles_full_train_split FAILED [100%]

=================================== FAILURES ===================================
____________ test_pinn_reconstruction_reassembles_full_train_split _____________

    def test_pinn_reconstruction_reassembles_full_train_split():
        """
        Test that ReassemblePatchesLayer can handle large dense datasets (>=5k patches)
        without Translation layer shape mismatches.
    
        Exit criteria:
        - Layer processes >=5k patches successfully via batched reassembly path
        - Translation layer receives matching patch/offset tensor shapes in each batch
        - Output shape is correct (1, padded_size, padded_size, 1) for complex tensors
        - No ValueError from Translation layer about mismatched input shapes
    
        This regression test guards the fix in ptycho/custom_layers.py:ReassemblePatchesLayer
        that switches to batched processing for large patch counts to avoid the Translation
        ValueError that blocked dense Phase G comparisons.
        """
        import tensorflow as tf
        import numpy as np
        from ptycho.custom_layers import ReassemblePatchesLayer
        from ptycho import params
    
        # Simulate dense dataset dimensions matching Phase G dense train split
        # Dense fly64 has ~5088 patches (B=159 batches of 32 patches each, C=4 channels)
        B = 159  # Number of prediction batches
        N = 138  # Patch size (fly64 reconstruction size)
        C = 4    # gridsizeÂ² = 2Â² = 4 channels for overlapping patches
    
        # Create synthetic patches in channel format (B, N, N, C)
        # Use small random values to keep memory reasonable
        patches = tf.random.normal((B, N, N, C), dtype=tf.float32)
    
        # Create synthetic positions in channel format (B, 1, 2, C)
        # Positions should be in range that makes sense for reassembly
        positions = tf.random.uniform((B, 1, 2, C), minval=-50, maxval=50, dtype=tf.float32)
    
        # Set up params.cfg for the layer (required by reassembly helpers)
        params.set('gridsize', 2)
        params.set('N', N)
        params.set('offset', 10)  # Required by get_bigN()
        params.set('max_position_jitter', 0)  # Required by get_padded_size()
        # Use get_padded_size() to match what tf_helper uses internally
        from ptycho.params import get_padded_size
        padded_size = get_padded_size()
    
        # Create layer with batch_size=64 (default) to trigger batching for B*C=636 patches
        layer = ReassemblePatchesLayer(batch_size=64)
    
        try:
            # Call layer - this should use batched reassembly internally
            # since B*C = 159*4 = 636 > 64
>           result = layer([patches, positions])
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/study/test_dose_overlap_comparison.py:796: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122: in error_handler
    raise e.with_traceback(filtered_tb) from None
ptycho/custom_layers.py:176: in call
    return tf.cond(
ptycho/custom_layers.py:167: in use_batched
    return hh.reassemble_patches(patches,
ptycho/tf_helper.py:1073: in reassemble_patches
    assembled_real = fn_reassemble_real(real, average = average, **kwargs) / mk_norm(real,
ptycho/tf_helper.py:1050: in mk_norm
    assembled_ones = fn_reassemble_real(ones, average = False)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ptycho/tf_helper.py:678: in newf
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
ptycho/tf_helper.py:1322: in reassemble_patches_position_batched_real
    return _reassemble_position_batched(imgs, input_positions, padded_size, batch_size, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ptycho/tf_helper.py:1004: in _reassemble_position_batched
    return tf.cond(
ptycho/tf_helper.py:989: in batched_approach
    _, final_canvas = tf.while_loop(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

i = <tf.Tensor: shape=(), dtype=int32, numpy=0>
canvas = <tf.Tensor: shape=(1, 148, 148, 1), dtype=float32, numpy=
array([[[[0.],
         [0.],
         [0.],
         ...,
 ...     [[0.],
         [0.],
         [0.],
         ...,
         [0.],
         [0.],
         [0.]]]], dtype=float32)>

    def body(i, canvas):
        # Calculate batch boundaries
        start_idx = i
        end_idx = tf.minimum(i + batch_size, num_patches)
    
        # Extract batch - handle case where batch might be smaller than batch_size
        batch_imgs = imgs_flat[start_idx:end_idx]
        batch_offsets = offsets_flat[start_idx:end_idx]
    
        # Ensure offsets have the right shape: (batch_size, 2)
        batch_offsets = tf.ensure_shape(batch_offsets, [None, 2])
    
        # Only process if we have images in the batch
        def process_batch():
            batch_imgs_padded = pad_patches(batch_imgs, padded_size)
            batch_translated = Translation(jitter_stddev=0.0, use_xla=should_use_xla())([batch_imgs_padded, -batch_offsets])
            # Sum directly over batch dimension to accumulate onto canvas
            # Shape: (batch_size_actual, padded_size?, padded_size?, 1) -> (1, padded_size?, padded_size?, 1)
            # Note: Translation may slightly change dimensions due to interpolation
            batch_summed = tf.reduce_sum(batch_translated, axis=0, keepdims=True)
            # Ensure shape matches canvas by padding or cropping if needed
            canvas_h = tf.shape(canvas)[1]
            canvas_w = tf.shape(canvas)[2]
            result_h = tf.shape(batch_summed)[1]
            result_w = tf.shape(batch_summed)[2]
    
            # Pad or crop to match canvas size
            # If result is smaller, pad it; if larger, crop it
            def pad_to_canvas():
                pad_h = (canvas_h - result_h) // 2
                pad_w = (canvas_w - result_w) // 2
                return tf.pad(batch_summed, [[0, 0], [pad_h, pad_h], [pad_w, pad_w], [0, 0]])
    
            def crop_to_canvas():
                crop_h = (result_h - canvas_h) // 2
                crop_w = (result_w - canvas_w) // 2
                return batch_summed[:, crop_h:crop_h+canvas_h, crop_w:crop_w+canvas_w, :]
    
            batch_summed = tf.cond(
                tf.less(result_h, canvas_h),
                pad_to_canvas,
                lambda: tf.cond(
                    tf.greater(result_h, canvas_h),
                    crop_to_canvas,
                    lambda: batch_summed
                )
            )
            return batch_summed
    
        def skip_batch():
            return tf.zeros_like(canvas)
    
        # Only process if we have a non-empty batch
        batch_result = tf.cond(
            tf.greater(end_idx, start_idx),
            process_batch,
            skip_batch
        )
    
>       return end_idx, canvas + batch_result
                        ^^^^^^^^^^^^^^^^^^^^^
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling ReassemblePatchesLayer.call().
E       
E       [1m{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} required broadcastable shapes [Op:AddV2] name: [0m
E       
E       Arguments received by ReassemblePatchesLayer.call():
E         â€¢ inputs=['tf.Tensor(shape=(159, 138, 138, 4), dtype=float32)', 'tf.Tensor(shape=(159, 1, 2, 4), dtype=float32)']

ptycho/tf_helper.py:987: InvalidArgumentError
----------------------------- Captured stdout call -----------------------------
DEBUG: Setting gridsize to 2 in params
DEBUG: Setting N to 138 in params
DEBUG: Setting offset to 10 in params
DEBUG: Setting max_position_jitter to 0 in params
----------------------------- Captured stderr call -----------------------------
I0000 00:00:1763055176.205460 2193739 gpu_process_state.cc:208] Using CUDA malloc Async allocator for GPU: 0
I0000 00:00:1763055176.207023 2193739 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22259 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:04:00.0, compute capability: 8.6
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1763055177.002180 2193739 service.cc:152] XLA service 0x13915930 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1763055177.002203 2193739 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6
2025-11-13 09:32:57.016170: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
I0000 00:00:1763055177.036906 2193739 cuda_dnn.cc:529] Loaded cuDNN version 91002
I0000 00:00:1763055177.609562 2193739 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2025-11-13 09:32:58.867307: W tensorflow/core/framework/op_kernel.cc:1844] INVALID_ARGUMENT: required broadcastable shapes
2025-11-13 09:32:58.867326: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: required broadcastable shapes
=========================== short test summary info ============================
FAILED tests/study/test_dose_overlap_comparison.py::test_pinn_reconstruction_reassembles_full_train_split - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling ReassemblePatchesLayer.call().

[1m{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} required broadcastable shapes [Op:AddV2] name: [0m

Arguments received by ReassemblePatchesLayer.call():
  â€¢ inputs=['tf.Tensor(shape=(159, 138, 138, 4), dtype=float32)', 'tf.Tensor(shape=(159, 1, 2, 4), dtype=float32)']
========================= 1 failed, 1 passed in 6.70s ==========================
