2025-11-12 18:40:48.700328: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1763001648.711696 1735778 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1763001648.715574 1735778 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1763001648.725764 1735778 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1763001648.725780 1735778 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1763001648.725782 1735778 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1763001648.725784 1735778 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-11-12 18:40:48.728529: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-12 18:40:51,664 - INFO - Configuration setup complete
2025-11-12 18:40:51,664 - INFO - Final configuration: TrainingConfig(model=ModelConfig(N=64, gridsize=1, n_filters_scale=2, model_type='supervised', amp_activation='sigmoid', object_big=True, probe_big=True, probe_mask=False, pad_object=True, probe_scale=4.0, gaussian_smoothing_sigma=0.0), train_data_file=PosixPath('tike_outputs/fly001_final_downsampled/fly001_final_downsampled_data_transposed.npz'), test_data_file=None, batch_size=16, nepochs=2, mae_weight=0.0, nll_weight=1.0, realspace_mae_weight=0.0, realspace_weight=0.0, nphotons=1000000000.0, n_groups=100, n_images=100, n_subsample=None, subsample_seed=None, neighbor_count=4, enable_oversampling=False, neighbor_pool_size=None, positions_provided=True, probe_trainable=False, intensity_scale_trainable=True, output_dir=PosixPath('plans/active/INTEGRATE-PYTORCH-001/reports/2025-11-13T150000Z/parity_reactivation/cli/pytorch_cli_smoke_training/train_outputs'), sequential_sampling=False, backend='pytorch')
2025-11-12 18:40:51,664 - INFO - Legacy mode: using 100 groups (gridsize=1)
2025-11-12 18:40:51,664 - INFO - Starting training with n_subsample=100, n_groups=100, stitching=disabled
2025-11-12 18:40:51,664 - INFO - Loading data from tike_outputs/fly001_final_downsampled/fly001_final_downsampled_data_transposed.npz with n_images=100, n_subsample=100
2025-11-12 18:40:51,976 - INFO - Independent sampling: subsampling 100 images from 2000 total
2025-11-12 18:40:51,976 - INFO - Randomly subsampled 100 images
diff3d shape: (100, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (100,)
objectGuess shape: (232, 232)
xcoords shape: (100,)
ycoords shape: (100,)
xcoords_start shape: (100,)
ycoords_start shape: (100,)
2025-11-12 18:40:53,313 - INFO - PyTorch execution config built: accelerator=cpu, num_workers=0, learning_rate=0.0005, logger_backend=csv
2025-11-12 18:40:53,313 - INFO - Backend dispatcher: params.cfg synchronized with TrainingConfig (backend=pytorch)
2025-11-12 18:40:53,313 - INFO - Backend dispatcher: routing to PyTorch workflow (ptycho_torch.workflows.components)
2025-11-12 18:40:53,313 - INFO - PyTorch workflow: params.cfg synchronized with TrainingConfig
2025-11-12 18:40:53,313 - INFO - Invoking PyTorch training orchestration via train_cdi_model_torch
2025-11-12 18:40:53,313 - INFO - Normalizing training data via _ensure_container
diff3d shape: (100, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (100,)
objectGuess shape: (232, 232)
xcoords shape: (100,)
ycoords shape: (100,)
xcoords_start shape: (100,)
ycoords_start shape: (100,)
2025-11-12 18:40:53,313 - INFO - [OVERSAMPLING DEBUG] generate_grouped_data called with: nsamples=100, n_points=100, C=1, K=4
2025-11-12 18:40:53,313 - INFO - [OVERSAMPLING DEBUG] Parameters: gridsize=1, N=64, sequential_sampling=False
2025-11-12 18:40:53,313 - INFO - [OVERSAMPLING DEBUG] Oversampling flags: enable_oversampling=False, neighbor_pool_size=None, effective_K=4
2025-11-12 18:40:53,313 - INFO - [OVERSAMPLING DEBUG] Oversampling check: nsamples > n_points = 100 > 100 = False
2025-11-12 18:40:53,313 - INFO - [OVERSAMPLING DEBUG] Oversampling check: C > 1 = 1 > 1 = False
2025-11-12 18:40:53,313 - INFO - [OVERSAMPLING DEBUG] needs_oversampling = False
DEBUG: nsamples: 100, gridsize: 1 (using efficient random sample-then-group strategy)
2025-11-12 18:40:53,313 - INFO - Using efficient random sampling strategy for gridsize=1
2025-11-12 18:40:53,313 - INFO - [OVERSAMPLING DEBUG] Taking efficient branch: standard sample-then-group
2025-11-12 18:40:53,313 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently called with: nsamples=100, K=4, C=1
2025-11-12 18:40:53,313 - INFO - Generating 100 groups efficiently from 100 points (K=4, C=1)
2025-11-12 18:40:53,313 - INFO - [OVERSAMPLING DEBUG] Standard case: using 100 groups from 100 points
2025-11-12 18:40:53,313 - INFO - [OVERSAMPLING DEBUG] Using all 100 points as seeds (no sampling needed)
2025-11-12 18:40:53,313 - INFO - Using all 100 points as seeds
2025-11-12 18:40:53,313 - INFO - Using seed indices directly for C=1 (gridsize=1) - no neighbor search needed
2025-11-12 18:40:53,313 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently completed: generated 100 groups
2025-11-12 18:40:53,313 - INFO - Successfully generated 100 groups with shape (100, 1)
2025-11-12 18:40:53,313 - INFO - [OVERSAMPLING DEBUG] Generated 100 groups in total
2025-11-12 18:40:53,313 - INFO - Generated 100 groups efficiently
I0000 00:00:1763001653.447353 1735778 gpu_process_state.cc:208] Using CUDA malloc Async allocator for GPU: 0
I0000 00:00:1763001653.448578 1735778 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22259 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:04:00.0, compute capability: 8.6
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1763001654.237267 1735778 service.cc:152] XLA service 0x3fcf5ba0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1763001654.237286 1735778 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6
2025-11-12 18:40:54.252093: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
I0000 00:00:1763001654.269565 1735778 cuda_dnn.cc:529] Loaded cuDNN version 91002
I0000 00:00:1763001654.401666 1735778 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
INFO: 'Y' array not found. Generating ground truth patches from 'objectGuess' as a fallback.
neighbor-sampled diffraction shape (100, 64, 64, 1)
2025-11-12 18:40:54,807 - INFO - Delegating to Lightning trainer via _train_with_lightning
2025-11-12 18:40:55,983 - INFO - _train_with_lightning orchestrating Lightning training
2025-11-12 18:40:55,983 - INFO - Training config: nepochs=2, n_groups=100
/home/ollie/Documents/PtychoPINN/ptycho_torch/config_factory.py:257: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
  tf_training_config = to_training_config(
/home/ollie/Documents/PtychoPINN/ptycho_torch/config_factory.py:613: UserWarning: params.cfg already populated. Set force=True to overwrite existing values.
  warnings.warn(
2025-11-12 18:40:55,985 - INFO - Backend override: supervised mode requires MAE loss (was Poisson), forcing loss_function='MAE'
INFO: Seed set to 42
Decoder Block 1: No attention module added.
Decoder Block 2: No attention module added.
Decoder Block 1: No attention module added.
Decoder Block 2: No attention module added.
2025-11-12 18:40:55,998 - INFO - Seed set to 42
2025-11-12 18:40:56,000 - INFO - Enabled CSVLogger: metrics saved to plans/active/INTEGRATE-PYTORCH-001/reports/2025-11-13T150000Z/parity_reactivation/cli/pytorch_cli_smoke_training/train_outputs/lightning_logs/
2025-11-12 18:40:56,027 - INFO - GPU available: True (cuda), used: False
2025-11-12 18:40:56,028 - INFO - TPU available: False, using: 0 TPU cores
2025-11-12 18:40:56,028 - INFO - HPU available: False, using: 0 HPUs
/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.
2025-11-12 18:40:56,028 - INFO - Starting Lightning training: 2 epochs
/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
2025-11-12 18:40:56,030 - ERROR - Lightning training failed: Automatic gradient accumulation is not supported for manual optimization. Remove `Trainer(accumulate_grad_batches=2)` or switch to automatic optimization.
2025-11-12 18:40:56,030 - ERROR - An error occurred during execution: Lightning training failed. See logs for details.
Traceback (most recent call last):
  File "/home/ollie/Documents/PtychoPINN/ptycho_torch/workflows/components.py", line 828, in _train_with_lightning
    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)
  File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py", line 961, in _run
    _verify_loop_configurations(self)
  File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py", line 37, in _verify_loop_configurations
    __verify_manual_optimization_support(trainer, model)
  File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py", line 129, in __verify_manual_optimization_support
    raise MisconfigurationException(
lightning.fabric.utilities.exceptions.MisconfigurationException: Automatic gradient accumulation is not supported for manual optimization. Remove `Trainer(accumulate_grad_batches=2)` or switch to automatic optimization.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ollie/Documents/PtychoPINN/scripts/training/train.py", line 407, in <module>
    main()
  File "/home/ollie/Documents/PtychoPINN/scripts/training/train.py", line 386, in main
    recon_amp, recon_phase, results = run_cdi_example_with_backend(
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ollie/Documents/PtychoPINN/ptycho/workflows/backend_selector.py", line 162, in run_cdi_example_with_backend
    recon_amp, recon_phase, results = torch_components.run_cdi_example_torch(
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ollie/Documents/PtychoPINN/ptycho_torch/workflows/components.py", line 161, in run_cdi_example_torch
    train_results = train_cdi_model_torch(train_data, test_data, config, execution_config=execution_config)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ollie/Documents/PtychoPINN/ptycho_torch/workflows/components.py", line 1080, in train_cdi_model_torch
    results = _train_with_lightning(train_container, test_container, config, execution_config=execution_config)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ollie/Documents/PtychoPINN/ptycho_torch/workflows/components.py", line 831, in _train_with_lightning
    raise RuntimeError(f"Lightning training failed. See logs for details.") from e
RuntimeError: Lightning training failed. See logs for details.
