Artifact Inventory — PyTorchExecutionConfig GPU-First Defaults
===============================================================

Date: 2025-11-13T210000Z
Focus: INTEGRATE-PYTORCH-PARITY-001
Objective: Make PyTorchExecutionConfig GPU-first by default (auto→cuda resolution)

Code Changes
------------
1. ptycho/config/config.py:221
   - Changed accelerator default from 'cpu' to 'auto'
   - Updated field comment to document auto→cuda/cpu behavior

2. ptycho/config/config.py:253-312
   - Extended __post_init__ with auto-resolution logic
   - Import torch and check torch.cuda.is_available()
   - Resolve 'auto' → 'cuda' when GPU available
   - Resolve 'auto' → 'cpu' with POLICY-001 warning when CPU-only
   - Handle ImportError gracefully (should not occur per POLICY-001)

3. ptycho_torch/workflows/components.py:552-555
   - Added logger.info() after PyTorchExecutionConfig() auto-instantiation in _build_inference_dataloader
   - Logs resolved accelerator value for visibility

4. ptycho_torch/workflows/components.py:730-733
   - Added logger.info() after PyTorchExecutionConfig() auto-instantiation in _train_with_lightning
   - Logs resolved accelerator value for visibility

5. tests/torch/test_execution_config_defaults.py (NEW)
   - Test class TestPyTorchExecutionConfigDefaults with 5 test methods
   - test_auto_prefers_cuda: Monkeypatch torch.cuda.is_available=True → assert 'cuda'
   - test_auto_warns_and_falls_back_to_cpu: Monkeypatch torch.cuda.is_available=False → assert 'cpu' + POLICY-001 warning
   - test_explicit_cpu_bypasses_auto_resolution: Explicit accelerator='cpu' → assert 'cpu'
   - test_explicit_cuda_bypasses_auto_resolution: Explicit accelerator='cuda' → assert 'cuda'
   - test_backend_selector_inherits_gpu_first_defaults: Mock backend_selector call with execution_config=None

Test Evidence
-------------
Selector: pytest tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_auto_prefers_cuda tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_auto_warns_and_falls_back_to_cpu -vv

Result: 2 PASSED in 0.83s

Log: plans/active/INTEGRATE-PYTORCH-001/reports/2025-11-13T150000Z/parity_reactivation/green/pytest_execution_config_defaults.log

Key Assertions:
- test_auto_prefers_cuda: PASSED (auto→cuda when CUDA available)
- test_auto_warns_and_falls_back_to_cpu: PASSED (auto→cpu with POLICY-001 warning when no CUDA)

Policy Compliance
-----------------
POLICY-001: PyTorch (torch>=2.2) is mandatory. Auto-resolution ensures GPU-first execution when hardware available.

CONFIG-001: PyTorchExecutionConfig is execution-only; does NOT populate params.cfg via update_legacy_dict.

CONFIG-LOGGER-001: Logging infrastructure already in place; added logger.info() calls to surface resolved accelerator.

EXEC-ACCUM-001: Not affected by this change (gradient accumulation guard remains separate).

DATA-SUP-001: Not affected by this change (supervised data contract detection remains separate).

DEVICE-MISMATCH-001: RESOLVED (commit 85478a67; separate from this work).

Exit Criteria Assessment
-------------------------
[x] PyTorchExecutionConfig.accelerator defaults to 'auto'
[x] __post_init__ resolves 'auto' → 'cuda' when torch.cuda.is_available() == True
[x] __post_init__ resolves 'auto' → 'cpu' with POLICY-001 warning when CUDA unavailable
[x] ptycho_torch/workflows/components.py logs resolved accelerator at execution_config=None sites
[x] Regression tests verify auto-resolution behavior (monkeypatch torch.cuda.is_available)
[x] GREEN pytest log captured
[x] Explicit accelerator values (cpu, cuda) bypass auto-resolution

Status: COMPLETE

Next Actions
------------
1. Commit changes with message referencing POLICY-001 and auto→cuda defaults
2. Update docs/fix_plan.md to mark this Do Now complete
3. Update plans/active/INTEGRATE-PYTORCH-001/summary.md with Turn Summary
4. Rerun full PyTorch test suite to ensure no regressions
