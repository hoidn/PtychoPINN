============================= test session starts ==============================
platform linux -- Python 3.11.13, pytest-8.4.1, pluggy-1.6.0 -- /home/ollie/miniconda3/envs/ptycho311/bin/python3.11
cachedir: .pytest_cache
PyTorch: available
PyTorch version: 2.8.0+cu128
rootdir: /home/ollie/Documents/PtychoPINN
configfile: pyproject.toml
plugins: anyio-4.9.0
collecting ... collected 6 items

tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_auto_prefers_cuda PASSED [ 16%]
tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_auto_warns_and_falls_back_to_cpu PASSED [ 33%]
tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_explicit_cpu_bypasses_auto_resolution PASSED [ 50%]
tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_explicit_cuda_bypasses_auto_resolution PASSED [ 66%]
tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_backend_selector_inherits_gpu_first_defaults FAILED [ 83%]
tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_backend_selector_cpu_fallback_with_warning FAILED [100%]

=================================== FAILURES ===================================
_ TestPyTorchExecutionConfigDefaults.test_backend_selector_inherits_gpu_first_defaults _

self = <test_execution_config_defaults.TestPyTorchExecutionConfigDefaults object at 0x7d75507a0490>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7d75508a5ed0>

    def test_backend_selector_inherits_gpu_first_defaults(self, monkeypatch):
        """
        Verify backend_selector delegates to train_cdi_model_torch with GPU default when execution_config=None.
    
        Integration test proving train_cdi_model_torch auto-instantiates PyTorchExecutionConfig
        with GPU-first defaults when callers (e.g., Ptychodus via backend_selector) omit execution_config.
    
        POLICY-001: External integrations must benefit from GPU-first defaults without explicit config.
        """
        # Mock torch.cuda.is_available to return True
        mock_torch = MagicMock()
        mock_torch.cuda.is_available.return_value = True
        monkeypatch.setitem(__import__('sys').modules, 'torch', mock_torch)
    
        # Import the real PyTorchExecutionConfig to spy on it
        from ptycho.config.config import PyTorchExecutionConfig
    
        # Create a spy that tracks instantiation and preserves real behavior
        instantiated_configs = []
        original_init = PyTorchExecutionConfig.__init__
    
        def spy_init(self, *args, **kwargs):
            original_init(self, *args, **kwargs)
            instantiated_configs.append(self)
    
        # Mock _ensure_container to bypass data prep
        def mock_ensure_container(data, config):
            mock_container = MagicMock()
            mock_container.probe = MagicMock()
            return mock_container
    
        # Mock Lightning Trainer to avoid actual training
        def mock_trainer_init(self, *args, **kwargs):
            pass
    
        def mock_trainer_fit(self, *args, **kwargs):
            pass
    
        # Apply all patches
>       with patch.object(PyTorchExecutionConfig, '__init__', spy_init), \
             patch('ptycho_torch.workflows.components._ensure_container', mock_ensure_container), \
             patch('lightning.pytorch.Trainer.__init__', mock_trainer_init), \
             patch('lightning.pytorch.Trainer.fit', mock_trainer_fit):

tests/torch/test_execution_config_defaults.py:156: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../miniconda3/envs/ptycho311/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
../../miniconda3/envs/ptycho311/lib/python3.11/pkgutil.py:700: in resolve_name
    mod = importlib.import_module(modname)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/ptycho311/lib/python3.11/importlib/__init__.py:126: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1204: in _gcd_import
    ???
<frozen importlib._bootstrap>:1176: in _find_and_load
    ???
<frozen importlib._bootstrap>:1147: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:690: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:940: in exec_module
    ???
<frozen importlib._bootstrap>:241: in _call_with_frames_removed
    ???
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/__init__.py:18: in <module>
    from lightning.fabric.fabric import Fabric  # noqa: E402
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/fabric/__init__.py:35: in <module>
    from lightning.fabric.fabric import Fabric  # noqa: E402
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/fabric/fabric.py:38: in <module>
    from lightning.fabric.accelerators.accelerator import Accelerator
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/fabric/accelerators/__init__.py:15: in <module>
    from lightning.fabric.accelerators.accelerator import Accelerator
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/fabric/accelerators/accelerator.py:19: in <module>
    from lightning.fabric.accelerators.registry import _AcceleratorRegistry
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/fabric/accelerators/registry.py:18: in <module>
    from lightning.fabric.utilities.exceptions import MisconfigurationException
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/fabric/utilities/__init__.py:16: in <module>
    from lightning.fabric.utilities.apply_func import move_data_to_device
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/fabric/utilities/apply_func.py:24: in <module>
    from lightning.fabric.utilities.imports import _NUMPY_AVAILABLE
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/fabric/utilities/imports.py:39: in <module>
    _TORCHMETRICS_GREATER_EQUAL_1_0_0 = compare_version("torchmetrics", operator.ge, "1.0.0")
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning_utilities/core/imports.py:78: in compare_version
    pkg = importlib.import_module(package)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/ptycho311/lib/python3.11/importlib/__init__.py:126: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/torchmetrics/__init__.py:37: in <module>
    from torchmetrics import functional  # noqa: E402
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/torchmetrics/functional/__init__.py:56: in <module>
    from torchmetrics.functional.image._deprecated import (
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/torchmetrics/functional/image/__init__.py:14: in <module>
    from torchmetrics.functional.image.arniqa import arniqa
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/torchmetrics/functional/image/arniqa.py:31: in <module>
    from torchvision import transforms
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/torchvision/__init__.py:9: in <module>
    from .extension import _HAS_OPS  # usort:skip
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/torchvision/extension.py:92: in <module>
    _check_cuda_version()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _check_cuda_version():
        """
        Make sure that CUDA versions match between the pytorch install and torchvision install
        """
        if not _HAS_OPS:
            return -1
        from torch.version import cuda as torch_version_cuda
    
        _version = torch.ops.torchvision._cuda_version()
        if _version != -1 and torch_version_cuda is not None:
            tv_version = str(_version)
>           if int(tv_version) < 10000:
               ^^^^^^^^^^^^^^^
E           ValueError: invalid literal for int() with base 10: "<MagicMock name='mock.ops.torchvision._cuda_version()' id='137939222362640'>"

../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/torchvision/extension.py:68: ValueError
----------------------------- Captured stderr call -----------------------------
2025-11-12 21:06:21.871551: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1763010381.882537 1851256 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1763010381.886291 1851256 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1763010381.896872 1851256 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1763010381.896886 1851256 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1763010381.896888 1851256 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1763010381.896889 1851256 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-11-12 21:06:21.899515: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
_ TestPyTorchExecutionConfigDefaults.test_backend_selector_cpu_fallback_with_warning _

self = <test_execution_config_defaults.TestPyTorchExecutionConfigDefaults object at 0x7d75507a0250>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7d7494107850>

    def test_backend_selector_cpu_fallback_with_warning(self, monkeypatch):
        """
        Verify train_cdi_model_torch falls back to CPU with POLICY-001 warning when no CUDA available.
    
        Integration test proving CPU-only hosts receive GPU-first execution_config that
        auto-resolves to 'cpu' with actionable warning per docs/workflows/pytorch.md ยง12.
    
        POLICY-001: CPU fallback must emit warning about GPU-first policy.
        """
        # Mock torch.cuda.is_available to return False (CPU-only host)
        mock_torch = MagicMock()
        mock_torch.cuda.is_available.return_value = False
        monkeypatch.setitem(__import__('sys').modules, 'torch', mock_torch)
    
        # Import the real PyTorchExecutionConfig to spy on it
        from ptycho.config.config import PyTorchExecutionConfig
    
        # Create a spy that tracks instantiation and preserves real behavior
        instantiated_configs = []
        original_init = PyTorchExecutionConfig.__init__
    
        def spy_init(self, *args, **kwargs):
            original_init(self, *args, **kwargs)
            instantiated_configs.append(self)
    
        # Mock _ensure_container to bypass data prep
        def mock_ensure_container(data, config):
            mock_container = MagicMock()
            mock_container.probe = MagicMock()
            return mock_container
    
        # Mock Lightning Trainer to avoid actual training
        def mock_trainer_init(self, *args, **kwargs):
            pass
    
        def mock_trainer_fit(self, *args, **kwargs):
            pass
    
        # Apply all patches and capture warnings
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter("always")
>           with patch.object(PyTorchExecutionConfig, '__init__', spy_init), \
                 patch('ptycho_torch.workflows.components._ensure_container', mock_ensure_container), \
                 patch('lightning.pytorch.Trainer.__init__', mock_trainer_init), \
                 patch('lightning.pytorch.Trainer.fit', mock_trainer_fit):

tests/torch/test_execution_config_defaults.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../miniconda3/envs/ptycho311/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
../../miniconda3/envs/ptycho311/lib/python3.11/pkgutil.py:700: in resolve_name
    mod = importlib.import_module(modname)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/ptycho311/lib/python3.11/importlib/__init__.py:126: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1204: in _gcd_import
    ???
<frozen importlib._bootstrap>:1176: in _find_and_load
    ???
<frozen importlib._bootstrap>:1147: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:690: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:940: in exec_module
    ???
<frozen importlib._bootstrap>:241: in _call_with_frames_removed
    ???
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/__init__.py:18: in <module>
    from lightning.fabric.fabric import Fabric  # noqa: E402
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/fabric/__init__.py:35: in <module>
    from lightning.fabric.fabric import Fabric  # noqa: E402
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/fabric/fabric.py:38: in <module>
    from lightning.fabric.accelerators.accelerator import Accelerator
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/fabric/accelerators/__init__.py:15: in <module>
    from lightning.fabric.accelerators.accelerator import Accelerator
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/fabric/accelerators/accelerator.py:19: in <module>
    from lightning.fabric.accelerators.registry import _AcceleratorRegistry
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/fabric/accelerators/registry.py:18: in <module>
    from lightning.fabric.utilities.exceptions import MisconfigurationException
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/fabric/utilities/__init__.py:16: in <module>
    from lightning.fabric.utilities.apply_func import move_data_to_device
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/fabric/utilities/apply_func.py:24: in <module>
    from lightning.fabric.utilities.imports import _NUMPY_AVAILABLE
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/fabric/utilities/imports.py:39: in <module>
    _TORCHMETRICS_GREATER_EQUAL_1_0_0 = compare_version("torchmetrics", operator.ge, "1.0.0")
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning_utilities/core/imports.py:78: in compare_version
    pkg = importlib.import_module(package)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/ptycho311/lib/python3.11/importlib/__init__.py:126: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/torchmetrics/__init__.py:37: in <module>
    from torchmetrics import functional  # noqa: E402
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/torchmetrics/functional/__init__.py:56: in <module>
    from torchmetrics.functional.image._deprecated import (
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/torchmetrics/functional/image/__init__.py:14: in <module>
    from torchmetrics.functional.image.arniqa import arniqa
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/torchmetrics/functional/image/arniqa.py:31: in <module>
    from torchvision import transforms
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/torchvision/__init__.py:9: in <module>
    from .extension import _HAS_OPS  # usort:skip
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/torchvision/extension.py:92: in <module>
    _check_cuda_version()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _check_cuda_version():
        """
        Make sure that CUDA versions match between the pytorch install and torchvision install
        """
        if not _HAS_OPS:
            return -1
        from torch.version import cuda as torch_version_cuda
    
        _version = torch.ops.torchvision._cuda_version()
        if _version != -1 and torch_version_cuda is not None:
            tv_version = str(_version)
>           if int(tv_version) < 10000:
               ^^^^^^^^^^^^^^^
E           ValueError: invalid literal for int() with base 10: "<MagicMock name='mock.ops.torchvision._cuda_version()' id='137939215414928'>"

../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/torchvision/extension.py:68: ValueError
=========================== short test summary info ============================
FAILED tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_backend_selector_inherits_gpu_first_defaults - ValueError: invalid literal for int() with base 10: "<MagicMock name='mock.ops.torchvision._cuda_version()' id='137939222362640'>"
FAILED tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_backend_selector_cpu_fallback_with_warning - ValueError: invalid literal for int() with base 10: "<MagicMock name='mock.ops.torchvision._cuda_version()' id='137939215414928'>"
========================= 2 failed, 4 passed in 4.20s ==========================
