============================= test session starts ==============================
platform linux -- Python 3.11.13, pytest-8.4.1, pluggy-1.6.0 -- /home/ollie/miniconda3/envs/ptycho311/bin/python3.11
cachedir: .pytest_cache
PyTorch: available
PyTorch version: 2.8.0+cu128
rootdir: /home/ollie/Documents/PtychoPINN2
configfile: pyproject.toml
plugins: anyio-4.9.0
collecting ... collected 46 items

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_direct_fields[N-direct] FAILED [  2%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_direct_fields[n_filters_scale-direct] FAILED [  4%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_direct_fields[object_big-direct] FAILED [  6%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_direct_fields[probe_big-direct] FAILED [  8%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_direct_fields[batch_size-direct] FAILED [ 10%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_transform_fields[gridsize-tuple-to-int] FAILED [ 13%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_transform_fields[model_type-unsupervised] FAILED [ 15%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_transform_fields[model_type-supervised] FAILED [ 17%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_transform_fields[amp_activation-silu] FAILED [ 19%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_transform_fields[amp_activation-SiLU] FAILED [ 21%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_transform_fields[amp_activation-passthrough] FAILED [ 23%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_transform_fields[nepochs-rename] FAILED [ 26%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_transform_fields[nll_weight-true] FAILED [ 28%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_transform_fields[nll_weight-false] FAILED [ 30%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_override_fields[pad_object-default] FAILED [ 32%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_override_fields[pad_object-override] FAILED [ 34%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_override_fields[gaussian_smoothing_sigma-default] FAILED [ 36%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_override_fields[gaussian_smoothing_sigma-override] FAILED [ 39%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[mae_weight-default] FAILED [ 41%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[mae_weight-override] FAILED [ 43%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[realspace_mae_weight-default] FAILED [ 45%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[realspace_weight-default] FAILED [ 47%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[positions_provided-default] FAILED [ 50%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[probe_trainable-default] FAILED [ 52%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[sequential_sampling-default] FAILED [ 54%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_inference_config_override_fields[debug-default] FAILED [ 56%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_inference_config_override_fields[debug-override] FAILED [ 58%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_probe_mask_translation[probe_mask-default] FAILED [ 60%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_probe_mask_override FAILED [ 63%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_default_divergence_detection[nphotons-divergence] FAILED [ 65%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_default_divergence_detection[probe_scale-divergence] PASSED [ 67%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_gridsize_error_handling[gridsize-non-square] PASSED [ 69%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_type_error_handling[model_type-invalid-enum] PASSED [ 71%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_activation_error_handling[amp_activation-unknown] PASSED [ 73%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_train_data_file_required_error FAILED [ 76%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_path_required_error FAILED [ 78%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_nphotons_default_divergence_error FAILED [ 80%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_nphotons_override_passes_validation FAILED [ 82%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_n_subsample_missing_override_uses_none FAILED [ 84%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_n_subsample_explicit_override FAILED [ 86%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_inference_config_n_subsample_missing_override_uses_none FAILED [ 89%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_inference_config_n_subsample_explicit_override FAILED [ 91%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_probe_scale_default_divergence_warning PASSED [ 93%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_n_groups_missing_override_warning PASSED [ 95%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_test_data_file_training_missing_warning PASSED [ 97%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_params_cfg_matches_baseline PASSED [100%]

=================================== FAILURES ===================================
_______ TestConfigBridgeParity.test_model_config_direct_fields[N-direct] _______

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afdf5ec9d0>
params_cfg_snapshot = None, field_name = 'N', pytorch_value = 128
expected_tf_value = 128

    @pytest.mark.parametrize('field_name,pytorch_value,expected_tf_value', [
        pytest.param('N', 128, 128, id='N-direct'),
        pytest.param('n_filters_scale', 2, 2, id='n_filters_scale-direct'),
        pytest.param('object_big', False, False, id='object_big-direct'),
        pytest.param('probe_big', False, False, id='probe_big-direct'),
    ])
    def test_model_config_direct_fields(self, params_cfg_snapshot, field_name, pytorch_value, expected_tf_value):
        """
        Test ModelConfig fields that translate directly without transformation.
    
        Spec coverage: §5.1:1 (N), §5.1:3 (n_filters_scale), §5.1:6 (object_big), §5.1:7 (probe_big)
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig
        from ptycho_torch import config_bridge
    
        # Create PyTorch config with test value
        if field_name in ['N']:
            pt_data = DataConfig(**{field_name: pytorch_value})
            pt_model = ModelConfig()
        else:
            pt_data = DataConfig()
            pt_model = ModelConfig(**{field_name: pytorch_value})
    
        # Translate to TensorFlow
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:212: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=128, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Near...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
---------------------------- Captured stderr setup -----------------------------
2025-10-16 23:46:06.449297: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1760683566.460388 3399746 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1760683566.464280 3399746 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1760683566.475009 3399746 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1760683566.475023 3399746 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1760683566.475026 3399746 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1760683566.475028 3399746 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-10-16 23:46:06.477713: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
_ TestConfigBridgeParity.test_model_config_direct_fields[n_filters_scale-direct] _

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afe9de6190>
params_cfg_snapshot = None, field_name = 'n_filters_scale', pytorch_value = 2
expected_tf_value = 2

    @pytest.mark.parametrize('field_name,pytorch_value,expected_tf_value', [
        pytest.param('N', 128, 128, id='N-direct'),
        pytest.param('n_filters_scale', 2, 2, id='n_filters_scale-direct'),
        pytest.param('object_big', False, False, id='object_big-direct'),
        pytest.param('probe_big', False, False, id='probe_big-direct'),
    ])
    def test_model_config_direct_fields(self, params_cfg_snapshot, field_name, pytorch_value, expected_tf_value):
        """
        Test ModelConfig fields that translate directly without transformation.
    
        Spec coverage: §5.1:1 (N), §5.1:3 (n_filters_scale), §5.1:6 (object_big), §5.1:7 (probe_big)
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig
        from ptycho_torch import config_bridge
    
        # Create PyTorch config with test value
        if field_name in ['N']:
            pt_data = DataConfig(**{field_name: pytorch_value})
            pt_model = ModelConfig()
        else:
            pt_data = DataConfig()
            pt_model = ModelConfig(**{field_name: pytorch_value})
    
        # Translate to TensorFlow
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:212: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
__ TestConfigBridgeParity.test_model_config_direct_fields[object_big-direct] ___

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda14dcd0>
params_cfg_snapshot = None, field_name = 'object_big', pytorch_value = False
expected_tf_value = False

    @pytest.mark.parametrize('field_name,pytorch_value,expected_tf_value', [
        pytest.param('N', 128, 128, id='N-direct'),
        pytest.param('n_filters_scale', 2, 2, id='n_filters_scale-direct'),
        pytest.param('object_big', False, False, id='object_big-direct'),
        pytest.param('probe_big', False, False, id='probe_big-direct'),
    ])
    def test_model_config_direct_fields(self, params_cfg_snapshot, field_name, pytorch_value, expected_tf_value):
        """
        Test ModelConfig fields that translate directly without transformation.
    
        Spec coverage: §5.1:1 (N), §5.1:3 (n_filters_scale), §5.1:6 (object_big), §5.1:7 (probe_big)
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig
        from ptycho_torch import config_bridge
    
        # Create PyTorch config with test value
        if field_name in ['N']:
            pt_data = DataConfig(**{field_name: pytorch_value})
            pt_model = ModelConfig()
        else:
            pt_data = DataConfig()
            pt_model = ModelConfig(**{field_name: pytorch_value})
    
        # Translate to TensorFlow
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:212: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
___ TestConfigBridgeParity.test_model_config_direct_fields[probe_big-direct] ___

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda14c050>
params_cfg_snapshot = None, field_name = 'probe_big', pytorch_value = False
expected_tf_value = False

    @pytest.mark.parametrize('field_name,pytorch_value,expected_tf_value', [
        pytest.param('N', 128, 128, id='N-direct'),
        pytest.param('n_filters_scale', 2, 2, id='n_filters_scale-direct'),
        pytest.param('object_big', False, False, id='object_big-direct'),
        pytest.param('probe_big', False, False, id='probe_big-direct'),
    ])
    def test_model_config_direct_fields(self, params_cfg_snapshot, field_name, pytorch_value, expected_tf_value):
        """
        Test ModelConfig fields that translate directly without transformation.
    
        Spec coverage: §5.1:1 (N), §5.1:3 (n_filters_scale), §5.1:6 (object_big), §5.1:7 (probe_big)
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig
        from ptycho_torch import config_bridge
    
        # Create PyTorch config with test value
        if field_name in ['N']:
            pt_data = DataConfig(**{field_name: pytorch_value})
            pt_model = ModelConfig()
        else:
            pt_data = DataConfig()
            pt_model = ModelConfig(**{field_name: pytorch_value})
    
        # Translate to TensorFlow
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:212: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
_ TestConfigBridgeParity.test_training_config_direct_fields[batch_size-direct] _

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda14e590>
params_cfg_snapshot = None, field_name = 'batch_size', pytorch_value = 32
expected_tf_value = 32

    @pytest.mark.parametrize('field_name,pytorch_value,expected_tf_value', [
        pytest.param('batch_size', 32, 32, id='batch_size-direct'),
    ])
    def test_training_config_direct_fields(self, params_cfg_snapshot, field_name, pytorch_value, expected_tf_value):
        """
        Test TrainingConfig fields that translate directly without transformation.
    
        Spec coverage: §5.2:3 (batch_size)
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig, TrainingConfig
        from ptycho_torch import config_bridge
    
        pt_data = DataConfig()
        pt_model = ModelConfig()
        pt_train = TrainingConfig(**{field_name: pytorch_value})
    
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:234: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
_ TestConfigBridgeParity.test_model_config_transform_fields[gridsize-tuple-to-int] _

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda14f910>
params_cfg_snapshot = None, pt_field = 'grid_size', pt_value = (3, 3)
tf_field = 'gridsize', tf_value = 3

    @pytest.mark.parametrize('pt_field,pt_value,tf_field,tf_value', [
        pytest.param('grid_size', (3, 3), 'gridsize', 3, id='gridsize-tuple-to-int'),
        pytest.param('mode', 'Unsupervised', 'model_type', 'pinn', id='model_type-unsupervised'),
        pytest.param('mode', 'Supervised', 'model_type', 'supervised', id='model_type-supervised'),
        pytest.param('amp_activation', 'silu', 'amp_activation', 'swish', id='amp_activation-silu'),
        pytest.param('amp_activation', 'SiLU', 'amp_activation', 'swish', id='amp_activation-SiLU'),
        pytest.param('amp_activation', 'sigmoid', 'amp_activation', 'sigmoid', id='amp_activation-passthrough'),
    ])
    def test_model_config_transform_fields(self, params_cfg_snapshot, pt_field, pt_value, tf_field, tf_value):
        """
        Test ModelConfig fields that require transformation during translation.
    
        Spec coverage: §5.1:2 (gridsize), §5.1:4 (model_type), §5.1:5 (amp_activation)
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig
        from ptycho_torch import config_bridge
    
        # Create PyTorch config with test value
        if pt_field in ['grid_size']:
            pt_data = DataConfig(**{pt_field: pt_value})
            pt_model = ModelConfig()
        else:
            pt_data = DataConfig()
            pt_model = ModelConfig(**{pt_field: pt_value})
    
        # Translate to TensorFlow
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:273: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(3, 3), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
_ TestConfigBridgeParity.test_model_config_transform_fields[model_type-unsupervised] _

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda14fb90>
params_cfg_snapshot = None, pt_field = 'mode', pt_value = 'Unsupervised'
tf_field = 'model_type', tf_value = 'pinn'

    @pytest.mark.parametrize('pt_field,pt_value,tf_field,tf_value', [
        pytest.param('grid_size', (3, 3), 'gridsize', 3, id='gridsize-tuple-to-int'),
        pytest.param('mode', 'Unsupervised', 'model_type', 'pinn', id='model_type-unsupervised'),
        pytest.param('mode', 'Supervised', 'model_type', 'supervised', id='model_type-supervised'),
        pytest.param('amp_activation', 'silu', 'amp_activation', 'swish', id='amp_activation-silu'),
        pytest.param('amp_activation', 'SiLU', 'amp_activation', 'swish', id='amp_activation-SiLU'),
        pytest.param('amp_activation', 'sigmoid', 'amp_activation', 'sigmoid', id='amp_activation-passthrough'),
    ])
    def test_model_config_transform_fields(self, params_cfg_snapshot, pt_field, pt_value, tf_field, tf_value):
        """
        Test ModelConfig fields that require transformation during translation.
    
        Spec coverage: §5.1:2 (gridsize), §5.1:4 (model_type), §5.1:5 (amp_activation)
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig
        from ptycho_torch import config_bridge
    
        # Create PyTorch config with test value
        if pt_field in ['grid_size']:
            pt_data = DataConfig(**{pt_field: pt_value})
            pt_model = ModelConfig()
        else:
            pt_data = DataConfig()
            pt_model = ModelConfig(**{pt_field: pt_value})
    
        # Translate to TensorFlow
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:273: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
_ TestConfigBridgeParity.test_model_config_transform_fields[model_type-supervised] _

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda14fe10>
params_cfg_snapshot = None, pt_field = 'mode', pt_value = 'Supervised'
tf_field = 'model_type', tf_value = 'supervised'

    @pytest.mark.parametrize('pt_field,pt_value,tf_field,tf_value', [
        pytest.param('grid_size', (3, 3), 'gridsize', 3, id='gridsize-tuple-to-int'),
        pytest.param('mode', 'Unsupervised', 'model_type', 'pinn', id='model_type-unsupervised'),
        pytest.param('mode', 'Supervised', 'model_type', 'supervised', id='model_type-supervised'),
        pytest.param('amp_activation', 'silu', 'amp_activation', 'swish', id='amp_activation-silu'),
        pytest.param('amp_activation', 'SiLU', 'amp_activation', 'swish', id='amp_activation-SiLU'),
        pytest.param('amp_activation', 'sigmoid', 'amp_activation', 'sigmoid', id='amp_activation-passthrough'),
    ])
    def test_model_config_transform_fields(self, params_cfg_snapshot, pt_field, pt_value, tf_field, tf_value):
        """
        Test ModelConfig fields that require transformation during translation.
    
        Spec coverage: §5.1:2 (gridsize), §5.1:4 (model_type), §5.1:5 (amp_activation)
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig
        from ptycho_torch import config_bridge
    
        # Create PyTorch config with test value
        if pt_field in ['grid_size']:
            pt_data = DataConfig(**{pt_field: pt_value})
            pt_model = ModelConfig()
        else:
            pt_data = DataConfig()
            pt_model = ModelConfig(**{pt_field: pt_value})
    
        # Translate to TensorFlow
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:273: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Supervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_mod...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
_ TestConfigBridgeParity.test_model_config_transform_fields[amp_activation-silu] _

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda1700d0>
params_cfg_snapshot = None, pt_field = 'amp_activation', pt_value = 'silu'
tf_field = 'amp_activation', tf_value = 'swish'

    @pytest.mark.parametrize('pt_field,pt_value,tf_field,tf_value', [
        pytest.param('grid_size', (3, 3), 'gridsize', 3, id='gridsize-tuple-to-int'),
        pytest.param('mode', 'Unsupervised', 'model_type', 'pinn', id='model_type-unsupervised'),
        pytest.param('mode', 'Supervised', 'model_type', 'supervised', id='model_type-supervised'),
        pytest.param('amp_activation', 'silu', 'amp_activation', 'swish', id='amp_activation-silu'),
        pytest.param('amp_activation', 'SiLU', 'amp_activation', 'swish', id='amp_activation-SiLU'),
        pytest.param('amp_activation', 'sigmoid', 'amp_activation', 'sigmoid', id='amp_activation-passthrough'),
    ])
    def test_model_config_transform_fields(self, params_cfg_snapshot, pt_field, pt_value, tf_field, tf_value):
        """
        Test ModelConfig fields that require transformation during translation.
    
        Spec coverage: §5.1:2 (gridsize), §5.1:4 (model_type), §5.1:5 (amp_activation)
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig
        from ptycho_torch import config_bridge
    
        # Create PyTorch config with test value
        if pt_field in ['grid_size']:
            pt_data = DataConfig(**{pt_field: pt_value})
            pt_model = ModelConfig()
        else:
            pt_data = DataConfig()
            pt_model = ModelConfig(**{pt_field: pt_value})
    
        # Translate to TensorFlow
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:273: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
_ TestConfigBridgeParity.test_model_config_transform_fields[amp_activation-SiLU] _

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda170390>
params_cfg_snapshot = None, pt_field = 'amp_activation', pt_value = 'SiLU'
tf_field = 'amp_activation', tf_value = 'swish'

    @pytest.mark.parametrize('pt_field,pt_value,tf_field,tf_value', [
        pytest.param('grid_size', (3, 3), 'gridsize', 3, id='gridsize-tuple-to-int'),
        pytest.param('mode', 'Unsupervised', 'model_type', 'pinn', id='model_type-unsupervised'),
        pytest.param('mode', 'Supervised', 'model_type', 'supervised', id='model_type-supervised'),
        pytest.param('amp_activation', 'silu', 'amp_activation', 'swish', id='amp_activation-silu'),
        pytest.param('amp_activation', 'SiLU', 'amp_activation', 'swish', id='amp_activation-SiLU'),
        pytest.param('amp_activation', 'sigmoid', 'amp_activation', 'sigmoid', id='amp_activation-passthrough'),
    ])
    def test_model_config_transform_fields(self, params_cfg_snapshot, pt_field, pt_value, tf_field, tf_value):
        """
        Test ModelConfig fields that require transformation during translation.
    
        Spec coverage: §5.1:2 (gridsize), §5.1:4 (model_type), §5.1:5 (amp_activation)
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig
        from ptycho_torch import config_bridge
    
        # Create PyTorch config with test value
        if pt_field in ['grid_size']:
            pt_data = DataConfig(**{pt_field: pt_value})
            pt_model = ModelConfig()
        else:
            pt_data = DataConfig()
            pt_model = ModelConfig(**{pt_field: pt_value})
    
        # Translate to TensorFlow
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:273: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
_ TestConfigBridgeParity.test_model_config_transform_fields[amp_activation-passthrough] _

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda170690>
params_cfg_snapshot = None, pt_field = 'amp_activation', pt_value = 'sigmoid'
tf_field = 'amp_activation', tf_value = 'sigmoid'

    @pytest.mark.parametrize('pt_field,pt_value,tf_field,tf_value', [
        pytest.param('grid_size', (3, 3), 'gridsize', 3, id='gridsize-tuple-to-int'),
        pytest.param('mode', 'Unsupervised', 'model_type', 'pinn', id='model_type-unsupervised'),
        pytest.param('mode', 'Supervised', 'model_type', 'supervised', id='model_type-supervised'),
        pytest.param('amp_activation', 'silu', 'amp_activation', 'swish', id='amp_activation-silu'),
        pytest.param('amp_activation', 'SiLU', 'amp_activation', 'swish', id='amp_activation-SiLU'),
        pytest.param('amp_activation', 'sigmoid', 'amp_activation', 'sigmoid', id='amp_activation-passthrough'),
    ])
    def test_model_config_transform_fields(self, params_cfg_snapshot, pt_field, pt_value, tf_field, tf_value):
        """
        Test ModelConfig fields that require transformation during translation.
    
        Spec coverage: §5.1:2 (gridsize), §5.1:4 (model_type), §5.1:5 (amp_activation)
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig
        from ptycho_torch import config_bridge
    
        # Create PyTorch config with test value
        if pt_field in ['grid_size']:
            pt_data = DataConfig(**{pt_field: pt_value})
            pt_model = ModelConfig()
        else:
            pt_data = DataConfig()
            pt_model = ModelConfig(**{pt_field: pt_value})
    
        # Translate to TensorFlow
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:273: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
_ TestConfigBridgeParity.test_training_config_transform_fields[nepochs-rename] _

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda171790>
params_cfg_snapshot = None, pt_field = 'epochs', pt_value = 100
tf_field = 'nepochs', tf_value = 100

    @pytest.mark.parametrize('pt_field,pt_value,tf_field,tf_value', [
        pytest.param('epochs', 100, 'nepochs', 100, id='nepochs-rename'),
        pytest.param('nll', True, 'nll_weight', 1.0, id='nll_weight-true'),
        pytest.param('nll', False, 'nll_weight', 0.0, id='nll_weight-false'),
    ])
    def test_training_config_transform_fields(self, params_cfg_snapshot, pt_field, pt_value, tf_field, tf_value):
        """
        Test TrainingConfig fields that require transformation during translation.
    
        Spec coverage: §5.2:4 (nepochs), §5.2:6 (nll_weight)
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig, TrainingConfig
        from ptycho_torch import config_bridge
    
        pt_data = DataConfig()
        pt_model = ModelConfig()
        pt_train = TrainingConfig(**{pt_field: pt_value})
    
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:297: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
_ TestConfigBridgeParity.test_training_config_transform_fields[nll_weight-true] _

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda171a10>
params_cfg_snapshot = None, pt_field = 'nll', pt_value = True
tf_field = 'nll_weight', tf_value = 1.0

    @pytest.mark.parametrize('pt_field,pt_value,tf_field,tf_value', [
        pytest.param('epochs', 100, 'nepochs', 100, id='nepochs-rename'),
        pytest.param('nll', True, 'nll_weight', 1.0, id='nll_weight-true'),
        pytest.param('nll', False, 'nll_weight', 0.0, id='nll_weight-false'),
    ])
    def test_training_config_transform_fields(self, params_cfg_snapshot, pt_field, pt_value, tf_field, tf_value):
        """
        Test TrainingConfig fields that require transformation during translation.
    
        Spec coverage: §5.2:4 (nepochs), §5.2:6 (nll_weight)
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig, TrainingConfig
        from ptycho_torch import config_bridge
    
        pt_data = DataConfig()
        pt_model = ModelConfig()
        pt_train = TrainingConfig(**{pt_field: pt_value})
    
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:297: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
_ TestConfigBridgeParity.test_training_config_transform_fields[nll_weight-false] _

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda171c90>
params_cfg_snapshot = None, pt_field = 'nll', pt_value = False
tf_field = 'nll_weight', tf_value = 0.0

    @pytest.mark.parametrize('pt_field,pt_value,tf_field,tf_value', [
        pytest.param('epochs', 100, 'nepochs', 100, id='nepochs-rename'),
        pytest.param('nll', True, 'nll_weight', 1.0, id='nll_weight-true'),
        pytest.param('nll', False, 'nll_weight', 0.0, id='nll_weight-false'),
    ])
    def test_training_config_transform_fields(self, params_cfg_snapshot, pt_field, pt_value, tf_field, tf_value):
        """
        Test TrainingConfig fields that require transformation during translation.
    
        Spec coverage: §5.2:4 (nepochs), §5.2:6 (nll_weight)
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig, TrainingConfig
        from ptycho_torch import config_bridge
    
        pt_data = DataConfig()
        pt_model = ModelConfig()
        pt_train = TrainingConfig(**{pt_field: pt_value})
    
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:297: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
_ TestConfigBridgeParity.test_model_config_override_fields[pad_object-default] _

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda172d10>
params_cfg_snapshot = None, field_name = 'pad_object', override_value = None
expected_value = True

    @pytest.mark.parametrize('field_name,override_value,expected_value', [
        pytest.param('pad_object', None, True, id='pad_object-default'),
        pytest.param('pad_object', False, False, id='pad_object-override'),
        pytest.param('gaussian_smoothing_sigma', None, 0.0, id='gaussian_smoothing_sigma-default'),
        pytest.param('gaussian_smoothing_sigma', 0.5, 0.5, id='gaussian_smoothing_sigma-override'),
    ])
    def test_model_config_override_fields(self, params_cfg_snapshot, field_name, override_value, expected_value):
        """
        Test ModelConfig fields missing from PyTorch that use defaults or overrides.
    
        Spec coverage: §5.1:9 (pad_object), §5.1:11 (gaussian_smoothing_sigma)
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig
        from ptycho_torch import config_bridge
    
        pt_data = DataConfig()
        pt_model = ModelConfig()
    
        # Build overrides dict
        overrides = {field_name: override_value} if override_value is not None else {}
    
>       tf_model = config_bridge.to_model_config(pt_data, pt_model, overrides=overrides)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:331: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
_ TestConfigBridgeParity.test_model_config_override_fields[pad_object-override] _

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda172f90>
params_cfg_snapshot = None, field_name = 'pad_object', override_value = False
expected_value = False

    @pytest.mark.parametrize('field_name,override_value,expected_value', [
        pytest.param('pad_object', None, True, id='pad_object-default'),
        pytest.param('pad_object', False, False, id='pad_object-override'),
        pytest.param('gaussian_smoothing_sigma', None, 0.0, id='gaussian_smoothing_sigma-default'),
        pytest.param('gaussian_smoothing_sigma', 0.5, 0.5, id='gaussian_smoothing_sigma-override'),
    ])
    def test_model_config_override_fields(self, params_cfg_snapshot, field_name, override_value, expected_value):
        """
        Test ModelConfig fields missing from PyTorch that use defaults or overrides.
    
        Spec coverage: §5.1:9 (pad_object), §5.1:11 (gaussian_smoothing_sigma)
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig
        from ptycho_torch import config_bridge
    
        pt_data = DataConfig()
        pt_model = ModelConfig()
    
        # Build overrides dict
        overrides = {field_name: override_value} if override_value is not None else {}
    
>       tf_model = config_bridge.to_model_config(pt_data, pt_model, overrides=overrides)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:331: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {'pad_object': False}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
_ TestConfigBridgeParity.test_model_config_override_fields[gaussian_smoothing_sigma-default] _

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda173210>
params_cfg_snapshot = None, field_name = 'gaussian_smoothing_sigma'
override_value = None, expected_value = 0.0

    @pytest.mark.parametrize('field_name,override_value,expected_value', [
        pytest.param('pad_object', None, True, id='pad_object-default'),
        pytest.param('pad_object', False, False, id='pad_object-override'),
        pytest.param('gaussian_smoothing_sigma', None, 0.0, id='gaussian_smoothing_sigma-default'),
        pytest.param('gaussian_smoothing_sigma', 0.5, 0.5, id='gaussian_smoothing_sigma-override'),
    ])
    def test_model_config_override_fields(self, params_cfg_snapshot, field_name, override_value, expected_value):
        """
        Test ModelConfig fields missing from PyTorch that use defaults or overrides.
    
        Spec coverage: §5.1:9 (pad_object), §5.1:11 (gaussian_smoothing_sigma)
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig
        from ptycho_torch import config_bridge
    
        pt_data = DataConfig()
        pt_model = ModelConfig()
    
        # Build overrides dict
        overrides = {field_name: override_value} if override_value is not None else {}
    
>       tf_model = config_bridge.to_model_config(pt_data, pt_model, overrides=overrides)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:331: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
_ TestConfigBridgeParity.test_model_config_override_fields[gaussian_smoothing_sigma-override] _

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda173490>
params_cfg_snapshot = None, field_name = 'gaussian_smoothing_sigma'
override_value = 0.5, expected_value = 0.5

    @pytest.mark.parametrize('field_name,override_value,expected_value', [
        pytest.param('pad_object', None, True, id='pad_object-default'),
        pytest.param('pad_object', False, False, id='pad_object-override'),
        pytest.param('gaussian_smoothing_sigma', None, 0.0, id='gaussian_smoothing_sigma-default'),
        pytest.param('gaussian_smoothing_sigma', 0.5, 0.5, id='gaussian_smoothing_sigma-override'),
    ])
    def test_model_config_override_fields(self, params_cfg_snapshot, field_name, override_value, expected_value):
        """
        Test ModelConfig fields missing from PyTorch that use defaults or overrides.
    
        Spec coverage: §5.1:9 (pad_object), §5.1:11 (gaussian_smoothing_sigma)
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig
        from ptycho_torch import config_bridge
    
        pt_data = DataConfig()
        pt_model = ModelConfig()
    
        # Build overrides dict
        overrides = {field_name: override_value} if override_value is not None else {}
    
>       tf_model = config_bridge.to_model_config(pt_data, pt_model, overrides=overrides)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:331: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {'gaussian_smoothing_sigma': 0.5}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
_ TestConfigBridgeParity.test_training_config_override_fields[mae_weight-default] _

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda178510>
params_cfg_snapshot = None, field_name = 'mae_weight', override_value = None
expected_value = 0.0

    @pytest.mark.parametrize('field_name,override_value,expected_value', [
        pytest.param('mae_weight', None, 0.0, id='mae_weight-default'),
        pytest.param('mae_weight', 0.3, 0.3, id='mae_weight-override'),
        pytest.param('realspace_mae_weight', None, 0.0, id='realspace_mae_weight-default'),
        pytest.param('realspace_weight', None, 0.0, id='realspace_weight-default'),
        pytest.param('positions_provided', None, True, id='positions_provided-default'),
        pytest.param('probe_trainable', None, False, id='probe_trainable-default'),
        pytest.param('sequential_sampling', None, False, id='sequential_sampling-default'),
    ])
    def test_training_config_override_fields(self, params_cfg_snapshot, field_name, override_value, expected_value):
        """
        Test TrainingConfig fields missing from PyTorch that require defaults/overrides.
    
        Spec coverage: §5.2:5 (mae_weight), §5.2:7 (realspace_mae_weight),
        §5.2:8 (realspace_weight), §5.2:15 (positions_provided),
        §5.2:16 (probe_trainable), §5.2:19 (sequential_sampling)
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig, TrainingConfig
        from ptycho_torch import config_bridge
    
        pt_data = DataConfig()
        pt_model = ModelConfig()
        pt_train = TrainingConfig()
    
        # Build overrides dict (always include required fields including nphotons to avoid validation error)
        overrides = dict(train_data_file=Path('train.npz'), n_groups=512, nphotons=1e9)
        if override_value is not None:
            overrides[field_name] = override_value
    
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:365: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
_ TestConfigBridgeParity.test_training_config_override_fields[mae_weight-override] _

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda178790>
params_cfg_snapshot = None, field_name = 'mae_weight', override_value = 0.3
expected_value = 0.3

    @pytest.mark.parametrize('field_name,override_value,expected_value', [
        pytest.param('mae_weight', None, 0.0, id='mae_weight-default'),
        pytest.param('mae_weight', 0.3, 0.3, id='mae_weight-override'),
        pytest.param('realspace_mae_weight', None, 0.0, id='realspace_mae_weight-default'),
        pytest.param('realspace_weight', None, 0.0, id='realspace_weight-default'),
        pytest.param('positions_provided', None, True, id='positions_provided-default'),
        pytest.param('probe_trainable', None, False, id='probe_trainable-default'),
        pytest.param('sequential_sampling', None, False, id='sequential_sampling-default'),
    ])
    def test_training_config_override_fields(self, params_cfg_snapshot, field_name, override_value, expected_value):
        """
        Test TrainingConfig fields missing from PyTorch that require defaults/overrides.
    
        Spec coverage: §5.2:5 (mae_weight), §5.2:7 (realspace_mae_weight),
        §5.2:8 (realspace_weight), §5.2:15 (positions_provided),
        §5.2:16 (probe_trainable), §5.2:19 (sequential_sampling)
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig, TrainingConfig
        from ptycho_torch import config_bridge
    
        pt_data = DataConfig()
        pt_model = ModelConfig()
        pt_train = TrainingConfig()
    
        # Build overrides dict (always include required fields including nphotons to avoid validation error)
        overrides = dict(train_data_file=Path('train.npz'), n_groups=512, nphotons=1e9)
        if override_value is not None:
            overrides[field_name] = override_value
    
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:365: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
_ TestConfigBridgeParity.test_training_config_override_fields[realspace_mae_weight-default] _

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda178a10>
params_cfg_snapshot = None, field_name = 'realspace_mae_weight'
override_value = None, expected_value = 0.0

    @pytest.mark.parametrize('field_name,override_value,expected_value', [
        pytest.param('mae_weight', None, 0.0, id='mae_weight-default'),
        pytest.param('mae_weight', 0.3, 0.3, id='mae_weight-override'),
        pytest.param('realspace_mae_weight', None, 0.0, id='realspace_mae_weight-default'),
        pytest.param('realspace_weight', None, 0.0, id='realspace_weight-default'),
        pytest.param('positions_provided', None, True, id='positions_provided-default'),
        pytest.param('probe_trainable', None, False, id='probe_trainable-default'),
        pytest.param('sequential_sampling', None, False, id='sequential_sampling-default'),
    ])
    def test_training_config_override_fields(self, params_cfg_snapshot, field_name, override_value, expected_value):
        """
        Test TrainingConfig fields missing from PyTorch that require defaults/overrides.
    
        Spec coverage: §5.2:5 (mae_weight), §5.2:7 (realspace_mae_weight),
        §5.2:8 (realspace_weight), §5.2:15 (positions_provided),
        §5.2:16 (probe_trainable), §5.2:19 (sequential_sampling)
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig, TrainingConfig
        from ptycho_torch import config_bridge
    
        pt_data = DataConfig()
        pt_model = ModelConfig()
        pt_train = TrainingConfig()
    
        # Build overrides dict (always include required fields including nphotons to avoid validation error)
        overrides = dict(train_data_file=Path('train.npz'), n_groups=512, nphotons=1e9)
        if override_value is not None:
            overrides[field_name] = override_value
    
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:365: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
_ TestConfigBridgeParity.test_training_config_override_fields[realspace_weight-default] _

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda178c90>
params_cfg_snapshot = None, field_name = 'realspace_weight'
override_value = None, expected_value = 0.0

    @pytest.mark.parametrize('field_name,override_value,expected_value', [
        pytest.param('mae_weight', None, 0.0, id='mae_weight-default'),
        pytest.param('mae_weight', 0.3, 0.3, id='mae_weight-override'),
        pytest.param('realspace_mae_weight', None, 0.0, id='realspace_mae_weight-default'),
        pytest.param('realspace_weight', None, 0.0, id='realspace_weight-default'),
        pytest.param('positions_provided', None, True, id='positions_provided-default'),
        pytest.param('probe_trainable', None, False, id='probe_trainable-default'),
        pytest.param('sequential_sampling', None, False, id='sequential_sampling-default'),
    ])
    def test_training_config_override_fields(self, params_cfg_snapshot, field_name, override_value, expected_value):
        """
        Test TrainingConfig fields missing from PyTorch that require defaults/overrides.
    
        Spec coverage: §5.2:5 (mae_weight), §5.2:7 (realspace_mae_weight),
        §5.2:8 (realspace_weight), §5.2:15 (positions_provided),
        §5.2:16 (probe_trainable), §5.2:19 (sequential_sampling)
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig, TrainingConfig
        from ptycho_torch import config_bridge
    
        pt_data = DataConfig()
        pt_model = ModelConfig()
        pt_train = TrainingConfig()
    
        # Build overrides dict (always include required fields including nphotons to avoid validation error)
        overrides = dict(train_data_file=Path('train.npz'), n_groups=512, nphotons=1e9)
        if override_value is not None:
            overrides[field_name] = override_value
    
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:365: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
_ TestConfigBridgeParity.test_training_config_override_fields[positions_provided-default] _

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda178f50>
params_cfg_snapshot = None, field_name = 'positions_provided'
override_value = None, expected_value = True

    @pytest.mark.parametrize('field_name,override_value,expected_value', [
        pytest.param('mae_weight', None, 0.0, id='mae_weight-default'),
        pytest.param('mae_weight', 0.3, 0.3, id='mae_weight-override'),
        pytest.param('realspace_mae_weight', None, 0.0, id='realspace_mae_weight-default'),
        pytest.param('realspace_weight', None, 0.0, id='realspace_weight-default'),
        pytest.param('positions_provided', None, True, id='positions_provided-default'),
        pytest.param('probe_trainable', None, False, id='probe_trainable-default'),
        pytest.param('sequential_sampling', None, False, id='sequential_sampling-default'),
    ])
    def test_training_config_override_fields(self, params_cfg_snapshot, field_name, override_value, expected_value):
        """
        Test TrainingConfig fields missing from PyTorch that require defaults/overrides.
    
        Spec coverage: §5.2:5 (mae_weight), §5.2:7 (realspace_mae_weight),
        §5.2:8 (realspace_weight), §5.2:15 (positions_provided),
        §5.2:16 (probe_trainable), §5.2:19 (sequential_sampling)
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig, TrainingConfig
        from ptycho_torch import config_bridge
    
        pt_data = DataConfig()
        pt_model = ModelConfig()
        pt_train = TrainingConfig()
    
        # Build overrides dict (always include required fields including nphotons to avoid validation error)
        overrides = dict(train_data_file=Path('train.npz'), n_groups=512, nphotons=1e9)
        if override_value is not None:
            overrides[field_name] = override_value
    
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:365: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
_ TestConfigBridgeParity.test_training_config_override_fields[probe_trainable-default] _

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda179290>
params_cfg_snapshot = None, field_name = 'probe_trainable'
override_value = None, expected_value = False

    @pytest.mark.parametrize('field_name,override_value,expected_value', [
        pytest.param('mae_weight', None, 0.0, id='mae_weight-default'),
        pytest.param('mae_weight', 0.3, 0.3, id='mae_weight-override'),
        pytest.param('realspace_mae_weight', None, 0.0, id='realspace_mae_weight-default'),
        pytest.param('realspace_weight', None, 0.0, id='realspace_weight-default'),
        pytest.param('positions_provided', None, True, id='positions_provided-default'),
        pytest.param('probe_trainable', None, False, id='probe_trainable-default'),
        pytest.param('sequential_sampling', None, False, id='sequential_sampling-default'),
    ])
    def test_training_config_override_fields(self, params_cfg_snapshot, field_name, override_value, expected_value):
        """
        Test TrainingConfig fields missing from PyTorch that require defaults/overrides.
    
        Spec coverage: §5.2:5 (mae_weight), §5.2:7 (realspace_mae_weight),
        §5.2:8 (realspace_weight), §5.2:15 (positions_provided),
        §5.2:16 (probe_trainable), §5.2:19 (sequential_sampling)
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig, TrainingConfig
        from ptycho_torch import config_bridge
    
        pt_data = DataConfig()
        pt_model = ModelConfig()
        pt_train = TrainingConfig()
    
        # Build overrides dict (always include required fields including nphotons to avoid validation error)
        overrides = dict(train_data_file=Path('train.npz'), n_groups=512, nphotons=1e9)
        if override_value is not None:
            overrides[field_name] = override_value
    
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:365: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
_ TestConfigBridgeParity.test_training_config_override_fields[sequential_sampling-default] _

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda179590>
params_cfg_snapshot = None, field_name = 'sequential_sampling'
override_value = None, expected_value = False

    @pytest.mark.parametrize('field_name,override_value,expected_value', [
        pytest.param('mae_weight', None, 0.0, id='mae_weight-default'),
        pytest.param('mae_weight', 0.3, 0.3, id='mae_weight-override'),
        pytest.param('realspace_mae_weight', None, 0.0, id='realspace_mae_weight-default'),
        pytest.param('realspace_weight', None, 0.0, id='realspace_weight-default'),
        pytest.param('positions_provided', None, True, id='positions_provided-default'),
        pytest.param('probe_trainable', None, False, id='probe_trainable-default'),
        pytest.param('sequential_sampling', None, False, id='sequential_sampling-default'),
    ])
    def test_training_config_override_fields(self, params_cfg_snapshot, field_name, override_value, expected_value):
        """
        Test TrainingConfig fields missing from PyTorch that require defaults/overrides.
    
        Spec coverage: §5.2:5 (mae_weight), §5.2:7 (realspace_mae_weight),
        §5.2:8 (realspace_weight), §5.2:15 (positions_provided),
        §5.2:16 (probe_trainable), §5.2:19 (sequential_sampling)
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig, TrainingConfig
        from ptycho_torch import config_bridge
    
        pt_data = DataConfig()
        pt_model = ModelConfig()
        pt_train = TrainingConfig()
    
        # Build overrides dict (always include required fields including nphotons to avoid validation error)
        overrides = dict(train_data_file=Path('train.npz'), n_groups=512, nphotons=1e9)
        if override_value is not None:
            overrides[field_name] = override_value
    
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:365: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
_ TestConfigBridgeParity.test_inference_config_override_fields[debug-default] __

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda17a490>
params_cfg_snapshot = None, field_name = 'debug', override_value = None
expected_value = False

    @pytest.mark.parametrize('field_name,override_value,expected_value', [
        pytest.param('debug', None, False, id='debug-default'),
        pytest.param('debug', True, True, id='debug-override'),
    ])
    def test_inference_config_override_fields(self, params_cfg_snapshot, field_name, override_value, expected_value):
        """
        Test InferenceConfig fields missing from PyTorch that require defaults/overrides.
    
        Spec coverage: §5.3:8 (debug)
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig, InferenceConfig
        from ptycho_torch import config_bridge
    
        pt_data = DataConfig()
        pt_model = ModelConfig()
        pt_infer = InferenceConfig()
    
        # Build overrides dict (always include required fields)
        overrides = dict(
            model_path=Path('model_dir'),
            test_data_file=Path('test.npz'),
            n_groups=512
        )
        if override_value is not None:
            overrides[field_name] = override_value
    
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:399: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
_ TestConfigBridgeParity.test_inference_config_override_fields[debug-override] _

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda17a710>
params_cfg_snapshot = None, field_name = 'debug', override_value = True
expected_value = True

    @pytest.mark.parametrize('field_name,override_value,expected_value', [
        pytest.param('debug', None, False, id='debug-default'),
        pytest.param('debug', True, True, id='debug-override'),
    ])
    def test_inference_config_override_fields(self, params_cfg_snapshot, field_name, override_value, expected_value):
        """
        Test InferenceConfig fields missing from PyTorch that require defaults/overrides.
    
        Spec coverage: §5.3:8 (debug)
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig, InferenceConfig
        from ptycho_torch import config_bridge
    
        pt_data = DataConfig()
        pt_model = ModelConfig()
        pt_infer = InferenceConfig()
    
        # Build overrides dict (always include required fields)
        overrides = dict(
            model_path=Path('model_dir'),
            test_data_file=Path('test.npz'),
            n_groups=512
        )
        if override_value is not None:
            overrides[field_name] = override_value
    
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:399: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
_ TestConfigBridgeParity.test_model_config_probe_mask_translation[probe_mask-default] _

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda17b2d0>
params_cfg_snapshot = None, pytorch_value = None, expected_tf_value = False
description = 'default-None→False'

    @pytest.mark.parametrize('pytorch_value,expected_tf_value,description', [
        pytest.param(None, False, 'default-None→False', id='probe_mask-default'),
        # Note: Cannot test Tensor→True case without torch runtime, but logic is implemented
    ])
    def test_model_config_probe_mask_translation(self, params_cfg_snapshot, pytorch_value, expected_tf_value, description):
        """
        Test probe_mask translation from Optional[Tensor] to bool.
    
        Without torch: None → False (default behavior)
        With torch: None → False, Tensor → True
    
        Spec coverage: §5.1:8 (probe_mask)
        Phase: B.B5.B2 parity extension
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig
        from ptycho_torch import config_bridge
    
        pt_data = DataConfig()
        # Create ModelConfig with probe_mask=None (PyTorch default)
        pt_model = ModelConfig(probe_mask=pytorch_value)
    
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:432: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
_________ TestConfigBridgeParity.test_model_config_probe_mask_override _________

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda17b790>
params_cfg_snapshot = None

    def test_model_config_probe_mask_override(self, params_cfg_snapshot):
        """
        Test that probe_mask can be explicitly overridden via overrides dict.
    
        This allows external callers to force True even when PyTorch config has None.
    
        Spec coverage: §5.1:8 (probe_mask override pattern)
        Phase: B.B5.B2 parity extension
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig
        from ptycho_torch import config_bridge
    
        pt_data = DataConfig()
        pt_model = ModelConfig(probe_mask=None)  # PyTorch default
    
        # Override to True
>       tf_model = config_bridge.to_model_config(
            pt_data, pt_model,
            overrides={'probe_mask': True}
        )

tests/torch/test_config_bridge.py:453: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {'probe_mask': True}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
_ TestConfigBridgeParity.test_default_divergence_detection[nphotons-divergence] _

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda184790>
params_cfg_snapshot = None, field_name = 'nphotons', pytorch_default = 100000.0
tf_default = 1000000000.0, test_value = 500000000.0

    @pytest.mark.parametrize('field_name,pytorch_default,tf_default,test_value', [
        pytest.param('nphotons', 1e5, 1e9, 5e8, id='nphotons-divergence',
                     marks=pytest.mark.mvp),
        pytest.param('probe_scale', 1.0, 4.0, 2.0, id='probe_scale-divergence'),
    ])
    def test_default_divergence_detection(self, params_cfg_snapshot, field_name, pytorch_default, tf_default, test_value):
        """
        Test that fields with different PyTorch/TensorFlow defaults use explicit values.
    
        This ensures the adapter doesn't silently fall back to incompatible defaults.
    
        Spec coverage: §5.2:9 (nphotons HIGH risk), §5.1:10 (probe_scale MEDIUM risk)
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig, TrainingConfig
        from ptycho_torch import config_bridge
    
        # Create PyTorch config with explicit non-default value
        pt_data = DataConfig(**{field_name: test_value})
        pt_model = ModelConfig()
        pt_train = TrainingConfig()
    
        # For model-level fields, test via to_model_config
        if field_name in ['probe_scale']:
            tf_model = config_bridge.to_model_config(pt_data, pt_model)
            actual_value = getattr(tf_model, field_name)
        # For training-level fields, test via to_training_config
        else:
>           tf_model = config_bridge.to_model_config(pt_data, pt_model)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:492: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=500000000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Ne...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
__________ TestConfigBridgeParity.test_train_data_file_required_error __________

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda187190>
params_cfg_snapshot = None

    def test_train_data_file_required_error(self, params_cfg_snapshot):
        """Test that missing train_data_file raises actionable ValueError (MVP field)."""
        from ptycho_torch.config_params import DataConfig, ModelConfig, TrainingConfig
        from ptycho_torch import config_bridge
    
        pt_data = DataConfig()
        pt_model = ModelConfig()
        pt_train = TrainingConfig()
    
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:572: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
____________ TestConfigBridgeParity.test_model_path_required_error _____________

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda187810>
params_cfg_snapshot = None

    def test_model_path_required_error(self, params_cfg_snapshot):
        """Test that missing model_path raises actionable ValueError (MVP field)."""
        from ptycho_torch.config_params import DataConfig, ModelConfig, InferenceConfig
        from ptycho_torch import config_bridge
    
        pt_data = DataConfig()
        pt_model = ModelConfig()
        pt_infer = InferenceConfig()
    
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:592: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
________ TestConfigBridgeParity.test_nphotons_default_divergence_error _________

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda187f10>
params_cfg_snapshot = None

    def test_nphotons_default_divergence_error(self, params_cfg_snapshot):
        """
        Test that using PyTorch default nphotons without override raises informative error.
    
        Regression test for Phase B.B5.B4: Validates that the adapter enforces explicit
        nphotons override when PyTorch default (1e5) differs from TensorFlow default (1e9).
    
        Error message should provide actionable guidance: overrides=dict(..., nphotons=1e9)
    
        Spec coverage: §5.2:9 (nphotons HIGH risk divergence)
        Phase: B.B5.B4 override validation
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig, TrainingConfig
        from ptycho_torch import config_bridge
    
        # Use PyTorch default nphotons (1e5)
        pt_data = DataConfig(nphotons=1e5)
        pt_model = ModelConfig()
        pt_train = TrainingConfig()
    
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:627: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
_______ TestConfigBridgeParity.test_nphotons_override_passes_validation ________

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda185f10>
params_cfg_snapshot = None

    def test_nphotons_override_passes_validation(self, params_cfg_snapshot):
        """
        Test that explicit nphotons override passes validation (green path).
    
        Paired with test_nphotons_default_divergence_error to confirm validation
        accepts explicit overrides and only rejects PyTorch defaults.
    
        Spec coverage: §5.2:9 (nphotons override pattern)
        Phase: B.B5.B4 override validation
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig, TrainingConfig
        from ptycho_torch import config_bridge
    
        # Use PyTorch default nphotons (1e5) but provide explicit override
        pt_data = DataConfig(nphotons=1e5)
        pt_model = ModelConfig()
        pt_train = TrainingConfig()
    
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:663: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
_ TestConfigBridgeParity.test_training_config_n_subsample_missing_override_uses_none _

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda172250>
params_cfg_snapshot = None

    def test_training_config_n_subsample_missing_override_uses_none(self, params_cfg_snapshot):
        """
        Test that missing n_subsample override defaults to None in TrainingConfig.
    
        Semantic collision context: PyTorch DataConfig.n_subsample exists but has
        different semantics (coordinate subsampling factor, not sample count). The
        adapter must NOT propagate PyTorch n_subsample to TensorFlow; explicit
        override is required to set training sample count.
    
        Spec coverage: §5.2:12 (n_subsample override_required)
        Phase: Phase C.C1-C2 (n_subsample parity)
        Reference: field_matrix.md row 51
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig, TrainingConfig
        from ptycho_torch import config_bridge
    
        # PyTorch config with n_subsample=7 (coordinate subsampling, not sample count)
        pt_data = DataConfig(n_subsample=7)
        pt_model = ModelConfig()
        pt_train = TrainingConfig()
    
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:704: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
__ TestConfigBridgeParity.test_training_config_n_subsample_explicit_override ___

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda18c5d0>
params_cfg_snapshot = None

    def test_training_config_n_subsample_explicit_override(self, params_cfg_snapshot):
        """
        Test that explicit n_subsample override is applied to TrainingConfig.
    
        Green path confirming that when caller provides explicit n_subsample value,
        adapter applies it to TensorFlow config regardless of PyTorch value.
    
        Spec coverage: §5.2:12 (n_subsample override pattern)
        Phase: Phase C.C1-C2 (n_subsample parity)
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig, TrainingConfig
        from ptycho_torch import config_bridge
    
        pt_data = DataConfig(n_subsample=7)  # PyTorch coordinate subsampling
        pt_model = ModelConfig()
        pt_train = TrainingConfig()
    
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:739: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
_ TestConfigBridgeParity.test_inference_config_n_subsample_missing_override_uses_none _

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda18c950>
params_cfg_snapshot = None

    def test_inference_config_n_subsample_missing_override_uses_none(self, params_cfg_snapshot):
        """
        Test that missing n_subsample override defaults to None in InferenceConfig.
    
        Same semantic collision as TrainingConfig: PyTorch n_subsample has different
        meaning, so adapter must not propagate without explicit override.
    
        Spec coverage: §5.3:5 (n_subsample override_required)
        Phase: Phase C.C1-C2 (n_subsample parity)
        Reference: field_matrix.md row 69
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig, InferenceConfig
        from ptycho_torch import config_bridge
    
        pt_data = DataConfig(n_subsample=7)  # PyTorch coordinate subsampling
        pt_model = ModelConfig()
        pt_infer = InferenceConfig()
    
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:776: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
__ TestConfigBridgeParity.test_inference_config_n_subsample_explicit_override __

self = <test_config_bridge.TestConfigBridgeParity object at 0x74afda18ccd0>
params_cfg_snapshot = None

    def test_inference_config_n_subsample_explicit_override(self, params_cfg_snapshot):
        """
        Test that explicit n_subsample override is applied to InferenceConfig.
    
        Green path for inference-time sample count control.
    
        Spec coverage: §5.3:5 (n_subsample override pattern)
        Phase: Phase C.C1-C2 (n_subsample parity)
        """
        from ptycho_torch.config_params import DataConfig, ModelConfig, InferenceConfig
        from ptycho_torch import config_bridge
    
        pt_data = DataConfig(n_subsample=7)
        pt_model = ModelConfig()
        pt_infer = InferenceConfig()
    
>       tf_model = config_bridge.to_model_config(pt_data, pt_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_config_bridge.py:810: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, grid_size=(2, 2), neighbor_function='Neare...e=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9))
model = ModelConfig(mode='Unsupervised', intensity_scale_trainable=False, intensity_scale=10000.0, max_position_jitter=10, C_m...fset=6, C_forward=4, loss_function='Poisson', amp_loss=None, phase_loss=None, amp_loss_coeff=1.0, phase_loss_coeff=1.0)
overrides = {}

    def to_model_config(
        data: DataConfig,
        model: ModelConfig,
        overrides: Optional[Dict[str, Any]] = None
    ) -> TFModelConfig:
        """
        Translate PyTorch DataConfig and ModelConfig to TensorFlow ModelConfig.
    
        Performs critical field transformations:
        - grid_size tuple → gridsize int (extracts first element, assumes square grids)
        - mode enum → model_type enum ('Unsupervised'→'pinn', 'Supervised'→'supervised')
        - amp_activation normalization (silu→swish, SiLU→swish)
        - Merges fields from both PyTorch configs into single TensorFlow ModelConfig
    
        Args:
            data: PyTorch DataConfig instance (provides N, grid_size, nphotons)
            model: PyTorch ModelConfig instance (provides mode, architecture params)
            overrides: Optional dict of additional fields to override defaults
    
        Returns:
            TensorFlow ModelConfig instance with translated fields
    
        Raises:
            ValueError: If grid_size is non-square, mode has invalid value, or activation unknown
        """
        overrides = overrides or {}
    
        # Extract gridsize from grid_size tuple (assumes square grids)
        grid_h, grid_w = data.grid_size
        if grid_h != grid_w:
            raise ValueError(
                f"Non-square grids not supported by TensorFlow backend: "
                f"grid_size={data.grid_size}. Use square grids (e.g., (2, 2))."
            )
        gridsize = grid_h
    
        # Map mode enum to model_type enum
        mode_to_model_type = {
            'Unsupervised': 'pinn',
            'Supervised': 'supervised'
        }
        if model.mode not in mode_to_model_type:
            raise ValueError(
                f"Invalid mode '{model.mode}'. Must be 'Unsupervised' or 'Supervised'."
            )
        model_type = mode_to_model_type[model.mode]
    
        # Map PyTorch activation names to TensorFlow equivalents
        activation_mapping = {
            'silu': 'swish',
            'SiLU': 'swish',
            'sigmoid': 'sigmoid',
            'swish': 'swish',
            'softplus': 'softplus',
            'relu': 'relu'
        }
        if model.amp_activation not in activation_mapping:
            raise ValueError(
                f"Unknown activation '{model.amp_activation}'. "
                f"Supported values: {list(activation_mapping.keys())}"
            )
        amp_activation = activation_mapping[model.amp_activation]
    
        # Translate probe_mask from Optional[Tensor] to bool
        # None → False (no masking), non-None tensor → True (masking enabled)
        # Can be overridden explicitly via overrides dict
        probe_mask_value = False  # Default when None
        if TORCH_AVAILABLE and model.probe_mask is not None:
            # If torch available and probe_mask is a tensor, enable masking
            probe_mask_value = True
    
        # Validate probe_scale: PyTorch default (1.0) differs from TensorFlow default (4.0)
        # Require explicit override to avoid silent divergence (Phase B.B5.D3)
        pytorch_default_probe_scale = 1.0
        tensorflow_default_probe_scale = 4.0
        if 'probe_scale' not in overrides and data.probe_scale == pytorch_default_probe_scale:
>           raise ValueError(
                f"probe_scale default divergence detected: PyTorch default ({pytorch_default_probe_scale}) "
                f"differs from TensorFlow default ({tensorflow_default_probe_scale}). "
                f"Provide explicit probe_scale override to resolve: "
                f"overrides=dict(..., probe_scale={tensorflow_default_probe_scale})"
            )
E           ValueError: probe_scale default divergence detected: PyTorch default (1.0) differs from TensorFlow default (4.0). Provide explicit probe_scale override to resolve: overrides=dict(..., probe_scale=4.0)

ptycho_torch/config_bridge.py:157: ValueError
=========================== short test summary info ============================
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_direct_fields[N-direct]
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_direct_fields[n_filters_scale-direct]
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_direct_fields[object_big-direct]
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_direct_fields[probe_big-direct]
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_direct_fields[batch_size-direct]
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_transform_fields[gridsize-tuple-to-int]
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_transform_fields[model_type-unsupervised]
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_transform_fields[model_type-supervised]
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_transform_fields[amp_activation-silu]
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_transform_fields[amp_activation-SiLU]
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_transform_fields[amp_activation-passthrough]
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_transform_fields[nepochs-rename]
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_transform_fields[nll_weight-true]
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_transform_fields[nll_weight-false]
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_override_fields[pad_object-default]
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_override_fields[pad_object-override]
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_override_fields[gaussian_smoothing_sigma-default]
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_override_fields[gaussian_smoothing_sigma-override]
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[mae_weight-default]
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[mae_weight-override]
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[realspace_mae_weight-default]
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[realspace_weight-default]
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[positions_provided-default]
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[probe_trainable-default]
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[sequential_sampling-default]
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_inference_config_override_fields[debug-default]
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_inference_config_override_fields[debug-override]
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_probe_mask_translation[probe_mask-default]
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_probe_mask_override
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_default_divergence_detection[nphotons-divergence]
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_train_data_file_required_error
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_path_required_error
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_nphotons_default_divergence_error
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_nphotons_override_passes_validation
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_n_subsample_missing_override_uses_none
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_n_subsample_explicit_override
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_inference_config_n_subsample_missing_override_uses_none
FAILED tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_inference_config_n_subsample_explicit_override
========================= 38 failed, 8 passed in 3.62s =========================
