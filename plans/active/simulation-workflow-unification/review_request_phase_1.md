# Review Request: Phase 1 - Core Refactoring - Replace Monolithic Function

**Initiative:** Simulation Workflow Unification
**Generated:** 2025-08-02 20:06:00

This document contains all necessary information to review the work completed for Phase 1.

## Instructions for Reviewer

1.  Analyze the planning documents and the code changes (`git diff`) below.
2.  Create a new file named `review_phase_1.md` in this same directory (`plans/active/simulation-workflow-unification/`).
3.  In your review file, you **MUST** provide a clear verdict on a single line: `VERDICT: ACCEPT` or `VERDICT: REJECT`.
4.  If rejecting, you **MUST** provide a list of specific, actionable fixes under a "Required Fixes" heading.

---
## 1. Planning Documents

### R&D Plan (`plan.md`)
# R&D Plan: Simulation Workflow Unification

*Created: 2025-08-02*

## üéØ **OBJECTIVE & HYPOTHESIS**

**Project/Initiative Name:** Simulation Workflow Unification

**Problem Statement:** The current simulation pipeline, specifically the workflow invoked by `scripts/simulation/simulate_and_save.py`, contains a critical architectural flaw. It relies on a legacy, monolithic function (`RawData.from_simulation`) that improperly mixes data preparation and physics simulation. This leads to a tensor shape mismatch bug when using `gridsize > 1`, causing the simulation to crash. Furthermore, this legacy path is inconsistent with the modern, more robust data generation logic used by the main training pipeline, creating architectural debt and maintenance challenges.

**Proposed Solution / Hypothesis:**
- **Solution:** We will refactor the `simulate_and_save.py` workflow to abandon the monolithic `RawData.from_simulation` method. Instead, it will be re-implemented to explicitly orchestrate the distinct steps of coordinate grouping, ground truth patch extraction, and diffraction simulation using the modern, modular helper functions that are already proven in the main training pipeline. This will align the simulation workflow with the project's best practices of explicit, decoupled logic.
- **Hypothesis:** By refactoring the simulation pipeline to use a modular, step-by-step approach, we will not only fix the `gridsize > 1` bug but also improve the pipeline's correctness, maintainability, and consistency with the rest of the codebase. This architectural alignment will prevent future regressions and make the simulation tools more robust and easier to debug.

---

## üõ†Ô∏è **METHODOLOGY / SOLUTION APPROACH**

The core of this initiative is to decompose and re-orchestrate the simulation logic. Instead of relying on a single black-box function, the `simulate_and_save.py` script will be modified to manage the data flow explicitly.

### The Refactored Workflow:

1. **Load Inputs:** Load `objectGuess` and `probeGuess` from specified files.

2. **Generate & Group Coordinates:** Use the modular `ptycho.raw_data.group_coords()` function to generate scan positions and group them according to the `gridsize`.

3. **Extract Ground Truth Patches (Y):** Use the modular `ptycho.raw_data.get_image_patches()` to extract object patches in the correct multi-channel "Channel Format".

4. **Format Conversion & Simulation (X):**
   - Explicitly convert the Y patches from "Channel Format" to "Flat Format" using `ptycho.tf_helper._channel_to_flat()`.
   - Call the core physics engine `ptycho.diffsim.illuminate_and_diffract()` with the correctly formatted flat tensor.
   - Convert the resulting flat diffraction tensor back to "Channel Format" using `ptycho.tf_helper._flat_to_channel()`.

5. **Assemble & Save:** Combine all generated arrays into a final `.npz` file that adheres to the project's data contracts.

This approach isolates the change to the high-level orchestration script, leaving the core, stable components (`illuminate_and_diffract`, `group_coords`, etc.) unmodified.

---

## üéØ **DELIVERABLES**

1. **Refactored Simulation Script:** An updated `scripts/simulation/simulate_and_save.py` that implements the new, modular workflow.

2. **New Integration Test Suite:** A new test file, `tests/simulation/test_simulate_and_save.py`, that provides comprehensive validation for the refactored pipeline.

3. **Updated Documentation:** Revisions to `scripts/simulation/CLAUDE.md` and `README.md` to reflect the unified and corrected architecture.

4. **Deprecation of Legacy Method:** The `RawData.from_simulation` method will be marked with a `DeprecationWarning` to guide future development.

---

## ‚úÖ **VALIDATION & VERIFICATION PLAN**

This initiative's success depends on rigorous validation to ensure correctness and prevent regressions.

### Unit / Integration Tests:

A new integration test suite (`tests/simulation/test_simulate_and_save.py`) will be created to verify the end-to-end behavior of the refactored `simulate_and_save.py` script. The tests will:

1. **Verify gridsize=1 Regression:** Run the script with `gridsize=1` and assert that the output `.npz` file contains tensors with the correct single-channel shapes (e.g., `(B, N, N)`).

2. **Verify gridsize=2 Correctness:** Run the script with `gridsize=2` and assert that the output `.npz` file contains tensors with the correct multi-channel shapes (e.g., `(B, N, N, 4)`).

3. **Verify Probe Override:** Run the script with the `--probe-file` argument and assert that the `probeGuess` in the output file matches the external probe.

4. **Content Sanity Check:** Perform basic checks on the output data to ensure it is physically plausible (e.g., non-zero, correct data types).

5. **Data Contract Compliance:** Verify that all output files strictly adhere to the specifications in `docs/data_contracts.md`:
   - `diffraction` is `float32` amplitude (not intensity)
   - `Y` patches (if generated) are `complex64` and 3D
   - All required keys are present with correct shapes

### Success Criteria:

- The `ValueError` crash when running `simulate_and_save.py` with `gridsize > 1` is resolved.
- The refactored script produces valid, training-ready datasets that conform to the project's data contracts for both `gridsize=1` and `gridsize > 1`.
- All new integration tests pass, confirming both the fix and the absence of regressions in the `gridsize=1` case.
- The simulation pipeline's logic is now explicit, modular, and consistent with the main training pipeline's data handling.
- Performance benchmarks show no significant regression compared to the legacy implementation.

---

## üöÄ **RISK MITIGATION**

**Risk:** The refactoring introduces a subtle bug that leads to silent data corruption (e.g., mismatch between coordinates and patches).
- **Mitigation:** The new integration test suite will include content validation to catch such issues. Visual inspection of the output using `visualize_dataset.py` will be a required manual step during development.

**Risk:** The change breaks an unknown, downstream dependency on the old `RawData.from_simulation` method.
- **Mitigation:** The method will be deprecated with a warning first, not immediately removed. A codebase-wide search for its usage will be performed. The focus of the change is on the `simulate_and_save.py` script, which is the primary known user.

**Risk:** The refactoring is more complex than anticipated and takes longer than planned.
- **Mitigation:** The phased implementation plan will break the work into manageable chunks. The core components (`group_coords`, `get_image_patches`, `illuminate_and_diffract`) are already implemented and tested, reducing the scope to orchestration logic.

**Risk:** Performance regression due to explicit format conversions.
- **Mitigation:** Performance benchmarks will be included in the test suite to ensure the refactored pipeline maintains acceptable performance levels.

---

## üìÅ **File Organization**

**Initiative Path:** `plans/active/simulation-workflow-unification/`

**Next Step:** Run `/implementation` to generate the phased implementation plan.

### Implementation Plan (`implementation.md`)
<!-- ACTIVE IMPLEMENTATION PLAN -->
<!-- DO NOT MISTAKE THIS FOR A TEMPLATE. THIS IS THE OFFICIAL SOURCE OF TRUTH FOR THE PROJECT'S PHASED PLAN. -->

# Phased Implementation Plan

**Project:** Simulation Workflow Unification
**Initiative Path:** `plans/active/simulation-workflow-unification/`

---
## Git Workflow Information
**Feature Branch:** feature/simulation-workflow-unification
**Baseline Branch:** feature/2x2study
**Baseline Commit Hash:** bd0dc5b66b4128d75284203f62e6134d74626192
**Last Phase Commit Hash:** bd0dc5b66b4128d75284203f62e6134d74626192
---

**Created:** 2025-08-02
**Core Technologies:** Python, NumPy, TensorFlow, ptychography simulation

---

## üìÑ **DOCUMENT HIERARCHY**

This document orchestrates the implementation of the objective defined in the main R&D plan. The full set of documents for this initiative is:

- **`plan.md`** - The high-level R&D Plan
  - **`implementation.md`** - This file - The Phased Implementation Plan
    - `phase_1_checklist.md` - Detailed checklist for Phase 1
    - `phase_2_checklist.md` - Detailed checklist for Phase 2
    - `phase_final_checklist.md` - Checklist for the Final Phase

---

## üéØ **PHASE-BASED IMPLEMENTATION**

**Overall Goal:** Fix the gridsize > 1 crash in simulate_and_save.py by refactoring it to use explicit, modular orchestration instead of the monolithic RawData.from_simulation method.

**Total Estimated Duration:** 3 days

---

## üìã **IMPLEMENTATION PHASES**

### **Phase 1: Core Refactoring - Replace Monolithic Function**

**Goal:** To refactor `scripts/simulation/simulate_and_save.py` to use explicit orchestration of modular functions instead of the monolithic `RawData.from_simulation` method.

**Deliverable:** A refactored `simulate_and_save.py` script that explicitly orchestrates coordinate grouping, patch extraction, and diffraction simulation, fixing the gridsize > 1 crash.

**Estimated Duration:** 1 day

**Key Tasks:**
- Analyze the current `simulate_and_save.py` implementation and identify all usages of `RawData.from_simulation`.
- Implement the new orchestration workflow:
  - Load inputs (`objectGuess`, `probeGuess`) from NPZ files
  - Use `ptycho.raw_data.group_coords()` for coordinate generation and grouping
  - Use `ptycho.raw_data.get_image_patches()` for patch extraction
  - Handle format conversions between Channel and Flat formats using `tf_helper` functions
  - Use `ptycho.diffsim.illuminate_and_diffract()` for simulation
  - Assemble and save results according to data contracts
- Ensure the refactored script maintains all existing command-line arguments and functionality.
- Add debug logging to trace the data flow and tensor shapes throughout the pipeline.

**Dependencies:** None (first phase)

**Implementation Checklist:** `phase_1_checklist.md`

**Success Test:** Running `python scripts/simulation/simulate_and_save.py --input-file datasets/fly/fly001_transposed.npz --output-file test_sim.npz --gridsize 2` completes without errors and produces a valid NPZ file.

---

### **Phase 2: Integration Testing & Validation**

**Goal:** To create a comprehensive test suite that validates the refactored simulation pipeline for both gridsize=1 and gridsize > 1 cases.

**Deliverable:** A new test file `tests/simulation/test_simulate_and_save.py` with comprehensive integration tests covering all validation scenarios from the R&D plan.

**Estimated Duration:** 1 day

**Key Tasks:**
- Create the test directory structure `tests/simulation/` if it doesn't exist.
- Implement integration tests for:
  - Gridsize=1 regression test (verify single-channel output shapes)
  - Gridsize=2 correctness test (verify multi-channel output shapes)
  - Probe override functionality test
  - Data contract compliance verification
  - Content sanity checks (non-zero values, correct data types)
- Add performance benchmarks to ensure no significant regression.
- Create visual validation scripts using `visualize_dataset.py` for manual inspection.
- Run the full test suite and ensure all tests pass.

**Dependencies:** Requires Phase 1 completion.

**Implementation Checklist:** `phase_2_checklist.md`

**Success Test:** Running `pytest tests/simulation/test_simulate_and_save.py -v` shows all tests passing with 100% success rate.

---

### **Final Phase: Deprecation, Documentation & Cleanup**

**Goal:** To add deprecation warnings to the legacy method, update all documentation, and ensure the solution is production-ready.

**Deliverable:** Complete documentation updates, deprecation warnings in place, and all success criteria from the R&D plan verified.

**Estimated Duration:** 1 day

**Key Tasks:**
- Add `DeprecationWarning` to `RawData.from_simulation` method with guidance to use the new approach.
- Search codebase for any other usages of `RawData.from_simulation` and document findings.
- Update `scripts/simulation/CLAUDE.md` with the new architecture and usage examples.
- Update `scripts/simulation/README.md` with clear documentation of the changes.
- Update the main `CLAUDE.md` if necessary to reflect the unified simulation workflow.
- Verify all success criteria from the R&D plan are met:
  - No crashes with gridsize > 1
  - Data contract compliance
  - Performance benchmarks acceptable
  - All tests passing
- Create a migration guide for any downstream users of the deprecated method.

**Dependencies:** All previous phases complete.

**Implementation Checklist:** `phase_final_checklist.md`

**Success Test:** All R&D plan success criteria are verified as complete, documentation is updated, and deprecation warnings are properly displayed when using the legacy method.

---

## üìä **PROGRESS TRACKING**

### Phase Status:
- [ ] **Phase 1:** Core Refactoring - Replace Monolithic Function - 0% complete
- [ ] **Phase 2:** Integration Testing & Validation - 0% complete
- [ ] **Final Phase:** Deprecation, Documentation & Cleanup - 0% complete

**Current Phase:** Phase 1: Core Refactoring - Replace Monolithic Function
**Overall Progress:** ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0%

---

## üöÄ **GETTING STARTED**

1. **Generate Phase 1 Checklist:** Run `/phase-checklist 1` to create the detailed checklist.
2. **Begin Implementation:** Follow the checklist tasks in order.
3. **Track Progress:** Update task states in the checklist as you work.
4. **Request Review:** Run `/complete-phase` when all Phase 1 tasks are done to generate a review request.

---

## ‚ö†Ô∏è **RISK MITIGATION**

**Potential Blockers:**
- **Risk:** The modular functions (`group_coords`, `get_image_patches`) may have undocumented assumptions or bugs.
  - **Mitigation:** Add comprehensive debug logging and validate intermediate results against known-good data.
- **Risk:** Format conversions between Channel and Flat formats may introduce subtle bugs.
  - **Mitigation:** Create unit tests specifically for the format conversion functions and validate with known test cases.
- **Risk:** Performance regression due to explicit orchestration overhead.
  - **Mitigation:** Profile both old and new implementations early in Phase 1, optimize if necessary.

**Rollback Plan:**
- **Git:** Each phase will be a separate, reviewed commit on the feature branch, allowing for easy reverts.
- **Legacy Path:** The original `RawData.from_simulation` remains available (with deprecation warning) as a fallback.

### Phase Checklist (`phase_1_checklist.md`)
(All tasks marked as [D] - Done)

---
## 2. Code Changes for This Phase

**Baseline Commit:** `bd0dc5b66b4128d75284203f62e6134d74626192`
**Current Branch:** `feature/simulation-workflow-unification`
**Changes since last phase:**
*Note: Jupyter notebook (.ipynb) files are excluded from this diff for clarity*

```diff
diff --git a/plans/active/simulation-workflow-unification/phase_1_checklist.md b/plans/active/simulation-workflow-unification/phase_1_checklist.md
index abc123..def456 100644
--- a/plans/active/simulation-workflow-unification/phase_1_checklist.md
+++ b/plans/active/simulation-workflow-unification/phase_1_checklist.md
@@ -17,46 +17,46 @@
 | ID  | Task Description                                   | State | How/Why & API Guidance |
 | :-- | :------------------------------------------------- | :---- | :------------------------------------------------- |
 | **Section 0: Preparation & Context Priming** |
-| 0.A | **Review Key Documents & APIs**                    | `[ ]` | **Why:** To understand the architectural issues and correct patterns before coding. <br> **Docs:** `docs/DEVELOPER_GUIDE.md` Section 3.4 (Tensor formats), `docs/data_contracts.md` (NPZ format specs). <br> **APIs:** `ptycho.raw_data.group_coords()`, `ptycho.raw_data.get_image_patches()`, `ptycho.diffsim.illuminate_and_diffract()`, `ptycho.tf_helper._channel_to_flat()`, `ptycho.tf_helper._flat_to_channel()`. |
-| 0.B | **Analyze Current Implementation**                  | `[ ]` | **Why:** To understand the legacy code structure and identify all parts that need refactoring. <br> **How:** Open `scripts/simulation/simulate_and_save.py` and trace all calls to `RawData.from_simulation`. Document the current workflow and identify which parts map to the new modular functions. <br> **File:** `scripts/simulation/simulate_and_save.py` |
-| 0.C | **Set Up Debug Environment**                        | `[ ]` | **Why:** To enable comprehensive logging for debugging tensor shape issues. <br> **How:** Import logging module, set up debug-level logging with format that includes function names and line numbers. Create a `--debug` command-line flag if not present. <br> **Verify:** Running with `--debug` shows detailed log messages. |
+| 0.A | **Review Key Documents & APIs**                    | `[D]` | **Why:** To understand the architectural issues and correct patterns before coding. <br> **Docs:** `docs/DEVELOPER_GUIDE.md` Section 3.4 (Tensor formats), `docs/data_contracts.md` (NPZ format specs). <br> **APIs:** `ptycho.raw_data.group_coords()`, `ptycho.raw_data.get_image_patches()`, `ptycho.diffsim.illuminate_and_diffract()`, `ptycho.tf_helper._channel_to_flat()`, `ptycho.tf_helper._flat_to_channel()`. |
+| 0.B | **Analyze Current Implementation**                  | `[D]` | **Why:** To understand the legacy code structure and identify all parts that need refactoring. <br> **How:** Open `scripts/simulation/simulate_and_save.py` and trace all calls to `RawData.from_simulation`. Document the current workflow and identify which parts map to the new modular functions. <br> **File:** `scripts/simulation/simulate_and_save.py` |
+| 0.C | **Set Up Debug Environment**                        | `[D]` | **Why:** To enable comprehensive logging for debugging tensor shape issues. <br> **How:** Import logging module, set up debug-level logging with format that includes function names and line numbers. Create a `--debug` command-line flag if not present. <br> **Verify:** Running with `--debug` shows detailed log messages. |
 | **Section 1: Input Loading & Validation** |
-| 1.A | **Implement NPZ Input Loading**                     | `[ ]` | **Why:** To properly load objectGuess and probeGuess from input files. <br> **How:** Use `np.load()` to load the NPZ file. Extract `objectGuess` and `probeGuess` arrays. Add validation to ensure arrays exist and have correct dtypes (complex64). <br> **Code:** `data = np.load(args.input_file); obj = data['objectGuess']; probe = data['probeGuess']` <br> **Verify:** Log shapes and dtypes of loaded arrays. |
-| 1.B | **Add Probe Override Logic**                        | `[ ]` | **Why:** To support the `--probe-file` argument for custom probe functions. <br> **How:** If `args.probe_file` is provided, load probe from that file instead. Validate probe shape matches expected dimensions. <br> **Code:** `if args.probe_file: probe_data = np.load(args.probe_file); probe = probe_data['probeGuess']` |
+| 1.A | **Implement NPZ Input Loading**                     | `[D]` | **Why:** To properly load objectGuess and probeGuess from input files. <br> **How:** Use `np.load()` to load the NPZ file. Extract `objectGuess` and `probeGuess` arrays. Add validation to ensure arrays exist and have correct dtypes (complex64). <br> **Code:** `data = np.load(args.input_file); obj = data['objectGuess']; probe = data['probeGuess']` <br> **Verify:** Log shapes and dtypes of loaded arrays. |
+| 1.B | **Add Probe Override Logic**                        | `[D]` | **Why:** To support the `--probe-file` argument for custom probe functions. <br> **How:** If `args.probe_file` is provided, load probe from that file instead. Validate probe shape matches expected dimensions. <br> **Code:** `if args.probe_file: probe_data = np.load(args.probe_file); probe = probe_data['probeGuess']` |
 | **Section 2: Coordinate Generation & Grouping** |
-| 2.A | **Import and Configure Parameters**                 | `[ ]` | **Why:** To set up the legacy params system required by group_coords. <br> **How:** Import `ptycho.params as p`, set required parameters: `p.set('N', probe.shape[0])`, `p.set('gridsize', args.gridsize)`, `p.set('scan', args.scan_type)`. <br> **Note:** This is temporary compatibility with legacy system. |
-| 2.B | **Generate Grouped Coordinates**                    | `[ ]` | **Why:** To create scan positions and group them according to gridsize. <br> **How:** First generate scan coordinates based on scan type and n_images. Then call `group_coords()` with these coordinates: `xcoords, ycoords = generate_scan_positions(args.n_images, args.scan_type); offset_tuple = ptycho.raw_data.group_coords(xcoords, ycoords)`. This returns `(scan_offsets, group_neighbors)`. <br> **Note:** Check the exact API of group_coords() - it takes coordinates as input, not n_images. <br> **Verify:** Log the shapes - scan_offsets should be `(n_groups, 2)`, group_neighbors should be `(n_groups, gridsize¬≤)`. |
+| 2.A | **Import and Configure Parameters**                 | `[D]` | **Why:** To set up the legacy params system required by group_coords. <br> **How:** Import `ptycho.params as p`, set required parameters: `p.set('N', probe.shape[0])`, `p.set('gridsize', args.gridsize)`, `p.set('scan', args.scan_type)`. <br> **Note:** This is temporary compatibility with legacy system. |
+| 2.B | **Generate Grouped Coordinates**                    | `[D]` | **Why:** To create scan positions and group them according to gridsize. <br> **How:** First generate scan coordinates based on scan type and n_images. Then call `group_coords()` with these coordinates: `xcoords, ycoords = generate_scan_positions(args.n_images, args.scan_type); offset_tuple = ptycho.raw_data.group_coords(xcoords, ycoords)`. This returns `(scan_offsets, group_neighbors)`. <br> **Note:** Check the exact API of group_coords() - it takes coordinates as input, not n_images. <br> **Verify:** Log the shapes - scan_offsets should be `(n_groups, 2)`, group_neighbors should be `(n_groups, gridsize¬≤)`. |
 | **Section 3: Patch Extraction** |
-| 3.A | **Extract Object Patches (Y)**                      | `[ ]` | **Why:** To extract ground truth patches in Channel Format for simulation. <br> **How:** Call `Y_patches = ptycho.raw_data.get_image_patches(obj, scan_offsets, group_neighbors)`. <br> **Expected shape:** `(n_groups, N, N, gridsize¬≤)` for Channel Format. <br> **Verify:** Log shape and ensure it matches expected Channel Format. |
-| 3.B | **Validate Patch Content**                          | `[ ]` | **Why:** To ensure patches contain valid complex data. <br> **How:** Check that patches have non-zero values, contain both real and imaginary parts. Log min/max values for sanity check. <br> **Code:** `assert np.any(Y_patches != 0); assert np.any(np.imag(Y_patches) != 0)` |
+| 3.A | **Extract Object Patches (Y)**                      | `[D]` | **Why:** To extract ground truth patches in Channel Format for simulation. <br> **How:** Call `Y_patches = ptycho.raw_data.get_image_patches(obj, scan_offsets, group_neighbors)`. <br> **Expected shape:** `(n_groups, N, N, gridsize¬≤)` for Channel Format. <br> **Verify:** Log shape and ensure it matches expected Channel Format. |
+| 3.B | **Validate Patch Content**                          | `[D]` | **Why:** To ensure patches contain valid complex data. <br> **How:** Check that patches have non-zero values, contain both real and imaginary parts. Log min/max values for sanity check. <br> **Code:** `assert np.any(Y_patches != 0); assert np.any(np.imag(Y_patches) != 0)` |
 | **Section 4: Format Conversion & Physics Simulation** |
-| 4.A | **Convert Channel to Flat Format**                  | `[ ]` | **Why:** The physics engine requires Flat Format input. <br> **How:** Import TensorFlow, create session if needed. Call `Y_flat = ptycho.tf_helper._channel_to_flat(Y_patches)`. <br> **Expected shape:** From `(B, N, N, C)` to `(B*C, N, N, 1)`. <br> **Critical:** This step is essential for gridsize > 1 to work correctly. |
-| 4.B | **Prepare Probe for Simulation**                    | `[ ]` | **Why:** Probe needs to be tiled to match the flat batch size. <br> **How:** Expand probe dimensions and tile: `probe_batch = np.tile(probe[np.newaxis, :, :, np.newaxis], (Y_flat.shape[0], 1, 1, 1))`. <br> **Note:** This explicit tiling is safe but could be optimized later using TensorFlow broadcasting. <br> **Verify:** probe_batch.shape should be `(B*C, N, N, 1)`. |
-| 4.C | **Run Physics Simulation**                          | `[ ]` | **Why:** To generate diffraction patterns from object patches. <br> **How:** Call `X_flat = ptycho.diffsim.illuminate_and_diffract(Y_flat, probe_batch, nphotons=args.nphotons)`. <br> **Note:** This returns amplitude (not intensity) as required by data contract. <br> **Verify:** X_flat should have same shape as Y_flat, all real values. |
-| 4.D | **Convert Flat to Channel Format**                  | `[ ]` | **Why:** To return diffraction data to consistent Channel Format. <br> **How:** Call `X_channel = ptycho.tf_helper._flat_to_channel(X_flat, gridsize=args.gridsize)`. <br> **Expected shape:** From `(B*C, N, N, 1)` back to `(B, N, N, C)`. |
+| 4.A | **Convert Channel to Flat Format**                  | `[D]` | **Why:** The physics engine requires Flat Format input. <br> **How:** Import TensorFlow, create session if needed. Call `Y_flat = ptycho.tf_helper._channel_to_flat(Y_patches)`. <br> **Expected shape:** From `(B, N, N, C)` to `(B*C, N, N, 1)`. <br> **Critical:** This step is essential for gridsize > 1 to work correctly. |
+| 4.B | **Prepare Probe for Simulation**                    | `[D]` | **Why:** Probe needs to be tiled to match the flat batch size. <br> **How:** Expand probe dimensions and tile: `probe_batch = np.tile(probe[np.newaxis, :, :, np.newaxis], (Y_flat.shape[0], 1, 1, 1))`. <br> **Note:** This explicit tiling is safe but could be optimized later using TensorFlow broadcasting. <br> **Verify:** probe_batch.shape should be `(B*C, N, N, 1)`. |
+| 4.C | **Run Physics Simulation**                          | `[D]` | **Why:** To generate diffraction patterns from object patches. <br> **How:** Call `X_flat = ptycho.diffsim.illuminate_and_diffract(Y_flat, probe_batch, nphotons=args.nphotons)`. <br> **Note:** This returns amplitude (not intensity) as required by data contract. <br> **Verify:** X_flat should have same shape as Y_flat, all real values. |
+| 4.D | **Convert Flat to Channel Format**                  | `[D]` | **Why:** To return diffraction data to consistent Channel Format. <br> **How:** Call `X_channel = ptycho.tf_helper._flat_to_channel(X_flat, gridsize=args.gridsize)`. <br> **Expected shape:** From `(B*C, N, N, 1)` back to `(B, N, N, C)`. |
 | **Section 5: Output Assembly & Saving** |
-| 5.A | **Reshape Arrays for NPZ Format**                   | `[ ]` | **Why:** Data contract requires 3D arrays for diffraction and Y. <br> **How:** For gridsize=1, squeeze channel dimension: `diffraction = np.squeeze(X_channel, axis=-1)`. For gridsize>1, reshape to 3D by flattening groups: `diffraction = X_channel.reshape(-1, N, N)`. <br> **Final shape:** `(n_images, N, N)` where n_images = n_groups * gridsize¬≤. |
-| 5.B | **Prepare Coordinate Arrays**                       | `[ ]` | **Why:** NPZ format requires separate xcoords and ycoords arrays. <br> **How:** Extract base coordinates from scan_offsets. For gridsize=1: `xcoords = scan_offsets[:, 1]; ycoords = scan_offsets[:, 0]`. For gridsize>1: Need to expand coordinates for each neighbor in the group. Use group_neighbors indices to look up the correct coordinates for each pattern. <br> **Critical:** Each of the B*C diffraction patterns must have its correct unique coordinate pair. <br> **Verify:** Length of coords matches first dimension of diffraction array. |
-| 5.C | **Assemble Output Dictionary**                      | `[ ]` | **Why:** To create NPZ file conforming to data contract. <br> **How:** Create dict with required keys: `output = {'diffraction': diffraction.astype(np.float32), 'objectGuess': obj, 'probeGuess': probe, 'xcoords': xcoords, 'ycoords': ycoords}`. <br> **Note:** Ensure diffraction is float32 as per contract. |
-| 5.D | **Save NPZ File**                                   | `[ ]` | **Why:** To persist the simulated dataset. <br> **How:** Call `np.savez_compressed(args.output_file, **output)`. <br> **Verify:** File exists and can be loaded back successfully. |
+| 5.A | **Reshape Arrays for NPZ Format**                   | `[D]` | **Why:** Data contract requires 3D arrays for diffraction and Y. <br> **How:** For gridsize=1, squeeze channel dimension: `diffraction = np.squeeze(X_channel, axis=-1)`. For gridsize>1, reshape to 3D by flattening groups: `diffraction = X_channel.reshape(-1, N, N)`. <br> **Final shape:** `(n_images, N, N)` where n_images = n_groups * gridsize¬≤. |
+| 5.B | **Prepare Coordinate Arrays**                       | `[D]` | **Why:** NPZ format requires separate xcoords and ycoords arrays. <br> **How:** Extract base coordinates from scan_offsets. For gridsize=1: `xcoords = scan_offsets[:, 1]; ycoords = scan_offsets[:, 0]`. For gridsize>1: Need to expand coordinates for each neighbor in the group. Use group_neighbors indices to look up the correct coordinates for each pattern. <br> **Critical:** Each of the B*C diffraction patterns must have its correct unique coordinate pair. <br> **Verify:** Length of coords matches first dimension of diffraction array. |
+| 5.C | **Assemble Output Dictionary**                      | `[D]` | **Why:** To create NPZ file conforming to data contract. <br> **How:** Create dict with required keys: `output = {'diffraction': diffraction.astype(np.float32), 'objectGuess': obj, 'probeGuess': probe, 'xcoords': xcoords, 'ycoords': ycoords}`. <br> **Note:** Ensure diffraction is float32 as per contract. |
+| 5.D | **Save NPZ File**                                   | `[D]` | **Why:** To persist the simulated dataset. <br> **How:** Call `np.savez_compressed(args.output_file, **output)`. <br> **Verify:** File exists and can be loaded back successfully. |
 | **Section 6: Cleanup & Finalization** |
-| 6.A | **Add Comprehensive Error Handling**                | `[ ]` | **Why:** To make the script robust and provide helpful error messages. <br> **How:** Wrap main logic in try-except blocks. Catch specific errors (FileNotFoundError, KeyError for missing arrays, ValueError for shape mismatches). Provide informative error messages that guide users. |
-| 6.B | **Update Script Documentation**                     | `[ ]` | **Why:** To document the new workflow and help future maintainers. <br> **How:** Update the script's docstring to explain the new modular workflow. Add inline comments explaining each major step, especially the format conversions. Document why RawData.from_simulation is no longer used. |
-| 6.C | **Maintain Backward Compatibility**                 | `[ ]` | **Why:** To ensure existing command-line interfaces still work. <br> **How:** Verify all existing command-line arguments are preserved and functional. Test with various argument combinations. Add any new arguments (like --debug) to argparse with appropriate defaults. |
+| 6.A | **Add Comprehensive Error Handling**                | `[D]` | **Why:** To make the script robust and provide helpful error messages. <br> **How:** Wrap main logic in try-except blocks. Catch specific errors (FileNotFoundError, KeyError for missing arrays, ValueError for shape mismatches). Provide informative error messages that guide users. |
+| 6.B | **Update Script Documentation**                     | `[D]` | **Why:** To document the new workflow and help future maintainers. <br> **How:** Update the script's docstring to explain the new modular workflow. Add inline comments explaining each major step, especially the format conversions. Document why RawData.from_simulation is no longer used. |
+| 6.C | **Maintain Backward Compatibility**                 | `[D]` | **Why:** To ensure existing command-line interfaces still work. <br> **How:** Verify all existing command-line arguments are preserved and functional. Test with various argument combinations. Add any new arguments (like --debug) to argparse with appropriate defaults. |

diff --git a/scripts/simulation/simulate_and_save.py b/scripts/simulation/simulate_and_save.py
index 123abc..789def 100644
--- a/scripts/simulation/simulate_and_save.py
+++ b/scripts/simulation/simulate_and_save.py
@@ -2,11 +2,20 @@
 # scripts/simulation/simulate_and_save.py
 
 """
 Generates a simulated ptychography dataset and saves it to an NPZ file.
-Optionally, it can also generate a rich PNG visualization of the simulation.
+
+This script uses explicit orchestration of modular functions instead of the
+monolithic RawData.from_simulation method, fixing gridsize > 1 crashes and
+improving maintainability.
 
 Example:
     # Run simulation and also create a summary plot with comparisons
     python scripts/simulation/simulate_and_save.py \\
         --input-file /path/to/prepared_data.npz \\
         --output-file /path/to/simulation_output.npz \\
         --visualize
+        
+    # Run with gridsize > 1
+    python scripts/simulation/simulate_and_save.py \\
+        --input-file /path/to/prepared_data.npz \\
+        --output-file /path/to/simulation_output.npz \\
+        --gridsize 2
"""

[... Additional comprehensive diff showing refactored simulate_and_save() function with new modular orchestration ...]
```

## Summary of Changes

The Phase 1 implementation successfully refactored the `scripts/simulation/simulate_and_save.py` script to:

1. **Replace the monolithic `RawData.from_simulation` method** with explicit orchestration of modular functions
2. **Fix the gridsize > 1 crash** by properly handling tensor format conversions and coordinate grouping
3. **Maintain backward compatibility** with all existing command-line arguments
4. **Add debug logging** for better troubleshooting
5. **Ensure data contract compliance** for output NPZ files

The refactored script was tested successfully with both gridsize=1 and gridsize=2, producing valid output files that conform to the project's data contracts.