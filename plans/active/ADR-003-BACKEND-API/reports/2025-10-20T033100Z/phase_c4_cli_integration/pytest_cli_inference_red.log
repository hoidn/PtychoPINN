============================= test session starts ==============================
platform linux -- Python 3.11.13, pytest-8.4.1, pluggy-1.6.0 -- /home/ollie/miniconda3/envs/ptycho311/bin/python3.11
cachedir: .pytest_cache
PyTorch: available
PyTorch version: 2.8.0+cu128
rootdir: /home/ollie/Documents/PtychoPINN2
configfile: pyproject.toml
plugins: anyio-4.9.0
collecting ... collected 4 items

tests/torch/test_cli_inference_torch.py::TestInferenceCLI::test_accelerator_flag_roundtrip FAILED [ 25%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLI::test_num_workers_flag_roundtrip FAILED [ 50%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLI::test_inference_batch_size_flag_roundtrip FAILED [ 75%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLI::test_multiple_execution_config_flags FAILED [100%]

=================================== FAILURES ===================================
_______________ TestInferenceCLI.test_accelerator_flag_roundtrip _______________

self = <test_cli_inference_torch.TestInferenceCLI object at 0x78fcfc7f4110>
minimal_inference_args = ['--model_path', '/tmp/pytest-of-ollie/pytest-669/test_accelerator_flag_roundtri0/model', '--test_data', '/tmp/pytest-...i0/test.npz', '--output_dir', '/tmp/pytest-of-ollie/pytest-669/test_accelerator_flag_roundtri0/inference_outputs', ...]
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x78fcf9dbca50>

    def test_accelerator_flag_roundtrip(self, minimal_inference_args, monkeypatch):
        """
        RED Test: --accelerator flag maps to execution_config.accelerator.
    
        Expected RED Failure:
        - argparse.ArgumentError: unrecognized arguments: --accelerator cpu
        OR
        - AssertionError: execution_config.accelerator != 'cpu'
        """
        mock_factory = MagicMock()
        mock_factory.return_value = MagicMock(
            tf_inference_config=MagicMock(),
            data_config=MagicMock(),
            execution_config=MagicMock(accelerator='cpu'),
        )
    
        with patch('ptycho_torch.config_factory.create_inference_payload', mock_factory):
            test_args = minimal_inference_args + ['--accelerator', 'cpu']
    
            from ptycho_torch.inference import cli_main
            monkeypatch.setattr('sys.argv', ['inference.py'] + test_args)
    
            try:
                cli_main()
            except SystemExit:
                pass
    
>       assert mock_factory.called, "Factory was not called"
E       AssertionError: Factory was not called
E       assert False
E        +  where False = <MagicMock id='133027962411600'>.called

tests/torch/test_cli_inference_torch.py:83: AssertionError
----------------------------- Captured stdout call -----------------------------
No GPU found, using CPU instead.
----------------------------- Captured stderr call -----------------------------
2025-10-19 20:44:16.126328: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1760931856.137481 1377313 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1760931856.141287 1377313 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1760931856.152131 1377313 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1760931856.152147 1377313 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1760931856.152150 1377313 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1760931856.152152 1377313 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-10-19 20:44:16.154830: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-10-19 20:44:18.220556: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2025-10-19 20:44:18.220592: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:167] env: CUDA_VISIBLE_DEVICES=""
2025-10-19 20:44:18.220598: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:170] CUDA_VISIBLE_DEVICES is set to an empty string - this hides all GPUs from CUDA
2025-10-19 20:44:18.220601: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module
2025-10-19 20:44:18.220606: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: ollie-System-Product-Name
2025-10-19 20:44:18.220609: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] hostname: ollie-System-Product-Name
2025-10-19 20:44:18.220634: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 570.172.8
2025-10-19 20:44:18.220649: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 570.172.8
2025-10-19 20:44:18.220653: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:291] kernel version seems to match DSO: 570.172.8
usage: inference.py [-h] --model_path MODEL_PATH --test_data TEST_DATA
                    --output_dir OUTPUT_DIR [--n_images N_IMAGES]
                    [--device {cpu,cuda}] [--quiet]
inference.py: error: unrecognized arguments: --accelerator cpu
_______________ TestInferenceCLI.test_num_workers_flag_roundtrip _______________

self = <test_cli_inference_torch.TestInferenceCLI object at 0x78fce779ecd0>
minimal_inference_args = ['--model_path', '/tmp/pytest-of-ollie/pytest-669/test_num_workers_flag_roundtri0/model', '--test_data', '/tmp/pytest-...i0/test.npz', '--output_dir', '/tmp/pytest-of-ollie/pytest-669/test_num_workers_flag_roundtri0/inference_outputs', ...]
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x78fc2877bf50>

    def test_num_workers_flag_roundtrip(self, minimal_inference_args, monkeypatch):
        """
        RED Test: --num-workers flag maps to execution_config.num_workers.
    
        Expected RED Failure:
        - argparse.ArgumentError: unrecognized arguments: --num-workers 4
        OR
        - AssertionError: execution_config.num_workers != 4
        """
        mock_factory = MagicMock()
        mock_factory.return_value = MagicMock(
            tf_inference_config=MagicMock(),
            execution_config=MagicMock(num_workers=4),
        )
    
        with patch('ptycho_torch.config_factory.create_inference_payload', mock_factory):
            test_args = minimal_inference_args + ['--num-workers', '4']
    
            from ptycho_torch.inference import cli_main
            monkeypatch.setattr('sys.argv', ['inference.py'] + test_args)
    
            try:
                cli_main()
            except SystemExit:
                pass
    
>       assert mock_factory.called
E       AssertionError: assert False
E        +  where False = <MagicMock id='133024406027984'>.called

tests/torch/test_cli_inference_torch.py:115: AssertionError
----------------------------- Captured stderr call -----------------------------
usage: inference.py [-h] --model_path MODEL_PATH --test_data TEST_DATA
                    --output_dir OUTPUT_DIR [--n_images N_IMAGES]
                    [--device {cpu,cuda}] [--quiet]
inference.py: error: unrecognized arguments: --num-workers 4
__________ TestInferenceCLI.test_inference_batch_size_flag_roundtrip ___________

self = <test_cli_inference_torch.TestInferenceCLI object at 0x78fce779c090>
minimal_inference_args = ['--model_path', '/tmp/pytest-of-ollie/pytest-669/test_inference_batch_size_flag0/model', '--test_data', '/tmp/pytest-...g0/test.npz', '--output_dir', '/tmp/pytest-of-ollie/pytest-669/test_inference_batch_size_flag0/inference_outputs', ...]
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x78fc287c3750>

    def test_inference_batch_size_flag_roundtrip(self, minimal_inference_args, monkeypatch):
        """
        RED Test: --inference-batch-size flag maps to execution_config.inference_batch_size.
    
        Expected RED Failure:
        - argparse.ArgumentError: unrecognized arguments: --inference-batch-size 32
        OR
        - AssertionError: execution_config.inference_batch_size != 32
        """
        mock_factory = MagicMock()
        mock_factory.return_value = MagicMock(
            tf_inference_config=MagicMock(),
            execution_config=MagicMock(inference_batch_size=32),
        )
    
        with patch('ptycho_torch.config_factory.create_inference_payload', mock_factory):
            test_args = minimal_inference_args + ['--inference-batch-size', '32']
    
            from ptycho_torch.inference import cli_main
            monkeypatch.setattr('sys.argv', ['inference.py'] + test_args)
    
            try:
                cli_main()
            except SystemExit:
                pass
    
>       assert mock_factory.called
E       AssertionError: assert False
E        +  where False = <MagicMock id='133024406321744'>.called

tests/torch/test_cli_inference_torch.py:147: AssertionError
----------------------------- Captured stderr call -----------------------------
usage: inference.py [-h] --model_path MODEL_PATH --test_data TEST_DATA
                    --output_dir OUTPUT_DIR [--n_images N_IMAGES]
                    [--device {cpu,cuda}] [--quiet]
inference.py: error: unrecognized arguments: --inference-batch-size 32
____________ TestInferenceCLI.test_multiple_execution_config_flags _____________

self = <test_cli_inference_torch.TestInferenceCLI object at 0x78fce779c450>
minimal_inference_args = ['--model_path', '/tmp/pytest-of-ollie/pytest-669/test_multiple_execution_config0/model', '--test_data', '/tmp/pytest-...g0/test.npz', '--output_dir', '/tmp/pytest-of-ollie/pytest-669/test_multiple_execution_config0/inference_outputs', ...]
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x78fc287b0b90>

    def test_multiple_execution_config_flags(self, minimal_inference_args, monkeypatch):
        """
        RED Test: Multiple execution config flags work together.
    
        Expected RED Failure:
        - argparse.ArgumentError: unrecognized arguments (any of the new flags)
        OR
        - AssertionError: execution_config fields do not match expected values
        """
        mock_factory = MagicMock()
        mock_factory.return_value = MagicMock(
            tf_inference_config=MagicMock(),
            execution_config=MagicMock(
                accelerator='gpu',
                num_workers=8,
                inference_batch_size=64,
            ),
        )
    
        with patch('ptycho_torch.config_factory.create_inference_payload', mock_factory):
            test_args = minimal_inference_args + [
                '--accelerator', 'gpu',
                '--num-workers', '8',
                '--inference-batch-size', '64',
            ]
    
            from ptycho_torch.inference import cli_main
            monkeypatch.setattr('sys.argv', ['inference.py'] + test_args)
    
            try:
                cli_main()
            except SystemExit:
                pass
    
>       assert mock_factory.called
E       AssertionError: assert False
E        +  where False = <MagicMock id='133024406253008'>.called

tests/torch/test_cli_inference_torch.py:187: AssertionError
----------------------------- Captured stderr call -----------------------------
usage: inference.py [-h] --model_path MODEL_PATH --test_data TEST_DATA
                    --output_dir OUTPUT_DIR [--n_images N_IMAGES]
                    [--device {cpu,cuda}] [--quiet]
inference.py: error: unrecognized arguments: --accelerator gpu --num-workers 8 --inference-batch-size 64
=========================== short test summary info ============================
FAILED tests/torch/test_cli_inference_torch.py::TestInferenceCLI::test_accelerator_flag_roundtrip - AssertionError: Factory was not called
assert False
 +  where False = <MagicMock id='133027962411600'>.called
FAILED tests/torch/test_cli_inference_torch.py::TestInferenceCLI::test_num_workers_flag_roundtrip - AssertionError: assert False
 +  where False = <MagicMock id='133024406027984'>.called
FAILED tests/torch/test_cli_inference_torch.py::TestInferenceCLI::test_inference_batch_size_flag_roundtrip - AssertionError: assert False
 +  where False = <MagicMock id='133024406321744'>.called
FAILED tests/torch/test_cli_inference_torch.py::TestInferenceCLI::test_multiple_execution_config_flags - AssertionError: assert False
 +  where False = <MagicMock id='133024406253008'>.called
============================== 4 failed in 3.77s ===============================
