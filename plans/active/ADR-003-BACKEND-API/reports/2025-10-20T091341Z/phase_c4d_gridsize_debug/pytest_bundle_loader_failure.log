============================= test session starts ==============================
platform linux -- Python 3.11.13, pytest-8.4.1, pluggy-1.6.0 -- /home/ollie/miniconda3/envs/ptycho311/bin/python3.11
cachedir: .pytest_cache
PyTorch: available
PyTorch version: 2.8.0+cu128
rootdir: /home/ollie/Documents/PtychoPINN
configfile: pyproject.toml
plugins: anyio-4.9.0
collecting ... collected 1 item

tests/torch/test_integration_workflow_torch.py::test_bundle_loader_returns_modules FAILED [100%]

=================================== FAILURES ===================================
______________________ test_bundle_loader_returns_modules ______________________

tmp_path = PosixPath('/tmp/pytest-of-ollie/pytest-758/test_bundle_loader_returns_mod0')
data_file = PosixPath('/home/ollie/Documents/PtychoPINN/tests/fixtures/pytorch_integration/minimal_dataset_v1.npz')
cuda_cpu_env = {'CODEX_MANAGED_BY_NPM': '1', 'CONDA_DEFAULT_ENV': 'ptycho311', 'CONDA_EXE': '/home/ollie/miniconda3/bin/conda', 'CONDA_PREFIX': '/home/ollie/miniconda3/envs/ptycho311', ...}

    def test_bundle_loader_returns_modules(tmp_path, data_file, cuda_cpu_env):
        """
        Regression test: bundle loader must return Lightning modules, not dicts.
    
        Phase C4.D.B4: This test validates that load_inference_bundle_torch returns
        nn.Module instances that support .eval(), not sentinel dicts. The bug was
        discovered during integration testing when inference.py raised AttributeError
        on models_dict['diffraction_to_obj'].eval().
    
        Test Strategy (TDD RED phase):
        1. Train a minimal model and save bundle
        2. Load bundle via load_inference_bundle_torch
        3. Assert both models in dict are torch.nn.Module instances
        4. Assert models support .eval() without AttributeError
    
        Expected Failure: Currently returns sentinel dict for autoencoder.
        """
        import torch.nn as nn
        from ptycho_torch.workflows.components import load_inference_bundle_torch
    
        # Setup: Train and save a bundle (reuse training command from integration test)
        training_output_dir = tmp_path / "training_outputs"
        train_command = [
            sys.executable, "-m", "ptycho_torch.train",
            "--train_data_file", str(data_file),
            "--test_data_file", str(data_file),
            "--output_dir", str(training_output_dir),
            "--max_epochs", "1",  # Minimal training for faster test
            "--n_images", "32",
            "--gridsize", "1",
            "--batch_size", "4",
            "--device", "cpu",
            "--disable_mlflow",
        ]
    
        train_result = subprocess.run(
            train_command, capture_output=True, text=True, env=cuda_cpu_env, check=False
        )
    
        if train_result.returncode != 0:
>           pytest.fail(f"Training failed:\n{train_result.stderr}")
E           Failed: Training failed:
E           2025-10-20 02:13:58.268104: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
E           WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E           E0000 00:00:1760951638.279161 1602221 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E           E0000 00:00:1760951638.282855 1602221 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E           W0000 00:00:1760951638.293264 1602221 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E           W0000 00:00:1760951638.293276 1602221 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E           W0000 00:00:1760951638.293278 1602221 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E           W0000 00:00:1760951638.293280 1602221 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E           2025-10-20 02:13:58.295895: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
E           To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
E           2025-10-20 02:14:00.276891: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
E           2025-10-20 02:14:00.276913: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:167] env: CUDA_VISIBLE_DEVICES=""
E           2025-10-20 02:14:00.276918: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:170] CUDA_VISIBLE_DEVICES is set to an empty string - this hides all GPUs from CUDA
E           2025-10-20 02:14:00.276921: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module
E           2025-10-20 02:14:00.276925: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: ollie-System-Product-Name
E           2025-10-20 02:14:00.276926: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] hostname: ollie-System-Product-Name
E           2025-10-20 02:14:00.276948: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 570.172.8
E           2025-10-20 02:14:00.276962: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 570.172.8
E           2025-10-20 02:14:00.276966: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:291] kernel version seems to match DSO: 570.172.8
E           /home/ollie/Documents/PtychoPINN/ptycho_torch/config_factory.py:608: UserWarning: params.cfg already populated. Set force=True to overwrite existing values.
E             warnings.warn(
E           WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E           I0000 00:00:1760951642.746550 1602221 service.cc:152] XLA service 0x44360070 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
E           I0000 00:00:1760951642.746577 1602221 service.cc:160]   StreamExecutor device (0): Host, Default Version
E           2025-10-20 02:14:02.761556: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
E           I0000 00:00:1760951642.862135 1602221 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
E           /home/ollie/Documents/PtychoPINN/ptycho_torch/config_factory.py:257: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
E             tf_training_config = to_training_config(
E           Seed set to 42
E           INFO:pytorch_lightning.utilities.rank_zero:ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
E           INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False
E           INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores
E           INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs
E           
E             | Name  | Type        | Params | Mode 
E           ----------------------------------------------
E           0 | model | PtychoPINN  | 2.3 M  | train
E           1 | Loss  | PoissonLoss | 0      | train
E           ----------------------------------------------
E           2.3 M     Trainable params
E           0         Non-trainable params
E           2.3 M     Total params
E           9.345     Total estimated model params size (MB)
E           73        Modules in train mode
E           0         Modules in eval mode
E           /home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.
E           ERROR:ptycho_torch.workflows.components:Lightning training failed: shape '[4, 2, 1]' is invalid for input of size 16
E           Traceback (most recent call last):
E             File "/home/ollie/Documents/PtychoPINN/ptycho_torch/workflows/components.py", line 700, in _train_with_lightning
E               trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)
E             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py", line 560, in fit
E               call._call_and_handle_interrupt(
E             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py", line 49, in _call_and_handle_interrupt
E               return trainer_fn(*args, **kwargs)
E                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py", line 598, in _fit_impl
E               self._run(model, ckpt_path=ckpt_path)
E             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py", line 1011, in _run
E               results = self._run_stage()
E                         ^^^^^^^^^^^^^^^^^
E             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py", line 1053, in _run_stage
E               self._run_sanity_check()
E             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py", line 1082, in _run_sanity_check
E               val_loop.run()
E             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/loops/utilities.py", line 179, in _decorator
E               return loop_run(self, *args, **kwargs)
E                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 145, in run
E               self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
E             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 437, in _evaluation_step
E               output = call._call_strategy_hook(trainer, hook_name, *step_args)
E                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py", line 329, in _call_strategy_hook
E               output = fn(*args, **kwargs)
E                        ^^^^^^^^^^^^^^^^^^^
E             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py", line 412, in validation_step
E               return self.lightning_module.validation_step(*args, **kwargs)
E                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E             File "/home/ollie/Documents/PtychoPINN/ptycho_torch/model.py", line 1257, in validation_step
E               val_loss = self.compute_loss(batch)
E                          ^^^^^^^^^^^^^^^^^^^^^^^^
E             File "/home/ollie/Documents/PtychoPINN/ptycho_torch/model.py", line 1187, in compute_loss
E               pred, amp, phase = self(x, positions, probe,
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^
E             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
E               return self._call_impl(*args, **kwargs)
E                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
E               return forward_call(*args, **kwargs)
E                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E             File "/home/ollie/Documents/PtychoPINN/ptycho_torch/model.py", line 1144, in forward
E               x_out = self.model(x, positions, probe, input_scale_factor, output_scale_factor)
E                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
E               return self._call_impl(*args, **kwargs)
E                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
E               return forward_call(*args, **kwargs)
E                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E             File "/home/ollie/Documents/PtychoPINN/ptycho_torch/model.py", line 900, in forward
E               x_out = self.forward_model.forward(x_combined, positions,
E                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E             File "/home/ollie/Documents/PtychoPINN/ptycho_torch/model.py", line 696, in forward
E               reassembled_obj, _, _ = hh.reassemble_patches_position_real(x, positions,
E                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E             File "/home/ollie/Documents/PtychoPINN/ptycho_torch/helper.py", line 90, in reassemble_patches_position_real
E               imgs_flat_bigN_translated = Translation(imgs_flat_bigN, offsets_flat, 0.).squeeze(1)
E                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E             File "/home/ollie/Documents/PtychoPINN/ptycho_torch/helper.py", line 425, in Translation
E               theta[:, :, 2:] = norm_offset.view(n, 2, 1)  # Translation matrix
E                                 ^^^^^^^^^^^^^^^^^^^^^^^^^
E           RuntimeError: shape '[4, 2, 1]' is invalid for input of size 16
E           
E           The above exception was the direct cause of the following exception:
E           
E           Traceback (most recent call last):
E             File "/home/ollie/Documents/PtychoPINN/ptycho_torch/train.py", line 642, in cli_main
E               amplitude, phase, results = run_cdi_example_torch(
E                                           ^^^^^^^^^^^^^^^^^^^^^^
E             File "/home/ollie/Documents/PtychoPINN/ptycho_torch/workflows/components.py", line 155, in run_cdi_example_torch
E               train_results = train_cdi_model_torch(train_data, test_data, config)
E                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E             File "/home/ollie/Documents/PtychoPINN/ptycho_torch/workflows/components.py", line 950, in train_cdi_model_torch
E               results = _train_with_lightning(train_container, test_container, config)
E                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E             File "/home/ollie/Documents/PtychoPINN/ptycho_torch/workflows/components.py", line 703, in _train_with_lightning
E               raise RuntimeError(f"Lightning training failed. See logs for details.") from e
E           RuntimeError: Lightning training failed. See logs for details.

tests/torch/test_integration_workflow_torch.py:216: Failed
----------------------------- Captured stdout call -----------------------------
No GPU found, using CPU instead.
----------------------------- Captured stderr call -----------------------------
2025-10-20 02:13:54.757314: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1760951634.768987 1602137 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1760951634.773057 1602137 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1760951634.784332 1602137 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1760951634.784351 1602137 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1760951634.784354 1602137 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1760951634.784356 1602137 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-10-20 02:13:54.787103: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-10-20 02:13:56.818940: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2025-10-20 02:13:56.818979: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:167] env: CUDA_VISIBLE_DEVICES=""
2025-10-20 02:13:56.818986: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:170] CUDA_VISIBLE_DEVICES is set to an empty string - this hides all GPUs from CUDA
2025-10-20 02:13:56.818989: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module
2025-10-20 02:13:56.818993: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: ollie-System-Product-Name
2025-10-20 02:13:56.818995: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] hostname: ollie-System-Product-Name
2025-10-20 02:13:56.819024: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 570.172.8
2025-10-20 02:13:56.819042: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 570.172.8
2025-10-20 02:13:56.819046: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:291] kernel version seems to match DSO: 570.172.8
=========================== short test summary info ============================
FAILED tests/torch/test_integration_workflow_torch.py::test_bundle_loader_returns_modules - Failed: Training failed:
2025-10-20 02:13:58.268104: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1760951638.279161 1602221 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1760951638.282855 1602221 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1760951638.293264 1602221 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1760951638.293276 1602221 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1760951638.293278 1602221 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1760951638.293280 1602221 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-10-20 02:13:58.295895: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-10-20 02:14:00.276891: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2025-10-20 02:14:00.276913: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:167] env: CUDA_VISIBLE_DEVICES=""
2025-10-20 02:14:00.276918: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:170] CUDA_VISIBLE_DEVICES is set to an empty string - this hides all GPUs from CUDA
2025-10-20 02:14:00.276921: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module
2025-10-20 02:14:00.276925: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: ollie-System-Product-Name
2025-10-20 02:14:00.276926: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] hostname: ollie-System-Product-Name
2025-10-20 02:14:00.276948: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 570.172.8
2025-10-20 02:14:00.276962: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 570.172.8
2025-10-20 02:14:00.276966: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:291] kernel version seems to match DSO: 570.172.8
/home/ollie/Documents/PtychoPINN/ptycho_torch/config_factory.py:608: UserWarning: params.cfg already populated. Set force=True to overwrite existing values.
  warnings.warn(
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1760951642.746550 1602221 service.cc:152] XLA service 0x44360070 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1760951642.746577 1602221 service.cc:160]   StreamExecutor device (0): Host, Default Version
2025-10-20 02:14:02.761556: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
I0000 00:00:1760951642.862135 1602221 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
/home/ollie/Documents/PtychoPINN/ptycho_torch/config_factory.py:257: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
  tf_training_config = to_training_config(
Seed set to 42
INFO:pytorch_lightning.utilities.rank_zero:ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False
INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs

  | Name  | Type        | Params | Mode 
----------------------------------------------
0 | model | PtychoPINN  | 2.3 M  | train
1 | Loss  | PoissonLoss | 0      | train
----------------------------------------------
2.3 M     Trainable params
0         Non-trainable params
2.3 M     Total params
9.345     Total estimated model params size (MB)
73        Modules in train mode
0         Modules in eval mode
/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.
ERROR:ptycho_torch.workflows.components:Lightning training failed: shape '[4, 2, 1]' is invalid for input of size 16
Traceback (most recent call last):
  File "/home/ollie/Documents/PtychoPINN/ptycho_torch/workflows/components.py", line 700, in _train_with_lightning
    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)
  File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py", line 1011, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py", line 1053, in _run_stage
    self._run_sanity_check()
  File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py", line 1082, in _run_sanity_check
    val_loop.run()
  File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/loops/utilities.py", line 179, in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 145, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 437, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py", line 412, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ollie/Documents/PtychoPINN/ptycho_torch/model.py", line 1257, in validation_step
    val_loss = self.compute_loss(batch)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ollie/Documents/PtychoPINN/ptycho_torch/model.py", line 1187, in compute_loss
    pred, amp, phase = self(x, positions, probe,
                       ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ollie/Documents/PtychoPINN/ptycho_torch/model.py", line 1144, in forward
    x_out = self.model(x, positions, probe, input_scale_factor, output_scale_factor)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ollie/Documents/PtychoPINN/ptycho_torch/model.py", line 900, in forward
    x_out = self.forward_model.forward(x_combined, positions,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ollie/Documents/PtychoPINN/ptycho_torch/model.py", line 696, in forward
    reassembled_obj, _, _ = hh.reassemble_patches_position_real(x, positions,
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ollie/Documents/PtychoPINN/ptycho_torch/helper.py", line 90, in reassemble_patches_position_real
    imgs_flat_bigN_translated = Translation(imgs_flat_bigN, offsets_flat, 0.).squeeze(1)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ollie/Documents/PtychoPINN/ptycho_torch/helper.py", line 425, in Translation
    theta[:, :, 2:] = norm_offset.view(n, 2, 1)  # Translation matrix
                      ^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: shape '[4, 2, 1]' is invalid for input of size 16

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ollie/Documents/PtychoPINN/ptycho_torch/train.py", line 642, in cli_main
    amplitude, phase, results = run_cdi_example_torch(
                                ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ollie/Documents/PtychoPINN/ptycho_torch/workflows/components.py", line 155, in run_cdi_example_torch
    train_results = train_cdi_model_torch(train_data, test_data, config)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ollie/Documents/PtychoPINN/ptycho_torch/workflows/components.py", line 950, in train_cdi_model_torch
    results = _train_with_lightning(train_container, test_container, config)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ollie/Documents/PtychoPINN/ptycho_torch/workflows/components.py", line 703, in _train_with_lightning
    raise RuntimeError(f"Lightning training failed. See logs for details.") from e
RuntimeError: Lightning training failed. See logs for details.
============================== 1 failed in 10.97s ==============================
