============================= test session starts ==============================
platform linux -- Python 3.11.13, pytest-8.4.1, pluggy-1.6.0 -- /home/ollie/miniconda3/envs/ptycho311/bin/python3.11
cachedir: .pytest_cache
PyTorch: available
PyTorch version: 2.8.0+cu128
rootdir: /home/ollie/Documents/PtychoPINN2
configfile: pyproject.toml
plugins: anyio-4.9.0
collecting ... collected 4 items

tests/torch/test_cli_inference_torch.py::TestInferenceCLI::test_accelerator_flag_roundtrip FAILED [ 25%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLI::test_num_workers_flag_roundtrip FAILED [ 50%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLI::test_inference_batch_size_flag_roundtrip FAILED [ 75%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLI::test_multiple_execution_config_flags FAILED [100%]

=================================== FAILURES ===================================
_______________ TestInferenceCLI.test_accelerator_flag_roundtrip _______________

self = <test_cli_inference_torch.TestInferenceCLI object at 0x746045777290>
minimal_inference_args = ['--model_path', '/tmp/pytest-of-ollie/pytest-675/test_accelerator_flag_roundtri0/model', '--test_data', '/tmp/pytest-...i0/test.npz', '--output_dir', '/tmp/pytest-of-ollie/pytest-675/test_accelerator_flag_roundtri0/inference_outputs', ...]
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x746036c43390>

    def test_accelerator_flag_roundtrip(self, minimal_inference_args, monkeypatch):
        """
        RED Test: --accelerator flag maps to execution_config.accelerator.
    
        Expected RED Failure:
        - argparse.ArgumentError: unrecognized arguments: --accelerator cpu
        OR
        - AssertionError: execution_config.accelerator != 'cpu'
        """
        mock_factory = MagicMock()
        mock_factory.return_value = MagicMock(
            tf_inference_config=MagicMock(),
            data_config=MagicMock(),
            execution_config=MagicMock(accelerator='cpu'),
        )
    
        with patch('ptycho_torch.config_factory.create_inference_payload', mock_factory):
            test_args = minimal_inference_args + ['--accelerator', 'cpu']
    
            from ptycho_torch.inference import cli_main
            monkeypatch.setattr('sys.argv', ['inference.py'] + test_args)
    
            try:
>               cli_main()

tests/torch/test_cli_inference_torch.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def cli_main():
        """
        CLI entrypoint for PyTorch Lightning checkpoint inference.
    
        This function implements Phase E2.C2 of INTEGRATE-PYTORCH-001, providing
        a command-line interface for loading trained PyTorch models and generating
        reconstructions from test data.
    
        Usage:
            python -m ptycho_torch.inference \\
                --model_path <training_output_dir> \\
                --test_data <npz_file> \\
                --output_dir <inference_output_dir> \\
                --n_images 32 \\
                --device cpu \\
                [--quiet]
    
        Expected Output Artifacts:
            - <output_dir>/reconstructed_amplitude.png
            - <output_dir>/reconstructed_phase.png
    
        References:
            - Phase E2 plan: plans/active/INTEGRATE-PYTORCH-001/phase_e2_implementation.md §E2.C2
            - Test contract: tests/torch/test_integration_workflow_torch.py
            - Red phase evidence: plans/active/INTEGRATE-PYTORCH-001/reports/2025-10-17T213500Z/red_phase.md §2.3
        """
        parser = argparse.ArgumentParser(
            description="PyTorch Lightning checkpoint inference for ptychography reconstruction",
            formatter_class=argparse.RawDescriptionHelpFormatter,
            epilog="""
    Examples:
      # Run inference on trained model
      python -m ptycho_torch.inference \\
          --model_path training_outputs \\
          --test_data datasets/Run1084_recon3_postPC_shrunk_3.npz \\
          --output_dir inference_outputs \\
          --n_images 32 \\
          --device cpu
    
      # Run with quiet output
      python -m ptycho_torch.inference \\
          --model_path training_outputs \\
          --test_data test.npz \\
          --output_dir outputs \\
          --n_images 64 \\
          --device cuda \\
          --quiet
            """
        )
    
        parser.add_argument(
            '--model_path',
            type=str,
            required=True,
            help='Path to training output directory containing Lightning checkpoint (expects checkpoints/last.ckpt or wts.pt)'
        )
        parser.add_argument(
            '--test_data',
            type=str,
            required=True,
            help='Path to test data NPZ file (must conform to specs/data_contracts.md)'
        )
        parser.add_argument(
            '--output_dir',
            type=str,
            required=True,
            help='Directory to save reconstruction outputs (amplitude/phase PNGs)'
        )
        parser.add_argument(
            '--n_images',
            type=int,
            default=32,
            help='Number of images to use for reconstruction (default: 32)'
        )
        parser.add_argument(
            '--device',
            type=str,
            choices=['cpu', 'cuda'],
            default='cpu',
            help='Device to run inference on (cpu or cuda, default: cpu)'
        )
        parser.add_argument(
            '--quiet',
            action='store_true',
            help='Suppress progress output'
        )
    
        # Execution config flags (Phase C4.C5 - ADR-003)
        parser.add_argument(
            '--accelerator',
            type=str,
            default='auto',
            choices=['auto', 'cpu', 'gpu', 'cuda', 'tpu', 'mps'],
            help=(
                'Hardware accelerator: auto (auto-detect, default), cpu (CPU-only), '
                'gpu (NVIDIA GPU), cuda (alias for gpu), tpu (Google TPU), mps (Apple Silicon).'
            )
        )
        parser.add_argument(
            '--num-workers',
            type=int,
            default=0,
            dest='num_workers',
            help=(
                'Number of DataLoader worker processes (default: 0 = synchronous). '
                'Typical values: 2-8 for multi-core systems.'
            )
        )
        parser.add_argument(
            '--inference-batch-size',
            type=int,
            default=None,
            dest='inference_batch_size',
            help=(
                'Batch size for inference DataLoader (default: None = use training batch_size). '
                'Larger values increase throughput. Typical: 16-64 for GPU, 4-8 for CPU.'
            )
        )
    
        args = parser.parse_args()
    
        # Phase C4.C6: Create execution config from CLI args (ADR-003)
        from ptycho.config.config import PyTorchExecutionConfig
    
        # Resolve accelerator (handle --device backward compatibility)
        resolved_accelerator = args.accelerator
        if args.device and args.accelerator == 'auto':
            # Map legacy --device to --accelerator if accelerator not explicitly set
            resolved_accelerator = 'cpu' if args.device == 'cpu' else 'gpu'
        elif args.device and args.accelerator != 'auto':
            # Warn if both specified
            import warnings
            warnings.warn(
                "--device is deprecated and will be removed in Phase D. "
                "Use --accelerator instead. Ignoring --device value.",
                DeprecationWarning
            )
    
        # Validate execution config args
        if args.num_workers < 0:
            raise ValueError(f"--num-workers must be >= 0, got {args.num_workers}")
        if args.inference_batch_size is not None and args.inference_batch_size <= 0:
            raise ValueError(f"--inference-batch-size must be > 0, got {args.inference_batch_size}")
    
        execution_config = PyTorchExecutionConfig(
            accelerator=resolved_accelerator,
            num_workers=args.num_workers,
            inference_batch_size=args.inference_batch_size,
            enable_progress_bar=(not args.quiet),
        )
    
        # Fail-fast: Check Lightning availability
        try:
            import lightning as L
            import torch
        except ImportError as e:
            raise RuntimeError(
                "PyTorch Lightning backend requires 'lightning' and 'torch' packages. "
                "Install via: pip install -e .[torch]\n"
                f"Import error: {e}"
            )
    
        # Phase C4.C6+C4.C7: Delegate to factory for CONFIG-001 compliance (ADR-003)
        # Replaces manual checkpoint loading and config construction with centralized
        # factory pattern. The factory handles:
        # 1. Path validation and checkpoint discovery
        # 2. CONFIG-001 bridging (update_legacy_dict before any IO)
        # 3. Config translation (PyTorch → TensorFlow canonical dataclasses)
        # 4. Execution config merging with override precedence
    
        from ptycho_torch.config_factory import create_inference_payload
        from ptycho.raw_data import RawData
    
        # Convert paths to Path objects
        model_path = Path(args.model_path)
        test_data_path = Path(args.test_data)
        output_dir = Path(args.output_dir)
    
        # Build overrides dict for factory
        overrides = {
            'n_groups': args.n_images,  # Map CLI arg to config field
        }
    
        # Call factory to construct all configs and populate params.cfg
        try:
            payload = create_inference_payload(
                model_path=model_path,
                test_data_file=test_data_path,
                output_dir=output_dir,
                overrides=overrides,
                execution_config=execution_config,
            )
    
            # Extract configs from payload (factory already populated params.cfg)
            pt_data_config = payload.pt_data_config
            tf_inference_config = payload.tf_inference_config
            execution_config = payload.execution_config
    
            if not args.quiet:
                print(f"Loaded configuration from model checkpoint")
                print(f"Test data: {test_data_path}")
                print(f"Output directory: {output_dir}")
                print(f"N groups: {tf_inference_config.n_groups}")
                print(f"Execution config: accelerator={execution_config.accelerator}, "
                      f"num_workers={execution_config.num_workers}")
    
        except Exception as e:
            raise RuntimeError(
                f"Failed to create inference payload.\n"
                f"Error: {e}\n"
                "Ensure model_path contains wts.h5.zip and test_data conforms to DATA-001."
            )
    
        # Load checkpoint via Lightning (factory validated path exists)
        checkpoint_candidates = [
            model_path / "checkpoints" / "last.ckpt",  # Lightning default
            model_path / "wts.pt",                      # Custom bundle format
            model_path / "model.pt",                    # Alternative naming
        ]
    
        checkpoint_path = None
        for candidate in checkpoint_candidates:
            if candidate.exists():
                checkpoint_path = candidate
                break
    
        if checkpoint_path is None:
>           raise FileNotFoundError(
                f"No Lightning checkpoint found in {model_path}.\n"
                f"Searched for: {[str(p) for p in checkpoint_candidates]}\n"
                "Factory validated model directory, but checkpoint file missing."
            )
E           FileNotFoundError: No Lightning checkpoint found in /tmp/pytest-of-ollie/pytest-675/test_accelerator_flag_roundtri0/model.
E           Searched for: ['/tmp/pytest-of-ollie/pytest-675/test_accelerator_flag_roundtri0/model/checkpoints/last.ckpt', '/tmp/pytest-of-ollie/pytest-675/test_accelerator_flag_roundtri0/model/wts.pt', '/tmp/pytest-of-ollie/pytest-675/test_accelerator_flag_roundtri0/model/model.pt']
E           Factory validated model directory, but checkpoint file missing.

ptycho_torch/inference.py:520: FileNotFoundError
----------------------------- Captured stdout call -----------------------------
No GPU found, using CPU instead.
Loaded configuration from model checkpoint
Test data: /tmp/pytest-of-ollie/pytest-675/test_accelerator_flag_roundtri0/test.npz
Output directory: /tmp/pytest-of-ollie/pytest-675/test_accelerator_flag_roundtri0/inference_outputs
N groups: <MagicMock name='mock().tf_inference_config.n_groups' id='127953380430672'>
Execution config: accelerator=cpu, num_workers=<MagicMock name='mock().execution_config.num_workers' id='127953088456400'>
----------------------------- Captured stderr call -----------------------------
2025-10-19 21:27:45.075435: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1760934465.086874 1401193 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1760934465.090788 1401193 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1760934465.101819 1401193 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1760934465.101835 1401193 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1760934465.101838 1401193 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1760934465.101840 1401193 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-10-19 21:27:45.104562: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-10-19 21:27:47.195090: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2025-10-19 21:27:47.195127: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:167] env: CUDA_VISIBLE_DEVICES=""
2025-10-19 21:27:47.195134: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:170] CUDA_VISIBLE_DEVICES is set to an empty string - this hides all GPUs from CUDA
2025-10-19 21:27:47.195137: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module
2025-10-19 21:27:47.195141: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: ollie-System-Product-Name
2025-10-19 21:27:47.195144: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] hostname: ollie-System-Product-Name
2025-10-19 21:27:47.195171: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 570.172.8
2025-10-19 21:27:47.195186: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 570.172.8
2025-10-19 21:27:47.195190: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:291] kernel version seems to match DSO: 570.172.8
_______________ TestInferenceCLI.test_num_workers_flag_roundtrip _______________

self = <test_cli_inference_torch.TestInferenceCLI object at 0x746036c42290>
minimal_inference_args = ['--model_path', '/tmp/pytest-of-ollie/pytest-675/test_num_workers_flag_roundtri0/model', '--test_data', '/tmp/pytest-...i0/test.npz', '--output_dir', '/tmp/pytest-of-ollie/pytest-675/test_num_workers_flag_roundtri0/inference_outputs', ...]
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x745f6664bb50>

    def test_num_workers_flag_roundtrip(self, minimal_inference_args, monkeypatch):
        """
        RED Test: --num-workers flag maps to execution_config.num_workers.
    
        Expected RED Failure:
        - argparse.ArgumentError: unrecognized arguments: --num-workers 4
        OR
        - AssertionError: execution_config.num_workers != 4
        """
        mock_factory = MagicMock()
        mock_factory.return_value = MagicMock(
            tf_inference_config=MagicMock(),
            execution_config=MagicMock(num_workers=4),
        )
    
        with patch('ptycho_torch.config_factory.create_inference_payload', mock_factory):
            test_args = minimal_inference_args + ['--num-workers', '4']
    
            from ptycho_torch.inference import cli_main
            monkeypatch.setattr('sys.argv', ['inference.py'] + test_args)
    
            try:
>               cli_main()

tests/torch/test_cli_inference_torch.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def cli_main():
        """
        CLI entrypoint for PyTorch Lightning checkpoint inference.
    
        This function implements Phase E2.C2 of INTEGRATE-PYTORCH-001, providing
        a command-line interface for loading trained PyTorch models and generating
        reconstructions from test data.
    
        Usage:
            python -m ptycho_torch.inference \\
                --model_path <training_output_dir> \\
                --test_data <npz_file> \\
                --output_dir <inference_output_dir> \\
                --n_images 32 \\
                --device cpu \\
                [--quiet]
    
        Expected Output Artifacts:
            - <output_dir>/reconstructed_amplitude.png
            - <output_dir>/reconstructed_phase.png
    
        References:
            - Phase E2 plan: plans/active/INTEGRATE-PYTORCH-001/phase_e2_implementation.md §E2.C2
            - Test contract: tests/torch/test_integration_workflow_torch.py
            - Red phase evidence: plans/active/INTEGRATE-PYTORCH-001/reports/2025-10-17T213500Z/red_phase.md §2.3
        """
        parser = argparse.ArgumentParser(
            description="PyTorch Lightning checkpoint inference for ptychography reconstruction",
            formatter_class=argparse.RawDescriptionHelpFormatter,
            epilog="""
    Examples:
      # Run inference on trained model
      python -m ptycho_torch.inference \\
          --model_path training_outputs \\
          --test_data datasets/Run1084_recon3_postPC_shrunk_3.npz \\
          --output_dir inference_outputs \\
          --n_images 32 \\
          --device cpu
    
      # Run with quiet output
      python -m ptycho_torch.inference \\
          --model_path training_outputs \\
          --test_data test.npz \\
          --output_dir outputs \\
          --n_images 64 \\
          --device cuda \\
          --quiet
            """
        )
    
        parser.add_argument(
            '--model_path',
            type=str,
            required=True,
            help='Path to training output directory containing Lightning checkpoint (expects checkpoints/last.ckpt or wts.pt)'
        )
        parser.add_argument(
            '--test_data',
            type=str,
            required=True,
            help='Path to test data NPZ file (must conform to specs/data_contracts.md)'
        )
        parser.add_argument(
            '--output_dir',
            type=str,
            required=True,
            help='Directory to save reconstruction outputs (amplitude/phase PNGs)'
        )
        parser.add_argument(
            '--n_images',
            type=int,
            default=32,
            help='Number of images to use for reconstruction (default: 32)'
        )
        parser.add_argument(
            '--device',
            type=str,
            choices=['cpu', 'cuda'],
            default='cpu',
            help='Device to run inference on (cpu or cuda, default: cpu)'
        )
        parser.add_argument(
            '--quiet',
            action='store_true',
            help='Suppress progress output'
        )
    
        # Execution config flags (Phase C4.C5 - ADR-003)
        parser.add_argument(
            '--accelerator',
            type=str,
            default='auto',
            choices=['auto', 'cpu', 'gpu', 'cuda', 'tpu', 'mps'],
            help=(
                'Hardware accelerator: auto (auto-detect, default), cpu (CPU-only), '
                'gpu (NVIDIA GPU), cuda (alias for gpu), tpu (Google TPU), mps (Apple Silicon).'
            )
        )
        parser.add_argument(
            '--num-workers',
            type=int,
            default=0,
            dest='num_workers',
            help=(
                'Number of DataLoader worker processes (default: 0 = synchronous). '
                'Typical values: 2-8 for multi-core systems.'
            )
        )
        parser.add_argument(
            '--inference-batch-size',
            type=int,
            default=None,
            dest='inference_batch_size',
            help=(
                'Batch size for inference DataLoader (default: None = use training batch_size). '
                'Larger values increase throughput. Typical: 16-64 for GPU, 4-8 for CPU.'
            )
        )
    
        args = parser.parse_args()
    
        # Phase C4.C6: Create execution config from CLI args (ADR-003)
        from ptycho.config.config import PyTorchExecutionConfig
    
        # Resolve accelerator (handle --device backward compatibility)
        resolved_accelerator = args.accelerator
        if args.device and args.accelerator == 'auto':
            # Map legacy --device to --accelerator if accelerator not explicitly set
            resolved_accelerator = 'cpu' if args.device == 'cpu' else 'gpu'
        elif args.device and args.accelerator != 'auto':
            # Warn if both specified
            import warnings
            warnings.warn(
                "--device is deprecated and will be removed in Phase D. "
                "Use --accelerator instead. Ignoring --device value.",
                DeprecationWarning
            )
    
        # Validate execution config args
        if args.num_workers < 0:
            raise ValueError(f"--num-workers must be >= 0, got {args.num_workers}")
        if args.inference_batch_size is not None and args.inference_batch_size <= 0:
            raise ValueError(f"--inference-batch-size must be > 0, got {args.inference_batch_size}")
    
        execution_config = PyTorchExecutionConfig(
            accelerator=resolved_accelerator,
            num_workers=args.num_workers,
            inference_batch_size=args.inference_batch_size,
            enable_progress_bar=(not args.quiet),
        )
    
        # Fail-fast: Check Lightning availability
        try:
            import lightning as L
            import torch
        except ImportError as e:
            raise RuntimeError(
                "PyTorch Lightning backend requires 'lightning' and 'torch' packages. "
                "Install via: pip install -e .[torch]\n"
                f"Import error: {e}"
            )
    
        # Phase C4.C6+C4.C7: Delegate to factory for CONFIG-001 compliance (ADR-003)
        # Replaces manual checkpoint loading and config construction with centralized
        # factory pattern. The factory handles:
        # 1. Path validation and checkpoint discovery
        # 2. CONFIG-001 bridging (update_legacy_dict before any IO)
        # 3. Config translation (PyTorch → TensorFlow canonical dataclasses)
        # 4. Execution config merging with override precedence
    
        from ptycho_torch.config_factory import create_inference_payload
        from ptycho.raw_data import RawData
    
        # Convert paths to Path objects
        model_path = Path(args.model_path)
        test_data_path = Path(args.test_data)
        output_dir = Path(args.output_dir)
    
        # Build overrides dict for factory
        overrides = {
            'n_groups': args.n_images,  # Map CLI arg to config field
        }
    
        # Call factory to construct all configs and populate params.cfg
        try:
            payload = create_inference_payload(
                model_path=model_path,
                test_data_file=test_data_path,
                output_dir=output_dir,
                overrides=overrides,
                execution_config=execution_config,
            )
    
            # Extract configs from payload (factory already populated params.cfg)
            pt_data_config = payload.pt_data_config
            tf_inference_config = payload.tf_inference_config
            execution_config = payload.execution_config
    
            if not args.quiet:
                print(f"Loaded configuration from model checkpoint")
                print(f"Test data: {test_data_path}")
                print(f"Output directory: {output_dir}")
                print(f"N groups: {tf_inference_config.n_groups}")
                print(f"Execution config: accelerator={execution_config.accelerator}, "
                      f"num_workers={execution_config.num_workers}")
    
        except Exception as e:
            raise RuntimeError(
                f"Failed to create inference payload.\n"
                f"Error: {e}\n"
                "Ensure model_path contains wts.h5.zip and test_data conforms to DATA-001."
            )
    
        # Load checkpoint via Lightning (factory validated path exists)
        checkpoint_candidates = [
            model_path / "checkpoints" / "last.ckpt",  # Lightning default
            model_path / "wts.pt",                      # Custom bundle format
            model_path / "model.pt",                    # Alternative naming
        ]
    
        checkpoint_path = None
        for candidate in checkpoint_candidates:
            if candidate.exists():
                checkpoint_path = candidate
                break
    
        if checkpoint_path is None:
>           raise FileNotFoundError(
                f"No Lightning checkpoint found in {model_path}.\n"
                f"Searched for: {[str(p) for p in checkpoint_candidates]}\n"
                "Factory validated model directory, but checkpoint file missing."
            )
E           FileNotFoundError: No Lightning checkpoint found in /tmp/pytest-of-ollie/pytest-675/test_num_workers_flag_roundtri0/model.
E           Searched for: ['/tmp/pytest-of-ollie/pytest-675/test_num_workers_flag_roundtri0/model/checkpoints/last.ckpt', '/tmp/pytest-of-ollie/pytest-675/test_num_workers_flag_roundtri0/model/wts.pt', '/tmp/pytest-of-ollie/pytest-675/test_num_workers_flag_roundtri0/model/model.pt']
E           Factory validated model directory, but checkpoint file missing.

ptycho_torch/inference.py:520: FileNotFoundError
----------------------------- Captured stdout call -----------------------------
Loaded configuration from model checkpoint
Test data: /tmp/pytest-of-ollie/pytest-675/test_num_workers_flag_roundtri0/test.npz
Output directory: /tmp/pytest-of-ollie/pytest-675/test_num_workers_flag_roundtri0/inference_outputs
N groups: <MagicMock name='mock().tf_inference_config.n_groups' id='127953098955088'>
Execution config: accelerator=<MagicMock name='mock().execution_config.accelerator' id='127953087630672'>, num_workers=4
__________ TestInferenceCLI.test_inference_batch_size_flag_roundtrip ___________

self = <test_cli_inference_torch.TestInferenceCLI object at 0x746036c42650>
minimal_inference_args = ['--model_path', '/tmp/pytest-of-ollie/pytest-675/test_inference_batch_size_flag0/model', '--test_data', '/tmp/pytest-...g0/test.npz', '--output_dir', '/tmp/pytest-of-ollie/pytest-675/test_inference_batch_size_flag0/inference_outputs', ...]
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x745f66694410>

    def test_inference_batch_size_flag_roundtrip(self, minimal_inference_args, monkeypatch):
        """
        RED Test: --inference-batch-size flag maps to execution_config.inference_batch_size.
    
        Expected RED Failure:
        - argparse.ArgumentError: unrecognized arguments: --inference-batch-size 32
        OR
        - AssertionError: execution_config.inference_batch_size != 32
        """
        mock_factory = MagicMock()
        mock_factory.return_value = MagicMock(
            tf_inference_config=MagicMock(),
            execution_config=MagicMock(inference_batch_size=32),
        )
    
        with patch('ptycho_torch.config_factory.create_inference_payload', mock_factory):
            test_args = minimal_inference_args + ['--inference-batch-size', '32']
    
            from ptycho_torch.inference import cli_main
            monkeypatch.setattr('sys.argv', ['inference.py'] + test_args)
    
            try:
>               cli_main()

tests/torch/test_cli_inference_torch.py:143: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def cli_main():
        """
        CLI entrypoint for PyTorch Lightning checkpoint inference.
    
        This function implements Phase E2.C2 of INTEGRATE-PYTORCH-001, providing
        a command-line interface for loading trained PyTorch models and generating
        reconstructions from test data.
    
        Usage:
            python -m ptycho_torch.inference \\
                --model_path <training_output_dir> \\
                --test_data <npz_file> \\
                --output_dir <inference_output_dir> \\
                --n_images 32 \\
                --device cpu \\
                [--quiet]
    
        Expected Output Artifacts:
            - <output_dir>/reconstructed_amplitude.png
            - <output_dir>/reconstructed_phase.png
    
        References:
            - Phase E2 plan: plans/active/INTEGRATE-PYTORCH-001/phase_e2_implementation.md §E2.C2
            - Test contract: tests/torch/test_integration_workflow_torch.py
            - Red phase evidence: plans/active/INTEGRATE-PYTORCH-001/reports/2025-10-17T213500Z/red_phase.md §2.3
        """
        parser = argparse.ArgumentParser(
            description="PyTorch Lightning checkpoint inference for ptychography reconstruction",
            formatter_class=argparse.RawDescriptionHelpFormatter,
            epilog="""
    Examples:
      # Run inference on trained model
      python -m ptycho_torch.inference \\
          --model_path training_outputs \\
          --test_data datasets/Run1084_recon3_postPC_shrunk_3.npz \\
          --output_dir inference_outputs \\
          --n_images 32 \\
          --device cpu
    
      # Run with quiet output
      python -m ptycho_torch.inference \\
          --model_path training_outputs \\
          --test_data test.npz \\
          --output_dir outputs \\
          --n_images 64 \\
          --device cuda \\
          --quiet
            """
        )
    
        parser.add_argument(
            '--model_path',
            type=str,
            required=True,
            help='Path to training output directory containing Lightning checkpoint (expects checkpoints/last.ckpt or wts.pt)'
        )
        parser.add_argument(
            '--test_data',
            type=str,
            required=True,
            help='Path to test data NPZ file (must conform to specs/data_contracts.md)'
        )
        parser.add_argument(
            '--output_dir',
            type=str,
            required=True,
            help='Directory to save reconstruction outputs (amplitude/phase PNGs)'
        )
        parser.add_argument(
            '--n_images',
            type=int,
            default=32,
            help='Number of images to use for reconstruction (default: 32)'
        )
        parser.add_argument(
            '--device',
            type=str,
            choices=['cpu', 'cuda'],
            default='cpu',
            help='Device to run inference on (cpu or cuda, default: cpu)'
        )
        parser.add_argument(
            '--quiet',
            action='store_true',
            help='Suppress progress output'
        )
    
        # Execution config flags (Phase C4.C5 - ADR-003)
        parser.add_argument(
            '--accelerator',
            type=str,
            default='auto',
            choices=['auto', 'cpu', 'gpu', 'cuda', 'tpu', 'mps'],
            help=(
                'Hardware accelerator: auto (auto-detect, default), cpu (CPU-only), '
                'gpu (NVIDIA GPU), cuda (alias for gpu), tpu (Google TPU), mps (Apple Silicon).'
            )
        )
        parser.add_argument(
            '--num-workers',
            type=int,
            default=0,
            dest='num_workers',
            help=(
                'Number of DataLoader worker processes (default: 0 = synchronous). '
                'Typical values: 2-8 for multi-core systems.'
            )
        )
        parser.add_argument(
            '--inference-batch-size',
            type=int,
            default=None,
            dest='inference_batch_size',
            help=(
                'Batch size for inference DataLoader (default: None = use training batch_size). '
                'Larger values increase throughput. Typical: 16-64 for GPU, 4-8 for CPU.'
            )
        )
    
        args = parser.parse_args()
    
        # Phase C4.C6: Create execution config from CLI args (ADR-003)
        from ptycho.config.config import PyTorchExecutionConfig
    
        # Resolve accelerator (handle --device backward compatibility)
        resolved_accelerator = args.accelerator
        if args.device and args.accelerator == 'auto':
            # Map legacy --device to --accelerator if accelerator not explicitly set
            resolved_accelerator = 'cpu' if args.device == 'cpu' else 'gpu'
        elif args.device and args.accelerator != 'auto':
            # Warn if both specified
            import warnings
            warnings.warn(
                "--device is deprecated and will be removed in Phase D. "
                "Use --accelerator instead. Ignoring --device value.",
                DeprecationWarning
            )
    
        # Validate execution config args
        if args.num_workers < 0:
            raise ValueError(f"--num-workers must be >= 0, got {args.num_workers}")
        if args.inference_batch_size is not None and args.inference_batch_size <= 0:
            raise ValueError(f"--inference-batch-size must be > 0, got {args.inference_batch_size}")
    
        execution_config = PyTorchExecutionConfig(
            accelerator=resolved_accelerator,
            num_workers=args.num_workers,
            inference_batch_size=args.inference_batch_size,
            enable_progress_bar=(not args.quiet),
        )
    
        # Fail-fast: Check Lightning availability
        try:
            import lightning as L
            import torch
        except ImportError as e:
            raise RuntimeError(
                "PyTorch Lightning backend requires 'lightning' and 'torch' packages. "
                "Install via: pip install -e .[torch]\n"
                f"Import error: {e}"
            )
    
        # Phase C4.C6+C4.C7: Delegate to factory for CONFIG-001 compliance (ADR-003)
        # Replaces manual checkpoint loading and config construction with centralized
        # factory pattern. The factory handles:
        # 1. Path validation and checkpoint discovery
        # 2. CONFIG-001 bridging (update_legacy_dict before any IO)
        # 3. Config translation (PyTorch → TensorFlow canonical dataclasses)
        # 4. Execution config merging with override precedence
    
        from ptycho_torch.config_factory import create_inference_payload
        from ptycho.raw_data import RawData
    
        # Convert paths to Path objects
        model_path = Path(args.model_path)
        test_data_path = Path(args.test_data)
        output_dir = Path(args.output_dir)
    
        # Build overrides dict for factory
        overrides = {
            'n_groups': args.n_images,  # Map CLI arg to config field
        }
    
        # Call factory to construct all configs and populate params.cfg
        try:
            payload = create_inference_payload(
                model_path=model_path,
                test_data_file=test_data_path,
                output_dir=output_dir,
                overrides=overrides,
                execution_config=execution_config,
            )
    
            # Extract configs from payload (factory already populated params.cfg)
            pt_data_config = payload.pt_data_config
            tf_inference_config = payload.tf_inference_config
            execution_config = payload.execution_config
    
            if not args.quiet:
                print(f"Loaded configuration from model checkpoint")
                print(f"Test data: {test_data_path}")
                print(f"Output directory: {output_dir}")
                print(f"N groups: {tf_inference_config.n_groups}")
                print(f"Execution config: accelerator={execution_config.accelerator}, "
                      f"num_workers={execution_config.num_workers}")
    
        except Exception as e:
            raise RuntimeError(
                f"Failed to create inference payload.\n"
                f"Error: {e}\n"
                "Ensure model_path contains wts.h5.zip and test_data conforms to DATA-001."
            )
    
        # Load checkpoint via Lightning (factory validated path exists)
        checkpoint_candidates = [
            model_path / "checkpoints" / "last.ckpt",  # Lightning default
            model_path / "wts.pt",                      # Custom bundle format
            model_path / "model.pt",                    # Alternative naming
        ]
    
        checkpoint_path = None
        for candidate in checkpoint_candidates:
            if candidate.exists():
                checkpoint_path = candidate
                break
    
        if checkpoint_path is None:
>           raise FileNotFoundError(
                f"No Lightning checkpoint found in {model_path}.\n"
                f"Searched for: {[str(p) for p in checkpoint_candidates]}\n"
                "Factory validated model directory, but checkpoint file missing."
            )
E           FileNotFoundError: No Lightning checkpoint found in /tmp/pytest-of-ollie/pytest-675/test_inference_batch_size_flag0/model.
E           Searched for: ['/tmp/pytest-of-ollie/pytest-675/test_inference_batch_size_flag0/model/checkpoints/last.ckpt', '/tmp/pytest-of-ollie/pytest-675/test_inference_batch_size_flag0/model/wts.pt', '/tmp/pytest-of-ollie/pytest-675/test_inference_batch_size_flag0/model/model.pt']
E           Factory validated model directory, but checkpoint file missing.

ptycho_torch/inference.py:520: FileNotFoundError
----------------------------- Captured stdout call -----------------------------
Loaded configuration from model checkpoint
Test data: /tmp/pytest-of-ollie/pytest-675/test_inference_batch_size_flag0/test.npz
Output directory: /tmp/pytest-of-ollie/pytest-675/test_inference_batch_size_flag0/inference_outputs
N groups: <MagicMock name='mock().tf_inference_config.n_groups' id='127953089312976'>
Execution config: accelerator=<MagicMock name='mock().execution_config.accelerator' id='127953089306448'>, num_workers=<MagicMock name='mock().execution_config.num_workers' id='127953089067664'>
____________ TestInferenceCLI.test_multiple_execution_config_flags _____________

self = <test_cli_inference_torch.TestInferenceCLI object at 0x746036c42c50>
minimal_inference_args = ['--model_path', '/tmp/pytest-of-ollie/pytest-675/test_multiple_execution_config0/model', '--test_data', '/tmp/pytest-...g0/test.npz', '--output_dir', '/tmp/pytest-of-ollie/pytest-675/test_multiple_execution_config0/inference_outputs', ...]
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x745f666beed0>

    def test_multiple_execution_config_flags(self, minimal_inference_args, monkeypatch):
        """
        RED Test: Multiple execution config flags work together.
    
        Expected RED Failure:
        - argparse.ArgumentError: unrecognized arguments (any of the new flags)
        OR
        - AssertionError: execution_config fields do not match expected values
        """
        mock_factory = MagicMock()
        mock_factory.return_value = MagicMock(
            tf_inference_config=MagicMock(),
            execution_config=MagicMock(
                accelerator='gpu',
                num_workers=8,
                inference_batch_size=64,
            ),
        )
    
        with patch('ptycho_torch.config_factory.create_inference_payload', mock_factory):
            test_args = minimal_inference_args + [
                '--accelerator', 'gpu',
                '--num-workers', '8',
                '--inference-batch-size', '64',
            ]
    
            from ptycho_torch.inference import cli_main
            monkeypatch.setattr('sys.argv', ['inference.py'] + test_args)
    
            try:
>               cli_main()

tests/torch/test_cli_inference_torch.py:183: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def cli_main():
        """
        CLI entrypoint for PyTorch Lightning checkpoint inference.
    
        This function implements Phase E2.C2 of INTEGRATE-PYTORCH-001, providing
        a command-line interface for loading trained PyTorch models and generating
        reconstructions from test data.
    
        Usage:
            python -m ptycho_torch.inference \\
                --model_path <training_output_dir> \\
                --test_data <npz_file> \\
                --output_dir <inference_output_dir> \\
                --n_images 32 \\
                --device cpu \\
                [--quiet]
    
        Expected Output Artifacts:
            - <output_dir>/reconstructed_amplitude.png
            - <output_dir>/reconstructed_phase.png
    
        References:
            - Phase E2 plan: plans/active/INTEGRATE-PYTORCH-001/phase_e2_implementation.md §E2.C2
            - Test contract: tests/torch/test_integration_workflow_torch.py
            - Red phase evidence: plans/active/INTEGRATE-PYTORCH-001/reports/2025-10-17T213500Z/red_phase.md §2.3
        """
        parser = argparse.ArgumentParser(
            description="PyTorch Lightning checkpoint inference for ptychography reconstruction",
            formatter_class=argparse.RawDescriptionHelpFormatter,
            epilog="""
    Examples:
      # Run inference on trained model
      python -m ptycho_torch.inference \\
          --model_path training_outputs \\
          --test_data datasets/Run1084_recon3_postPC_shrunk_3.npz \\
          --output_dir inference_outputs \\
          --n_images 32 \\
          --device cpu
    
      # Run with quiet output
      python -m ptycho_torch.inference \\
          --model_path training_outputs \\
          --test_data test.npz \\
          --output_dir outputs \\
          --n_images 64 \\
          --device cuda \\
          --quiet
            """
        )
    
        parser.add_argument(
            '--model_path',
            type=str,
            required=True,
            help='Path to training output directory containing Lightning checkpoint (expects checkpoints/last.ckpt or wts.pt)'
        )
        parser.add_argument(
            '--test_data',
            type=str,
            required=True,
            help='Path to test data NPZ file (must conform to specs/data_contracts.md)'
        )
        parser.add_argument(
            '--output_dir',
            type=str,
            required=True,
            help='Directory to save reconstruction outputs (amplitude/phase PNGs)'
        )
        parser.add_argument(
            '--n_images',
            type=int,
            default=32,
            help='Number of images to use for reconstruction (default: 32)'
        )
        parser.add_argument(
            '--device',
            type=str,
            choices=['cpu', 'cuda'],
            default='cpu',
            help='Device to run inference on (cpu or cuda, default: cpu)'
        )
        parser.add_argument(
            '--quiet',
            action='store_true',
            help='Suppress progress output'
        )
    
        # Execution config flags (Phase C4.C5 - ADR-003)
        parser.add_argument(
            '--accelerator',
            type=str,
            default='auto',
            choices=['auto', 'cpu', 'gpu', 'cuda', 'tpu', 'mps'],
            help=(
                'Hardware accelerator: auto (auto-detect, default), cpu (CPU-only), '
                'gpu (NVIDIA GPU), cuda (alias for gpu), tpu (Google TPU), mps (Apple Silicon).'
            )
        )
        parser.add_argument(
            '--num-workers',
            type=int,
            default=0,
            dest='num_workers',
            help=(
                'Number of DataLoader worker processes (default: 0 = synchronous). '
                'Typical values: 2-8 for multi-core systems.'
            )
        )
        parser.add_argument(
            '--inference-batch-size',
            type=int,
            default=None,
            dest='inference_batch_size',
            help=(
                'Batch size for inference DataLoader (default: None = use training batch_size). '
                'Larger values increase throughput. Typical: 16-64 for GPU, 4-8 for CPU.'
            )
        )
    
        args = parser.parse_args()
    
        # Phase C4.C6: Create execution config from CLI args (ADR-003)
        from ptycho.config.config import PyTorchExecutionConfig
    
        # Resolve accelerator (handle --device backward compatibility)
        resolved_accelerator = args.accelerator
        if args.device and args.accelerator == 'auto':
            # Map legacy --device to --accelerator if accelerator not explicitly set
            resolved_accelerator = 'cpu' if args.device == 'cpu' else 'gpu'
        elif args.device and args.accelerator != 'auto':
            # Warn if both specified
            import warnings
            warnings.warn(
                "--device is deprecated and will be removed in Phase D. "
                "Use --accelerator instead. Ignoring --device value.",
                DeprecationWarning
            )
    
        # Validate execution config args
        if args.num_workers < 0:
            raise ValueError(f"--num-workers must be >= 0, got {args.num_workers}")
        if args.inference_batch_size is not None and args.inference_batch_size <= 0:
            raise ValueError(f"--inference-batch-size must be > 0, got {args.inference_batch_size}")
    
        execution_config = PyTorchExecutionConfig(
            accelerator=resolved_accelerator,
            num_workers=args.num_workers,
            inference_batch_size=args.inference_batch_size,
            enable_progress_bar=(not args.quiet),
        )
    
        # Fail-fast: Check Lightning availability
        try:
            import lightning as L
            import torch
        except ImportError as e:
            raise RuntimeError(
                "PyTorch Lightning backend requires 'lightning' and 'torch' packages. "
                "Install via: pip install -e .[torch]\n"
                f"Import error: {e}"
            )
    
        # Phase C4.C6+C4.C7: Delegate to factory for CONFIG-001 compliance (ADR-003)
        # Replaces manual checkpoint loading and config construction with centralized
        # factory pattern. The factory handles:
        # 1. Path validation and checkpoint discovery
        # 2. CONFIG-001 bridging (update_legacy_dict before any IO)
        # 3. Config translation (PyTorch → TensorFlow canonical dataclasses)
        # 4. Execution config merging with override precedence
    
        from ptycho_torch.config_factory import create_inference_payload
        from ptycho.raw_data import RawData
    
        # Convert paths to Path objects
        model_path = Path(args.model_path)
        test_data_path = Path(args.test_data)
        output_dir = Path(args.output_dir)
    
        # Build overrides dict for factory
        overrides = {
            'n_groups': args.n_images,  # Map CLI arg to config field
        }
    
        # Call factory to construct all configs and populate params.cfg
        try:
            payload = create_inference_payload(
                model_path=model_path,
                test_data_file=test_data_path,
                output_dir=output_dir,
                overrides=overrides,
                execution_config=execution_config,
            )
    
            # Extract configs from payload (factory already populated params.cfg)
            pt_data_config = payload.pt_data_config
            tf_inference_config = payload.tf_inference_config
            execution_config = payload.execution_config
    
            if not args.quiet:
                print(f"Loaded configuration from model checkpoint")
                print(f"Test data: {test_data_path}")
                print(f"Output directory: {output_dir}")
                print(f"N groups: {tf_inference_config.n_groups}")
                print(f"Execution config: accelerator={execution_config.accelerator}, "
                      f"num_workers={execution_config.num_workers}")
    
        except Exception as e:
            raise RuntimeError(
                f"Failed to create inference payload.\n"
                f"Error: {e}\n"
                "Ensure model_path contains wts.h5.zip and test_data conforms to DATA-001."
            )
    
        # Load checkpoint via Lightning (factory validated path exists)
        checkpoint_candidates = [
            model_path / "checkpoints" / "last.ckpt",  # Lightning default
            model_path / "wts.pt",                      # Custom bundle format
            model_path / "model.pt",                    # Alternative naming
        ]
    
        checkpoint_path = None
        for candidate in checkpoint_candidates:
            if candidate.exists():
                checkpoint_path = candidate
                break
    
        if checkpoint_path is None:
>           raise FileNotFoundError(
                f"No Lightning checkpoint found in {model_path}.\n"
                f"Searched for: {[str(p) for p in checkpoint_candidates]}\n"
                "Factory validated model directory, but checkpoint file missing."
            )
E           FileNotFoundError: No Lightning checkpoint found in /tmp/pytest-of-ollie/pytest-675/test_multiple_execution_config0/model.
E           Searched for: ['/tmp/pytest-of-ollie/pytest-675/test_multiple_execution_config0/model/checkpoints/last.ckpt', '/tmp/pytest-of-ollie/pytest-675/test_multiple_execution_config0/model/wts.pt', '/tmp/pytest-of-ollie/pytest-675/test_multiple_execution_config0/model/model.pt']
E           Factory validated model directory, but checkpoint file missing.

ptycho_torch/inference.py:520: FileNotFoundError
----------------------------- Captured stdout call -----------------------------
Loaded configuration from model checkpoint
Test data: /tmp/pytest-of-ollie/pytest-675/test_multiple_execution_config0/test.npz
Output directory: /tmp/pytest-of-ollie/pytest-675/test_multiple_execution_config0/inference_outputs
N groups: <MagicMock name='mock().tf_inference_config.n_groups' id='127953087855440'>
Execution config: accelerator=gpu, num_workers=8
=========================== short test summary info ============================
FAILED tests/torch/test_cli_inference_torch.py::TestInferenceCLI::test_accelerator_flag_roundtrip - FileNotFoundError: No Lightning checkpoint found in /tmp/pytest-of-ollie/pytest-675/test_accelerator_flag_roundtri0/model.
Searched for: ['/tmp/pytest-of-ollie/pytest-675/test_accelerator_flag_roundtri0/model/checkpoints/last.ckpt', '/tmp/pytest-of-ollie/pytest-675/test_accelerator_flag_roundtri0/model/wts.pt', '/tmp/pytest-of-ollie/pytest-675/test_accelerator_flag_roundtri0/model/model.pt']
Factory validated model directory, but checkpoint file missing.
FAILED tests/torch/test_cli_inference_torch.py::TestInferenceCLI::test_num_workers_flag_roundtrip - FileNotFoundError: No Lightning checkpoint found in /tmp/pytest-of-ollie/pytest-675/test_num_workers_flag_roundtri0/model.
Searched for: ['/tmp/pytest-of-ollie/pytest-675/test_num_workers_flag_roundtri0/model/checkpoints/last.ckpt', '/tmp/pytest-of-ollie/pytest-675/test_num_workers_flag_roundtri0/model/wts.pt', '/tmp/pytest-of-ollie/pytest-675/test_num_workers_flag_roundtri0/model/model.pt']
Factory validated model directory, but checkpoint file missing.
FAILED tests/torch/test_cli_inference_torch.py::TestInferenceCLI::test_inference_batch_size_flag_roundtrip - FileNotFoundError: No Lightning checkpoint found in /tmp/pytest-of-ollie/pytest-675/test_inference_batch_size_flag0/model.
Searched for: ['/tmp/pytest-of-ollie/pytest-675/test_inference_batch_size_flag0/model/checkpoints/last.ckpt', '/tmp/pytest-of-ollie/pytest-675/test_inference_batch_size_flag0/model/wts.pt', '/tmp/pytest-of-ollie/pytest-675/test_inference_batch_size_flag0/model/model.pt']
Factory validated model directory, but checkpoint file missing.
FAILED tests/torch/test_cli_inference_torch.py::TestInferenceCLI::test_multiple_execution_config_flags - FileNotFoundError: No Lightning checkpoint found in /tmp/pytest-of-ollie/pytest-675/test_multiple_execution_config0/model.
Searched for: ['/tmp/pytest-of-ollie/pytest-675/test_multiple_execution_config0/model/checkpoints/last.ckpt', '/tmp/pytest-of-ollie/pytest-675/test_multiple_execution_config0/model/wts.pt', '/tmp/pytest-of-ollie/pytest-675/test_multiple_execution_config0/model/model.pt']
Factory validated model directory, but checkpoint file missing.
============================== 4 failed in 4.64s ===============================
