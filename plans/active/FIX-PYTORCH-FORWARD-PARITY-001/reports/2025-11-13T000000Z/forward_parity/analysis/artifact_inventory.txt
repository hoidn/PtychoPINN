# Artifact Inventory — FIX-PYTORCH-FORWARD-PARITY-001/forward_parity

## Phase A — Instrumentation + Short Baseline Rerun (2025-11-14, latest post-dc5415ba refresh)

### Green Evidence (Tests Passing)
- green/pytest_patch_stats_rerun.log — Selector GREEN after fresh rerun (1/1 PASSED, 7.20s)
  - Confirms torch_patch_stats.json/torch_patch_grid.png emission
  - log_patch_stats/patch_stats_limit flags working via payload handoff (dc5415ba)
  - Test timestamp: 2025-11-14 03:38:46 (latest fresh evidence post-dc5415ba)

### CLI Evidence (Training + Inference)
- cli/train_patch_stats_rerun.log — 10-epoch training run with instrumentation (fresh 2025-11-14 03:39)
  - Dataset: fly001_reconstructed_final_downsampled_data_train.npz (256 images, gridsize=2)
  - Flags: --log-patch-stats --patch-stats-limit 2 --accelerator gpu --deterministic
  - Output: outputs/torch_forward_parity_baseline
  - Completed successfully, model saved to wts.h5.zip
  - Torch patch stats logged for train batch 0 (mean=0.002091, std=0.003645, var_zero_mean=1.33e-05)
- cli/inference_patch_stats_rerun.log — Inference run with debug dump (fresh 2025-11-14 03:40)
  - Model: outputs/torch_forward_parity_baseline
  - Dataset: fly001_reconstructed_final_downsampled_data_test.npz (128 images)
  - Flags: --log-patch-stats --patch-stats-limit 2 --debug-dump --accelerator gpu
  - Completed successfully
  - Torch patch stats logged for inference batch 0 (mean=765.820, std=1397.806, var_zero_mean=1953579.0)

### Analysis Artifacts
- analysis/torch_patch_stats.json — Per-patch statistics (2 training batches, fresh 2025-11-14 03:39)
  - Train phase batch 0 (first occurrence): mean=0.002091, std=0.003645, var_zero_mean=1.33e-05
  - Train phase batch 0 (second occurrence): mean=0.002091, std=0.003645, var_zero_mean=1.33e-05
  - Non-zero variance confirms patches have meaningful structure
  - 4 patches per batch, consistent metrics across both logged batches
- analysis/torch_patch_grid.png — Normalized patch visualization (7.5KB, fresh 2025-11-14 03:39)
  - Shows first 2 batches of training patches (8 patches total)
- analysis/torch_patch_stats_inference.json — Per-patch statistics (1 inference batch, fresh 2025-11-14 03:40)
  - Inference phase batch 0: mean=765.820, std=1397.806, var_zero_mean=1953579.0
  - Much higher variance than training (expected for unnormalized outputs)
  - 4 patches logged (batch_size=128, but only first 4 captured for stats)
- analysis/torch_patch_grid_inference.png — Normalized inference patch visualization (fresh 2025-11-14 03:40)
- analysis/forward_parity_debug/ — Inference debug dump (fresh 2025-11-14 03:40)
  - canvas.json — Canvas metadata (140 bytes)
  - offsets.json — Patch offset information (1.4KB, 128 groups)
  - pred_patches_amp_grid.png — Amplitude patches unnormalized (92.9KB)
  - pred_patches_amp_grid_normalized.png — Amplitude patches normalized (97.6KB)
  - stats.json — Inference statistics (301 bytes)

### Code Changes (Commit dc5415ba)
- ptycho_torch/workflows/components.py — Training payload threading
  - Added training_payload parameter to run_cdi_example_torch, train_cdi_model_torch, _train_with_lightning
  - Conditional logic: reuse payload when provided, rebuild via factory otherwise
- ptycho_torch/train.py — Pass payload at line 756

### Summary
- All Phase A evidence refreshed with timestamps after dc5415ba (2025-11-14 03:38-03:40)
- Pytest selector GREEN (1/1 PASSED, 7.20s), training/inference logs captured with instrumentation active
- torch_patch_stats.json shows non-zero training variance (1.33e-05) proving patches have structure
- Inference shows much higher variance (1953579.0) due to unnormalized outputs
- Debug dumps contain expected canvas/offsets/patch grids
- Phase A checklist items A0/A1/A2/A3 complete
- Ready for Phase B: scaling/config alignment (object_big defaults, intensity_scale persistence)

### Key Metrics Comparison
| Phase     | Mean      | Std       | Var(zero-mean) |
|-----------|-----------|-----------|----------------|
| Training  | 0.002091  | 0.003645  | 1.33e-05       |
| Inference | 765.820   | 1397.806  | 1953579.0      |

The ~365,000x variance increase from training→inference is expected because training uses normalized inputs while inference outputs are unnormalized reconstructed patches.
