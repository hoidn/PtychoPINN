# Artifact Inventory — FIX-PYTORCH-FORWARD-PARITY-001/forward_parity

## Phase A — Instrumentation + Short Baseline Rerun (2025-11-14, post-dc5415ba refresh on 2025-11-14 04:35-04:36)

### Green Evidence (Tests Passing)
- green/pytest_patch_stats_rerun.log — Selector GREEN after fresh rerun (1/1 PASSED, 7.15s)
  - Confirms torch_patch_stats.json/torch_patch_grid.png emission
  - log_patch_stats/patch_stats_limit flags working via payload handoff (dc5415ba)
  - Test timestamp: 2025-11-14 04:35:22 (latest fresh evidence post-dc5415ba)

### CLI Evidence (Training + Inference)
- cli/train_patch_stats_rerun.log — 10-epoch training run with instrumentation (fresh 2025-11-14 04:35)
  - Dataset: fly001_reconstructed_final_downsampled_data_train.npz (256 images, gridsize=2)
  - Flags: --log-patch-stats --patch-stats-limit 2 --accelerator gpu --deterministic
  - Output: outputs/torch_forward_parity_baseline
  - Completed successfully, model saved to wts.h5.zip
  - Torch patch stats logged for train batch 0 (mean=0.002909, std=0.005068, var_zero_mean=2.57e-05)
- cli/inference_patch_stats_rerun.log — Inference run with debug dump (fresh 2025-11-14 04:36)
  - Model: outputs/torch_forward_parity_baseline
  - Dataset: fly001_reconstructed_final_downsampled_data_test.npz (128 images)
  - Flags: --log-patch-stats --patch-stats-limit 2 --debug-dump --accelerator gpu
  - Completed successfully
  - Torch patch stats logged for inference batch 0 (mean=1276.072, std=2333.519, var_zero_mean=5443325.0)

### Analysis Artifacts
- analysis/torch_patch_stats.json — Per-patch statistics (2 training batches, fresh 2025-11-14 04:36)
  - Train phase batch 0 (first occurrence): mean=0.002909, std=0.005068, var_zero_mean=2.57e-05
  - Train phase batch 0 (second occurrence): mean=0.002909, std=0.005068, var_zero_mean=2.57e-05
  - Non-zero variance confirms patches have meaningful structure
  - 4 patches per batch, consistent metrics across both logged batches
- analysis/torch_patch_grid.png — Normalized patch visualization (7.4KB, fresh 2025-11-14 04:36)
  - Shows first 2 batches of training patches (8 patches total)
- analysis/forward_parity_debug/ — Inference debug dump (fresh 2025-11-14 04:36)
  - canvas.json — Canvas metadata (140 bytes)
  - offsets.json — Patch offset information (1.4KB, 128 groups)
  - pred_patches_amp_grid.png — Amplitude patches unnormalized (88KB)
  - pred_patches_amp_grid_normalized.png — Amplitude patches normalized (92KB)
  - stats.json — Inference statistics (300 bytes)

### Code Changes (Commit dc5415ba)
- ptycho_torch/workflows/components.py — Training payload threading
  - Added training_payload parameter to run_cdi_example_torch, train_cdi_model_torch, _train_with_lightning
  - Conditional logic: reuse payload when provided, rebuild via factory otherwise
- ptycho_torch/train.py — Pass payload at line 756

### Summary
- All Phase A evidence refreshed with timestamps after dc5415ba (2025-11-14 04:35-04:36)
- Pytest selector GREEN (1/1 PASSED, 7.15s), training/inference logs captured with instrumentation active
- torch_patch_stats.json shows non-zero training variance (2.57e-05) proving patches have structure
- Inference shows much higher variance (5443325.0) due to unnormalized outputs
- Debug dumps contain expected canvas/offsets/patch grids
- Phase A checklist items A0/A1/A2/A3 complete
- Ready for Phase B: scaling/config alignment (object_big defaults, intensity_scale persistence)

### Key Metrics Comparison
| Phase     | Mean      | Std       | Var(zero-mean) |
|-----------|-----------|-----------|----------------|
| Training  | 0.002909  | 0.005068  | 2.57e-05       |
| Inference | 1276.072  | 2333.519  | 5443325.0      |

The ~2.1M-fold variance increase from training→inference is expected because training uses normalized inputs while inference outputs are unnormalized reconstructed patches.
