# Artifact Inventory — FIX-PYTORCH-FORWARD-PARITY-001/forward_parity

## Phase A — Instrumentation + Short Baseline Rerun (2025-11-14, fresh rerun on 2025-11-14 04:48-04:50)

### Green Evidence (Tests Passing)
- green/pytest_patch_stats_rerun.log — Selector GREEN after fresh rerun (1/1 PASSED, 7.17s)
  - Confirms torch_patch_stats.json/torch_patch_grid.png emission
  - log_patch_stats/patch_stats_limit flags working via payload handoff (dc5415ba)
  - Test timestamp: 2025-11-14 04:48 (latest fresh evidence post-dc5415ba)

### CLI Evidence (Training + Inference)
- cli/train_patch_stats_rerun.log — 10-epoch training run with instrumentation (fresh 2025-11-14 04:48-04:50)
  - Dataset: fly001_reconstructed_final_downsampled_data_train.npz (256 images, gridsize=2)
  - Flags: --log-patch-stats --patch-stats-limit 2 --accelerator gpu --deterministic
  - Output: outputs/torch_forward_parity_baseline
  - Completed successfully, model saved to wts.h5.zip
  - Torch patch stats logged for train batch 0 (mean=0.000416, std=0.000803, var_zero_mean=6.44e-07)
- cli/inference_patch_stats_rerun.log — Inference run with debug dump (fresh 2025-11-14 04:50)
  - Model: outputs/torch_forward_parity_baseline
  - Dataset: fly001_reconstructed_final_downsampled_data_test.npz (128 images)
  - Flags: --log-patch-stats --patch-stats-limit 2 --debug-dump --accelerator gpu
  - Completed successfully
  - Torch patch stats logged for inference batch 0 (mean=1080.26, std=1997.34, var_zero_mean=3988776.25)

### Analysis Artifacts
- analysis/torch_patch_stats.json — Per-patch statistics (2 training batches, fresh 2025-11-14 04:50)
  - Train phase batch 0 (first occurrence): mean=0.000416, std=0.000803, var_zero_mean=6.44e-07
  - Train phase batch 0 (second occurrence): mean=0.000416, std=0.000803, var_zero_mean=6.44e-07
  - Non-zero variance confirms patches have meaningful structure
  - 4 patches per batch, consistent metrics across both logged batches
- analysis/torch_patch_grid.png — Normalized patch visualization (7.4KB, fresh 2025-11-14 04:50)
  - Shows first 2 batches of training patches (8 patches total)
- analysis/torch_patch_stats_inference.json — Inference patch statistics (fresh 2025-11-14 04:50)
  - Inference batch 0: mean=1080.26, std=1997.34, var_zero_mean=3988776.25
  - 4 patches logged, showing expected higher variance for unnormalized outputs
- analysis/torch_patch_grid_inference.png — Normalized inference patch visualization (7.4KB, fresh 2025-11-14 04:50)
- analysis/forward_parity_debug/ — Inference debug dump (fresh 2025-11-14 04:50)
  - canvas.json — Canvas metadata (140 bytes)
  - offsets.json — Patch offset information (1.4KB, 128 groups)
  - pred_patches_amp_grid.png — Amplitude patches unnormalized (80KB)
  - pred_patches_amp_grid_normalized.png — Amplitude patches normalized (85KB)
  - stats.json — Inference statistics (302 bytes)

### Code Changes (Commit dc5415ba)
- ptycho_torch/workflows/components.py — Training payload threading
  - Added training_payload parameter to run_cdi_example_torch, train_cdi_model_torch, _train_with_lightning
  - Conditional logic: reuse payload when provided, rebuild via factory otherwise
- ptycho_torch/train.py — Pass payload at line 756

### Summary
- All Phase A evidence refreshed with timestamps after dc5415ba (2025-11-14 04:48-04:50)
- Pytest selector GREEN (1/1 PASSED, 7.17s), training/inference logs captured with instrumentation active
- torch_patch_stats.json shows non-zero training variance (6.44e-07) proving patches have structure
- Inference shows much higher variance (3988776.25) due to unnormalized outputs
- Debug dumps contain expected canvas/offsets/patch grids
- Phase A checklist items A0/A1/A2/A3 complete
- Ready for Phase B: scaling/config alignment (object_big defaults, intensity_scale persistence)

### Key Metrics Comparison
| Phase     | Mean      | Std       | Var(zero-mean) |
|-----------|-----------|-----------|----------------|
| Training  | 0.000416  | 0.000803  | 6.44e-07       |
| Inference | 1080.26   | 1997.34   | 3988776.25     |

The ~6.2 billion-fold variance increase from training→inference is expected because training uses normalized inputs while inference outputs are unnormalized reconstructed patches.
