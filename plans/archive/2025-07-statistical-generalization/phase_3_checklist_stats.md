# Agent Implementation Checklist: Phase 3 - Documentation and Final Validation

**Overall Goal for this Phase:** To update all relevant documentation, perform a full end-to-end test, and validate the success criteria for the statistical generalization study enhancement.

**Instructions for Agent:**
1. Copy this checklist into your working memory.
2. Update the `State` for each item as you progress: `[ ]` (Open) -> `[P]` (In Progress) -> `[D]` (Done).
3. Follow the `How/Why & API Guidance` column carefully for implementation details.

---

| ID | Task Description | State | How/Why & API Guidance |
| :-- | :-- | :-- | :-- |
| **Section 0: Preparation & Context Validation** |
| 0.A | **Verify Phase 2 Success Test** | `[D]` | **Why:** Ensure Phase 2 is actually complete before proceeding. <br> **How:** Run `aggregate_and_plot_results.py` on the mock multi-trial dataset from Phase 2. Verify that all tasks in `phase_2_checklist_stats.md` are marked `[D]`, the plot shows percentile bands, and the CSV contains `_median`, `_p25`, `_p75` columns. <br> **File:** Test execution and checklist verification. |
| 0.B | **Review R&D Plan Success Criteria** | `[D]` | **Why:** To understand exactly what constitutes completion for this initiative. <br> **Docs:** `docs/studies/multirun/plan_statistical_generalization.md` section "Success Criteria (How we know we're done)". <br> **How:** Read and note the 5 success criteria that must be met for the initiative to be considered complete. |
| **Section 1: Documentation Updates** |
| 1.A | **Update QUICK_REFERENCE.md** | `[ ]` | **Why:** Users need to know about the new `--num-trials` flag and multi-trial capabilities. <br> **How:** Add documentation for the `--num-trials` flag in `run_complete_generalization_study.sh`. Include example usage showing how to run multi-trial studies. Document the new statistical output format (median, percentiles). <br> **File:** `docs/studies/QUICK_REFERENCE.md`. |
| 1.B | **Update main README.md** | `[ ]` | **Why:** The main README should reflect the enhanced capabilities. <br> **How:** Update the generalization study section to mention multi-trial support and statistical robustness. Add a brief note about the enhanced plotting with uncertainty bands. Keep changes concise and user-focused. <br> **File:** `README.md` (project root). |
| 1.C | **Update script help documentation** | `[ ]` | **Why:** Command-line help should reflect the new capabilities. <br> **How:** Verify that `aggregate_and_plot_results.py --help` accurately describes the new multi-trial detection and statistical features. The enhanced docstring should be sufficient, but check that it's complete and clear. <br> **File:** Verify `scripts/studies/aggregate_and_plot_results.py` docstring. |
| **Section 2: End-to-End Integration Testing** |
| 2.A | **Create minimal end-to-end test scenario** | `[ ]` | **Why:** To verify the complete workflow works as designed. <br> **How:** Design a minimal test using `--train-sizes "512" --num-trials 2` that can complete quickly. This will test the Phase 1 script integration with Phase 2 aggregation. Document the expected directory structure and outputs. <br> **Planning:** Design test parameters and expected outcomes. |
| 2.B | **Execute end-to-end micro-study** | `[ ]` | **Why:** To prove the integrated workflow functions correctly. <br> **How:** Run `run_complete_generalization_study.sh --train-sizes "512" --num-trials 2 --skip-data-prep` (if available) or equivalent minimal test. Verify that: 1) Trial directories are created correctly, 2) Both trials complete successfully, 3) `aggregate_and_plot_results.py` processes the results. <br> **File:** Execute and monitor the complete workflow. |
| 2.C | **Validate end-to-end outputs** | `[ ]` | **Why:** To ensure the final artifacts meet expectations. <br> **How:** Check that the end-to-end test produces: 1) Trial subdirectories with comparison_metrics.csv files, 2) A final plot showing median lines with percentile bands, 3) A results.csv with statistical columns, 4) No critical errors in logs. <br> **Validation:** Inspect all generated files and verify quality. |
| **Section 3: Success Criteria Verification** |
| 3.A | **Verify Criterion 1: num-trials argument** | `[ ]` | **Why:** Success criterion from R&D plan. <br> **How:** Confirm that `run_complete_generalization_study.sh` accepts a `--num-trials` argument and creates the corresponding number of trial subdirectories for each training size. Test with different values (1, 2, 5). <br> **Test:** Command-line argument parsing and directory creation. |
| 3.B | **Verify Criterion 2: nested directory parsing** | `[ ]` | **Why:** Success criterion from R&D plan. <br> **How:** Confirm that `aggregate_and_plot_results.py` successfully parses the new nested directory structure (`train_SIZE/trial_N/comparison_metrics.csv`). Test with both multi-trial and legacy single-trial data. <br> **Test:** File discovery and parsing logic. |
| 3.C | **Verify Criterion 3: statistical CSV output** | `[ ]` | **Why:** Success criterion from R&D plan. <br> **How:** Confirm that the final `results.csv` file contains columns for the median and percentiles of each metric (e.g., `psnr_amp_median`, `psnr_amp_p25`, `psnr_amp_p75`). Verify column naming consistency and data completeness. <br> **Test:** CSV format and content validation. |
| 3.D | **Verify Criterion 4: percentile band visualization** | `[ ]` | **Why:** Success criterion from R&D plan. <br> **How:** Confirm that the final plot (`.png`) correctly displays the median metric values as lines and the interquartile range (25th-75th percentile) as a shaded region around the lines. Verify visual clarity and legend accuracy. <br> **Test:** Plot visual inspection and validation. |
| 3.E | **Verify Criterion 5: documentation completeness** | `[ ]` | **Why:** Success criterion from R&D plan. <br> **How:** Confirm that the `QUICK_REFERENCE.md` is updated with the new `--num-trials` flag and includes a complete example. Verify that all user-facing documentation accurately reflects the new capabilities. <br> **Test:** Documentation review and completeness check. |
| **Section 4: Performance and Edge Case Testing** |
| 4.A | **Test backward compatibility** | `[ ]` | **Why:** Ensure existing single-trial studies still work. <br> **How:** Test `aggregate_and_plot_results.py` on a legacy directory structure (direct `train_SIZE/comparison_metrics.csv` files). Verify that the script correctly detects single-trial mode and produces appropriate output without percentile bands. <br> **Test:** Legacy compatibility verification. |
| 4.B | **Test mixed trial scenarios** | `[ ]` | **Why:** Handle real-world scenarios where different training sizes have different numbers of trials. <br> **How:** Create a test dataset where some training sizes have multiple trials and others have single trials. Verify that the aggregation handles this gracefully and produces appropriate warnings. <br> **Test:** Mixed data structure handling. |
| 4.C | **Test failure resilience** | `[ ]` | **Why:** Ensure the system handles various failure modes gracefully. <br> **How:** Test scenarios with: 1) Missing CSV files, 2) CSV files with all NaN values, 3) Incomplete trial directories. Verify that appropriate warnings are logged and the system continues processing available data. <br> **Test:** Error handling and resilience validation. |
| **Section 5: Code Quality and Maintenance** |
| 5.A | **Code review and cleanup** | `[ ]` | **Why:** Ensure code quality and maintainability. <br> **How:** Review the modified `aggregate_and_plot_results.py` for: 1) Clear function documentation, 2) Consistent error handling, 3) No debug print statements, 4) Appropriate logging levels. Clean up any temporary code or comments. <br> **File:** `scripts/studies/aggregate_and_plot_results.py`. |
| 5.B | **Verify logging consistency** | `[ ]` | **Why:** Ensure appropriate logging for debugging and monitoring. <br> **How:** Run the script with `--verbose` and verify that log messages are: 1) Informative and appropriate, 2) Consistently formatted, 3) Not overly verbose, 4) Include key statistics (trial counts, file counts, processing summaries). <br> **Test:** Logging output review. |
| 5.C | **Performance baseline documentation** | `[ ]` | **Why:** Document expected performance characteristics for future reference. <br> **How:** Time the execution of `aggregate_and_plot_results.py` on the test multi-trial dataset. Document approximate processing time per trial and any performance considerations for large studies. <br> **Documentation:** Performance characteristics summary. |
| **Section 6: Final Integration and Sign-off** |
| 6.A | **Update implementation plan status** | `[ ]` | **Why:** Mark Phase 3 as complete in the official planning documents. <br> **How:** Update `docs/studies/multirun/implementation_statistical_generalization.md` to mark Phase 3 as complete (`[âœ…]`) and update the "Current Phase" line to indicate completion. <br> **File:** `docs/studies/multirun/implementation_statistical_generalization.md`. |
| 6.B | **Prepare PROJECT_STATUS.md update** | `[ ]` | **Why:** Ready the project status for initiative completion. <br> **How:** Prepare (but do not yet apply) the changes needed to move this initiative from "Current Active Initiative" to "Completed Initiatives" in `docs/PROJECT_STATUS.md`. Document the key deliverables and planning documents. <br> **Planning:** Prepare status update content. |
| 6.C | **Create completion summary** | `[ ]` | **Why:** Provide a clear summary of what was accomplished. <br> **How:** Document a concise summary of: 1) All Phase 1-3 deliverables, 2) Key features implemented, 3) Testing completed, 4) Documentation updated, 5) Files modified/created. This serves as a project completion record. <br> **Documentation:** Completion summary report. |
