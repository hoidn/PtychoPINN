============================= test session starts ==============================
platform linux -- Python 3.11.13, pytest-8.4.1, pluggy-1.6.0 -- /home/ollie/miniconda3/envs/ptycho311/bin/python
cachedir: .pytest_cache
PyTorch: available
PyTorch version: 2.9.1+cu128
rootdir: /home/ollie/Documents/tmp/PtychoPINN
configfile: pyproject.toml
plugins: typeguard-2.13.3, anyio-4.9.0
collecting ... collected 278 items

tests/torch/test_api_deprecation.py::TestLegacyAPIDeprecation::test_example_train_import_emits_deprecation_warning PASSED [  0%]
tests/torch/test_api_deprecation.py::TestLegacyAPIDeprecation::test_api_package_import_is_idempotent PASSED [  0%]
tests/torch/test_backend_selection.py::TestBackendSelection::test_defaults_to_tensorflow_backend PASSED [  1%]
tests/torch/test_backend_selection.py::TestBackendSelection::test_selects_pytorch_backend PASSED [  1%]
tests/torch/test_backend_selection.py::TestBackendSelection::test_pytorch_backend_calls_update_legacy_dict PASSED [  1%]
tests/torch/test_backend_selection.py::TestBackendSelection::test_pytorch_unavailable_raises_error PASSED [  2%]
tests/torch/test_backend_selection.py::TestBackendSelection::test_inference_config_supports_backend_selection PASSED [  2%]
tests/torch/test_backend_selection.py::TestBackendSelection::test_backend_selection_preserves_api_parity PASSED [  2%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLI::test_accelerator_flag_roundtrip FAILED [  3%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLI::test_num_workers_flag_roundtrip PASSED [  3%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLI::test_inference_batch_size_flag_roundtrip PASSED [  3%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLI::test_multiple_execution_config_flags PASSED [  4%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLIThinWrapper::test_cli_delegates_to_validate_paths PASSED [  4%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLIThinWrapper::test_cli_delegates_to_helper_for_data_loading PASSED [  5%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLIThinWrapper::test_cli_delegates_to_inference_helper PASSED [  5%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLIThinWrapper::test_cli_calls_save_individual_reconstructions PASSED [  5%]
tests/torch/test_cli_inference_torch.py::TestInferenceCLIThinWrapper::test_quiet_flag_suppresses_progress_output PASSED [  6%]
tests/torch/test_cli_shared.py::TestResolveAccelerator::test_default_no_device PASSED [  6%]
tests/torch/test_cli_shared.py::TestResolveAccelerator::test_legacy_device_cpu PASSED [  6%]
tests/torch/test_cli_shared.py::TestResolveAccelerator::test_legacy_device_cuda_maps_to_gpu PASSED [  7%]
tests/torch/test_cli_shared.py::TestResolveAccelerator::test_conflict_accelerator_wins PASSED [  7%]
tests/torch/test_cli_shared.py::TestResolveAccelerator::test_all_accelerator_values_passthrough PASSED [  7%]
tests/torch/test_cli_shared.py::TestResolveAccelerator::test_resolve_accelerator_auto_defaults PASSED [  8%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfig::test_training_mode_defaults PASSED [  8%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfig::test_training_mode_custom_values PASSED [  8%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfig::test_inference_mode PASSED [  9%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfig::test_emits_deterministic_warning PASSED [  9%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfig::test_handles_quiet_flag PASSED [ 10%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfig::test_handles_disable_mlflow_flag PASSED [ 10%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfig::test_quiet_or_disable_mlflow_both_true PASSED [ 10%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfig::test_invalid_mode_raises_value_error PASSED [ 11%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfig::test_resolves_accelerator_from_device_flag PASSED [ 11%]
tests/torch/test_cli_shared.py::TestValidatePaths::test_creates_output_dir PASSED [ 11%]
tests/torch/test_cli_shared.py::TestValidatePaths::test_raises_if_train_file_missing PASSED [ 12%]
tests/torch/test_cli_shared.py::TestValidatePaths::test_raises_if_test_file_missing PASSED [ 12%]
tests/torch/test_cli_shared.py::TestValidatePaths::test_accepts_none_test_file PASSED [ 12%]
tests/torch/test_cli_shared.py::TestValidatePaths::test_works_with_pathlib_path_objects PASSED [ 13%]
tests/torch/test_cli_shared.py::TestValidatePaths::test_accepts_none_train_file_for_inference_mode PASSED [ 13%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfigInferenceMode::test_inference_mode_defaults PASSED [ 14%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfigInferenceMode::test_inference_mode_custom_batch_size PASSED [ 14%]
tests/torch/test_cli_shared.py::TestBuildExecutionConfigInferenceMode::test_inference_mode_respects_quiet PASSED [ 14%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_accelerator_flag_roundtrip PASSED [ 15%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_deterministic_flag_roundtrip PASSED [ 15%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_no_deterministic_flag_roundtrip PASSED [ 15%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_num_workers_flag_roundtrip PASSED [ 16%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_learning_rate_flag_roundtrip PASSED [ 16%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_multiple_execution_config_flags PASSED [ 16%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_bundle_persistence PASSED [ 17%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_enable_checkpointing_flag PASSED [ 17%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_checkpoint_save_top_k_flag PASSED [ 17%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_checkpoint_monitor_flag PASSED [ 18%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_checkpoint_mode_flag PASSED [ 18%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_early_stop_patience_flag PASSED [ 19%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_scheduler_flag_roundtrip PASSED [ 19%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_accumulate_grad_batches_roundtrip PASSED [ 19%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_logger_backend_csv_default PASSED [ 20%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_logger_backend_tensorboard PASSED [ 20%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_logger_backend_none PASSED [ 20%]
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_disable_mlflow_deprecation_warning PASSED [ 21%]
tests/torch/test_cli_train_torch.py::TestPatchStatsCLI::test_patch_stats_dump FAILED [ 21%]
tests/torch/test_config_bridge.py::TestConfigBridgeMVP::test_mvp_config_bridge_populates_params_cfg PASSED [ 21%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_direct_fields[N-direct] PASSED [ 22%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_direct_fields[n_filters_scale-direct] PASSED [ 22%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_direct_fields[object_big-direct] PASSED [ 23%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_direct_fields[probe_big-direct] PASSED [ 23%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_direct_fields[batch_size-direct] PASSED [ 23%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_transform_fields[gridsize-tuple-to-int] PASSED [ 24%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_transform_fields[model_type-unsupervised] PASSED [ 24%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_transform_fields[model_type-supervised] PASSED [ 24%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_transform_fields[amp_activation-silu] PASSED [ 25%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_transform_fields[amp_activation-SiLU] PASSED [ 25%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_transform_fields[amp_activation-passthrough] PASSED [ 25%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_transform_fields[nepochs-rename] PASSED [ 26%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_transform_fields[nll_weight-true] PASSED [ 26%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_transform_fields[nll_weight-false] PASSED [ 26%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_override_fields[pad_object-default] PASSED [ 27%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_override_fields[pad_object-override] PASSED [ 27%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_override_fields[gaussian_smoothing_sigma-default] PASSED [ 28%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_override_fields[gaussian_smoothing_sigma-override] PASSED [ 28%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[mae_weight-default] PASSED [ 28%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[mae_weight-override] PASSED [ 29%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[realspace_mae_weight-default] PASSED [ 29%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[realspace_weight-default] PASSED [ 29%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[positions_provided-default] PASSED [ 30%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[probe_trainable-default] PASSED [ 30%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[sequential_sampling-default] PASSED [ 30%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_inference_config_override_fields[debug-default] PASSED [ 31%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_inference_config_override_fields[debug-override] PASSED [ 31%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_probe_mask_translation[probe_mask-default] PASSED [ 32%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_config_probe_mask_override PASSED [ 32%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_default_divergence_detection[nphotons-divergence] PASSED [ 32%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_default_divergence_detection[probe_scale-divergence] PASSED [ 33%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_gridsize_error_handling[gridsize-non-square] PASSED [ 33%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_type_error_handling[model_type-invalid-enum] PASSED [ 33%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_activation_error_handling[amp_activation-unknown] PASSED [ 34%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_train_data_file_required_error PASSED [ 34%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_model_path_required_error PASSED [ 34%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_nphotons_default_divergence_error PASSED [ 35%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_nphotons_override_passes_validation PASSED [ 35%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_n_subsample_missing_override_uses_none PASSED [ 35%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_n_subsample_explicit_override PASSED [ 36%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_inference_config_n_subsample_missing_override_uses_none PASSED [ 36%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_inference_config_n_subsample_explicit_override PASSED [ 37%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_subsample_seed_from_dataconfig PASSED [ 37%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_subsample_seed_override PASSED [ 37%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_n_groups_missing_override_warning PASSED [ 38%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_test_data_file_training_missing_warning PASSED [ 38%]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_params_cfg_matches_baseline PASSED [ 38%]
tests/torch/test_config_bridge.py::TestConfigBridgeArchitecture::test_config_bridge_architecture_override PASSED [ 39%]
tests/torch/test_config_bridge.py::TestConfigBridgeArchitecture::test_config_bridge_architecture_default PASSED [ 39%]
tests/torch/test_config_bridge.py::TestConfigBridgeArchitecture::test_config_bridge_architecture_from_pt_model PASSED [ 39%]
tests/torch/test_config_factory.py::TestTrainingPayloadStructure::test_training_payload_returns_dataclass PASSED [ 40%]
tests/torch/test_config_factory.py::TestTrainingPayloadStructure::test_training_payload_contains_tf_config PASSED [ 40%]
tests/torch/test_config_factory.py::TestTrainingPayloadStructure::test_training_payload_contains_pytorch_configs PASSED [ 41%]
tests/torch/test_config_factory.py::TestTrainingPayloadStructure::test_training_payload_contains_overrides_dict PASSED [ 41%]
tests/torch/test_config_factory.py::TestTrainingPayloadStructure::test_gridsize_sets_channel_count FAILED [ 41%]
tests/torch/test_config_factory.py::TestInferencePayloadStructure::test_inference_payload_returns_dataclass PASSED [ 42%]
tests/torch/test_config_factory.py::TestInferencePayloadStructure::test_inference_payload_contains_tf_config PASSED [ 42%]
tests/torch/test_config_factory.py::TestInferencePayloadStructure::test_inference_payload_contains_pytorch_configs PASSED [ 42%]
tests/torch/test_config_factory.py::TestConfigBridgeTranslation::test_grid_size_tuple_to_gridsize_int PASSED [ 43%]
tests/torch/test_config_factory.py::TestConfigBridgeTranslation::test_epochs_to_nepochs_conversion FAILED [ 43%]
tests/torch/test_config_factory.py::TestConfigBridgeTranslation::test_k_to_neighbor_count_conversion FAILED [ 43%]
tests/torch/test_config_factory.py::TestLegacyParamsPopulation::test_factory_populates_params_cfg PASSED [ 44%]
tests/torch/test_config_factory.py::TestLegacyParamsPopulation::test_populate_legacy_params_helper PASSED [ 44%]
tests/torch/test_config_factory.py::TestOverridePrecedence::test_override_dict_wins_over_defaults PASSED [ 44%]
tests/torch/test_config_factory.py::TestOverridePrecedence::test_probe_size_override_wins_over_inference PASSED [ 45%]
tests/torch/test_config_factory.py::TestFactoryValidation::test_missing_n_groups_raises_error PASSED [ 45%]
tests/torch/test_config_factory.py::TestFactoryValidation::test_nonexistent_train_data_file_raises_error PASSED [ 46%]
tests/torch/test_config_factory.py::TestFactoryValidation::test_missing_checkpoint_raises_error PASSED [ 46%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_training_payload_execution_config_not_none PASSED [ 46%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_inference_payload_execution_config_not_none PASSED [ 47%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_execution_config_defaults_applied PASSED [ 47%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_execution_config_explicit_instance_propagates PASSED [ 47%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_execution_config_fields_accessible PASSED [ 48%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_overrides_applied_records_execution_knobs PASSED [ 48%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_checkpoint_knobs_propagate_through_factory PASSED [ 48%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_checkpoint_defaults_respected PASSED [ 49%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_scheduler_override_applied PASSED [ 49%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_accum_steps_override_applied PASSED [ 50%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_logger_backend_csv_default PASSED [ 50%]
tests/torch/test_config_factory.py::TestExecutionConfigOverrides::test_logger_backend_tensorboard PASSED [ 50%]
tests/torch/test_config_factory.py::TestProbeSizeInference::test_infer_probe_size_from_npz PASSED [ 51%]
tests/torch/test_config_factory.py::TestProbeSizeInference::test_infer_probe_size_missing_file_fallback PASSED [ 51%]
tests/torch/test_data_pipeline.py::TestRawDataTorchAdapter::test_raw_data_torch_matches_tensorflow PASSED [ 51%]
tests/torch/test_data_pipeline.py::TestDataContainerParity::test_data_container_shapes_and_dtypes PASSED [ 52%]
tests/torch/test_data_pipeline.py::TestGroundTruthLoading::test_y_patches_are_complex64 PASSED [ 52%]
tests/torch/test_data_pipeline.py::TestMemmapBridgeParity::test_memmap_loader_matches_raw_data_torch PASSED [ 52%]
tests/torch/test_data_pipeline.py::TestMemmapBridgeParity::test_deterministic_generation_validation PASSED [ 53%]
tests/torch/test_data_pipeline.py::TestMemmapBridgeParity::test_memmap_bridge_accepts_diffraction_legacy PASSED [ 53%]
tests/torch/test_dataloader.py::TestDataloaderCanonicalKeySupport::test_backward_compat_legacy_diff3d FAILED [ 53%]
tests/torch/test_dataloader.py::TestDataloaderCanonicalKeySupport::test_error_when_no_diffraction_key PASSED [ 54%]
tests/torch/test_dataloader.py::TestDataloaderCanonicalKeySupport::test_loads_canonical_diffraction FAILED [ 54%]
tests/torch/test_dataloader.py::TestDataloaderFormatAutoTranspose::test_auto_transposes_legacy_hwn_format PASSED [ 55%]
tests/torch/test_dataloader.py::TestDataloaderFormatAutoTranspose::test_handles_edge_case_square_dataset PASSED [ 55%]
tests/torch/test_dataloader.py::TestDataloaderFormatAutoTranspose::test_npz_headers_also_transposes_shape FAILED [ 55%]
tests/torch/test_dataloader.py::TestDataloaderFormatAutoTranspose::test_preserves_canonical_nwh_format PASSED [ 56%]
tests/torch/test_dataloader.py::TestDataloaderFormatAutoTranspose::test_real_dataset_dimensions PASSED [ 56%]
tests/torch/test_dataloader.py::TestDataloaderFormatAutoTranspose::test_works_with_diff3d_legacy_key PASSED [ 56%]
tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_auto_prefers_cuda PASSED [ 57%]
tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_auto_warns_and_falls_back_to_cpu PASSED [ 57%]
tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_explicit_cpu_bypasses_auto_resolution PASSED [ 57%]
tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_explicit_cuda_bypasses_auto_resolution PASSED [ 58%]
tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_workflow_auto_instantiates_with_hardware_detection PASSED [ 58%]
tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_backend_selector_warns_on_cpu_only_hosts SKIPPED [ 58%]
tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_backend_selector_inherits_gpu_first_defaults PASSED [ 59%]
tests/torch/test_execution_config_defaults.py::TestPyTorchExecutionConfigDefaults::test_backend_selector_cpu_fallback_with_warning PASSED [ 59%]
tests/torch/test_fixture_pytorch_integration.py::TestFixtureContract::test_fixture_file_exists SKIPPED [ 60%]
tests/torch/test_fixture_pytorch_integration.py::TestFixtureContract::test_fixture_outputs_match_contract SKIPPED [ 60%]
tests/torch/test_fixture_pytorch_integration.py::TestFixtureContract::test_metadata_sidecar_exists SKIPPED [ 60%]
tests/torch/test_fixture_pytorch_integration.py::TestFixtureContract::test_metadata_content_valid SKIPPED [ 61%]
tests/torch/test_fixture_pytorch_integration.py::TestFixtureContract::test_coordinate_coverage SKIPPED [ 61%]
tests/torch/test_fixture_pytorch_integration.py::TestFixtureIntegrationSmoke::test_fixture_loads_with_rawdata SKIPPED [ 61%]
tests/torch/test_fixture_pytorch_integration.py::TestFixtureIntegrationSmoke::test_fixture_compatible_with_pytorch_dataloader SKIPPED [ 62%]
tests/torch/test_fno_generators.py::TestSpatialLifter::test_lifter_preserves_spatial_dims PASSED [ 62%]
tests/torch/test_fno_generators.py::TestSpatialLifter::test_lifter_with_custom_hidden PASSED [ 62%]
tests/torch/test_fno_generators.py::TestPtychoBlock::test_block_preserves_dims PASSED [ 63%]
tests/torch/test_fno_generators.py::TestPtychoBlock::test_block_has_residual PASSED [ 63%]
tests/torch/test_fno_generators.py::TestHybridUNOGenerator::test_output_shape PASSED [ 64%]
tests/torch/test_fno_generators.py::TestHybridUNOGenerator::test_output_contract_real_imag PASSED [ 64%]
tests/torch/test_fno_generators.py::TestCascadedFNOGenerator::test_output_shape PASSED [ 64%]
tests/torch/test_fno_generators.py::TestCascadedFNOGenerator::test_output_is_differentiable PASSED [ 65%]
tests/torch/test_fno_generators.py::TestGeneratorRegistry::test_resolve_fno_generator PASSED [ 65%]
tests/torch/test_fno_generators.py::TestGeneratorRegistry::test_resolve_hybrid_generator PASSED [ 65%]
tests/torch/test_fno_generators.py::TestGeneratorRegistry::test_fno_generator_builds_model PASSED [ 66%]
tests/torch/test_fno_generators.py::TestGeneratorRegistry::test_hybrid_generator_builds_model PASSED [ 66%]
tests/torch/test_fno_integration.py::test_synthetic_npz_fixture_contract PASSED [ 66%]
tests/torch/test_fno_integration.py::test_fno_generator_forward_pass PASSED [ 67%]
tests/torch/test_fno_integration.py::test_hybrid_generator_forward_pass PASSED [ 67%]
tests/torch/test_fno_integration.py::test_neuraloperator_spectral_conv_available PASSED [ 67%]
tests/torch/test_fno_integration.py::test_ptychoblock_uses_neuraloperator_when_available SKIPPED [ 68%]
tests/torch/test_fno_integration.py::test_fno_generator_training_loop PASSED [ 68%]
tests/torch/test_fno_lightning_integration.py::test_loss_history_callback_collects_per_epoch PASSED [ 69%]
tests/torch/test_fno_lightning_integration.py::test_loss_history_callback_handles_missing_metrics PASSED [ 69%]
tests/torch/test_fno_lightning_integration.py::test_train_history_collects_epochs PASSED [ 69%]
tests/torch/test_fno_reconstruction_quality.py::test_fno_quality_comparable_to_random_baseline PASSED [ 70%]
tests/torch/test_fno_reconstruction_quality.py::test_hybrid_produces_different_output_than_fno PASSED [ 70%]
tests/torch/test_fno_reconstruction_quality.py::test_generator_output_improves_with_training PASSED [ 70%]
tests/torch/test_fno_reconstruction_quality.py::test_generators_handle_different_input_sizes PASSED [ 71%]
tests/torch/test_generator_registry.py::test_resolve_generator_cnn PASSED [ 71%]
tests/torch/test_generator_registry.py::test_resolve_generator_unknown_raises PASSED [ 71%]
tests/torch/test_grid_lines_torch_runner.py::TestTorchRunnerConfig::test_config_creation PASSED [ 72%]
tests/torch/test_grid_lines_torch_runner.py::TestLoadCachedDataset::test_load_valid_npz PASSED [ 72%]
tests/torch/test_grid_lines_torch_runner.py::TestLoadCachedDataset::test_load_missing_key_raises PASSED [ 73%]
tests/torch/test_grid_lines_torch_runner.py::TestSetupTorchConfigs::test_creates_training_config PASSED [ 73%]
tests/torch/test_grid_lines_torch_runner.py::TestSetupTorchConfigs::test_creates_execution_config PASSED [ 73%]
tests/torch/test_grid_lines_torch_runner.py::TestRunGridLinesTorchScaffold::test_runner_emits_metrics_json PASSED [ 74%]
tests/torch/test_grid_lines_torch_runner.py::TestRunGridLinesTorchScaffold::test_runner_creates_run_directory_structure PASSED [ 74%]
tests/torch/test_grid_lines_torch_runner.py::TestChannelGridsizeAlignment::test_runner_sets_gridsize_from_config PASSED [ 74%]
tests/torch/test_grid_lines_torch_runner.py::TestChannelGridsizeAlignment::test_runner_channels_derived_from_gridsize PASSED [ 75%]
tests/torch/test_grid_lines_torch_runner.py::TestForwardSignatureEnforcement::test_fno_inference_uses_single_input PASSED [ 75%]
tests/torch/test_grid_lines_torch_runner.py::TestOutputContractConversion::test_to_complex_patches_basic PASSED [ 75%]
tests/torch/test_grid_lines_torch_runner.py::TestOutputContractConversion::test_to_complex_patches_preserves_values PASSED [ 76%]
tests/torch/test_grid_lines_torch_runner.py::TestOutputContractConversion::test_runner_returns_predictions_complex PASSED [ 76%]
tests/torch/test_grid_lines_torch_runner.py::TestTorchTrainingPath::test_runner_uses_generator_registry PASSED [ 76%]
tests/torch/test_grid_lines_torch_runner.py::TestTorchTrainingPath::test_runner_returns_scaffold_when_generator_unavailable PASSED [ 77%]
tests/torch/test_inference_reassembly_parity.py::test_inference_helper_uses_reassembly PASSED [ 77%]
tests/torch/test_inference_reassembly_parity.py::test_reassembly_canvas_padding_invariants PASSED [ 78%]
tests/torch/test_integration_workflow_torch.py::test_bundle_loader_returns_modules FAILED [ 78%]
tests/torch/test_integration_workflow_torch.py::test_run_pytorch_train_save_load_infer FAILED [ 78%]
tests/torch/test_integration_workflow_torch.py::TestPyTorchIntegrationWorkflow::test_pytorch_train_save_load_infer_cycle_legacy SKIPPED [ 79%]
tests/torch/test_integration_workflow_torch.py::TestPyTorchIntegrationWorkflow::test_pytorch_tf_output_parity SKIPPED [ 79%]
tests/torch/test_lightning_checkpoint.py::TestLightningCheckpointSerialization::test_checkpoint_contains_hyperparameters PASSED [ 79%]
tests/torch/test_lightning_checkpoint.py::TestLightningCheckpointSerialization::test_load_from_checkpoint_without_kwargs PASSED [ 80%]
tests/torch/test_lightning_checkpoint.py::TestLightningCheckpointSerialization::test_checkpoint_configs_are_serializable PASSED [ 80%]
tests/torch/test_loss_modes.py::test_poisson_loss_mode_logs_poisson_metrics FAILED [ 80%]
tests/torch/test_loss_modes.py::test_mae_loss_mode_logs_mae_metrics_only FAILED [ 81%]
tests/torch/test_model_manager.py::TestSaveTorchBundle::test_archive_structure PASSED [ 81%]
tests/torch/test_model_manager.py::TestSaveTorchBundle::test_params_snapshot PASSED [ 82%]
tests/torch/test_model_manager.py::TestSaveTorchBundle::test_save_bundle_with_intensity_scale PASSED [ 82%]
tests/torch/test_model_manager.py::TestLoadTorchBundle::test_load_round_trip_updates_params_cfg PASSED [ 82%]
tests/torch/test_model_manager.py::TestLoadTorchBundle::test_missing_params_raises_value_error PASSED [ 83%]
tests/torch/test_model_manager.py::TestLoadTorchBundle::test_load_round_trip_returns_model_stub PASSED [ 83%]
tests/torch/test_model_manager.py::TestLoadTorchBundle::test_reconstructs_models_from_bundle PASSED [ 83%]
tests/torch/test_patch_stats_cli.py::TestPatchStatsCLI::test_patch_stats_flags_accepted FAILED [ 84%]
tests/torch/test_patch_stats_cli.py::TestPatchStatsCLI::test_patch_stats_default_disabled FAILED [ 84%]
tests/torch/test_patch_stats_cli.py::TestPatchStatsCLI::test_factory_creates_inference_config_with_patch_stats FAILED [ 84%]
tests/torch/test_patch_stats_cli.py::TestPatchStatsCLI::test_factory_inference_config_defaults FAILED [ 85%]
tests/torch/test_tf_helper.py::TestTorchTFHelper::test_combine_complex SKIPPED [ 85%]
tests/torch/test_tf_helper.py::TestTorchTFHelper::test_get_mask SKIPPED  [ 85%]
tests/torch/test_tf_helper.py::TestTorchTFHelper::test_placeholder_torch_functions SKIPPED [ 86%]
tests/torch/test_train_probe_size.py::TestNPZProbeSizeExtraction::test_infer_probe_size_128 FAILED [ 86%]
tests/torch/test_train_probe_size.py::TestNPZProbeSizeExtraction::test_infer_probe_size_from_npz FAILED [ 87%]
tests/torch/test_train_probe_size.py::TestNPZProbeSizeExtraction::test_infer_probe_size_missing_probe PASSED [ 87%]
tests/torch/test_train_probe_size.py::TestNPZProbeSizeExtraction::test_infer_probe_size_real_dataset FAILED [ 87%]
tests/torch/test_train_probe_size.py::TestNPZProbeSizeExtraction::test_infer_probe_size_rectangular FAILED [ 88%]
tests/torch/test_workflows_components.py::TestWorkflowsComponentsScaffold::test_run_cdi_example_calls_update_legacy_dict FAILED [ 88%]
tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_train_cdi_model_torch_invokes_lightning FAILED [ 88%]
tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_lightning_dataloader_tensor_dict_structure FAILED [ 89%]
tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_lightning_poisson_count_contract FAILED [ 89%]
tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_lightning_training_respects_gridsize FAILED [ 89%]
tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_coords_relative_layout FAILED [ 90%]
tests/torch/test_workflows_components.py::TestWorkflowsComponentsRun::test_run_cdi_example_invokes_training FAILED [ 90%]
tests/torch/test_workflows_components.py::TestWorkflowsComponentsRun::test_run_cdi_example_persists_models FAILED [ 91%]
tests/torch/test_workflows_components.py::TestWorkflowsComponentsRun::test_load_inference_bundle_handles_bundle PASSED [ 91%]
tests/torch/test_workflows_components.py::TestTrainWithLightningRed::test_train_with_lightning_instantiates_module PASSED [ 91%]
tests/torch/test_workflows_components.py::TestTrainWithLightningRed::test_train_with_lightning_runs_trainer_fit FAILED [ 92%]
tests/torch/test_workflows_components.py::TestTrainWithLightningRed::test_train_with_lightning_returns_models_dict FAILED [ 92%]
tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_reassemble_cdi_image_torch_guard_without_train_results PASSED [ 92%]
tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_reassemble_cdi_image_torch_flip_transpose_contract[False-False-False] FAILED [ 93%]
tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_reassemble_cdi_image_torch_flip_transpose_contract[True-False-False] FAILED [ 93%]
tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_reassemble_cdi_image_torch_flip_transpose_contract[False-True-False] FAILED [ 93%]
tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_reassemble_cdi_image_torch_flip_transpose_contract[False-False-True] FAILED [ 94%]
tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_reassemble_cdi_image_torch_flip_transpose_contract[True-True-True] FAILED [ 94%]
tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_run_cdi_example_torch_do_stitching_delegates_to_reassemble FAILED [ 94%]
tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_reassemble_cdi_image_torch_return_contract FAILED [ 95%]
tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchFloat32::test_batches_remain_float32 PASSED [ 95%]
tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchFloat32::test_dataloader_casts_float64_to_float32 PASSED [ 96%]
tests/torch/test_workflows_components.py::TestDecoderLastShapeParity::test_probe_big_shape_alignment FAILED [ 96%]
tests/torch/test_workflows_components.py::TestDecoderLastShapeParity::test_probe_big_false_no_mismatch PASSED [ 96%]
tests/torch/test_workflows_components.py::TestTrainWithLightningGreen::test_execution_config_overrides_trainer FAILED [ 97%]
tests/torch/test_workflows_components.py::TestTrainWithLightningGreen::test_execution_config_controls_determinism FAILED [ 97%]
tests/torch/test_workflows_components.py::TestInferenceExecutionConfig::test_inference_uses_execution_batch_size PASSED [ 97%]
tests/torch/test_workflows_components.py::TestLightningCheckpointCallbacks::test_model_checkpoint_callback_configured PASSED [ 98%]
tests/torch/test_workflows_components.py::TestLightningCheckpointCallbacks::test_early_stopping_callback_configured PASSED [ 98%]
tests/torch/test_workflows_components.py::TestLightningCheckpointCallbacks::test_disable_checkpointing_skips_callbacks PASSED [ 98%]
tests/torch/test_workflows_components.py::TestLightningExecutionConfig::test_trainer_receives_accumulation FAILED [ 99%]
tests/torch/test_workflows_components.py::TestLightningExecutionConfig::test_monitor_uses_val_loss_name PASSED [ 99%]
tests/torch/test_workflows_components.py::TestLightningExecutionConfig::test_trainer_receives_logger PASSED [100%]

=================================== FAILURES ===================================
_______________ TestInferenceCLI.test_accelerator_flag_roundtrip _______________

self = <test_cli_inference_torch.TestInferenceCLI object at 0x7d54e730f3d0>
minimal_inference_args = ['--model_path', '/tmp/pytest-of-ollie/pytest-184/test_accelerator_flag_roundtri0/model', '--test_data', '/tmp/pytest-...i0/test.npz', '--output_dir', '/tmp/pytest-of-ollie/pytest-184/test_accelerator_flag_roundtri0/inference_outputs', ...]
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7d54e7901b10>

    def test_accelerator_flag_roundtrip(self, minimal_inference_args, monkeypatch):
        """
        Test that accelerator flag is properly handled by _run_inference_and_reconstruct (DEVICE-MISMATCH-001).
    
        Expected behavior:
        - CLI parses --accelerator flag and builds execution_config
        - execution_config accelerator maps to device string ('cuda', 'mps', 'cpu')
        - _run_inference_and_reconstruct receives device and moves model to it
        - Model.to(device) and model.eval() are called inside helper
    
        Phase: R (DEVICE-MISMATCH-001 fix)
        Reference: input.md Do Now step 3, DEVICE-MISMATCH-001 finding
        """
        import numpy as np
        from unittest.mock import MagicMock, patch
    
        # Mock RawData with minimal required fields
        mock_raw_data = MagicMock()
        mock_raw_data.diff3d = np.random.rand(10, 64, 64).astype(np.float32)
        mock_raw_data.probeGuess = np.random.rand(64, 64).astype(np.complex64)
        mock_raw_data.xcoords = np.random.rand(10)
        mock_raw_data.ycoords = np.random.rand(10)
    
        # Mock model with .to() and .eval() tracking
        mock_model = MagicMock()
        device_calls = []
        eval_calls = []
    
        def track_to_call(device):
            device_calls.append(device)
            return mock_model
    
        def track_eval_call():
            eval_calls.append(True)
            return mock_model
    
        mock_model.to = MagicMock(side_effect=track_to_call)
        mock_model.eval = MagicMock(side_effect=track_eval_call)
        mock_model.forward_predict = MagicMock(
            return_value=MagicMock(
                cpu=MagicMock(
                    return_value=MagicMock(
                        numpy=MagicMock(return_value=np.random.rand(1, 1, 64, 64).astype(np.complex64))
                    )
                )
            )
        )
    
        # Import and call _run_inference_and_reconstruct directly
        from ptycho_torch.inference import _run_inference_and_reconstruct
        from ptycho.config.config import InferenceConfig, ModelConfig, PyTorchExecutionConfig
    
        config = InferenceConfig(
            model=ModelConfig(N=64, gridsize=1),
            model_path=Path('outputs/test/bundle.zip'),
            test_data_file=Path('test.npz'),
            backend='pytorch',
            output_dir=Path('outputs/inference'),
            n_groups=10
        )
    
        execution_config = PyTorchExecutionConfig(
            accelerator='cuda',  # Request CUDA device
            num_workers=0,
            inference_batch_size=None
        )
    
        # Call helper with 'cuda' device
>       _run_inference_and_reconstruct(
            mock_model, mock_raw_data, config, execution_config, 'cuda', quiet=True
        )

tests/torch/test_cli_inference_torch.py:270: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

model = <MagicMock id='137799749155152'>
raw_data = <MagicMock id='137803607899664'>
config = InferenceConfig(model=ModelConfig(N=64, gridsize=1, n_filters_scale=2, model_type='pinn', architecture='cnn', amp_acti...oversampling=False, neighbor_pool_size=None, debug=False, output_dir=PosixPath('outputs/inference'), backend='pytorch')
execution_config = PyTorchExecutionConfig(accelerator='cuda', strategy='auto', deterministic=True, gradient_clip_val=None, accum_steps=1,...nt_mode='min', early_stop_patience=100, logger_backend='csv', inference_batch_size=None, middle_trim=0, pad_eval=False)
device = 'cuda', quiet = True

    def _run_inference_and_reconstruct(model, raw_data, config, execution_config, device, quiet=False):
        """
        Extract inference logic into testable helper function (Phase D.C C3).
    
        Args:
            model: Loaded Lightning module (should be in eval mode)
            raw_data: RawData instance with test data
            config: TFInferenceConfig with n_groups, etc.
            execution_config: PyTorchExecutionConfig with device, batch size, etc.
            device: Torch device string ('cpu', 'cuda', 'mps')
            quiet: Suppress progress output (default: False)
    
        Returns:
            Tuple of (amplitude, phase) numpy arrays
    
        Notes:
            - Wraps existing simplified inference logic (lines 563-641)
            - Enforces DTYPE-001 (float32 for diffraction, complex64 for probe)
            - Handles shape permutations (H,W,N → N,H,W)
            - Averages across batch for single reconstruction
            - DEVICE-MISMATCH-001: Ensures model is on the correct device
        """
        import torch
        import numpy as np
    
        # DEVICE-MISMATCH-001 fix: Ensure model is on the requested device and in eval mode
        model.to(device)
        model.eval()
    
        # DTYPE ENFORCEMENT (Phase D1d): Cast to float32 per DATA-001
        diffraction = torch.from_numpy(raw_data.diff3d).to(device, dtype=torch.float32)
        probe = torch.from_numpy(raw_data.probeGuess).to(device, dtype=torch.complex64)
    
        # Handle different diffraction shapes (H, W, n) vs (n, H, W)
        # Auto-detect legacy (H, W, n) format where the last dim (n) is the largest
        if diffraction.ndim == 3 and diffraction.shape[-1] > max(diffraction.shape[0], diffraction.shape[1]):
            # Transpose from (H, W, n) to (n, H, W)
            diffraction = diffraction.permute(2, 0, 1)
    
        # Limit to n_groups
        diffraction = diffraction[:config.n_groups]
    
        # Add channel dimension if needed: (n, H, W) -> (n, 1, H, W)
        if diffraction.ndim == 3:
            diffraction = diffraction.unsqueeze(1)
    
        # Ensure probe is complex64
        if not torch.is_complex(probe):
            probe = probe.to(torch.complex64)
    
        # Add batch dimension to probe if needed
        if probe.ndim == 2:
            probe = probe.unsqueeze(0).unsqueeze(0).unsqueeze(0)  # (1, 1, 1, H, W)
    
        # Prepare positions (API requires it), real offsets computed for reassembly below
        batch_size = diffraction.shape[0]
        N = diffraction.shape[-1]
        positions = torch.zeros((batch_size, 1, 1, 2), device=device)
    
        # Prepare scaling factors (match training normalization)
        from ptycho_torch import helper as hh
        from ptycho_torch.config_params import DataConfig as PTDataConfig
    
        data_cfg_norm = PTDataConfig(N=int(N), grid_size=(1, 1))
        rms_scale = hh.get_rms_scaling_factor(diffraction.squeeze(1), data_cfg_norm)
        physics_scale = hh.get_physics_scaling_factor(diffraction.squeeze(1), data_cfg_norm)
        if not isinstance(rms_scale, torch.Tensor):
            rms_scale = torch.from_numpy(rms_scale)
        if not isinstance(physics_scale, torch.Tensor):
            physics_scale = torch.from_numpy(physics_scale)
        rms_scale = rms_scale.to(device=device, dtype=torch.float32)
        physics_scale = physics_scale.to(device=device, dtype=torch.float32)
        if rms_scale.ndim == 1:
            rms_scale = rms_scale.view(-1, 1, 1, 1)
        if physics_scale.ndim == 1:
            physics_scale = physics_scale.view(-1, 1, 1, 1)
    
        physics_weight = 1.0 if getattr(model, 'torch_loss_mode', 'poisson') == 'poisson' else 0.0
        input_scale_factor = rms_scale
        output_scale_factor = (1.0 - physics_weight) * rms_scale + physics_weight * physics_scale
    
        if not quiet:
            print(f"Running inference on {batch_size} images...")
    
        # Forward pass through model to get per-patch complex predictions
        with torch.no_grad():
            patch_complex = model.forward_predict(
                diffraction,
                positions,
                probe,
                input_scale_factor
            )
    
        # Compute pixel offsets relative to center-of-mass (B, 1, 1, 2)
        x = torch.from_numpy(raw_data.xcoords[:batch_size]).to(device=device, dtype=torch.float32)
        y = torch.from_numpy(raw_data.ycoords[:batch_size]).to(device=device, dtype=torch.float32)
        dx = x - torch.mean(x)
        dy = y - torch.mean(y)
        offsets = torch.stack([dx, dy], dim=-1).view(batch_size, 1, 1, 2)
    
        # Position-aware reassembly using torch helper to produce stitched canvas
        from ptycho_torch.config_params import DataConfig, ModelConfig
        from ptycho_torch import helper as hh
    
        # Minimal configs required for padding and translation
        N = patch_complex.shape[-1]
        data_cfg = DataConfig(N=int(N), grid_size=(1, 1))
        model_cfg = ModelConfig()
        # Ensure channel consistency for reassembly (C_forward must match predicted channels)
        model_cfg.C_forward = int(patch_complex.shape[1])
    
        # Compute dynamic canvas size to avoid clipping: M >= N + 2*max(|dx|, |dy|)
        max_shift = torch.max(torch.stack([dx.abs(), dy.abs()], dim=0)).item()
>       M = int(np.ceil(N + 2 * max_shift))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: only 0-dimensional arrays can be converted to Python scalars

ptycho_torch/inference.py:407: TypeError
___________________ TestPatchStatsCLI.test_patch_stats_dump ____________________

self = <test_cli_train_torch.TestPatchStatsCLI object at 0x7d54e715a950>
minimal_train_args = ['--train_data_file', '/tmp/pytest-of-ollie/pytest-184/test_patch_stats_dump0/train.npz', '--output_dir', '/tmp/pytest-of-ollie/pytest-184/test_patch_stats_dump0/outputs', '--n_images', '8', ...]
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7d53cb8d2dd0>
tmp_path = PosixPath('/tmp/pytest-of-ollie/pytest-184/test_patch_stats_dump0')

    def test_patch_stats_dump(self, minimal_train_args, monkeypatch, tmp_path):
        """
        Test that --log-patch-stats produces JSON and PNG artifacts.
    
        This is the selector required by input.md Phase A Do Now:
        pytest tests/torch/test_cli_train_torch.py::TestPatchStatsCLI::test_patch_stats_dump
    
        Expected behavior:
        - CLI accepts --log-patch-stats and --patch-stats-limit flags
        - After training, <output_dir>/analysis/ contains:
          - torch_patch_stats.json
          - torch_patch_grid.png
        """
        from ptycho_torch.train import cli_main
    
        output_dir = tmp_path / 'outputs'
        test_args = minimal_train_args + [
            '--log-patch-stats',
            '--patch-stats-limit', '2',
            '--accelerator', 'cpu',
            '--quiet',
        ]
    
        monkeypatch.setattr('sys.argv', ['train.py'] + test_args)
    
        # Run training
        try:
            exit_code = cli_main()
        except SystemExit as e:
            exit_code = e.code
    
        # Assert training completed
>       assert exit_code == 0 or exit_code is None, \
            f"Training CLI failed with exit code {exit_code}"
E       AssertionError: Training CLI failed with exit code 2
E       assert (2 == 0 or 2 is None)

tests/torch/test_cli_train_torch.py:860: AssertionError
----------------------------- Captured stderr call -----------------------------
usage: train.py [-h] [--train_data_file TRAIN_DATA_FILE]
                [--test_data_file TEST_DATA_FILE] [--output_dir OUTPUT_DIR]
                [--max_epochs MAX_EPOCHS] [--n_images N_IMAGES]
                [--gridsize GRIDSIZE] [--batch_size BATCH_SIZE]
                [--device {cpu,cuda}] [--disable_mlflow] [--quiet]
                [--torch-loss-mode {poisson,mae}]
                [--accelerator {auto,cpu,gpu,cuda,tpu,mps}] [--deterministic]
                [--no-deterministic] [--num-workers NUM_WORKERS]
                [--learning-rate LEARNING_RATE]
                [--logger {none,csv,tensorboard,mlflow}]
                [--enable-checkpointing] [--disable-checkpointing]
                [--checkpoint-save-top-k CHECKPOINT_SAVE_TOP_K]
                [--checkpoint-monitor CHECKPOINT_MONITOR_METRIC]
                [--checkpoint-mode {min,max}]
                [--early-stop-patience EARLY_STOP_PATIENCE]
                [--scheduler {Default,Exponential,MultiStage,Adaptive}]
                [--accumulate-grad-batches ACCUMULATE_GRAD_BATCHES]
                [--ptycho_dir PTYCHO_DIR] [--config CONFIG]
train.py: error: unrecognized arguments: --log-patch-stats --patch-stats-limit 2
________ TestTrainingPayloadStructure.test_gridsize_sets_channel_count _________

self = <test_config_factory.TestTrainingPayloadStructure object at 0x7d5401193310>
mock_train_npz = PosixPath('/tmp/pytest-of-ollie/pytest-184/test_gridsize_sets_channel_cou0/mock_train.npz')
temp_output_dir = PosixPath('/tmp/factory_test_dmlnbnkt')

    def test_gridsize_sets_channel_count(self, mock_train_npz, temp_output_dir):
        """
        Gridsize override synchronizes C_forward and C_model with data channel count.
    
        Regression test for ADR-003 C4.D3: create_training_payload() must set
        pt_model_config.C_forward and C_model to match pt_data_config.C when
        gridsize is specified. This ensures PyTorch helpers (reassemble_patches_position_real)
        receive tensor shapes consistent with the grouping strategy.
    
        Expected behavior:
            - gridsize=1 → C=1, C_forward=1, C_model=1
            - gridsize=2 → C=4, C_forward=4, C_model=4
            - Default (no gridsize override) → C=4, C_forward=4, C_model=4
    
        Reference: plans/active/ADR-003-BACKEND-API/reports/2025-10-20T061500Z/
                   phase_c4_cli_integration_debug/coords_relative_investigation.md
        """
        # Case 1: gridsize=1 (single-position groups)
        payload_gs1 = create_training_payload(
            train_data_file=mock_train_npz,
            output_dir=temp_output_dir,
            overrides={'gridsize': 1, 'n_groups': 512},
        )
>       assert payload_gs1.pt_data_config.C == 1, "DataConfig.C should match gridsize**2 (1)"
E       AssertionError: DataConfig.C should match gridsize**2 (1)
E       assert 4 == 1
E        +  where 4 = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, subsample_seed=None, grid_size=(2, 2), neighbor_function='Nearest', min_neighbor_distance=0.0, max_neighbor_distance=3.0, scan_pattern='Isotropic', normalize='Batch', probe_scale=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9)).C
E        +    where DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, subsample_seed=None, grid_size=(2, 2), neighbor_function='Nearest', min_neighbor_distance=0.0, max_neighbor_distance=3.0, scan_pattern='Isotropic', normalize='Batch', probe_scale=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9)) = TrainingPayload(tf_training_config=TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', architecture='cnn', amp_activation='swish', object_big=False, probe_big=True, probe_mask=False, pad_object=True, probe_scale=1.0, gaussian_smoothing_sigma=0.0), train_data_file=PosixPath('/tmp/pytest-of-ollie/pytest-184/test_gridsize_sets_channel_cou0/mock_train.npz'), test_data_file=None, batch_size=16, nepochs=50, mae_weight=0.0, nll_weight=1.0, realspace_mae_weight=0.0, realspace_weight=0.0, nphotons=100000.0, n_groups=512, n_images=None, n_subsample=None, subsample_seed=None, neighbor_count=6, enable_oversampling=False, neighbor_pool_size=None, positions_provided=True, probe_trainable=False, intensity_scale_trainable=False, output_dir=PosixPath('/tmp/factory_test_dmlnbnkt'), sequential_sampling=False, backend='pytorch', torch_loss_mode='poisson'), pt_data_config=DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, subsample_seed=None, grid_size=(2, 2), neighbor_function='Nearest', min_neighbor_distance=0.0, max_neighbor_distance=3.0, scan_pattern='Isotropic', normalize='Batch', probe_scale=1.0, probe_normalize=True, data_scaling='P...'Default', num_workers=4, accum_steps=1, gradient_clip_val=None, stage_1_epochs=0, stage_2_epochs=0, stage_3_epochs=0, physics_weight_schedule='cosine', stage_3_lr_factor=0.1, torch_loss_mode='poisson', experiment_name='Synthetic_Runs', notes='', model_name='PtychoPINNv2', output_dir='/tmp/factory_test_dmlnbnkt', train_data_file='/tmp/pytest-of-ollie/pytest-184/test_gridsize_sets_channel_cou0/mock_train.npz', test_data_file=None, n_groups=512), execution_config=PyTorchExecutionConfig(accelerator='cuda', strategy='auto', deterministic=True, gradient_clip_val=None, accum_steps=1, num_workers=0, pin_memory=False, persistent_workers=False, prefetch_factor=None, learning_rate=0.001, scheduler='Default', enable_progress_bar=False, enable_checkpointing=True, checkpoint_save_top_k=1, checkpoint_monitor_metric='val_loss', checkpoint_mode='min', early_stop_patience=100, logger_backend='csv', inference_batch_size=None, middle_trim=0, pad_eval=False), overrides_applied={'gridsize': 1, 'n_groups': 512, 'N': 64, 'accelerator': 'cuda', 'deterministic': True, 'num_workers': 0, 'enable_progress_bar': False, 'learning_rate': 0.001, 'scheduler': 'Default', 'accum_steps': 1, 'logger_backend': 'csv'}).pt_data_config

tests/torch/test_config_factory.py:222: AssertionError
________ TestConfigBridgeTranslation.test_epochs_to_nepochs_conversion _________

self = <test_config_factory.TestConfigBridgeTranslation object at 0x7d5401199810>
mock_train_npz = PosixPath('/tmp/pytest-of-ollie/pytest-184/test_epochs_to_nepochs_convers0/mock_train.npz')
temp_output_dir = PosixPath('/tmp/factory_test_ob88f268')

    def test_epochs_to_nepochs_conversion(self, mock_train_npz, temp_output_dir):
        """Factory maps epochs → nepochs via bridge."""
        payload = create_training_payload(
            train_data_file=mock_train_npz,
            output_dir=temp_output_dir,
            overrides={'n_groups': 512, 'max_epochs': 20},
        )
        # GREEN phase assertions:
>       assert payload.pt_training_config.epochs == 20  # PyTorch naming
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AssertionError: assert 50 == 20
E        +  where 50 = TrainingConfig(training_directories=[], nll=True, device='cuda', strategy='ddp', n_devices=1, framework='Lightning', orchestrator='Mlflow', learning_rate=0.001, epochs=50, batch_size=16, epochs_fine_tune=0, fine_tune_gamma=0.1, scheduler='Default', num_workers=4, accum_steps=1, gradient_clip_val=None, stage_1_epochs=0, stage_2_epochs=0, stage_3_epochs=0, physics_weight_schedule='cosine', stage_3_lr_factor=0.1, torch_loss_mode='poisson', experiment_name='Synthetic_Runs', notes='', model_name='PtychoPINNv2', output_dir='/tmp/factory_test_ob88f268', train_data_file='/tmp/pytest-of-ollie/pytest-184/test_epochs_to_nepochs_convers0/mock_train.npz', test_data_file=None, n_groups=512).epochs
E        +    where TrainingConfig(training_directories=[], nll=True, device='cuda', strategy='ddp', n_devices=1, framework='Lightning', orchestrator='Mlflow', learning_rate=0.001, epochs=50, batch_size=16, epochs_fine_tune=0, fine_tune_gamma=0.1, scheduler='Default', num_workers=4, accum_steps=1, gradient_clip_val=None, stage_1_epochs=0, stage_2_epochs=0, stage_3_epochs=0, physics_weight_schedule='cosine', stage_3_lr_factor=0.1, torch_loss_mode='poisson', experiment_name='Synthetic_Runs', notes='', model_name='PtychoPINNv2', output_dir='/tmp/factory_test_ob88f268', train_data_file='/tmp/pytest-of-ollie/pytest-184/test_epochs_to_nepochs_convers0/mock_train.npz', test_data_file=None, n_groups=512) = TrainingPayload(tf_training_config=TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', architecture='cnn', amp_activation='swish', object_big=False, probe_big=True, probe_mask=False, pad_object=True, probe_scale=1.0, gaussian_smoothing_sigma=0.0), train_data_file=PosixPath('/tmp/pytest-of-ollie/pytest-184/test_epochs_to_nepochs_convers0/mock_train.npz'), test_data_file=None, batch_size=16, nepochs=50, mae_weight=0.0, nll_weight=1.0, realspace_mae_weight=0.0, realspace_weight=0.0, nphotons=100000.0, n_groups=512, n_images=None, n_subsample=None, subsample_seed=None, neighbor_count=6, enable_oversampling=False, neighbor_pool_size=None, positions_provided=True, probe_trainable=False, intensity_scale_trainable=False, output_dir=PosixPath('/tmp/factory_test_ob88f268'), sequential_sampling=False, backend='pytorch', torch_loss_mode='poisson'), pt_data_config=DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, subsample_seed=None, grid_size=(2, 2), neighbor_function='Nearest', min_neighbor_distance=0.0, max_neighbor_distance=3.0, scan_pattern='Isotropic', normalize='Batch', probe_scale=1.0, probe_normalize=True, data_scaling='P...fault', num_workers=4, accum_steps=1, gradient_clip_val=None, stage_1_epochs=0, stage_2_epochs=0, stage_3_epochs=0, physics_weight_schedule='cosine', stage_3_lr_factor=0.1, torch_loss_mode='poisson', experiment_name='Synthetic_Runs', notes='', model_name='PtychoPINNv2', output_dir='/tmp/factory_test_ob88f268', train_data_file='/tmp/pytest-of-ollie/pytest-184/test_epochs_to_nepochs_convers0/mock_train.npz', test_data_file=None, n_groups=512), execution_config=PyTorchExecutionConfig(accelerator='cuda', strategy='auto', deterministic=True, gradient_clip_val=None, accum_steps=1, num_workers=0, pin_memory=False, persistent_workers=False, prefetch_factor=None, learning_rate=0.001, scheduler='Default', enable_progress_bar=False, enable_checkpointing=True, checkpoint_save_top_k=1, checkpoint_monitor_metric='val_loss', checkpoint_mode='min', early_stop_patience=100, logger_backend='csv', inference_batch_size=None, middle_trim=0, pad_eval=False), overrides_applied={'n_groups': 512, 'max_epochs': 20, 'N': 64, 'accelerator': 'cuda', 'deterministic': True, 'num_workers': 0, 'enable_progress_bar': False, 'learning_rate': 0.001, 'scheduler': 'Default', 'accum_steps': 1, 'logger_backend': 'csv'}).pt_training_config

tests/torch/test_config_factory.py:324: AssertionError
_______ TestConfigBridgeTranslation.test_k_to_neighbor_count_conversion ________

self = <test_config_factory.TestConfigBridgeTranslation object at 0x7d5401199f50>
mock_train_npz = PosixPath('/tmp/pytest-of-ollie/pytest-184/test_k_to_neighbor_count_conve0/mock_train.npz')
temp_output_dir = PosixPath('/tmp/factory_test_x9ypx0ow')

    def test_k_to_neighbor_count_conversion(self, mock_train_npz, temp_output_dir):
        """Factory maps K → neighbor_count via bridge."""
        payload = create_training_payload(
            train_data_file=mock_train_npz,
            output_dir=temp_output_dir,
            overrides={'n_groups': 512, 'neighbor_count': 7},
        )
        # GREEN phase assertions:
>       assert payload.pt_data_config.K == 7  # PyTorch K
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AssertionError: assert 6 == 7
E        +  where 6 = DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, subsample_seed=None, grid_size=(2, 2), neighbor_function='Nearest', min_neighbor_distance=0.0, max_neighbor_distance=3.0, scan_pattern='Isotropic', normalize='Batch', probe_scale=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9)).K
E        +    where DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, subsample_seed=None, grid_size=(2, 2), neighbor_function='Nearest', min_neighbor_distance=0.0, max_neighbor_distance=3.0, scan_pattern='Isotropic', normalize='Batch', probe_scale=1.0, probe_normalize=True, data_scaling='Parseval', phase_subtraction=True, x_bounds=(0.1, 0.9), y_bounds=(0.1, 0.9)) = TrainingPayload(tf_training_config=TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', architecture='cnn', amp_activation='swish', object_big=False, probe_big=True, probe_mask=False, pad_object=True, probe_scale=1.0, gaussian_smoothing_sigma=0.0), train_data_file=PosixPath('/tmp/pytest-of-ollie/pytest-184/test_k_to_neighbor_count_conve0/mock_train.npz'), test_data_file=None, batch_size=16, nepochs=50, mae_weight=0.0, nll_weight=1.0, realspace_mae_weight=0.0, realspace_weight=0.0, nphotons=100000.0, n_groups=512, n_images=None, n_subsample=None, subsample_seed=None, neighbor_count=6, enable_oversampling=False, neighbor_pool_size=None, positions_provided=True, probe_trainable=False, intensity_scale_trainable=False, output_dir=PosixPath('/tmp/factory_test_x9ypx0ow'), sequential_sampling=False, backend='pytorch', torch_loss_mode='poisson'), pt_data_config=DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, subsample_seed=None, grid_size=(2, 2), neighbor_function='Nearest', min_neighbor_distance=0.0, max_neighbor_distance=3.0, scan_pattern='Isotropic', normalize='Batch', probe_scale=1.0, probe_normalize=True, data_scaling='P...lt', num_workers=4, accum_steps=1, gradient_clip_val=None, stage_1_epochs=0, stage_2_epochs=0, stage_3_epochs=0, physics_weight_schedule='cosine', stage_3_lr_factor=0.1, torch_loss_mode='poisson', experiment_name='Synthetic_Runs', notes='', model_name='PtychoPINNv2', output_dir='/tmp/factory_test_x9ypx0ow', train_data_file='/tmp/pytest-of-ollie/pytest-184/test_k_to_neighbor_count_conve0/mock_train.npz', test_data_file=None, n_groups=512), execution_config=PyTorchExecutionConfig(accelerator='cuda', strategy='auto', deterministic=True, gradient_clip_val=None, accum_steps=1, num_workers=0, pin_memory=False, persistent_workers=False, prefetch_factor=None, learning_rate=0.001, scheduler='Default', enable_progress_bar=False, enable_checkpointing=True, checkpoint_save_top_k=1, checkpoint_monitor_metric='val_loss', checkpoint_mode='min', early_stop_patience=100, logger_backend='csv', inference_batch_size=None, middle_trim=0, pad_eval=False), overrides_applied={'n_groups': 512, 'neighbor_count': 7, 'N': 64, 'accelerator': 'cuda', 'deterministic': True, 'num_workers': 0, 'enable_progress_bar': False, 'learning_rate': 0.001, 'scheduler': 'Default', 'accum_steps': 1, 'logger_backend': 'csv'}).pt_data_config

tests/torch/test_config_factory.py:335: AssertionError
_____ TestDataloaderCanonicalKeySupport.test_backward_compat_legacy_diff3d _____

self = <test_dataloader.TestDataloaderCanonicalKeySupport testMethod=test_backward_compat_legacy_diff3d>

    def test_backward_compat_legacy_diff3d(self):
        """
        Test dataloader maintains backward compatibility with legacy 'diff3d' key.
    
        Should pass in both red and green phases (legacy fallback required).
        """
        # Arrange
        from ptycho_torch.dataloader import npz_headers
    
        legacy_npz = self._create_minimal_npz("legacy_dataset.npz", use_canonical_key=False)
    
        # Act & Assert
        try:
>           shape, xcoords, ycoords = npz_headers(legacy_npz)
                                      ^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_dataloader.py:139: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

npz = PosixPath('/tmp/tmp6w61z6lr/legacy_dataset.npz')

    def npz_headers(npz):
        """
        Takes a path to an .npz file, which is a Zip archive of .npy files.
        We can use this to determine shape of the scan tensor in the npz file without loading it
        This will be useful in the __len__ method for the dataset
    
        Prefers canonical 'diffraction' key per DATA-001 spec (specs/data_contracts.md),
        with graceful fallback to legacy 'diff3d' key for backward compatibility.
    
        Taken from: https://stackoverflow.com/questions/68224572/how-to-determine-the-shape-size-of-npz-file
        Modified to quickly grab dimension we care about
        """
        with zipfile.ZipFile(npz) as archive:
            npy_header_found = False
            diffraction_shape = None
            xcoords = None
            ycoords = None
    
            # First pass: try canonical 'diffraction' key (DATA-001 spec)
            for name in archive.namelist():
                if name.startswith('diffraction') and name.endswith('.npy'):
                    npy = archive.open(name)
                    version = np.lib.format.read_magic(npy)
                    shape, _, _ = np.lib.format._read_array_header(npy, version)
                    diffraction_shape = shape
                    npy_header_found = True
                    break
    
            # Fallback: try legacy 'diff3d' key for backward compatibility
            if not npy_header_found:
                for name in archive.namelist():
                    if name.startswith('diff3d') and name.endswith('.npy'):
                        npy = archive.open(name)
                        version = np.lib.format.read_magic(npy)
>                       shape, _, _ = np.lib.format._read_array_header(npy, version)
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                       AttributeError: module 'numpy.lib.format' has no attribute '_read_array_header'

ptycho_torch/dataloader.py:63: AttributeError
______ TestDataloaderCanonicalKeySupport.test_loads_canonical_diffraction ______

self = <test_dataloader.TestDataloaderCanonicalKeySupport testMethod=test_loads_canonical_diffraction>

    def test_loads_canonical_diffraction(self):
        """
        Test dataloader can load NPZ files with canonical 'diffraction' key.
    
        Red Phase: Expected to fail because npz_headers() only searches for 'diff3d'.
        Green Phase: Should pass after implementing canonical key preference.
        """
        # Arrange
        from ptycho_torch.dataloader import npz_headers
    
        canonical_npz = self._create_minimal_npz("canonical_dataset.npz", use_canonical_key=True)
    
        # Act & Assert
        try:
>           shape, xcoords, ycoords = npz_headers(canonical_npz)
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_dataloader.py:108: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

npz = PosixPath('/tmp/tmp89j5__7n/canonical_dataset.npz')

    def npz_headers(npz):
        """
        Takes a path to an .npz file, which is a Zip archive of .npy files.
        We can use this to determine shape of the scan tensor in the npz file without loading it
        This will be useful in the __len__ method for the dataset
    
        Prefers canonical 'diffraction' key per DATA-001 spec (specs/data_contracts.md),
        with graceful fallback to legacy 'diff3d' key for backward compatibility.
    
        Taken from: https://stackoverflow.com/questions/68224572/how-to-determine-the-shape-size-of-npz-file
        Modified to quickly grab dimension we care about
        """
        with zipfile.ZipFile(npz) as archive:
            npy_header_found = False
            diffraction_shape = None
            xcoords = None
            ycoords = None
    
            # First pass: try canonical 'diffraction' key (DATA-001 spec)
            for name in archive.namelist():
                if name.startswith('diffraction') and name.endswith('.npy'):
                    npy = archive.open(name)
                    version = np.lib.format.read_magic(npy)
>                   shape, _, _ = np.lib.format._read_array_header(npy, version)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                   AttributeError: module 'numpy.lib.format' has no attribute '_read_array_header'

ptycho_torch/dataloader.py:52: AttributeError
___ TestDataloaderFormatAutoTranspose.test_npz_headers_also_transposes_shape ___

self = <test_dataloader.TestDataloaderFormatAutoTranspose testMethod=test_npz_headers_also_transposes_shape>

    def test_npz_headers_also_transposes_shape(self):
        """
        Test that npz_headers() returns transposed shape for legacy format.
    
        This is CRITICAL because npz_headers() is used to pre-allocate memory maps.
        If the shape isn't transposed here, the memory map will have wrong dimensions
        even if _get_diffraction_stack() transposes the data.
    
        Regression test for the RuntimeError: tensor size mismatch during assignment.
        """
        # Arrange: Create NPZ with legacy (H, W, N) format
        from ptycho_torch.dataloader import npz_headers
    
        H, W, N = 64, 64, 1087
        legacy_diffraction = np.random.rand(H, W, N).astype(np.float32)
        xcoords = np.random.rand(N).astype(np.float64) * 100
        ycoords = np.random.rand(N).astype(np.float64) * 100
    
        npz_path = self.data_path / "legacy_for_headers.npz"
        np.savez(str(npz_path), diffraction=legacy_diffraction, xcoords=xcoords, ycoords=ycoords)
    
        # Act
>       shape, coords_x, coords_y = npz_headers(npz_path)
                                    ^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_dataloader.py:357: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

npz = PosixPath('/tmp/tmpmx_q49gv/legacy_for_headers.npz')

    def npz_headers(npz):
        """
        Takes a path to an .npz file, which is a Zip archive of .npy files.
        We can use this to determine shape of the scan tensor in the npz file without loading it
        This will be useful in the __len__ method for the dataset
    
        Prefers canonical 'diffraction' key per DATA-001 spec (specs/data_contracts.md),
        with graceful fallback to legacy 'diff3d' key for backward compatibility.
    
        Taken from: https://stackoverflow.com/questions/68224572/how-to-determine-the-shape-size-of-npz-file
        Modified to quickly grab dimension we care about
        """
        with zipfile.ZipFile(npz) as archive:
            npy_header_found = False
            diffraction_shape = None
            xcoords = None
            ycoords = None
    
            # First pass: try canonical 'diffraction' key (DATA-001 spec)
            for name in archive.namelist():
                if name.startswith('diffraction') and name.endswith('.npy'):
                    npy = archive.open(name)
                    version = np.lib.format.read_magic(npy)
>                   shape, _, _ = np.lib.format._read_array_header(npy, version)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                   AttributeError: module 'numpy.lib.format' has no attribute '_read_array_header'

ptycho_torch/dataloader.py:52: AttributeError
______________________ test_bundle_loader_returns_modules ______________________

tmp_path = PosixPath('/tmp/pytest-of-ollie/pytest-184/test_bundle_loader_returns_mod0')
data_file = PosixPath('/home/ollie/Documents/tmp/PtychoPINN/tests/fixtures/pytorch_integration/minimal_dataset_v1.npz')
cuda_gpu_env = {'CLAUDECODE': '1', 'CLAUDE_CODE_ENTRYPOINT': 'cli', 'CONDA_DEFAULT_ENV': 'ptycho311', 'CONDA_EXE': '/home/ollie/miniconda3/bin/conda', ...}

    def test_bundle_loader_returns_modules(tmp_path, data_file, cuda_gpu_env):
        """
        Regression test: bundle loader must return Lightning modules, not dicts.
    
        Phase C4.D.B4: This test validates that load_inference_bundle_torch returns
        nn.Module instances that support .eval(), not sentinel dicts. The bug was
        discovered during integration testing when inference.py raised AttributeError
        on models_dict['diffraction_to_obj'].eval().
    
        Test Strategy (TDD RED phase):
        1. Train a minimal model and save bundle
        2. Load bundle via load_inference_bundle_torch
        3. Assert both models in dict are torch.nn.Module instances
        4. Assert models support .eval() without AttributeError
    
        Expected Failure: Currently returns sentinel dict for autoencoder.
        """
        import torch.nn as nn
        from ptycho_torch.workflows.components import load_inference_bundle_torch
    
        # Setup: Train and save a bundle (reuse training command from integration test)
        training_output_dir = tmp_path / "training_outputs"
        train_command = [
            sys.executable, "-m", "ptycho_torch.train",
            "--train_data_file", str(data_file),
            "--test_data_file", str(data_file),
            "--output_dir", str(training_output_dir),
            "--max_epochs", "1",  # Minimal training for faster test
            "--n_images", "32",
            "--gridsize", "1",
            "--batch_size", "4",
            "--accelerator", "cuda",
            "--disable_mlflow",
        ]
    
        train_result = subprocess.run(
            train_command, capture_output=True, text=True, env=cuda_gpu_env, check=False
        )
    
        if train_result.returncode != 0:
>           pytest.fail(f"Training failed:\n{train_result.stderr}")
E           Failed: Training failed:
E           2026-01-26 21:39:07.776720: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
E           WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E           E0000 00:00:1769492347.789801 4031865 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E           E0000 00:00:1769492347.793901 4031865 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E           W0000 00:00:1769492347.805435 4031865 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E           W0000 00:00:1769492347.805458 4031865 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E           W0000 00:00:1769492347.805459 4031865 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E           W0000 00:00:1769492347.805461 4031865 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E           2026-01-26 21:39:07.808254: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
E           To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.

tests/torch/test_integration_workflow_torch.py:226: Failed
____________________ test_run_pytorch_train_save_load_infer ____________________

tmp_path = PosixPath('/tmp/pytest-of-ollie/pytest-184/test_run_pytorch_train_save_lo0')
data_file = PosixPath('/home/ollie/Documents/tmp/PtychoPINN/tests/fixtures/pytorch_integration/minimal_dataset_v1.npz')
cuda_gpu_env = {'CLAUDECODE': '1', 'CLAUDE_CODE_ENTRYPOINT': 'cli', 'CONDA_DEFAULT_ENV': 'ptycho311', 'CONDA_EXE': '/home/ollie/miniconda3/bin/conda', ...}

    def test_run_pytorch_train_save_load_infer(tmp_path, data_file, cuda_gpu_env):
        """
        Tests the complete PyTorch train → save → load → infer workflow.
    
        This validates the PyTorch model persistence layer by simulating a real
        user workflow across separate processes, mirroring the TensorFlow integration test.
    
        Phase: C2 (GREEN)
        Behavior:
        1. Training subprocess creates Lightning checkpoint at checkpoints/last.ckpt
        2. Inference subprocess loads checkpoint and generates reconstructions
        3. Output images created in inference output directory
        4. Assertions verify artifact existence and non-empty file sizes
    
        Implementation: _run_pytorch_workflow executes train/infer via subprocess
        """
        # Execute complete workflow via subprocess helper (Phase C2 implementation)
>       result = _run_pytorch_workflow(tmp_path, data_file, cuda_gpu_env)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_integration_workflow_torch.py:272: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tmp_path = PosixPath('/tmp/pytest-of-ollie/pytest-184/test_run_pytorch_train_save_lo0')
data_file = PosixPath('/home/ollie/Documents/tmp/PtychoPINN/tests/fixtures/pytorch_integration/minimal_dataset_v1.npz')
cuda_gpu_env = {'CLAUDECODE': '1', 'CLAUDE_CODE_ENTRYPOINT': 'cli', 'CONDA_DEFAULT_ENV': 'ptycho311', 'CONDA_EXE': '/home/ollie/miniconda3/bin/conda', ...}

    def _run_pytorch_workflow(tmp_path, data_file, cuda_gpu_env):
        """
        Execute PyTorch train→save→load→infer workflow via subprocess calls.
    
        Parameters:
            tmp_path: pytest tmp_path fixture for output directories
            data_file: Path to NPZ dataset
            cuda_gpu_env: Environment dict with CUDA_VISIBLE_DEVICES="0"
    
        Returns:
            SimpleNamespace with:
                - training_output_dir: Path to training outputs
                - inference_output_dir: Path to inference outputs
                - checkpoint_path: Path to Lightning checkpoint
                - recon_amp_path: Path to amplitude reconstruction PNG
                - recon_phase_path: Path to phase reconstruction PNG
    
        Raises:
            RuntimeError: If training or inference subprocess fails
    
        Phase: C2 (GREEN)
        Implementation: Ported from legacy unittest harness with subprocess commands
        """
        from types import SimpleNamespace
    
        # Define output paths
        training_output_dir = tmp_path / "training_outputs"
        inference_output_dir = tmp_path / "pytorch_output"
    
        # --- 1. Training Step (PyTorch) ---
        # CLI parameters aligned with Phase B1 scope (fixture n_subset=64, deterministic config)
        # Preserves CONFIG-001 ordering per docs/workflows/pytorch.md §12
        train_command = [
            sys.executable, "-m", "ptycho_torch.train",
            "--train_data_file", str(data_file),
            "--test_data_file", str(data_file),
            "--output_dir", str(training_output_dir),
            "--max_epochs", "2",  # Aligned with Phase B1 runtime budget (<45s)
            "--n_images", "64",   # Matches fixture subset size
            "--gridsize", "1",
            "--batch_size", "4",
            "--accelerator", "cuda",  # Deterministic single-GPU execution per cuda_gpu_env fixture
            "--disable_mlflow",
        ]
    
        train_result = subprocess.run(
            train_command,
            capture_output=True,
            text=True,
            env=cuda_gpu_env,
            check=False
        )
    
        if train_result.returncode != 0:
>           raise RuntimeError(
                f"PyTorch training failed with return code {train_result.returncode}\n"
                f"STDOUT:\n{train_result.stdout}\n"
                f"STDERR:\n{train_result.stderr}"
            )
E           RuntimeError: PyTorch training failed with return code 1
E           STDOUT:
E           Using new CLI interface with factory-based config (ADR-003)
E           ERROR: Training data file not found: /home/ollie/Documents/tmp/PtychoPINN/tests/fixtures/pytorch_integration/minimal_dataset_v1.npz
E           
E           STDERR:
E           2026-01-26 21:39:15.401719: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
E           WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E           E0000 00:00:1769492355.414734 4032139 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E           E0000 00:00:1769492355.418723 4032139 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E           W0000 00:00:1769492355.430275 4032139 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E           W0000 00:00:1769492355.430301 4032139 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E           W0000 00:00:1769492355.430302 4032139 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E           W0000 00:00:1769492355.430304 4032139 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
E           2026-01-26 21:39:15.433143: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
E           To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.

tests/torch/test_integration_workflow_torch.py:130: RuntimeError
_________________ test_poisson_loss_mode_logs_poisson_metrics __________________

    @pytest.mark.torch
    def test_poisson_loss_mode_logs_poisson_metrics():
        module = _build_stub_module('poisson')
        batch = _make_stub_batch()
>       module.compute_loss(batch)

tests/torch/test_loss_modes.py:61: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = _LoggingCaptureModule(
  (model): PtychoPINN(
    (autoencoder): Autoencoder(
      (encoder): Encoder(
        (block...  (probe_illumination): ProbeIllumination()
      (pad_and_diffract): LambdaLayer()
    )
  )
  (Loss): PoissonLoss()
)
batch = ({'coords_relative': tensor([[[[0.]]],


        [[[0.]]]]), 'images': tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],
     ...  ..., 1.+0.j, 1.+0.j, 1.+0.j],
          [1.+0.j, 1.+0.j, 1.+0.j,  ..., 1.+0.j, 1.+0.j, 1.+0.j]]]]), tensor([1., 1.]))

    def compute_loss(self, batch):
        """
        Enhanced loss computation supporting multi-stage training
        """
        # Grab required data fields from TensorDict
        x = batch[0]['images']
        positions = batch[0]['coords_relative']
        probe = batch[1]
        rms_scale = batch[0]['rms_scaling_constant']  # RMS scaling
        physics_scale = batch[0]['physics_scaling_constant']
>       experiment_ids = batch[0]['experiment_id']
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'experiment_id'

ptycho_torch/model.py:1140: KeyError
----------------------------- Captured stdout call -----------------------------
Decoder Block 1: No attention module added.
Decoder Block 2: No attention module added.
Decoder Block 1: No attention module added.
Decoder Block 2: No attention module added.
___________________ test_mae_loss_mode_logs_mae_metrics_only ___________________

    @pytest.mark.torch
    def test_mae_loss_mode_logs_mae_metrics_only():
        module = _build_stub_module('mae')
        batch = _make_stub_batch()
>       module.compute_loss(batch)

tests/torch/test_loss_modes.py:72: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = _LoggingCaptureModule(
  (model): PtychoPINN(
    (autoencoder): Autoencoder(
      (encoder): Encoder(
        (block...on): ProbeIllumination()
      (pad_and_diffract): LambdaLayer()
    )
  )
  (Loss): MAELoss(
    (mae): L1Loss()
  )
)
batch = ({'coords_relative': tensor([[[[0.]]],


        [[[0.]]]]), 'images': tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],
     ...  ..., 1.+0.j, 1.+0.j, 1.+0.j],
          [1.+0.j, 1.+0.j, 1.+0.j,  ..., 1.+0.j, 1.+0.j, 1.+0.j]]]]), tensor([1., 1.]))

    def compute_loss(self, batch):
        """
        Enhanced loss computation supporting multi-stage training
        """
        # Grab required data fields from TensorDict
        x = batch[0]['images']
        positions = batch[0]['coords_relative']
        probe = batch[1]
        rms_scale = batch[0]['rms_scaling_constant']  # RMS scaling
        physics_scale = batch[0]['physics_scaling_constant']
>       experiment_ids = batch[0]['experiment_id']
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'experiment_id'

ptycho_torch/model.py:1140: KeyError
----------------------------- Captured stdout call -----------------------------
Decoder Block 1: No attention module added.
Decoder Block 2: No attention module added.
Decoder Block 1: No attention module added.
Decoder Block 2: No attention module added.
______________ TestPatchStatsCLI.test_patch_stats_flags_accepted _______________

self = <test_patch_stats_cli.TestPatchStatsCLI object at 0x7d53fea037d0>
minimal_train_args = ['--train_data_file', '/tmp/pytest-of-ollie/pytest-184/test_patch_stats_flags_accepte0/train.npz', '--output_dir', '/tmp/pytest-of-ollie/pytest-184/test_patch_stats_flags_accepte0/outputs', '--n_images', '64', ...]
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7d54e72a1b50>
tmp_path = PosixPath('/tmp/pytest-of-ollie/pytest-184/test_patch_stats_flags_accepte0')

    def test_patch_stats_flags_accepted(self, minimal_train_args, monkeypatch, tmp_path):
        """
        Test that --log-patch-stats and --patch-stats-limit are accepted and forwarded.
    
        Expected GREEN behavior (Phase A):
        - CLI accepts both flags without argparse errors
        - Overrides dict contains log_patch_stats=True and patch_stats_limit=2
        - Factory receives these values and workflow can access them
        """
        # Mock the factory to intercept arguments
        mock_factory = MagicMock()
    
        # Create a minimal mock payload to avoid downstream errors
        mock_payload = MagicMock()
        mock_payload.pt_data_config = MagicMock(N=64, grid_size=(2,2))
        mock_payload.pt_training_config = MagicMock(epochs=2)
        mock_payload.pt_model_config = MagicMock()
        mock_payload.tf_training_config = MagicMock()
        mock_factory.return_value = mock_payload
    
        # Mock main() to avoid full training execution
        mock_main = MagicMock()
    
        with patch('ptycho_torch.config_factory.create_training_payload', mock_factory), \
             patch('ptycho_torch.train.main', mock_main):
    
            # CLI invocation with patch stats flags
            test_args = minimal_train_args + [
                '--log-patch-stats',
                '--patch-stats-limit', '2',
            ]
    
            from ptycho_torch.train import cli_main
            monkeypatch.setattr('sys.argv', ['train.py'] + test_args)
    
            try:
                cli_main()
            except SystemExit as e:
                # CLI may exit with 0 or non-zero; we only care that flags were parsed
                pass
    
        # Assert factory was called
>       assert mock_factory.called, "Factory was not called (CLI may have failed before factory invocation)"
E       AssertionError: Factory was not called (CLI may have failed before factory invocation)
E       assert False
E        +  where False = <MagicMock id='137803609022160'>.called

tests/torch/test_patch_stats_cli.py:86: AssertionError
----------------------------- Captured stderr call -----------------------------
usage: train.py [-h] [--train_data_file TRAIN_DATA_FILE]
                [--test_data_file TEST_DATA_FILE] [--output_dir OUTPUT_DIR]
                [--max_epochs MAX_EPOCHS] [--n_images N_IMAGES]
                [--gridsize GRIDSIZE] [--batch_size BATCH_SIZE]
                [--device {cpu,cuda}] [--disable_mlflow] [--quiet]
                [--torch-loss-mode {poisson,mae}]
                [--accelerator {auto,cpu,gpu,cuda,tpu,mps}] [--deterministic]
                [--no-deterministic] [--num-workers NUM_WORKERS]
                [--learning-rate LEARNING_RATE]
                [--logger {none,csv,tensorboard,mlflow}]
                [--enable-checkpointing] [--disable-checkpointing]
                [--checkpoint-save-top-k CHECKPOINT_SAVE_TOP_K]
                [--checkpoint-monitor CHECKPOINT_MONITOR_METRIC]
                [--checkpoint-mode {min,max}]
                [--early-stop-patience EARLY_STOP_PATIENCE]
                [--scheduler {Default,Exponential,MultiStage,Adaptive}]
                [--accumulate-grad-batches ACCUMULATE_GRAD_BATCHES]
                [--ptycho_dir PTYCHO_DIR] [--config CONFIG]
train.py: error: unrecognized arguments: --log-patch-stats --patch-stats-limit 2
_____________ TestPatchStatsCLI.test_patch_stats_default_disabled ______________

self = <test_patch_stats_cli.TestPatchStatsCLI object at 0x7d53fea03ed0>
minimal_train_args = ['--train_data_file', '/tmp/pytest-of-ollie/pytest-184/test_patch_stats_default_disab0/train.npz', '--output_dir', '/tmp/pytest-of-ollie/pytest-184/test_patch_stats_default_disab0/outputs', '--n_images', '64', ...]
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7d53feab2d90>
tmp_path = PosixPath('/tmp/pytest-of-ollie/pytest-184/test_patch_stats_default_disab0')

    def test_patch_stats_default_disabled(self, minimal_train_args, monkeypatch, tmp_path):
        """
        Test that patch stats logging is disabled by default.
    
        Expected behavior:
        - Without --log-patch-stats, log_patch_stats should be False
        - patch_stats_limit should be None when not specified
        """
        mock_factory = MagicMock()
        mock_payload = MagicMock()
        mock_payload.pt_data_config = MagicMock(N=64, grid_size=(2,2))
        mock_payload.pt_training_config = MagicMock(epochs=2)
        mock_payload.pt_model_config = MagicMock()
        mock_payload.tf_training_config = MagicMock()
        mock_factory.return_value = mock_payload
        mock_main = MagicMock()
    
        with patch('ptycho_torch.config_factory.create_training_payload', mock_factory), \
             patch('ptycho_torch.train.main', mock_main):
    
            # No patch stats flags
            test_args = minimal_train_args
    
            from ptycho_torch.train import cli_main
            monkeypatch.setattr('sys.argv', ['train.py'] + test_args)
    
            try:
                cli_main()
            except SystemExit:
                pass
    
        assert mock_factory.called
        call_kwargs = mock_factory.call_args.kwargs
        overrides = call_kwargs.get('overrides', {})
    
        # Defaults should be False and None
>       assert overrides.get('log_patch_stats') is False, \
            "Expected log_patch_stats=False by default"
E       AssertionError: Expected log_patch_stats=False by default
E       assert None is False
E        +  where None = <built-in method get of dict object at 0x7d53c814b080>('log_patch_stats')
E        +    where <built-in method get of dict object at 0x7d53c814b080> = {'batch_size': 4, 'gridsize': 2, 'max_epochs': 2, 'n_groups': 64, ...}.get

tests/torch/test_patch_stats_cli.py:137: AssertionError
----------------------------- Captured stdout call -----------------------------
Using new CLI interface with factory-based config (ADR-003)
Creating configuration via factory (CONFIG-001 compliance)...
✓ Factory created configs: N=64, gridsize=(2, 2), epochs=2
✓ Execution config: accelerator=cpu, deterministic=True, learning_rate=0.001
Starting training with 2 epochs...
Training failed: No data left in file
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/home/ollie/Documents/tmp/PtychoPINN/ptycho_torch/train.py", line 861, in cli_main
    train_data = RawData.from_file(str(train_data_file))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ollie/Documents/tmp/PtychoPINN/ptycho/raw_data.py", line 303, in from_file
    data_dict, metadata = MetadataManager.load_with_metadata(train_data_file_path)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ollie/Documents/tmp/PtychoPINN/ptycho/metadata.py", line 133, in load_with_metadata
    with np.load(file_path, allow_pickle=True) as data:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/numpy/lib/_npyio_impl.py", line 463, in load
    raise EOFError("No data left in file")
EOFError: No data left in file
___ TestPatchStatsCLI.test_factory_creates_inference_config_with_patch_stats ___

self = <test_patch_stats_cli.TestPatchStatsCLI object at 0x7d53fea10690>
tmp_path = PosixPath('/tmp/pytest-of-ollie/pytest-184/test_factory_creates_inference0')

    def test_factory_creates_inference_config_with_patch_stats(self, tmp_path):
        """
        Test that factory creates PTInferenceConfig with patch stats fields.
    
        This test validates the fix for the blocker documented in:
        plans/active/.../red/blocked_20251114T014035Z_factory_inference_config.md
    
        Expected behavior:
        - create_training_payload creates pt_inference_config
        - pt_inference_config has log_patch_stats and patch_stats_limit from overrides
        - Payload includes pt_inference_config field
        """
        from ptycho_torch.config_factory import create_training_payload
        from pathlib import Path
        import numpy as np
    
        # Create minimal NPZ fixture
        train_file = tmp_path / "train.npz"
        np.savez(
            train_file,
            diffraction=np.random.rand(10, 64, 64).astype(np.float32),
            xcoords=np.random.rand(10).astype(np.float32),
            ycoords=np.random.rand(10).astype(np.float32),
            probeGuess=np.random.rand(64, 64).astype(np.complex64),
            objectGuess=np.random.rand(200, 200).astype(np.complex64),
        )
    
        # Create payload with patch stats overrides
        payload = create_training_payload(
            train_data_file=train_file,
            output_dir=tmp_path / "outputs",
            overrides={
                'n_groups': 8,
                'log_patch_stats': True,
                'patch_stats_limit': 2,
            }
        )
    
        # Assert payload has pt_inference_config
>       assert hasattr(payload, 'pt_inference_config'), \
            "TrainingPayload missing pt_inference_config field"
E       AssertionError: TrainingPayload missing pt_inference_config field
E       assert False
E        +  where False = hasattr(TrainingPayload(tf_training_config=TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', architecture='cnn', amp_activation='swish', object_big=False, probe_big=True, probe_mask=False, pad_object=True, probe_scale=1.0, gaussian_smoothing_sigma=0.0), train_data_file=PosixPath('/tmp/pytest-of-ollie/pytest-184/test_factory_creates_inference0/train.npz'), test_data_file=None, batch_size=16, nepochs=50, mae_weight=0.0, nll_weight=1.0, realspace_mae_weight=0.0, realspace_weight=0.0, nphotons=100000.0, n_groups=8, n_images=None, n_subsample=None, subsample_seed=None, neighbor_count=6, enable_oversampling=False, neighbor_pool_size=None, positions_provided=True, probe_trainable=False, intensity_scale_trainable=False, output_dir=PosixPath('/tmp/pytest-of-ollie/pytest-184/test_factory_creates_inference0/outputs'), sequential_sampling=False, backend='pytorch', torch_loss_mode='poisson'), pt_data_config=DataConfig(nphotons=100000.0, N=64, C=4, K=6, K_quadrant=30, n_subsample=7, subsample_seed=None, grid_size=(2, 2), neighbor_function='Nearest', min_neighbor_distance=0.0, max_neighbor_distance=3.0, scan_pattern='Isotropic', normalize='Batch', probe_scale=1.0,..._1_epochs=0, stage_2_epochs=0, stage_3_epochs=0, physics_weight_schedule='cosine', stage_3_lr_factor=0.1, torch_loss_mode='poisson', experiment_name='Synthetic_Runs', notes='', model_name='PtychoPINNv2', output_dir='/tmp/pytest-of-ollie/pytest-184/test_factory_creates_inference0/outputs', train_data_file='/tmp/pytest-of-ollie/pytest-184/test_factory_creates_inference0/train.npz', test_data_file=None, n_groups=8), execution_config=PyTorchExecutionConfig(accelerator='cuda', strategy='auto', deterministic=True, gradient_clip_val=None, accum_steps=1, num_workers=0, pin_memory=False, persistent_workers=False, prefetch_factor=None, learning_rate=0.001, scheduler='Default', enable_progress_bar=False, enable_checkpointing=True, checkpoint_save_top_k=1, checkpoint_monitor_metric='val_loss', checkpoint_mode='min', early_stop_patience=100, logger_backend='csv', inference_batch_size=None, middle_trim=0, pad_eval=False), overrides_applied={'n_groups': 8, 'log_patch_stats': True, 'patch_stats_limit': 2, 'N': 64, 'accelerator': 'cuda', 'deterministic': True, 'num_workers': 0, 'enable_progress_bar': False, 'learning_rate': 0.001, 'scheduler': 'Default', 'accum_steps': 1, 'logger_backend': 'csv'}), 'pt_inference_config')

tests/torch/test_patch_stats_cli.py:181: AssertionError
___________ TestPatchStatsCLI.test_factory_inference_config_defaults ___________

self = <test_patch_stats_cli.TestPatchStatsCLI object at 0x7d53fea10e50>
tmp_path = PosixPath('/tmp/pytest-of-ollie/pytest-184/test_factory_inference_config_0')

    def test_factory_inference_config_defaults(self, tmp_path):
        """
        Test that factory creates PTInferenceConfig with defaults when flags not provided.
        """
        from ptycho_torch.config_factory import create_training_payload
        import numpy as np
    
        # Create minimal NPZ fixture
        train_file = tmp_path / "train.npz"
        np.savez(
            train_file,
            diffraction=np.random.rand(10, 64, 64).astype(np.float32),
            xcoords=np.random.rand(10).astype(np.float32),
            ycoords=np.random.rand(10).astype(np.float32),
            probeGuess=np.random.rand(64, 64).astype(np.complex64),
            objectGuess=np.random.rand(200, 200).astype(np.complex64),
        )
    
        # Create payload without patch stats overrides
        payload = create_training_payload(
            train_data_file=train_file,
            output_dir=tmp_path / "outputs",
            overrides={'n_groups': 8}
        )
    
        # Assert pt_inference_config has defaults
>       assert payload.pt_inference_config.log_patch_stats is False, \
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
            "Expected log_patch_stats=False by default"
E       AttributeError: 'TrainingPayload' object has no attribute 'pt_inference_config'

tests/torch/test_patch_stats_cli.py:216: AttributeError
_____________ TestNPZProbeSizeExtraction.test_infer_probe_size_128 _____________

self = <test_train_probe_size.TestNPZProbeSizeExtraction testMethod=test_infer_probe_size_128>

    def test_infer_probe_size_128(self):
        """
        Test utility correctly handles 128x128 probe (different from 64 default).
    
        This ensures the function works for probe sizes other than the common 64x64.
        """
        npz_file = self._create_minimal_training_npz("test_128.npz", probe_size=128)
    
        try:
            from ptycho_torch.train import _infer_probe_size
        except ImportError:
            self.skip("_infer_probe_size() not implemented yet")
    
>       inferred_N = _infer_probe_size(str(npz_file))
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_train_probe_size.py:140: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

npz_file = '/tmp/tmpuom2b3oy/test_128.npz'

    def _infer_probe_size(npz_file):
        """
        Infer probe size (N) from NPZ metadata without loading full arrays.
    
        This function reads the probeGuess array header from an NPZ file using the
        zipfile approach, following the pattern established in ptycho_torch/dataloader.py:npz_headers().
    
        Args:
            npz_file (str or Path): Path to NPZ file containing probeGuess key
    
        Returns:
            int or None: First dimension of probeGuess shape (N), or None if probeGuess key missing
    
        References:
            - specs/data_contracts.md §1 — probeGuess is required key for canonical NPZ format
            - ptycho_torch/dataloader.py:29-83 — npz_headers() implementation pattern
            - INTEGRATE-PYTORCH-001-PROBE-SIZE — Probe size mismatch resolution
    
        Example:
            >>> N = _infer_probe_size("datasets/Run1084_recon3_postPC_shrunk_3.npz")
            >>> print(N)  # 64 (for this dataset)
        """
        import zipfile
        import numpy as np
    
        try:
            with zipfile.ZipFile(npz_file) as archive:
                # Search for probeGuess key in NPZ archive
                for name in archive.namelist():
                    if name.startswith('probeGuess') and name.endswith('.npy'):
                        # Open the .npy file inside the archive
                        npy = archive.open(name)
                        # Read array header without loading data
                        version = np.lib.format.read_magic(npy)
>                       shape, _, _ = np.lib.format._read_array_header(npy, version)
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                       AttributeError: module 'numpy.lib.format' has no attribute '_read_array_header'

ptycho_torch/train.py:130: AttributeError
__________ TestNPZProbeSizeExtraction.test_infer_probe_size_from_npz ___________

self = <test_train_probe_size.TestNPZProbeSizeExtraction testMethod=test_infer_probe_size_from_npz>

    def test_infer_probe_size_from_npz(self):
        """
        Test utility function extracts probe size from NPZ metadata without loading full arrays.
    
        Red Phase: Fails because _infer_probe_size() function doesn't exist in train.py yet.
        Green Phase: Should pass after implementing NPZ metadata reader using zipfile approach.
    
        This test validates the core requirement: derive N from probeGuess.shape[0]
        efficiently (no full array load) before DataConfig instantiation.
        """
        # Create NPZ with 64x64 probe
        npz_file = self._create_minimal_training_npz("test_train.npz", probe_size=64)
    
        # Import the utility function (should fail in RED phase)
        try:
            from ptycho_torch.train import _infer_probe_size
        except ImportError:
            self.fail("_infer_probe_size() function not found in ptycho_torch.train module. "
                     "Implement this utility to extract probe size from NPZ metadata.")
    
        # Call the utility
>       inferred_N = _infer_probe_size(str(npz_file))
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_train_probe_size.py:119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

npz_file = '/tmp/tmpx5vm2ozd/test_train.npz'

    def _infer_probe_size(npz_file):
        """
        Infer probe size (N) from NPZ metadata without loading full arrays.
    
        This function reads the probeGuess array header from an NPZ file using the
        zipfile approach, following the pattern established in ptycho_torch/dataloader.py:npz_headers().
    
        Args:
            npz_file (str or Path): Path to NPZ file containing probeGuess key
    
        Returns:
            int or None: First dimension of probeGuess shape (N), or None if probeGuess key missing
    
        References:
            - specs/data_contracts.md §1 — probeGuess is required key for canonical NPZ format
            - ptycho_torch/dataloader.py:29-83 — npz_headers() implementation pattern
            - INTEGRATE-PYTORCH-001-PROBE-SIZE — Probe size mismatch resolution
    
        Example:
            >>> N = _infer_probe_size("datasets/Run1084_recon3_postPC_shrunk_3.npz")
            >>> print(N)  # 64 (for this dataset)
        """
        import zipfile
        import numpy as np
    
        try:
            with zipfile.ZipFile(npz_file) as archive:
                # Search for probeGuess key in NPZ archive
                for name in archive.namelist():
                    if name.startswith('probeGuess') and name.endswith('.npy'):
                        # Open the .npy file inside the archive
                        npy = archive.open(name)
                        # Read array header without loading data
                        version = np.lib.format.read_magic(npy)
>                       shape, _, _ = np.lib.format._read_array_header(npy, version)
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                       AttributeError: module 'numpy.lib.format' has no attribute '_read_array_header'

ptycho_torch/train.py:130: AttributeError
________ TestNPZProbeSizeExtraction.test_infer_probe_size_real_dataset _________

self = <test_train_probe_size.TestNPZProbeSizeExtraction testMethod=test_infer_probe_size_real_dataset>

    def test_infer_probe_size_real_dataset(self):
        """
        Test utility on actual project dataset (integration smoke test).
    
        This validates the fix works for the canonical test dataset used in
        test_integration_workflow_torch.py where the original mismatch occurred.
        """
        real_dataset = project_root / "datasets" / "Run1084_recon3_postPC_shrunk_3.npz"
    
        if not real_dataset.exists():
            self.skipTest(f"Real dataset not found: {real_dataset}")
    
        try:
            from ptycho_torch.train import _infer_probe_size
        except ImportError:
            self.skip("_infer_probe_size() not implemented yet")
    
>       inferred_N = _infer_probe_size(str(real_dataset))
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_train_probe_size.py:228: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

npz_file = '/home/ollie/Documents/tmp/PtychoPINN/datasets/Run1084_recon3_postPC_shrunk_3.npz'

    def _infer_probe_size(npz_file):
        """
        Infer probe size (N) from NPZ metadata without loading full arrays.
    
        This function reads the probeGuess array header from an NPZ file using the
        zipfile approach, following the pattern established in ptycho_torch/dataloader.py:npz_headers().
    
        Args:
            npz_file (str or Path): Path to NPZ file containing probeGuess key
    
        Returns:
            int or None: First dimension of probeGuess shape (N), or None if probeGuess key missing
    
        References:
            - specs/data_contracts.md §1 — probeGuess is required key for canonical NPZ format
            - ptycho_torch/dataloader.py:29-83 — npz_headers() implementation pattern
            - INTEGRATE-PYTORCH-001-PROBE-SIZE — Probe size mismatch resolution
    
        Example:
            >>> N = _infer_probe_size("datasets/Run1084_recon3_postPC_shrunk_3.npz")
            >>> print(N)  # 64 (for this dataset)
        """
        import zipfile
        import numpy as np
    
        try:
            with zipfile.ZipFile(npz_file) as archive:
                # Search for probeGuess key in NPZ archive
                for name in archive.namelist():
                    if name.startswith('probeGuess') and name.endswith('.npy'):
                        # Open the .npy file inside the archive
                        npy = archive.open(name)
                        # Read array header without loading data
                        version = np.lib.format.read_magic(npy)
>                       shape, _, _ = np.lib.format._read_array_header(npy, version)
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                       AttributeError: module 'numpy.lib.format' has no attribute '_read_array_header'

ptycho_torch/train.py:130: AttributeError
_________ TestNPZProbeSizeExtraction.test_infer_probe_size_rectangular _________

self = <test_train_probe_size.TestNPZProbeSizeExtraction testMethod=test_infer_probe_size_rectangular>

    def test_infer_probe_size_rectangular(self):
        """
        Test utility handles rectangular probes by using first dimension.
    
        Some edge cases may have non-square probes (e.g., 64x32).
        Utility should use probe.shape[0] to determine N.
        """
        n_images = 10
        probe_h, probe_w = 64, 32  # Rectangular
    
        # Create NPZ with rectangular probe
        npz_file = self.data_path / "rect_probe.npz"
        np.savez(
            str(npz_file),
            diffraction=np.random.rand(n_images, probe_h, probe_w).astype(np.float32) * 0.5,
            xcoords=np.random.rand(n_images).astype(np.float64) * 100,
            ycoords=np.random.rand(n_images).astype(np.float64) * 100,
            probeGuess=np.random.rand(probe_h, probe_w).astype(np.complex64),
            objectGuess=np.random.rand(probe_h*2, probe_w*2).astype(np.complex64),
            Y=np.random.rand(n_images, probe_h, probe_w).astype(np.complex64),
            scan_index=np.arange(n_images, dtype=np.int32),
        )
    
        try:
            from ptycho_torch.train import _infer_probe_size
        except ImportError:
            self.skip("_infer_probe_size() not implemented yet")
    
>       inferred_N = _infer_probe_size(str(npz_file))
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_train_probe_size.py:171: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

npz_file = '/tmp/tmpssi1g9m3/rect_probe.npz'

    def _infer_probe_size(npz_file):
        """
        Infer probe size (N) from NPZ metadata without loading full arrays.
    
        This function reads the probeGuess array header from an NPZ file using the
        zipfile approach, following the pattern established in ptycho_torch/dataloader.py:npz_headers().
    
        Args:
            npz_file (str or Path): Path to NPZ file containing probeGuess key
    
        Returns:
            int or None: First dimension of probeGuess shape (N), or None if probeGuess key missing
    
        References:
            - specs/data_contracts.md §1 — probeGuess is required key for canonical NPZ format
            - ptycho_torch/dataloader.py:29-83 — npz_headers() implementation pattern
            - INTEGRATE-PYTORCH-001-PROBE-SIZE — Probe size mismatch resolution
    
        Example:
            >>> N = _infer_probe_size("datasets/Run1084_recon3_postPC_shrunk_3.npz")
            >>> print(N)  # 64 (for this dataset)
        """
        import zipfile
        import numpy as np
    
        try:
            with zipfile.ZipFile(npz_file) as archive:
                # Search for probeGuess key in NPZ archive
                for name in archive.namelist():
                    if name.startswith('probeGuess') and name.endswith('.npy'):
                        # Open the .npy file inside the archive
                        npy = archive.open(name)
                        # Read array header without loading data
                        version = np.lib.format.read_magic(npy)
>                       shape, _, _ = np.lib.format._read_array_header(npy, version)
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                       AttributeError: module 'numpy.lib.format' has no attribute '_read_array_header'

ptycho_torch/train.py:130: AttributeError
_ TestWorkflowsComponentsScaffold.test_run_cdi_example_calls_update_legacy_dict _

self = <test_workflows_components.TestWorkflowsComponentsScaffold object at 0x7d53fea45490>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7d53cb8b62d0>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'sigmoid', 'architecture': 'cnn', 'backend': 'tensorflow', ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', architecture='cnn', amp_activ..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')

    def test_run_cdi_example_calls_update_legacy_dict(
        self,
        monkeypatch,
        params_cfg_snapshot,
        minimal_training_config
    ):
        """
        CRITICAL PARITY TEST: run_cdi_example_torch must call update_legacy_dict.
    
        Requirement: specs/ptychodus_api_spec.md:187 mandates that PyTorch entry
        points must synchronize params.cfg via update_legacy_dict() to prevent
        silent CONFIG-001 violations (shape mismatch errors from empty params.cfg).
    
        Red-phase contract:
        - Entry signature: run_cdi_example_torch(train_data, test_data, config, ...)
        - MUST call ptycho.config.config.update_legacy_dict(params.cfg, config)
        - Stub implementation may raise NotImplementedError for paths Phase D2.B/C fill
    
        Test mechanism:
        - Use monkeypatch to spy on update_legacy_dict calls
        - Pass minimal dummy data (no actual training execution required)
        - Assert update_legacy_dict was invoked with correct params.cfg + config args
        """
        # Import the module under test
        # This import must succeed even when torch unavailable (torch-optional)
        from ptycho_torch.workflows import components as torch_components
        from ptycho.config.config import update_legacy_dict
        from ptycho.raw_data import RawData
    
        # Spy flag to track update_legacy_dict invocation
        update_legacy_dict_called = {"called": False, "args": None}
    
        def mock_update_legacy_dict(cfg_dict, config_obj):
            """Spy that records invocation and delegates to real function."""
            update_legacy_dict_called["called"] = True
            update_legacy_dict_called["args"] = (cfg_dict, config_obj)
            # Call the real function to populate params.cfg for validation
            update_legacy_dict(cfg_dict, config_obj)
    
        # Patch update_legacy_dict with spy
        monkeypatch.setattr(
            "ptycho.config.config.update_legacy_dict",
            mock_update_legacy_dict
        )
    
        # Create minimal dummy train_data (RawData-compatible stub)
        # For scaffold test, we don't need valid NPZ data — just RawData structure
        dummy_coords = np.array([0.0, 1.0, 2.0])
        dummy_diff = np.random.rand(3, 64, 64).astype(np.float32)
        dummy_probe = np.ones((64, 64), dtype=np.complex64)
        dummy_scan_index = np.array([0, 1, 2], dtype=int)
    
        train_data = RawData(
            xcoords=dummy_coords,
            ycoords=dummy_coords,
            xcoords_start=dummy_coords,
            ycoords_start=dummy_coords,
            diff3d=dummy_diff,
            probeGuess=dummy_probe,
            scan_index=dummy_scan_index,
        )
    
        # Attempt to call run_cdi_example_torch
        # Phase D2.A: expects NotImplementedError (scaffold only) — COMPLETED
        # Phase D2.B/C: IMPLEMENTED — now returns results tuple
        # Test validates update_legacy_dict was called, but doesn't fully exercise training
        # (training path tested separately in TestWorkflowsComponentsTraining)
    
        # Monkeypatch train_cdi_model_torch to prevent full training execution in this test
        def mock_train_cdi_model_torch(train_data, test_data, config):
            """Minimal stub to prevent full training in scaffold test."""
            return {"history": {"train_loss": [0.5]}, "train_container": None, "test_container": None}
    
        monkeypatch.setattr(
            "ptycho_torch.workflows.components.train_cdi_model_torch",
            mock_train_cdi_model_torch
        )
    
        # Call should now succeed (Phase D2.C implemented)
>       recon_amp, recon_phase, results = torch_components.run_cdi_example_torch(
            train_data=train_data,
            test_data=None,  # Optional
            config=minimal_training_config,
            flip_x=False,
            flip_y=False,
            transpose=False,
            M=20,
            do_stitching=False,
        )

tests/torch/test_workflows_components.py:156: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

train_data = <ptycho.raw_data.RawData object at 0x7d53cb8b6990>
test_data = None
config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', architecture='cnn', amp_activ..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
flip_x = False, flip_y = False, transpose = False, M = 20, do_stitching = False
execution_config = None

    def run_cdi_example_torch(
        train_data: Union[RawData, 'RawDataTorch', 'PtychoDataContainerTorch'],
        test_data: Optional[Union[RawData, 'RawDataTorch', 'PtychoDataContainerTorch']],
        config: TrainingConfig,
        flip_x: bool = False,
        flip_y: bool = False,
        transpose: bool = False,
        M: int = 20,
        do_stitching: bool = False,
        execution_config: Optional[Any] = None
    ) -> Tuple[Optional[Any], Optional[Any], Dict[str, Any]]:
        """
        Run the main CDI example execution flow using PyTorch backend.
    
        This function provides API parity with ptycho.workflows.components.run_cdi_example,
        enabling transparent backend selection from Ptychodus per specs/ptychodus_api_spec.md §4.5.
    
        CRITICAL: This function MUST call update_legacy_dict(params.cfg, config) before
        delegating to core modules to prevent CONFIG-001 violations (empty params.cfg
        causing silent shape mismatches downstream).
    
        Args:
            train_data: Training data (RawData, RawDataTorch, or PtychoDataContainerTorch)
            test_data: Optional test data (same type constraints as train_data)
            config: TrainingConfig instance (TensorFlow dataclass, translated via config_bridge)
            flip_x: Whether to flip the x coordinates during reconstruction
            flip_y: Whether to flip the y coordinates during reconstruction
            transpose: Whether to transpose the image by swapping dimensions
            M: Parameter for reassemble_position function (default: 20)
            do_stitching: Whether to perform image stitching after training
            execution_config: Optional PyTorchExecutionConfig for runtime control (accelerator,
                             num_workers, learning_rate, scheduler, logger, checkpointing).
                             See CONFIG-002, CONFIG-LOGGER-001.
    
        Returns:
            Tuple containing:
            - reconstructed amplitude (or None if stitching disabled)
            - reconstructed phase (or None if stitching disabled)
            - results dictionary (training history, containers, metrics)
    
        Raises:
            NotImplementedError: Phase D2.B/C not yet implemented (scaffold only)
    
        Phase D2.A Scaffold Status:
            - Entry signature: ✅ COMPLETE (matches TensorFlow)
            - update_legacy_dict call: ✅ COMPLETE (CONFIG-001 compliance)
            - Placeholder logic: ✅ COMPLETE (raises NotImplementedError)
            - Torch-optional: ✅ COMPLETE (importable without torch)
    
        Phase D2.B/C TODO:
            - Implement train_cdi_model_torch delegation (Lightning trainer orchestration)
            - Implement reassemble_cdi_image_torch (optional stitching path)
            - Add MLflow disable flag handling
            - Validate deterministic seeds from config
    
        Example (Post D2.B/C):
            >>> from ptycho_torch.workflows.components import run_cdi_example_torch
            >>> from ptycho.config.config import TrainingConfig, ModelConfig
            >>> from ptycho.raw_data import RawData
            >>>
            >>> # Load data
            >>> train_data = RawData.from_file("train.npz")
            >>> config = TrainingConfig(model=ModelConfig(N=64), ...)
            >>>
            >>> # Execute PyTorch pipeline
            >>> amp, phase, results = run_cdi_example_torch(
            ...     train_data, None, config, do_stitching=False
            ... )
        """
        # CRITICAL: Update params.cfg before delegating (CONFIG-001 compliance)
        # This ensures legacy modules invoked downstream observe correct configuration state
        ptycho_config.update_legacy_dict(params.cfg, config)
        logger.info("PyTorch workflow: params.cfg synchronized with TrainingConfig")
    
        # Step 1: Train the model (Phase D2.B — delegates to Lightning trainer stub)
        logger.info("Invoking PyTorch training orchestration via train_cdi_model_torch")
        # Note: train_cdi_model_torch will need to be updated to accept execution_config
        # For now, we pass it as a keyword argument for forward compatibility
>       train_results = train_cdi_model_torch(train_data, test_data, config, execution_config=execution_config)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: TestWorkflowsComponentsScaffold.test_run_cdi_example_calls_update_legacy_dict.<locals>.mock_train_cdi_model_torch() got an unexpected keyword argument 'execution_config'

ptycho_torch/workflows/components.py:163: TypeError
----------------------------- Captured stdout call -----------------------------
diff3d shape: (3, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (3,)
xcoords shape: (3,)
ycoords shape: (3,)
xcoords_start shape: (3,)
ycoords_start shape: (3,)
_ TestWorkflowsComponentsTraining.test_train_cdi_model_torch_invokes_lightning _

self = <test_workflows_components.TestWorkflowsComponentsTraining object at 0x7d53fea45f10>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7d53feaa0390>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'sigmoid', 'architecture': 'cnn', 'backend': 'tensorflow', ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=1, n_filters_scale=2, model_type='pinn', architecture='cnn', amp_activ..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x7d564bde8450>

    def test_train_cdi_model_torch_invokes_lightning(
        self,
        monkeypatch,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data
    ):
        """
        CRITICAL TRAINING PATH TEST: train_cdi_model_torch must delegate to Lightning.
    
        Requirement: Phase D2.B must implement training orchestration following the
        pattern in plans/active/INTEGRATE-PYTORCH-001/reports/2025-10-17T093500Z/
        phase_d2_training_analysis.md.
    
        Red-phase contract:
        - Entry signature: train_cdi_model_torch(train_data, test_data, config)
        - MUST call _ensure_container(data, config) for train/test inputs
        - MUST delegate to Lightning trainer with normalized config
        - MUST return dict with keys: history, train_container, test_container
        - Stub implementation may raise NotImplementedError initially
    
        Test mechanism:
        - Use monkeypatch to spy on _ensure_container and Lightning orchestration calls
        - Pass minimal RawData (no actual training execution required)
        - Assert expected orchestration order without running full training
        """
        # Import the module under test
        from ptycho_torch.workflows import components as torch_components
    
        # Spy flags to track internal calls
        ensure_container_calls = []
        lightning_trainer_called = {"called": False, "config": None}
    
        def mock_ensure_container(data, config):
            """Spy that records _ensure_container invocations."""
            ensure_container_calls.append({
                "data": data,
                "config": config
            })
            # Return a sentinel PtychoDataContainerTorch-like object
            # In Phase D2.B implementation, this would be a real container
            return {"X": np.ones((2, 64, 64)), "Y": np.ones((2, 64, 64), dtype=np.complex64)}
    
        def mock_lightning_orchestrator(train_container, test_container, config):
            """Spy that records Lightning trainer invocation."""
            lightning_trainer_called["called"] = True
            lightning_trainer_called["config"] = config
            # Return minimal training results dict
            return {
                "history": {"train_loss": [0.5, 0.3], "val_loss": [0.6, 0.4]},
                "train_container": train_container,
                "test_container": test_container,
            }
    
        # Patch internal helpers (Phase D2.B implemented)
        monkeypatch.setattr(
            "ptycho_torch.workflows.components._ensure_container",
            mock_ensure_container
        )
        monkeypatch.setattr(
            "ptycho_torch.workflows.components._train_with_lightning",
            mock_lightning_orchestrator
        )
    
        # Call train_cdi_model_torch (Phase D2.B green phase)
>       results = torch_components.train_cdi_model_torch(
            train_data=dummy_raw_data,
            test_data=None,  # Optional
            config=minimal_training_config
        )

tests/torch/test_workflows_components.py:318: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

train_data = <ptycho.raw_data.RawData object at 0x7d564bde8450>
test_data = None
config = TrainingConfig(model=ModelConfig(N=64, gridsize=1, n_filters_scale=2, model_type='pinn', architecture='cnn', amp_activ..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
execution_config = None

    def train_cdi_model_torch(
        train_data: Union[RawData, 'RawDataTorch', 'PtychoDataContainerTorch', 'PtychoDataset'],
        test_data: Optional[Union[RawData, 'RawDataTorch', 'PtychoDataContainerTorch']],
        config: TrainingConfig,
        execution_config: Optional[Any] = None
    ) -> Dict[str, Any]:
        """
        Train the CDI model using PyTorch Lightning backend.
    
        This function provides API parity with ptycho.workflows.components.train_cdi_model,
        orchestrating data preparation, probe initialization, and Lightning trainer execution.
    
        Args:
            train_data: Training data (RawData, RawDataTorch, or PtychoDataContainerTorch)
            test_data: Optional test data for validation
            config: TrainingConfig instance (TensorFlow dataclass)
            execution_config: Optional PyTorchExecutionConfig for runtime control
    
        Returns:
            Dict[str, Any]: Results dictionary containing:
            - 'history': Training history (losses, metrics)
            - 'train_container': PtychoDataContainerTorch for training data
            - 'test_container': Optional PtychoDataContainerTorch for test data
            - Additional outputs from Lightning trainer
    
        Raises:
            ImportError: If Phase C adapters not available
            TypeError: If input data types are invalid
    
        Phase D2.B Status:
            - Entry signature: ✅ COMPLETE (matches TensorFlow)
            - _ensure_container helper: ✅ COMPLETE (normalizes inputs via Phase C adapters)
            - Lightning orchestration: 🔶 STUB (returns minimal dict, full impl pending)
            - Torch-optional: ✅ COMPLETE (importable without torch)
    
        Example:
            >>> config = TrainingConfig(model=ModelConfig(N=64), nepochs=10, ...)
            >>> results = train_cdi_model_torch(train_data, test_data, config)
            >>> print(results['history']['train_loss'][-1])
        """
        # Step 1: Normalize train_data to PtychoDataContainerTorch
        logger.info("Normalizing training data via _ensure_container")
        train_container = _ensure_container(train_data, config)
    
        # Step 2: Normalize test_data if provided
        test_container = None
        if test_data is not None:
            logger.info("Normalizing test data via _ensure_container")
            test_container = _ensure_container(test_data, config)
    
        # Step 3: Initialize probe (TODO: implement probe handling for PyTorch)
        # TensorFlow baseline: probe.set_probe_guess(None, train_container.probe)
        # For Phase D2.B stub, skip probe initialization
        logger.debug("Probe initialization deferred to full Lightning implementation")
    
        # Step 4: Delegate to Lightning trainer
        logger.info("Delegating to Lightning trainer via _train_with_lightning")
>       results = _train_with_lightning(train_container, test_container, config, execution_config=execution_config)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: TestWorkflowsComponentsTraining.test_train_cdi_model_torch_invokes_lightning.<locals>.mock_lightning_orchestrator() got an unexpected keyword argument 'execution_config'

ptycho_torch/workflows/components.py:1309: TypeError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (10, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (10,)
xcoords shape: (10,)
ycoords shape: (10,)
xcoords_start shape: (10,)
ycoords_start shape: (10,)
_ TestWorkflowsComponentsTraining.test_lightning_dataloader_tensor_dict_structure _

self = <test_workflows_components.TestWorkflowsComponentsTraining object at 0x7d53fea46650>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'sigmoid', 'architecture': 'cnn', 'backend': 'tensorflow', ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=1, n_filters_scale=2, model_type='pinn', architecture='cnn', amp_activ..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x7d53c03d19d0>

    def test_lightning_dataloader_tensor_dict_structure(
        self,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data
    ):
        """
        CRITICAL PARITY TEST: Lightning dataloaders must yield TensorDict-style batches.
    
        Requirement: ADR-003-BACKEND-API Phase C4.D3 — _build_lightning_dataloaders must
        produce batches matching the contract expected by PtychoPINN_Lightning.compute_loss
        (ptycho_torch/model.py:1118-1128).
    
        Expected batch structure:
        - batch[0]: dict-like with keys ['images', 'coords_relative',
                    'rms_scaling_constant', 'physics_scaling_constant']
        - batch[1]: probe tensor
        - batch[2]: scaling constant tensor
    
        Red-phase contract:
        - _build_lightning_dataloaders currently wraps tensors in TensorDataset
        - This yields (Tensor, Tensor) tuples, causing IndexError in compute_loss
        - Must refactor to use TensorDictDataLoader + Collate_Lightning pattern
        - Reference: ptycho_torch/dataloader.py:771-856 (TensorDictDataLoader + Collate_Lightning)
    
        Test mechanism:
        - Call _build_lightning_dataloaders with minimal container fixture
        - Extract first batch from train_loader
        - Assert batch structure matches compute_loss expectations
        - Validate presence of required keys in batch[0] dict
        """
        # Import the module under test
        from ptycho_torch.workflows import components as torch_components
        from ptycho.config.config import update_legacy_dict
        from ptycho import params
        import torch
    
        # Initialize params.cfg via CONFIG-001
        update_legacy_dict(params.cfg, minimal_training_config)
    
        # Create minimal container fixture matching _ensure_container output
        # (duck-typed dict for testing; production uses PtychoDataContainerTorch)
        N = minimal_training_config.model.N
        gridsize = minimal_training_config.model.gridsize
        n_samples = 8  # Small batch for testing
        n_channels = gridsize * gridsize
    
        train_container = {
            "X": torch.randn(n_samples, n_channels, N, N, dtype=torch.float32),
            "coords_nominal": torch.randn(n_samples, n_channels, 1, 2, dtype=torch.float32),
            "coords_relative": torch.randn(n_samples, n_channels, 1, 2, dtype=torch.float32),
            "rms_scaling_constant": torch.ones(n_samples, 1, 1, 1, dtype=torch.float32),
            "physics_scaling_constant": torch.ones(n_samples, 1, 1, 1, dtype=torch.float32),
            "probe": torch.randn(N, N, dtype=torch.complex64),
            "scaling_constant": torch.ones(1, dtype=torch.float32),
        }
    
        # Call _build_lightning_dataloaders
>       train_loader, _ = torch_components._build_lightning_dataloaders(
            train_container=train_container,
            test_container=None,
            config=minimal_training_config
        )
E       TypeError: _build_lightning_dataloaders() missing 1 required positional argument: 'payload'

tests/torch/test_workflows_components.py:402: TypeError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (10, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (10,)
xcoords shape: (10,)
ycoords shape: (10,)
xcoords_start shape: (10,)
ycoords_start shape: (10,)
____ TestWorkflowsComponentsTraining.test_lightning_poisson_count_contract _____

self = <test_workflows_components.TestWorkflowsComponentsTraining object at 0x7d53fea46d10>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'sigmoid', 'architecture': 'cnn', 'backend': 'tensorflow', ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=1, n_filters_scale=2, model_type='pinn', architecture='cnn', amp_activ..._poisson_count_c0/lightning_poisson_test'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x7d53cb8fdd90>
tmp_path = PosixPath('/tmp/pytest-of-ollie/pytest-184/test_lightning_poisson_count_c0')

    def test_lightning_poisson_count_contract(
        self,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data,
        tmp_path
    ):
        """
        CRITICAL PARITY TEST: Poisson loss must accept photon counts, not raw amplitudes.
    
        Requirement: ADR-003-BACKEND-API Phase C4.D3 — PyTorch Poisson loss path must
        replicate TensorFlow behavior: square amplitudes and apply nphotons scaling
        before computing log-likelihood. Current implementation passes normalized
        amplitude floats directly to torch.distributions.Poisson, violating the
        integer support constraint.
    
        TensorFlow behavior (ptycho/model.py:541-561):
        - Squares amplitudes: pred², raw²
        - Applies intensity_scale rescaling
        - Passes squared values to tf.nn.log_poisson_loss
    
        PyTorch current behavior (ptycho_torch/model.py:714-720):
        - PoissonIntensityLayer squares predictions: Lambda = amplitudes²
        - forward() receives raw amplitudes (float32, 0.0-0.07 range)
        - Poisson.log_prob(raw) raises ValueError: "expected within support IntegerGreaterThan(0)"
    
        Red-phase contract:
        - Test constructs minimal Lightning module + dataloader
        - Executes one forward + compute_loss pass
        - Current implementation MUST fail with Poisson support violation
        - Captures the exact error message for documentation
    
        Fix requirement:
        - PoissonIntensityLayer.forward() must convert both pred and raw to counts
        - Apply nphotons scaling via rms_scaling_constant (available in batch dict)
        - Square amplitudes before Poisson.log_prob()
        - Reference: ptycho/loader.py:355-363 (TF normalization pattern)
    
        Test mechanism:
        - Initialize params.cfg via CONFIG-001
        - Build minimal container + Lightning module
        - Call compute_loss with one batch (no full trainer needed)
        - Assert ValueError is raised with "support" in message (RED phase)
        - After fix: Assert loss is finite scalar (GREEN phase)
        """
        # Import required modules
        from ptycho_torch.workflows import components as torch_components
        from ptycho_torch.model import PtychoPINN_Lightning
        from ptycho.config.config import update_legacy_dict
        from ptycho import params
        import torch
    
        # CONFIG-001: Initialize params.cfg before data operations
        update_legacy_dict(params.cfg, minimal_training_config)
    
        # Set output_dir for checkpoint path resolution
        minimal_training_config.output_dir = tmp_path / "lightning_poisson_test"
        minimal_training_config.output_dir.mkdir(parents=True, exist_ok=True)
    
        # Build minimal container (uses _ensure_container internally)
        train_container = torch_components._ensure_container(
            dummy_raw_data,
            minimal_training_config
        )
    
        # Build Lightning dataloaders
>       train_loader, _ = torch_components._build_lightning_dataloaders(
            train_container=train_container,
            test_container=None,
            config=minimal_training_config
        )
E       TypeError: _build_lightning_dataloaders() missing 1 required positional argument: 'payload'

tests/torch/test_workflows_components.py:512: TypeError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (10, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (10,)
xcoords shape: (10,)
ycoords shape: (10,)
xcoords_start shape: (10,)
ycoords_start shape: (10,)
----------------------------- Captured stdout call -----------------------------
diff3d shape: (10, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (10,)
xcoords shape: (10,)
ycoords shape: (10,)
xcoords_start shape: (10,)
ycoords_start shape: (10,)
DEBUG: nsamples: 10, gridsize: 1 (using efficient random sample-then-group strategy)
INFO: No ground truth data ('Y' array or 'objectGuess') found.
INFO: This is expected for PINN training which doesn't require ground truth.
neighbor-sampled diffraction shape (10, 64, 64, 1)
PtychoDataContainerTorch: setting dummy Y ground truth with correct channel shape.
__ TestWorkflowsComponentsTraining.test_lightning_training_respects_gridsize ___

self = <test_workflows_components.TestWorkflowsComponentsTraining object at 0x7d53fea47450>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7d53cb9d8790>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'sigmoid', 'architecture': 'cnn', 'backend': 'pytorch', ...}
dummy_raw_data = <ptycho.raw_data.RawData object at 0x7d53cb9da950>

    def test_lightning_training_respects_gridsize(
        self,
        monkeypatch,
        params_cfg_snapshot,
        dummy_raw_data
    ):
        """
        Gridsize Channel Parity Test — _train_with_lightning MUST propagate gridsize to model channels.
    
        Requirement: docs/findings.md#BUG-TF-001 — gridsize > 1 yields channel mismatches
        unless params.cfg['gridsize'] AND PyTorch model configs synchronize C = gridsize**2.
    
        Design contract (phase_c4d_blockers/plan.md §B1-B2):
        - When config.model.gridsize=2, PyTorch DataConfig MUST set C=4 (2×2)
        - ModelConfig MUST set C_model=4 and C_forward=4 to match grouping
        - PtychoPINN_Lightning first conv layer MUST expect 4 input channels (not 1)
    
        Test mechanism:
        - Create TrainingConfig with gridsize=2
        - Monkeypatch PtychoPINN_Lightning to inspect first conv layer input channels
        - Invoke _train_with_lightning
        - Assert first conv layer has in_channels == gridsize**2 == 4
    
        Expected failure mode (RED phase):
        - _train_with_lightning manually builds PTDataConfig with default C=1
        - Lightning module created with C_model=1 → first conv expects 1 channel
        - Assertion fails: in_channels=1 != expected 4
    
        GREEN phase fix:
        - Refactor _train_with_lightning to reuse config_factory.create_training_payload
        - Factory propagates gridsize → C via grid_size tuple → C = grid_size[0]*grid_size[1]
        - ModelConfig receives C_model=4, Lightning module conv layers match
        """
        from ptycho.config.config import TrainingConfig, ModelConfig
        from ptycho_torch.workflows import components as torch_components
    
        # Spy to track Lightning module instantiation and inspect model structure
        lightning_init_spy = {"called": False, "first_conv_in_channels": None}
    
        # Store original PtychoPINN_Lightning before patching
        from ptycho_torch.model import PtychoPINN_Lightning as OriginalLightningModule
    
        def mock_lightning_init(model_config, data_config, training_config, inference_config):
            """Spy that captures PtychoPINN_Lightning model structure."""
            import torch
            import lightning.pytorch as L
    
            # Record that init was called
            lightning_init_spy["called"] = True
    
            # Use the ORIGINAL Lightning module (not the patched version) to inspect architecture
            module = OriginalLightningModule(
                model_config=model_config,
                data_config=data_config,
                training_config=training_config,
                inference_config=inference_config
            )
    
            # Extract first conv layer input channels from the model
            # The PtychoPINN architecture: model.autoencoder.encoder contains Conv2d layers
            # Find the first Conv2d layer and record its in_channels
            for layer in module.model.autoencoder.encoder.modules():
                if isinstance(layer, torch.nn.Conv2d):
                    lightning_init_spy["first_conv_in_channels"] = layer.in_channels
                    break
    
            # Return a stub module to avoid full training execution
            class StubLightningModule(L.LightningModule):
                def __init__(self):
                    super().__init__()
                    self.dummy_param = torch.nn.Parameter(torch.zeros(1))
    
                def training_step(self, batch, batch_idx):
                    return torch.tensor(0.0, requires_grad=True)
    
                def configure_optimizers(self):
                    return torch.optim.Adam(self.parameters(), lr=1e-3)
    
            return StubLightningModule()
    
        # Monkeypatch Lightning module constructor
        monkeypatch.setattr(
            "ptycho_torch.model.PtychoPINN_Lightning",
            mock_lightning_init
        )
    
        # Create minimal dummy NPZ file for factory validation
        # (Factory validates file exists before proceeding with config construction)
        import tempfile
        tmpdir = Path(tempfile.mkdtemp())
        dummy_npz = tmpdir / "dummy_train.npz"
    
        # Create minimal NPZ with required keys for DATA-001 compliance
        np.savez(
            dummy_npz,
            diffraction=np.ones((10, 64, 64), dtype=np.float32),
            xcoords=np.linspace(0, 9, 10),
            ycoords=np.linspace(0, 9, 10),
            probeGuess=np.ones((64, 64), dtype=np.complex64),
            objectGuess=np.ones((128, 128), dtype=np.complex64),
        )
    
        # Create TrainingConfig with gridsize=2 (requires 4 input channels)
        model_config = ModelConfig(
            N=64,
            gridsize=2,  # CRITICAL: 2×2 grouping → 4 channels expected
            model_type='pinn',
        )
    
        training_config = TrainingConfig(
            model=model_config,
            train_data_file=dummy_npz,  # Use temp file for factory validation
            test_data_file=dummy_npz,   # Reuse for test data
            n_groups=10,
            neighbor_count=4,
            nphotons=1e9,
            nepochs=2,
        )
    
        # Create minimal train_container
        train_container = {
            "X": np.ones((10, 64, 64)),
            "Y": np.ones((10, 64, 64), dtype=np.complex64),
        }
    
        # Call _train_with_lightning with gridsize=2 config
        results = torch_components._train_with_lightning(
            train_container=train_container,
            test_container=None,
            config=training_config
        )
    
        # Assert Lightning module was instantiated
        assert lightning_init_spy["called"], \
            "_train_with_lightning must instantiate PtychoPINN_Lightning module"
    
        # RED PHASE ASSERTION: Channel count must match gridsize**2
        expected_channels = training_config.model.gridsize ** 2
        actual_channels = lightning_init_spy["first_conv_in_channels"]
    
>       assert actual_channels == expected_channels, (
            f"Lightning model first conv layer MUST have in_channels={expected_channels} "
            f"when gridsize={training_config.model.gridsize}, but got in_channels={actual_channels}. "
            f"This indicates _train_with_lightning is not propagating gridsize to PyTorch configs. "
            f"See docs/findings.md#BUG-TF-001 for channel mismatch pattern."
        )
E       AssertionError: Lightning model first conv layer MUST have in_channels=4 when gridsize=2, but got in_channels=1. This indicates _train_with_lightning is not propagating gridsize to PyTorch configs. See docs/findings.md#BUG-TF-001 for channel mismatch pattern.
E       assert 1 == 4

tests/torch/test_workflows_components.py:708: AssertionError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (10, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (10,)
xcoords shape: (10,)
ycoords shape: (10,)
xcoords_start shape: (10,)
ycoords_start shape: (10,)
----------------------------- Captured stdout call -----------------------------
Decoder Block 1: No attention module added.
Decoder Block 2: No attention module added.
Decoder Block 1: No attention module added.
Decoder Block 2: No attention module added.
----------------------------- Captured stderr call -----------------------------
INFO: Seed set to 42
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name         | Type | Params | Mode
---------------------------------------------
  | other params | n/a  | 1      | n/a 
---------------------------------------------
1         Trainable params
0         Non-trainable params
1         Total params
0.000     Total estimated model params size (MB)
0         Modules in train mode
0         Modules in eval mode
------------------------------ Captured log call -------------------------------
INFO     lightning.fabric.utilities.seed:seed.py:57 Seed set to 42
INFO     pytorch_lightning.utilities.rank_zero:setup.py:156 GPU available: True (cuda), used: True
INFO     pytorch_lightning.utilities.rank_zero:setup.py:159 TPU available: False, using: 0 TPU cores
INFO     pytorch_lightning.utilities.rank_zero:setup.py:169 HPU available: False, using: 0 HPUs
INFO     lightning.pytorch.accelerators.cuda:cuda.py:61 LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO     lightning.pytorch.callbacks.model_summary:model_summary.py:104 
  | Name         | Type | Params | Mode
---------------------------------------------
  | other params | n/a  | 1      | n/a 
---------------------------------------------
1         Trainable params
0         Non-trainable params
1         Total params
0.000     Total estimated model params size (MB)
0         Modules in train mode
0         Modules in eval mode
INFO     pytorch_lightning.utilities.rank_zero:fit_loop.py:191 `Trainer.fit` stopped: `max_epochs=2` reached.
_________ TestWorkflowsComponentsTraining.test_coords_relative_layout __________

self = <test_workflows_components.TestWorkflowsComponentsTraining object at 0x7d53fea47b50>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7d53cb9e6ed0>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'sigmoid', 'architecture': 'cnn', 'backend': 'tensorflow', ...}
dummy_raw_data = <ptycho.raw_data.RawData object at 0x7d53cb9e4bd0>

    def test_coords_relative_layout(
        self,
        monkeypatch,
        params_cfg_snapshot,
        dummy_raw_data
    ):
        """
        Coords Relative Tensor Layout Test — dataloader MUST provide (batch, C, 1, 2) shaped coords_relative.
    
        Requirement: input.md Do Now #1 (2025-10-20 Phase C4.D.B2) — The current axis ordering bug
        causes coords_relative to arrive as (batch, 1, 2, C) which triggers Translation reshape crash
        when reassemble_patches_position_real broadcasts with norm_factor.
    
        Design contract (phase_c4d_coords_debug/summary.md):
        - _build_lightning_dataloaders MUST permute coords_relative from raw (batch, 1, 2, C)
          to expected (batch, C, 1, 2) before batching
        - Tensors MUST be .contiguous() before .view() operations
        - This layout matches the TensorFlow tf_helper contract for Translation inputs
    
        Test mechanism:
        - Create TrainingConfig with gridsize=2 (C=4)
        - Build Lightning dataloaders via _build_lightning_dataloaders
        - Extract a batch from train_loader
        - Assert batch['coords_relative'].shape == (batch_size, 4, 1, 2)
    
        Expected failure mode (RED phase):
        - Current implementation hands coords_relative shaped (batch, 1, 2, 4)
        - Assertion fails: shape[-3:] == (1, 2, 4) != expected (4, 1, 2)
    
        GREEN phase fix:
        - Refactor _build_lightning_dataloaders to permute coords_relative axes
        - Apply .contiguous() before batching to keep view() happy
        - Rerun test → assertion passes with (batch, 4, 1, 2) shape
        """
        from ptycho.config.config import TrainingConfig, ModelConfig
        from ptycho_torch.workflows import components as torch_components
        from ptycho.config.config import update_legacy_dict
        from ptycho import params
        import torch
    
        # Create TrainingConfig with gridsize=2 (C = 2×2 = 4)
        model_config = ModelConfig(
            N=64,
            gridsize=2,  # C = 4 channels
            model_type='pinn',
        )
    
        training_config = TrainingConfig(
            model=model_config,
            train_data_file=Path("/tmp/dummy_train.npz"),
            n_groups=10,
            neighbor_count=4,
            nphotons=1e9,
            nepochs=2,
            batch_size=4,  # Explicit batch size for shape check
        )
    
        # Populate params.cfg (CONFIG-001 requirement)
        update_legacy_dict(params.cfg, training_config)
    
        # Convert RawData to PtychoDataContainerTorch via _ensure_container
        # This will call generate_grouped_data internally
        train_container = torch_components._ensure_container(
            data=dummy_raw_data,
            config=training_config
        )
    
        # Build Lightning dataloaders (the function under test)
>       train_loader, _ = torch_components._build_lightning_dataloaders(
            train_container=train_container,
            test_container=None,
            config=training_config
        )
E       TypeError: _build_lightning_dataloaders() missing 1 required positional argument: 'payload'

tests/torch/test_workflows_components.py:783: TypeError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (10, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (10,)
xcoords shape: (10,)
ycoords shape: (10,)
xcoords_start shape: (10,)
ycoords_start shape: (10,)
----------------------------- Captured stdout call -----------------------------
diff3d shape: (10, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (10,)
xcoords shape: (10,)
ycoords shape: (10,)
xcoords_start shape: (10,)
ycoords_start shape: (10,)
DEBUG: nsamples: 10, gridsize: 2 (using efficient random sample-then-group strategy)
INFO: No ground truth data ('Y' array or 'objectGuess') found.
INFO: This is expected for PINN training which doesn't require ground truth.
neighbor-sampled diffraction shape (10, 64, 64, 4)
PtychoDataContainerTorch: setting dummy Y ground truth with correct channel shape.
_______ TestWorkflowsComponentsRun.test_run_cdi_example_invokes_training _______

self = <test_workflows_components.TestWorkflowsComponentsRun object at 0x7d53fea4c590>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7d53fe9eb290>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'sigmoid', 'architecture': 'cnn', 'backend': 'tensorflow', ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', architecture='cnn', amp_activ..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x7d53fe9e8750>

    def test_run_cdi_example_invokes_training(
        self,
        monkeypatch,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data
    ):
        """
        CRITICAL PARITY TEST: run_cdi_example_torch must invoke training orchestration.
    
        Requirement: Phase D2.C must implement full workflow orchestration following
        TensorFlow baseline ptycho/workflows/components.py:676-723 and mirroring
        the reconstructor lifecycle per specs/ptychodus_api_spec.md §4.5.
    
        Red-phase contract:
        - Entry signature: run_cdi_example_torch(train_data, test_data, config, do_stitching=False, ...)
        - MUST call train_cdi_model_torch(train_data, test_data, config) first
        - When do_stitching=False: return (None, None, results_dict)
        - When do_stitching=True + test_data: invoke reassemble helper, return (amp, phase, results)
        - results dict MUST contain keys from training (history, containers)
    
        Test mechanism:
        - Use monkeypatch to spy on train_cdi_model_torch call
        - Pass minimal RawData + do_stitching=False (no inference path required)
        - Assert train_cdi_model_torch was invoked with correct args
        - Validate return signature matches TensorFlow baseline
        """
        # Import the module under test
        from ptycho_torch.workflows import components as torch_components
    
        # Spy flag to track train_cdi_model_torch invocation
        train_cdi_model_torch_called = {"called": False, "args": None}
    
        def mock_train_cdi_model_torch(train_data, test_data, config):
            """Spy that records train_cdi_model_torch invocation."""
            train_cdi_model_torch_called["called"] = True
            train_cdi_model_torch_called["args"] = (train_data, test_data, config)
            # Return minimal training results dict
            return {
                "history": {"train_loss": [0.5, 0.3]},
                "train_container": {"sentinel": "train"},
                "test_container": None,
            }
    
        # Patch train_cdi_model_torch
        monkeypatch.setattr(
            "ptycho_torch.workflows.components.train_cdi_model_torch",
            mock_train_cdi_model_torch
        )
    
        # Call run_cdi_example_torch with do_stitching=False (Phase D2.C red phase)
>       recon_amp, recon_phase, results = torch_components.run_cdi_example_torch(
            train_data=dummy_raw_data,
            test_data=None,
            config=minimal_training_config,
            flip_x=False,
            flip_y=False,
            transpose=False,
            M=20,
            do_stitching=False,
        )

tests/torch/test_workflows_components.py:925: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

train_data = <ptycho.raw_data.RawData object at 0x7d53fe9e8750>
test_data = None
config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', architecture='cnn', amp_activ..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
flip_x = False, flip_y = False, transpose = False, M = 20, do_stitching = False
execution_config = None

    def run_cdi_example_torch(
        train_data: Union[RawData, 'RawDataTorch', 'PtychoDataContainerTorch'],
        test_data: Optional[Union[RawData, 'RawDataTorch', 'PtychoDataContainerTorch']],
        config: TrainingConfig,
        flip_x: bool = False,
        flip_y: bool = False,
        transpose: bool = False,
        M: int = 20,
        do_stitching: bool = False,
        execution_config: Optional[Any] = None
    ) -> Tuple[Optional[Any], Optional[Any], Dict[str, Any]]:
        """
        Run the main CDI example execution flow using PyTorch backend.
    
        This function provides API parity with ptycho.workflows.components.run_cdi_example,
        enabling transparent backend selection from Ptychodus per specs/ptychodus_api_spec.md §4.5.
    
        CRITICAL: This function MUST call update_legacy_dict(params.cfg, config) before
        delegating to core modules to prevent CONFIG-001 violations (empty params.cfg
        causing silent shape mismatches downstream).
    
        Args:
            train_data: Training data (RawData, RawDataTorch, or PtychoDataContainerTorch)
            test_data: Optional test data (same type constraints as train_data)
            config: TrainingConfig instance (TensorFlow dataclass, translated via config_bridge)
            flip_x: Whether to flip the x coordinates during reconstruction
            flip_y: Whether to flip the y coordinates during reconstruction
            transpose: Whether to transpose the image by swapping dimensions
            M: Parameter for reassemble_position function (default: 20)
            do_stitching: Whether to perform image stitching after training
            execution_config: Optional PyTorchExecutionConfig for runtime control (accelerator,
                             num_workers, learning_rate, scheduler, logger, checkpointing).
                             See CONFIG-002, CONFIG-LOGGER-001.
    
        Returns:
            Tuple containing:
            - reconstructed amplitude (or None if stitching disabled)
            - reconstructed phase (or None if stitching disabled)
            - results dictionary (training history, containers, metrics)
    
        Raises:
            NotImplementedError: Phase D2.B/C not yet implemented (scaffold only)
    
        Phase D2.A Scaffold Status:
            - Entry signature: ✅ COMPLETE (matches TensorFlow)
            - update_legacy_dict call: ✅ COMPLETE (CONFIG-001 compliance)
            - Placeholder logic: ✅ COMPLETE (raises NotImplementedError)
            - Torch-optional: ✅ COMPLETE (importable without torch)
    
        Phase D2.B/C TODO:
            - Implement train_cdi_model_torch delegation (Lightning trainer orchestration)
            - Implement reassemble_cdi_image_torch (optional stitching path)
            - Add MLflow disable flag handling
            - Validate deterministic seeds from config
    
        Example (Post D2.B/C):
            >>> from ptycho_torch.workflows.components import run_cdi_example_torch
            >>> from ptycho.config.config import TrainingConfig, ModelConfig
            >>> from ptycho.raw_data import RawData
            >>>
            >>> # Load data
            >>> train_data = RawData.from_file("train.npz")
            >>> config = TrainingConfig(model=ModelConfig(N=64), ...)
            >>>
            >>> # Execute PyTorch pipeline
            >>> amp, phase, results = run_cdi_example_torch(
            ...     train_data, None, config, do_stitching=False
            ... )
        """
        # CRITICAL: Update params.cfg before delegating (CONFIG-001 compliance)
        # This ensures legacy modules invoked downstream observe correct configuration state
        ptycho_config.update_legacy_dict(params.cfg, config)
        logger.info("PyTorch workflow: params.cfg synchronized with TrainingConfig")
    
        # Step 1: Train the model (Phase D2.B — delegates to Lightning trainer stub)
        logger.info("Invoking PyTorch training orchestration via train_cdi_model_torch")
        # Note: train_cdi_model_torch will need to be updated to accept execution_config
        # For now, we pass it as a keyword argument for forward compatibility
>       train_results = train_cdi_model_torch(train_data, test_data, config, execution_config=execution_config)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: TestWorkflowsComponentsRun.test_run_cdi_example_invokes_training.<locals>.mock_train_cdi_model_torch() got an unexpected keyword argument 'execution_config'

ptycho_torch/workflows/components.py:163: TypeError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (10, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (10,)
xcoords shape: (10,)
ycoords shape: (10,)
xcoords_start shape: (10,)
ycoords_start shape: (10,)
_______ TestWorkflowsComponentsRun.test_run_cdi_example_persists_models ________

self = <test_workflows_components.TestWorkflowsComponentsRun object at 0x7d53fea4cc90>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7d564af87850>
tmp_path = PosixPath('/tmp/pytest-of-ollie/pytest-184/test_run_cdi_example_persists_0')
params_cfg_snapshot = {'N': 64, 'amp_activation': 'sigmoid', 'architecture': 'cnn', 'backend': 'tensorflow', ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', architecture='cnn', amp_activ..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x7d564af85a10>

    def test_run_cdi_example_persists_models(
        self,
        monkeypatch,
        tmp_path,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data
    ):
        """
        REGRESSION TEST: run_cdi_example_torch must persist models when config.output_dir set.
    
        Requirement: Phase D4.B2 — validate PyTorch orchestration maintains persistence
        parity with TensorFlow baseline per specs/ptychodus_api_spec.md:§4.6.
    
        TensorFlow baseline (ptycho/workflows/components.py:709-723):
        - When config.output_dir is provided, calls save_model() or ModelManager.save()
        - Produces wts.h5.zip archive in output_dir with dual-model bundle
        - Persistence happens after training completes successfully
    
        Red-phase expectation:
        - run_cdi_example_torch currently does NOT call save_torch_bundle
        - Once Phase D4.C1 complete, SHOULD invoke save_torch_bundle when output_dir set
        - Test will FAIL until orchestration wiring is complete
    
        Test mechanism:
        - Monkeypatch save_torch_bundle to spy on invocation
        - Set config.output_dir to tmp_path
        - Call run_cdi_example_torch
        - Validate save_torch_bundle was called with correct models dict + base_path
        """
        # Import the module under test
        from ptycho_torch.workflows import components as torch_components
        from ptycho.config.config import TrainingConfig, ModelConfig
    
        # Spy flag to track save_torch_bundle invocation
        save_torch_bundle_called = {"called": False, "args": None, "kwargs": None}
    
        def mock_save_torch_bundle(models_dict, base_path, config, **kwargs):
            """Spy that records save_torch_bundle invocation."""
            save_torch_bundle_called["called"] = True
            save_torch_bundle_called["args"] = (models_dict, base_path, config)
            save_torch_bundle_called["kwargs"] = kwargs
    
        # Monkeypatch save_torch_bundle
        monkeypatch.setattr(
            "ptycho_torch.workflows.components.save_torch_bundle",
            mock_save_torch_bundle
        )
    
        # Monkeypatch train_cdi_model_torch to return minimal results with models
        def mock_train_cdi_model_torch(train_data, test_data, config):
            """Return stub results including models dict for persistence."""
            return {
                "history": {"train_loss": [0.5, 0.3]},
                "train_container": {"sentinel": "train"},
                "test_container": None,
                "models": {
                    'autoencoder': {'_sentinel': 'trained_autoencoder'},
                    'diffraction_to_obj': {'_sentinel': 'trained_diffraction'},
                },
            }
    
        monkeypatch.setattr(
            "ptycho_torch.workflows.components.train_cdi_model_torch",
            mock_train_cdi_model_torch
        )
    
        # Create config with output_dir set
        model_config = ModelConfig(N=64, gridsize=2, model_type='pinn')
        config_with_output = TrainingConfig(
            model=model_config,
            train_data_file=Path("/tmp/dummy_train.npz"),
            test_data_file=Path("/tmp/dummy_test.npz"),
            n_groups=10,
            neighbor_count=4,
            nphotons=1e9,
            output_dir=tmp_path,  # Enable persistence
        )
    
        # Call run_cdi_example_torch
>       recon_amp, recon_phase, results = torch_components.run_cdi_example_torch(
            train_data=dummy_raw_data,
            test_data=None,
            config=config_with_output,
            flip_x=False,
            flip_y=False,
            transpose=False,
            M=20,
            do_stitching=False,
        )

tests/torch/test_workflows_components.py:1036: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

train_data = <ptycho.raw_data.RawData object at 0x7d564af85a10>
test_data = None
config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', architecture='cnn', amp_activ...test-184/test_run_cdi_example_persists_0'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
flip_x = False, flip_y = False, transpose = False, M = 20, do_stitching = False
execution_config = None

    def run_cdi_example_torch(
        train_data: Union[RawData, 'RawDataTorch', 'PtychoDataContainerTorch'],
        test_data: Optional[Union[RawData, 'RawDataTorch', 'PtychoDataContainerTorch']],
        config: TrainingConfig,
        flip_x: bool = False,
        flip_y: bool = False,
        transpose: bool = False,
        M: int = 20,
        do_stitching: bool = False,
        execution_config: Optional[Any] = None
    ) -> Tuple[Optional[Any], Optional[Any], Dict[str, Any]]:
        """
        Run the main CDI example execution flow using PyTorch backend.
    
        This function provides API parity with ptycho.workflows.components.run_cdi_example,
        enabling transparent backend selection from Ptychodus per specs/ptychodus_api_spec.md §4.5.
    
        CRITICAL: This function MUST call update_legacy_dict(params.cfg, config) before
        delegating to core modules to prevent CONFIG-001 violations (empty params.cfg
        causing silent shape mismatches downstream).
    
        Args:
            train_data: Training data (RawData, RawDataTorch, or PtychoDataContainerTorch)
            test_data: Optional test data (same type constraints as train_data)
            config: TrainingConfig instance (TensorFlow dataclass, translated via config_bridge)
            flip_x: Whether to flip the x coordinates during reconstruction
            flip_y: Whether to flip the y coordinates during reconstruction
            transpose: Whether to transpose the image by swapping dimensions
            M: Parameter for reassemble_position function (default: 20)
            do_stitching: Whether to perform image stitching after training
            execution_config: Optional PyTorchExecutionConfig for runtime control (accelerator,
                             num_workers, learning_rate, scheduler, logger, checkpointing).
                             See CONFIG-002, CONFIG-LOGGER-001.
    
        Returns:
            Tuple containing:
            - reconstructed amplitude (or None if stitching disabled)
            - reconstructed phase (or None if stitching disabled)
            - results dictionary (training history, containers, metrics)
    
        Raises:
            NotImplementedError: Phase D2.B/C not yet implemented (scaffold only)
    
        Phase D2.A Scaffold Status:
            - Entry signature: ✅ COMPLETE (matches TensorFlow)
            - update_legacy_dict call: ✅ COMPLETE (CONFIG-001 compliance)
            - Placeholder logic: ✅ COMPLETE (raises NotImplementedError)
            - Torch-optional: ✅ COMPLETE (importable without torch)
    
        Phase D2.B/C TODO:
            - Implement train_cdi_model_torch delegation (Lightning trainer orchestration)
            - Implement reassemble_cdi_image_torch (optional stitching path)
            - Add MLflow disable flag handling
            - Validate deterministic seeds from config
    
        Example (Post D2.B/C):
            >>> from ptycho_torch.workflows.components import run_cdi_example_torch
            >>> from ptycho.config.config import TrainingConfig, ModelConfig
            >>> from ptycho.raw_data import RawData
            >>>
            >>> # Load data
            >>> train_data = RawData.from_file("train.npz")
            >>> config = TrainingConfig(model=ModelConfig(N=64), ...)
            >>>
            >>> # Execute PyTorch pipeline
            >>> amp, phase, results = run_cdi_example_torch(
            ...     train_data, None, config, do_stitching=False
            ... )
        """
        # CRITICAL: Update params.cfg before delegating (CONFIG-001 compliance)
        # This ensures legacy modules invoked downstream observe correct configuration state
        ptycho_config.update_legacy_dict(params.cfg, config)
        logger.info("PyTorch workflow: params.cfg synchronized with TrainingConfig")
    
        # Step 1: Train the model (Phase D2.B — delegates to Lightning trainer stub)
        logger.info("Invoking PyTorch training orchestration via train_cdi_model_torch")
        # Note: train_cdi_model_torch will need to be updated to accept execution_config
        # For now, we pass it as a keyword argument for forward compatibility
>       train_results = train_cdi_model_torch(train_data, test_data, config, execution_config=execution_config)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: TestWorkflowsComponentsRun.test_run_cdi_example_persists_models.<locals>.mock_train_cdi_model_torch() got an unexpected keyword argument 'execution_config'

ptycho_torch/workflows/components.py:163: TypeError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (10, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (10,)
xcoords shape: (10,)
ycoords shape: (10,)
xcoords_start shape: (10,)
ycoords_start shape: (10,)
_____ TestTrainWithLightningRed.test_train_with_lightning_runs_trainer_fit _____

self = <test_workflows_components.TestTrainWithLightningRed object at 0x7d53fea4e410>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7d56477e5310>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'sigmoid', 'architecture': 'cnn', 'backend': 'pytorch', ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', architecture='cnn', amp_activ..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x7d564af9a590>

    def test_train_with_lightning_runs_trainer_fit(
        self,
        monkeypatch,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data
    ):
        """
        RED TEST 2: _train_with_lightning MUST invoke Trainer.fit with dataloaders.
    
        Requirement: docs/workflows/pytorch.md §5 Lightning trainer expectations
        require Trainer.fit orchestration with train/val dataloaders.
    
        Design contract (phase_b_test_design.md §2):
        - _train_with_lightning MUST construct lightning.pytorch.Trainer
        - MUST invoke trainer.fit(module, train_dataloader, val_dataloader)
        - Dataloaders MUST be derived from provided train/test containers
        - Validation dataloader is None when test_container is None
    
        Test mechanism:
        - Monkeypatch Trainer constructor to return stub exposing fit_called flag
        - Monkeypatch dataloader builders (future helpers) with sentinels
        - Invoke _train_with_lightning
        - Assert Trainer.fit was called with correct dataloaders
    
        Expected red-phase failure:
        - Stub never constructs Trainer or calls fit
        - fit_called flag remains False → assertion fails
        """
        from ptycho_torch.workflows import components as torch_components
    
        # Spy to track Trainer.fit invocation
        trainer_fit_called = {"called": False, "args": None, "kwargs": None}
    
        class MockTrainer:
            """Stub Trainer that records fit() calls."""
            def fit(self, module, train_dataloaders=None, val_dataloaders=None, **kwargs):
                trainer_fit_called["called"] = True
                trainer_fit_called["args"] = (module, train_dataloaders, val_dataloaders)
                trainer_fit_called["kwargs"] = kwargs
    
        # Monkeypatch Lightning Trainer
        monkeypatch.setattr(
            "lightning.pytorch.Trainer",
            lambda **kwargs: MockTrainer()
        )
    
        # Monkeypatch Lightning module to prevent import errors
        class StubLightningModule:
            def save_hyperparameters(self):
                pass
    
        monkeypatch.setattr(
            "ptycho_torch.model.PtychoPINN_Lightning",
            lambda *args, **kwargs: StubLightningModule()
        )
    
        # Create sentinel dataloaders (Phase B2 will wire real loader builders)
        sentinel_train_loader = {"_sentinel": "train_dataloader"}
        sentinel_val_loader = None  # test_container is None
    
        # Monkeypatch future dataloader builder helper
        # (Phase B2 will add _build_lightning_dataloaders or similar)
        def mock_build_dataloaders(container, config, shuffle=True):
            """Sentinel that returns mock dataloader."""
            if container is not None:
                return sentinel_train_loader
            return None
    
        # For red phase, assume _train_with_lightning will eventually call helper
        # For now, test just validates fit() invocation pattern
    
        # Create minimal containers
        train_container = {"X": np.ones((10, 64, 64))}
    
        # Call _train_with_lightning
>       results = torch_components._train_with_lightning(
            train_container=train_container,
            test_container=None,
            config=minimal_training_config
        )

tests/torch/test_workflows_components.py:1433: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

train_container = {'X': array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., ... ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.]]], shape=(10, 64, 64))}
test_container = None
config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', architecture='cnn', amp_activ..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
execution_config = PyTorchExecutionConfig(accelerator='cuda', strategy='auto', deterministic=True, gradient_clip_val=None, accum_steps=1,...nt_mode='min', early_stop_patience=100, logger_backend='csv', inference_batch_size=None, middle_trim=0, pad_eval=False)
overrides = None

    def _train_with_lightning(
        train_container: Union['PtychoDataContainerTorch','PtychoDataset'],
        test_container: Optional['PtychoDataContainerTorch'],
        config: TrainingConfig,
        execution_config: Optional['PyTorchExecutionConfig'] = None,
        overrides: Optional[dict] = None
    ) -> Dict[str, Any]:
        """
        Orchestrate Lightning trainer execution for PyTorch model training.
    
        This function implements the Lightning training workflow per Phase D2.B blueprint:
        1. Derives PyTorch config objects from TensorFlow TrainingConfig
        2. Instantiates PtychoPINN_Lightning module with all four config dependencies
        3. Builds train/val dataloaders via _build_lightning_dataloaders helper
        4. Configures Lightning Trainer with checkpoint/logging settings (ADR-003 Phase C3)
        5. Executes training via trainer.fit()
        6. Returns structured results dict with history, containers, and module handle
    
        Args:
            train_container: Normalized training data container
            test_container: Optional normalized test data container
            config: TrainingConfig with training hyperparameters
            execution_config: Optional PyTorchExecutionConfig with runtime knobs (Phase C3.A2)
    
        Returns:
            Dict[str, Any]: Training results including:
                - history: Dict with train_loss and optional val_loss trajectories
                - train_container: Original training container
                - test_container: Original test container
                - models: Dict with 'diffraction_to_obj' (Lightning module) and 'autoencoder' (sentinel)
                          for dual-model bundle persistence per spec §4.6
    
        Raises:
            RuntimeError: If torch or lightning packages are not installed (POLICY-001)
    
        References:
            - Blueprint: plans/active/INTEGRATE-PYTORCH-001/reports/2025-10-18T020940Z/phase_d2_completion/phase_b2_implementation.md
            - Spec: specs/ptychodus_api_spec.md:187 (reconstructor lifecycle contract)
            - Findings: POLICY-001 (PyTorch mandatory), CONFIG-001 (params.cfg already populated by caller)
            - ADR-003 Phase C3: execution_config controls Trainer kwargs (accelerator, deterministic, gradient_clip_val)
        """
        # B2.2: torch-optional imports with POLICY-001 compliant error messaging
        try:
            import torch
            import lightning.pytorch as L
            from ptycho_torch.model import PtychoPINN_Lightning
            from ptycho_torch.config_params import (
                DataConfig as PTDataConfig,
                ModelConfig as PTModelConfig,
                TrainingConfig as PTTrainingConfig,
                InferenceConfig as PTInferenceConfig
            )
            from ptycho_torch.train_utils import PrebuiltPtychoDataModule
        except ImportError as e:
            raise RuntimeError(
                "PyTorch backend requires torch>=2.2 and lightning. "
                "Install with: pip install -e .[torch]\n"
                "See docs/findings.md#policy-001 for PyTorch requirement policy."
            ) from e
    
        logger.info("_train_with_lightning orchestrating Lightning training")
        logger.info(f"Training config: nepochs={config.nepochs}, n_groups={config.n_groups}")
    
        # B2.1: Use config_factory to derive PyTorch configs with correct channel propagation
        # CRITICAL (Phase C4.D B2): Factory ensures C = gridsize**2 is propagated to
        # pt_model_config.C_model and pt_model_config.C_forward, preventing channel mismatch
        # when gridsize > 1 (see docs/findings.md#BUG-TF-001).
        from ptycho_torch.config_factory import create_training_payload
    
        # Build factory overrides from TrainingConfig fields
        # Factory requires n_groups in overrides dict; train_data_file and output_dir as positional
        # Note: Factory expects model_type in PyTorch naming ('Unsupervised'/'Supervised')
        #       but TrainingConfig uses TensorFlow naming ('pinn'/'supervised')
        mode_map = {'pinn': 'Unsupervised', 'supervised': 'Supervised'}
        factory_overrides = {
            'n_groups': config.n_groups,  # Required by factory validation
            'gridsize': config.model.gridsize,
            'model_type': mode_map.get(config.model.model_type, 'Unsupervised'),
            'amp_activation': config.model.amp_activation,
            'n_filters_scale': config.model.n_filters_scale,
            'nphotons': config.nphotons,
            'neighbor_count': config.neighbor_count,
            'max_epochs': config.nepochs,
            'batch_size': getattr(config, 'batch_size', 16),
            'subsample_seed': getattr(config, 'subsample_seed', None),
            'torch_loss_mode': getattr(config, 'torch_loss_mode', 'poisson'),
        }
    
        # Create payload with factory-derived PyTorch configs
        payload = create_training_payload(
            train_data_file=Path(config.train_data_file),
            output_dir=Path(getattr(config, 'output_dir', './outputs')),
            execution_config=execution_config,  # Pass through from caller
            overrides=factory_overrides
        )
    
        # Extract PyTorch configs from payload (gridsize → C propagation already applied)
        pt_data_config = payload.pt_data_config
        pt_model_config = payload.pt_model_config
        pt_training_config = payload.pt_training_config
    
        # CRITICAL: Supervised mode REQUIRES a compatible loss function (MAE)
        # The Lightning module expects loss_name to be defined, which only happens when:
        #   1. mode='Unsupervised' AND loss_function='Poisson' → sets loss_name='poisson_train'
        #   2. mode='Unsupervised' AND loss_function='MAE' → sets loss_name='mae_train'
        #   3. mode='Supervised' AND loss_function='MAE' → sets loss_name='mae_train'
        # Without this override, supervised mode with default loss_function='Poisson' causes
        # AttributeError: 'PtychoPINN_Lightning' object has no attribute 'loss_name'
        # See: ptycho_torch/model.py:1052-1066
        if pt_model_config.mode == 'Supervised' and pt_model_config.loss_function != 'MAE':
            logger.info(
                f"Backend override: supervised mode requires MAE loss "
                f"(was {pt_model_config.loss_function}), forcing loss_function='MAE'"
            )
            # Create new ModelConfig with corrected loss_function
            from dataclasses import replace
            pt_model_config = replace(pt_model_config, loss_function='MAE')
    
        # Create minimal InferenceConfig for Lightning module (training payload doesn't include it)
        pt_inference_config = PTInferenceConfig()
    
        # B2.4: Instantiate PtychoPINN_Lightning model with factory-derived config objects
        model = PtychoPINN_Lightning(
            model_config=pt_model_config,
            data_config=pt_data_config,
            training_config=pt_training_config,
            inference_config=pt_inference_config
        )
    
        # Save hyperparameters so checkpoint can reconstruct module without external state
        model.save_hyperparameters()
    
        # B2.3: Build dataloaders via helper
        data_product = _build_lightning_dataloaders(
            train_container, test_container, config, payload = payload
        )
    
        # Data product is a lightning datamodule if ddp strategy selected, otherwise
        # it is a regular train/val loader tuple
        # Note: Use execution_config.strategy for runtime check, not pt_training_config.strategy
        effective_strategy = execution_config.strategy if execution_config else 'auto'
        if effective_strategy != 'ddp':
            train_loader, val_loader = data_product
        else:
            train_loader, val_loader = None, None  # Set to None when using datamodule
    
        # DATA-SUP-001: Supervised mode requires labeled data
        # Check if supervised mode is requested but training data lacks required labels
        if pt_model_config.mode == 'Supervised':
            # Inspect first batch to verify label keys exist
            try:
                first_batch = next(iter(train_loader))
                batch_dict = first_batch[0]  # Extract tensor dict from batch tuple
                if 'label_amp' not in batch_dict or 'label_phase' not in batch_dict:
                    raise RuntimeError(
                        f"Supervised mode (model_type='supervised') requires labeled datasets with "
                        f"'label_amp' and 'label_phase' keys, but training data lacks these fields. "
                        f"Either: (1) Use a labeled NPZ dataset (see ptycho_torch/notebooks/create_supervised_datasets.ipynb), "
                        f"or (2) Switch to PINN mode (--model_type pinn) for self-supervised physics-based training. "
                        f"See DATA-SUP-001 in docs/findings.md for details."
                    )
            except StopIteration:
                raise RuntimeError(
                    f"Training dataloader is empty. Check dataset path and n_groups configuration."
                )
    
        # B2.5: Configure Trainer with settings from config
        # C3.A3: Thread execution config values to Trainer kwargs
        output_dir = Path(getattr(config, 'output_dir', './outputs'))
        debug_mode = getattr(config, 'debug', False)
    
        # Import execution config defaults if not provided
        if execution_config is None:
            from ptycho.config.config import PyTorchExecutionConfig
            execution_config = PyTorchExecutionConfig()
            logger.info(f"PyTorchExecutionConfig auto-instantiated for Lightning training (accelerator resolved to '{execution_config.accelerator}')")
    
        # Custom callback to track loss history across epochs
        class _LossHistoryCallback(L.Callback):
            """Callback to collect train/val loss per epoch for history dict.
    
            The model logs metrics with dynamic names like 'poisson_train_Amp_loss'
            based on model configuration. This callback searches for any metric
            containing 'train' and 'loss' (or 'val' and 'loss') to capture the loss.
            """
    
            def __init__(self):
                self.train_loss = []
                self.val_loss = []
    
            def _find_loss_metric(self, metrics, prefix):
                """Find loss metric by prefix ('train' or 'val')."""
                for key in metrics:
                    if prefix in key and 'loss' in key:
                        return float(metrics[key])
                return None
    
            def on_train_epoch_end(self, trainer, pl_module):
                metrics = trainer.callback_metrics
                loss_val = self._find_loss_metric(metrics, 'train')
                if loss_val is not None:
                    self.train_loss.append(loss_val)
    
            def on_validation_epoch_end(self, trainer, pl_module):
                metrics = trainer.callback_metrics
                loss_val = self._find_loss_metric(metrics, 'val')
                if loss_val is not None:
                    self.val_loss.append(loss_val)
    
        loss_history_cb = _LossHistoryCallback()
    
        # EB1.D: Configure checkpoint/early-stop callbacks (ADR-003 Phase EB1)
        callbacks: list = [loss_history_cb]
        if execution_config.enable_checkpointing:
            from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping
    
            # Determine if we have validation data to use val metrics
            # Ptycho Datamodule automatically creates a validation dataset on instantiation (see train_utils.py)
            # so this means validation set exists if data product is a datamodule.
            has_validation = test_container is not None or isinstance(data_product, PrebuiltPtychoDataModule)
    
            # EB2.B: Derive monitor metric from model.val_loss_name (ADR-003 Phase EB2)
            # The model's val_loss_name is dynamically constructed based on model_type and loss configuration
            # (e.g., 'poisson_val_Amp_loss' for PINN with amplitude loss, 'mae_val_Phase_loss' for supervised)
            # This ensures checkpoint/early-stop callbacks watch the correct logged metric
            if has_validation and hasattr(model, 'val_loss_name'):
                # Use the model's dynamic validation loss name
                monitor_metric = model.val_loss_name
            else:
                # Fall back to execution config default or train loss
                monitor_metric = execution_config.checkpoint_monitor_metric
                if 'val_' in monitor_metric and not has_validation:
                    # Fall back to train_loss if val metric requested but no validation data
                    monitor_metric = monitor_metric.replace('val_', 'train_')
    
            # Build checkpoint filename template using dynamic metric name
            # Format: epoch={epoch:02d}-<metric_short_name>={<full_metric_name>:.4f}
            if has_validation:
                # Extract short name for filename (remove '_loss' suffix if present)
                metric_short_name = monitor_metric.replace('_loss', '')
                filename_template = f'epoch={{epoch:02d}}-{metric_short_name}={{{monitor_metric}:.4f}}'
            else:
                filename_template = 'epoch={epoch:02d}'
    
            checkpoint_callback = ModelCheckpoint(
                dirpath=str(output_dir / "checkpoints"),
                filename=filename_template,
                monitor=monitor_metric,
                mode=execution_config.checkpoint_mode,
                save_top_k=execution_config.checkpoint_save_top_k,
                save_last=True,  # Always keep last checkpoint for recovery
                verbose=False,
            )
            callbacks.append(checkpoint_callback)
    
            # EarlyStopping callback (ADR-003 Phase EB1.D)
            # Only add early stopping if validation data is available (otherwise no metric to monitor)
            if has_validation:
                early_stop_callback = EarlyStopping(
                    monitor=monitor_metric,
                    mode=execution_config.checkpoint_mode,
                    patience=execution_config.early_stop_patience,
                    verbose=False,
                )
                callbacks.append(early_stop_callback)
    
        # Instantiate logger based on execution config (Phase EB3.B - ADR-003)
        lightning_logger = False  # Default: no logger
        if execution_config.logger_backend is not None:
            try:
                if execution_config.logger_backend == 'csv':
                    from lightning.pytorch.loggers import CSVLogger
                    lightning_logger = CSVLogger(
                        save_dir=str(output_dir),
                        name='lightning_logs',
                    )
                    logger.info(f"Enabled CSVLogger: metrics saved to {output_dir}/lightning_logs/")
                elif execution_config.logger_backend == 'tensorboard':
                    from lightning.pytorch.loggers import TensorBoardLogger
                    lightning_logger = TensorBoardLogger(
                        save_dir=str(output_dir),
                        name='lightning_logs',
                    )
                    logger.info(f"Enabled TensorBoardLogger: run `tensorboard --logdir={output_dir}/lightning_logs/`")
                elif execution_config.logger_backend == 'mlflow':
                    from lightning.pytorch.loggers import MLFlowLogger
                    lightning_logger = MLFlowLogger(
                        experiment_name=getattr(config, 'experiment_name', 'PtychoPINN'),
                        tracking_uri=str(output_dir / 'mlruns'),
                    )
                    logger.info(f"Enabled MLFlowLogger: tracking URI={output_dir}/mlruns")
                else:
                    logger.warning(
                        f"Unknown logger_backend '{execution_config.logger_backend}'. "
                        f"Falling back to logger=False. Supported: 'csv', 'tensorboard', 'mlflow'."
                    )
            except ImportError as e:
                logger.warning(
                    f"Failed to import Lightning logger '{execution_config.logger_backend}': {e}. "
                    f"Metrics logging disabled. Install the required package to enable logging."
                )
                lightning_logger = False
        else:
            logger.info("Logger disabled (logger_backend=None). Loss metrics will not be saved to disk.")
    
        # EXEC-ACCUM-001: Guard against manual optimization + gradient accumulation
        # Lightning's manual optimization (automatic_optimization=False) is incompatible with
        # Trainer(accumulate_grad_batches>1). The PtychoPINN_Lightning module uses manual optimization
        # for custom physics loss integration, so gradient accumulation must be disabled.
>       if not model.automatic_optimization and execution_config.accum_steps > 1:
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'StubLightningModule' object has no attribute 'automatic_optimization'

ptycho_torch/workflows/components.py:979: AttributeError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (10, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (10,)
xcoords shape: (10,)
ycoords shape: (10,)
xcoords_start shape: (10,)
ycoords_start shape: (10,)
----------------------------- Captured stderr call -----------------------------
INFO: Seed set to 42
------------------------------ Captured log call -------------------------------
INFO     lightning.fabric.utilities.seed:seed.py:57 Seed set to 42
___ TestTrainWithLightningRed.test_train_with_lightning_returns_models_dict ____

self = <test_workflows_components.TestTrainWithLightningRed object at 0x7d53fea4ead0>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7d53cb8bbb50>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'sigmoid', 'architecture': 'cnn', 'backend': 'pytorch', ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', architecture='cnn', amp_activ..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x7d53cb8bbbd0>

    def test_train_with_lightning_returns_models_dict(
        self,
        monkeypatch,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data
    ):
        """
        RED TEST 3: _train_with_lightning MUST return results dict with 'models' key.
    
        Requirement: Phase D4 persistence tests require trained module handles
        for save_torch_bundle orchestration (mirrors TensorFlow train_cdi_model).
    
        Design contract (phase_b_test_design.md §3):
        - _train_with_lightning returns Dict[str, Any]
        - Results dict MUST contain 'models' key
        - models['lightning_module'] (or models['diffraction_to_obj']) MUST point to trained module
        - This enables downstream save_torch_bundle to persist checkpoint
    
        Test mechanism:
        - Monkeypatch Lightning components to return stub module
        - Invoke _train_with_lightning
        - Assert results dict contains 'models' key with module handle
    
        Expected red-phase failure:
        - Stub returns only history/containers → missing 'models' key
        - Assertion fails
        """
        from ptycho_torch.workflows import components as torch_components
    
        # Stub Lightning module with sentinel identity
        class StubLightningModule:
            def save_hyperparameters(self):
                pass
    
            _sentinel = "trained_lightning_module"
    
        stub_module = StubLightningModule()
    
        # Monkeypatch Lightning module constructor
        monkeypatch.setattr(
            "ptycho_torch.model.PtychoPINN_Lightning",
            lambda *args, **kwargs: stub_module
        )
    
        # Monkeypatch Trainer to skip actual training
        class MockTrainer:
            def fit(self, module, train_dataloaders=None, val_dataloaders=None, **kwargs):
                pass
    
        monkeypatch.setattr(
            "lightning.pytorch.Trainer",
            lambda **kwargs: MockTrainer()
        )
    
        # Create minimal containers
        train_container = {"X": np.ones((10, 64, 64))}
    
        # Call _train_with_lightning
>       results = torch_components._train_with_lightning(
            train_container=train_container,
            test_container=None,
            config=minimal_training_config
        )

tests/torch/test_workflows_components.py:1513: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

train_container = {'X': array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., ... ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.]]], shape=(10, 64, 64))}
test_container = None
config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', architecture='cnn', amp_activ..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
execution_config = PyTorchExecutionConfig(accelerator='cuda', strategy='auto', deterministic=True, gradient_clip_val=None, accum_steps=1,...nt_mode='min', early_stop_patience=100, logger_backend='csv', inference_batch_size=None, middle_trim=0, pad_eval=False)
overrides = None

    def _train_with_lightning(
        train_container: Union['PtychoDataContainerTorch','PtychoDataset'],
        test_container: Optional['PtychoDataContainerTorch'],
        config: TrainingConfig,
        execution_config: Optional['PyTorchExecutionConfig'] = None,
        overrides: Optional[dict] = None
    ) -> Dict[str, Any]:
        """
        Orchestrate Lightning trainer execution for PyTorch model training.
    
        This function implements the Lightning training workflow per Phase D2.B blueprint:
        1. Derives PyTorch config objects from TensorFlow TrainingConfig
        2. Instantiates PtychoPINN_Lightning module with all four config dependencies
        3. Builds train/val dataloaders via _build_lightning_dataloaders helper
        4. Configures Lightning Trainer with checkpoint/logging settings (ADR-003 Phase C3)
        5. Executes training via trainer.fit()
        6. Returns structured results dict with history, containers, and module handle
    
        Args:
            train_container: Normalized training data container
            test_container: Optional normalized test data container
            config: TrainingConfig with training hyperparameters
            execution_config: Optional PyTorchExecutionConfig with runtime knobs (Phase C3.A2)
    
        Returns:
            Dict[str, Any]: Training results including:
                - history: Dict with train_loss and optional val_loss trajectories
                - train_container: Original training container
                - test_container: Original test container
                - models: Dict with 'diffraction_to_obj' (Lightning module) and 'autoencoder' (sentinel)
                          for dual-model bundle persistence per spec §4.6
    
        Raises:
            RuntimeError: If torch or lightning packages are not installed (POLICY-001)
    
        References:
            - Blueprint: plans/active/INTEGRATE-PYTORCH-001/reports/2025-10-18T020940Z/phase_d2_completion/phase_b2_implementation.md
            - Spec: specs/ptychodus_api_spec.md:187 (reconstructor lifecycle contract)
            - Findings: POLICY-001 (PyTorch mandatory), CONFIG-001 (params.cfg already populated by caller)
            - ADR-003 Phase C3: execution_config controls Trainer kwargs (accelerator, deterministic, gradient_clip_val)
        """
        # B2.2: torch-optional imports with POLICY-001 compliant error messaging
        try:
            import torch
            import lightning.pytorch as L
            from ptycho_torch.model import PtychoPINN_Lightning
            from ptycho_torch.config_params import (
                DataConfig as PTDataConfig,
                ModelConfig as PTModelConfig,
                TrainingConfig as PTTrainingConfig,
                InferenceConfig as PTInferenceConfig
            )
            from ptycho_torch.train_utils import PrebuiltPtychoDataModule
        except ImportError as e:
            raise RuntimeError(
                "PyTorch backend requires torch>=2.2 and lightning. "
                "Install with: pip install -e .[torch]\n"
                "See docs/findings.md#policy-001 for PyTorch requirement policy."
            ) from e
    
        logger.info("_train_with_lightning orchestrating Lightning training")
        logger.info(f"Training config: nepochs={config.nepochs}, n_groups={config.n_groups}")
    
        # B2.1: Use config_factory to derive PyTorch configs with correct channel propagation
        # CRITICAL (Phase C4.D B2): Factory ensures C = gridsize**2 is propagated to
        # pt_model_config.C_model and pt_model_config.C_forward, preventing channel mismatch
        # when gridsize > 1 (see docs/findings.md#BUG-TF-001).
        from ptycho_torch.config_factory import create_training_payload
    
        # Build factory overrides from TrainingConfig fields
        # Factory requires n_groups in overrides dict; train_data_file and output_dir as positional
        # Note: Factory expects model_type in PyTorch naming ('Unsupervised'/'Supervised')
        #       but TrainingConfig uses TensorFlow naming ('pinn'/'supervised')
        mode_map = {'pinn': 'Unsupervised', 'supervised': 'Supervised'}
        factory_overrides = {
            'n_groups': config.n_groups,  # Required by factory validation
            'gridsize': config.model.gridsize,
            'model_type': mode_map.get(config.model.model_type, 'Unsupervised'),
            'amp_activation': config.model.amp_activation,
            'n_filters_scale': config.model.n_filters_scale,
            'nphotons': config.nphotons,
            'neighbor_count': config.neighbor_count,
            'max_epochs': config.nepochs,
            'batch_size': getattr(config, 'batch_size', 16),
            'subsample_seed': getattr(config, 'subsample_seed', None),
            'torch_loss_mode': getattr(config, 'torch_loss_mode', 'poisson'),
        }
    
        # Create payload with factory-derived PyTorch configs
        payload = create_training_payload(
            train_data_file=Path(config.train_data_file),
            output_dir=Path(getattr(config, 'output_dir', './outputs')),
            execution_config=execution_config,  # Pass through from caller
            overrides=factory_overrides
        )
    
        # Extract PyTorch configs from payload (gridsize → C propagation already applied)
        pt_data_config = payload.pt_data_config
        pt_model_config = payload.pt_model_config
        pt_training_config = payload.pt_training_config
    
        # CRITICAL: Supervised mode REQUIRES a compatible loss function (MAE)
        # The Lightning module expects loss_name to be defined, which only happens when:
        #   1. mode='Unsupervised' AND loss_function='Poisson' → sets loss_name='poisson_train'
        #   2. mode='Unsupervised' AND loss_function='MAE' → sets loss_name='mae_train'
        #   3. mode='Supervised' AND loss_function='MAE' → sets loss_name='mae_train'
        # Without this override, supervised mode with default loss_function='Poisson' causes
        # AttributeError: 'PtychoPINN_Lightning' object has no attribute 'loss_name'
        # See: ptycho_torch/model.py:1052-1066
        if pt_model_config.mode == 'Supervised' and pt_model_config.loss_function != 'MAE':
            logger.info(
                f"Backend override: supervised mode requires MAE loss "
                f"(was {pt_model_config.loss_function}), forcing loss_function='MAE'"
            )
            # Create new ModelConfig with corrected loss_function
            from dataclasses import replace
            pt_model_config = replace(pt_model_config, loss_function='MAE')
    
        # Create minimal InferenceConfig for Lightning module (training payload doesn't include it)
        pt_inference_config = PTInferenceConfig()
    
        # B2.4: Instantiate PtychoPINN_Lightning model with factory-derived config objects
        model = PtychoPINN_Lightning(
            model_config=pt_model_config,
            data_config=pt_data_config,
            training_config=pt_training_config,
            inference_config=pt_inference_config
        )
    
        # Save hyperparameters so checkpoint can reconstruct module without external state
        model.save_hyperparameters()
    
        # B2.3: Build dataloaders via helper
        data_product = _build_lightning_dataloaders(
            train_container, test_container, config, payload = payload
        )
    
        # Data product is a lightning datamodule if ddp strategy selected, otherwise
        # it is a regular train/val loader tuple
        # Note: Use execution_config.strategy for runtime check, not pt_training_config.strategy
        effective_strategy = execution_config.strategy if execution_config else 'auto'
        if effective_strategy != 'ddp':
            train_loader, val_loader = data_product
        else:
            train_loader, val_loader = None, None  # Set to None when using datamodule
    
        # DATA-SUP-001: Supervised mode requires labeled data
        # Check if supervised mode is requested but training data lacks required labels
        if pt_model_config.mode == 'Supervised':
            # Inspect first batch to verify label keys exist
            try:
                first_batch = next(iter(train_loader))
                batch_dict = first_batch[0]  # Extract tensor dict from batch tuple
                if 'label_amp' not in batch_dict or 'label_phase' not in batch_dict:
                    raise RuntimeError(
                        f"Supervised mode (model_type='supervised') requires labeled datasets with "
                        f"'label_amp' and 'label_phase' keys, but training data lacks these fields. "
                        f"Either: (1) Use a labeled NPZ dataset (see ptycho_torch/notebooks/create_supervised_datasets.ipynb), "
                        f"or (2) Switch to PINN mode (--model_type pinn) for self-supervised physics-based training. "
                        f"See DATA-SUP-001 in docs/findings.md for details."
                    )
            except StopIteration:
                raise RuntimeError(
                    f"Training dataloader is empty. Check dataset path and n_groups configuration."
                )
    
        # B2.5: Configure Trainer with settings from config
        # C3.A3: Thread execution config values to Trainer kwargs
        output_dir = Path(getattr(config, 'output_dir', './outputs'))
        debug_mode = getattr(config, 'debug', False)
    
        # Import execution config defaults if not provided
        if execution_config is None:
            from ptycho.config.config import PyTorchExecutionConfig
            execution_config = PyTorchExecutionConfig()
            logger.info(f"PyTorchExecutionConfig auto-instantiated for Lightning training (accelerator resolved to '{execution_config.accelerator}')")
    
        # Custom callback to track loss history across epochs
        class _LossHistoryCallback(L.Callback):
            """Callback to collect train/val loss per epoch for history dict.
    
            The model logs metrics with dynamic names like 'poisson_train_Amp_loss'
            based on model configuration. This callback searches for any metric
            containing 'train' and 'loss' (or 'val' and 'loss') to capture the loss.
            """
    
            def __init__(self):
                self.train_loss = []
                self.val_loss = []
    
            def _find_loss_metric(self, metrics, prefix):
                """Find loss metric by prefix ('train' or 'val')."""
                for key in metrics:
                    if prefix in key and 'loss' in key:
                        return float(metrics[key])
                return None
    
            def on_train_epoch_end(self, trainer, pl_module):
                metrics = trainer.callback_metrics
                loss_val = self._find_loss_metric(metrics, 'train')
                if loss_val is not None:
                    self.train_loss.append(loss_val)
    
            def on_validation_epoch_end(self, trainer, pl_module):
                metrics = trainer.callback_metrics
                loss_val = self._find_loss_metric(metrics, 'val')
                if loss_val is not None:
                    self.val_loss.append(loss_val)
    
        loss_history_cb = _LossHistoryCallback()
    
        # EB1.D: Configure checkpoint/early-stop callbacks (ADR-003 Phase EB1)
        callbacks: list = [loss_history_cb]
        if execution_config.enable_checkpointing:
            from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping
    
            # Determine if we have validation data to use val metrics
            # Ptycho Datamodule automatically creates a validation dataset on instantiation (see train_utils.py)
            # so this means validation set exists if data product is a datamodule.
            has_validation = test_container is not None or isinstance(data_product, PrebuiltPtychoDataModule)
    
            # EB2.B: Derive monitor metric from model.val_loss_name (ADR-003 Phase EB2)
            # The model's val_loss_name is dynamically constructed based on model_type and loss configuration
            # (e.g., 'poisson_val_Amp_loss' for PINN with amplitude loss, 'mae_val_Phase_loss' for supervised)
            # This ensures checkpoint/early-stop callbacks watch the correct logged metric
            if has_validation and hasattr(model, 'val_loss_name'):
                # Use the model's dynamic validation loss name
                monitor_metric = model.val_loss_name
            else:
                # Fall back to execution config default or train loss
                monitor_metric = execution_config.checkpoint_monitor_metric
                if 'val_' in monitor_metric and not has_validation:
                    # Fall back to train_loss if val metric requested but no validation data
                    monitor_metric = monitor_metric.replace('val_', 'train_')
    
            # Build checkpoint filename template using dynamic metric name
            # Format: epoch={epoch:02d}-<metric_short_name>={<full_metric_name>:.4f}
            if has_validation:
                # Extract short name for filename (remove '_loss' suffix if present)
                metric_short_name = monitor_metric.replace('_loss', '')
                filename_template = f'epoch={{epoch:02d}}-{metric_short_name}={{{monitor_metric}:.4f}}'
            else:
                filename_template = 'epoch={epoch:02d}'
    
            checkpoint_callback = ModelCheckpoint(
                dirpath=str(output_dir / "checkpoints"),
                filename=filename_template,
                monitor=monitor_metric,
                mode=execution_config.checkpoint_mode,
                save_top_k=execution_config.checkpoint_save_top_k,
                save_last=True,  # Always keep last checkpoint for recovery
                verbose=False,
            )
            callbacks.append(checkpoint_callback)
    
            # EarlyStopping callback (ADR-003 Phase EB1.D)
            # Only add early stopping if validation data is available (otherwise no metric to monitor)
            if has_validation:
                early_stop_callback = EarlyStopping(
                    monitor=monitor_metric,
                    mode=execution_config.checkpoint_mode,
                    patience=execution_config.early_stop_patience,
                    verbose=False,
                )
                callbacks.append(early_stop_callback)
    
        # Instantiate logger based on execution config (Phase EB3.B - ADR-003)
        lightning_logger = False  # Default: no logger
        if execution_config.logger_backend is not None:
            try:
                if execution_config.logger_backend == 'csv':
                    from lightning.pytorch.loggers import CSVLogger
                    lightning_logger = CSVLogger(
                        save_dir=str(output_dir),
                        name='lightning_logs',
                    )
                    logger.info(f"Enabled CSVLogger: metrics saved to {output_dir}/lightning_logs/")
                elif execution_config.logger_backend == 'tensorboard':
                    from lightning.pytorch.loggers import TensorBoardLogger
                    lightning_logger = TensorBoardLogger(
                        save_dir=str(output_dir),
                        name='lightning_logs',
                    )
                    logger.info(f"Enabled TensorBoardLogger: run `tensorboard --logdir={output_dir}/lightning_logs/`")
                elif execution_config.logger_backend == 'mlflow':
                    from lightning.pytorch.loggers import MLFlowLogger
                    lightning_logger = MLFlowLogger(
                        experiment_name=getattr(config, 'experiment_name', 'PtychoPINN'),
                        tracking_uri=str(output_dir / 'mlruns'),
                    )
                    logger.info(f"Enabled MLFlowLogger: tracking URI={output_dir}/mlruns")
                else:
                    logger.warning(
                        f"Unknown logger_backend '{execution_config.logger_backend}'. "
                        f"Falling back to logger=False. Supported: 'csv', 'tensorboard', 'mlflow'."
                    )
            except ImportError as e:
                logger.warning(
                    f"Failed to import Lightning logger '{execution_config.logger_backend}': {e}. "
                    f"Metrics logging disabled. Install the required package to enable logging."
                )
                lightning_logger = False
        else:
            logger.info("Logger disabled (logger_backend=None). Loss metrics will not be saved to disk.")
    
        # EXEC-ACCUM-001: Guard against manual optimization + gradient accumulation
        # Lightning's manual optimization (automatic_optimization=False) is incompatible with
        # Trainer(accumulate_grad_batches>1). The PtychoPINN_Lightning module uses manual optimization
        # for custom physics loss integration, so gradient accumulation must be disabled.
>       if not model.automatic_optimization and execution_config.accum_steps > 1:
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'StubLightningModule' object has no attribute 'automatic_optimization'

ptycho_torch/workflows/components.py:979: AttributeError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (10, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (10,)
xcoords shape: (10,)
ycoords shape: (10,)
xcoords_start shape: (10,)
ycoords_start shape: (10,)
----------------------------- Captured stderr call -----------------------------
INFO: Seed set to 42
------------------------------ Captured log call -------------------------------
INFO     lightning.fabric.utilities.seed:seed.py:57 Seed set to 42
_ TestReassembleCdiImageTorchGreen.test_reassemble_cdi_image_torch_flip_transpose_contract[False-False-False] _

self = <test_workflows_components.TestReassembleCdiImageTorchGreen object at 0x7d53fea4fd50>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'silu', 'architecture': 'cnn', 'backend': 'tensorflow', ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=1, model_type='pinn', architecture='cnn', amp_activ...test-184/test_reassemble_cdi_image_torc1'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x7d53cb849190>
stitch_train_results = {'history': {'train_loss': [0.1, 0.05]}, 'models': {'autoencoder': {'_sentinel': 'autoencoder'}, 'diffraction_to_obj': MockLightningModule()}}
flip_x = False, flip_y = False, transpose = False

    @pytest.mark.parametrize("flip_x,flip_y,transpose", [
        (False, False, False),  # No transforms
        (True, False, False),   # Flip X only
        (False, True, False),   # Flip Y only
        (False, False, True),   # Transpose only
        (True, True, True),     # All transforms
    ])
    def test_reassemble_cdi_image_torch_flip_transpose_contract(
        self,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data,
        stitch_train_results,
        flip_x,
        flip_y,
        transpose
    ):
        """
        GREEN TEST: _reassemble_cdi_image_torch honors flip/transpose parameters.
    
        Requirement: TensorFlow parity per specs/ptychodus_api_spec.md §4.5.
        TensorFlow baseline (ptycho/workflows/components.py:582-666) applies coordinate
        transforms via flip_x, flip_y, transpose parameters before reassembly.
    
        Expected behavior (all parameter combinations):
        - Function accepts flip_x, flip_y, transpose parameters
        - Returns (recon_amp, recon_phase, results) tuple
        - Amplitude/phase are 2D numpy arrays with finite values
        - Output shape invariant under flip/transpose (same canvas size)
        - global_offsets in results dict reflect coordinate transforms
    
        Test mechanism:
        - Parametrize over 5 representative flag combinations
        - Supply train_results with mock Lightning module
        - Assert successful execution (no exceptions)
        - Validate output structure and finiteness
    
        Rationale for parametrization:
        - Documents TF parity requirement explicitly in test corpus
        - Ensures implementation handles all transform combinations
        - Provides clear failure message surfacing which transform broke
        """
        from ptycho_torch.workflows import components as torch_components
        from ptycho.config.config import update_legacy_dict
        from ptycho import params
    
        # Bridge config (CONFIG-001)
        update_legacy_dict(params.cfg, minimal_training_config)
    
        # GREEN PHASE VALIDATION: expect successful stitching with all transform combos
>       recon_amp, recon_phase, results = torch_components._reassemble_cdi_image_torch(
            test_data=dummy_raw_data,
            config=minimal_training_config,
            flip_x=flip_x,
            flip_y=flip_y,
            transpose=transpose,
            M=128,
            train_results=stitch_train_results  # CRITICAL: provide trained model
        )

tests/torch/test_workflows_components.py:1808: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ptycho_torch/workflows/components.py:1228: in _reassemble_cdi_image_torch
    obj_image = hh.reassemble_position(obj_tensor_np, global_offsets_np, M=M)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ptycho/tf_helper.py:1324: in reassemble_position
    return shift_and_sum(obj_tensor, global_offsets, M = M) /\
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

op_name = '__inference_shift_and_sum_50612', num_outputs = 1
inputs = [<tf.Tensor: shape=(10, 64, 64, 1), dtype=complex64, numpy=
array([[[[1.+0.5j],
         [1.+0.5j],
         [1.+0.5j]...],


       [[[12.5],
         [25. ]]],


       [[[10. ],
         [10. ]]],


       [[[35. ],
         [17.5]]]])>]
attrs = ('executor_type', '', 'config_proto', b'\n\x07\n\x03CPU\x10\x01\n\x07\n\x03GPU\x10\x012\x07 \x01*\x010J\x008\x01\x82\x01\x00\x92\x01\x02J\x00')
ctx = <tensorflow.python.eager.context.Context object at 0x7d542d257110>
name = None

    def quick_execute(op_name, num_outputs, inputs, attrs, ctx, name=None):
      """Execute a TensorFlow operation.
    
      Args:
        op_name: Name of the TensorFlow operation (see REGISTER_OP in C++ code) to
          execute.
        num_outputs: The number of outputs of the operation to fetch. (Explicitly
          provided instead of being inferred for performance reasons).
        inputs: A list of inputs to the operation. Each entry should be a Tensor, or
          a value which can be passed to the Tensor constructor to create one.
        attrs: A tuple with alternating string attr names and attr values for this
          operation.
        ctx: The value of context.context().
        name: Customized name for the operation.
    
      Returns:
        List of output Tensor objects. The list is empty if there are no outputs
    
      Raises:
        An exception on error.
      """
      device_name = ctx.device_name
      # pylint: disable=protected-access
      try:
        ctx.ensure_initialized()
>       tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
                                            inputs, attrs, num_outputs)
E                                           tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:
E                                           
E                                           Detected at node while/add_2 defined at (most recent call last):
E                                             File "<frozen runpy>", line 198, in _run_module_as_main
E                                           
E                                             File "<frozen runpy>", line 88, in _run_code
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pytest/__main__.py", line 9, in <module>
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/config/__init__.py", line 201, in console_main
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/config/__init__.py", line 175, in main
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_hooks.py", line 512, in __call__
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_manager.py", line 120, in _hookexec
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_callers.py", line 121, in _multicall
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/main.py", line 336, in pytest_cmdline_main
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/main.py", line 289, in wrap_session
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/main.py", line 343, in _main
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_hooks.py", line 512, in __call__
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_manager.py", line 120, in _hookexec
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_callers.py", line 121, in _multicall
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/main.py", line 367, in pytest_runtestloop
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_hooks.py", line 512, in __call__
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_manager.py", line 120, in _hookexec
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_callers.py", line 121, in _multicall
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 117, in pytest_runtest_protocol
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 136, in runtestprotocol
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 245, in call_and_report
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 344, in from_call
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 246, in <lambda>
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_hooks.py", line 512, in __call__
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_manager.py", line 120, in _hookexec
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_callers.py", line 121, in _multicall
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 178, in pytest_runtest_call
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/python.py", line 1671, in runtest
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_hooks.py", line 512, in __call__
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_manager.py", line 120, in _hookexec
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_callers.py", line 121, in _multicall
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/python.py", line 157, in pytest_pyfunc_call
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/tests/torch/test_workflows_components.py", line 1808, in test_reassemble_cdi_image_torch_flip_transpose_contract
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/ptycho_torch/workflows/components.py", line 1228, in _reassemble_cdi_image_torch
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/ptycho/tf_helper.py", line 1324, in reassemble_position
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/ptycho/tf_helper.py", line 1252, in shift_and_sum
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/ptycho/tf_helper.py", line 1248, in _streaming
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/ptycho/tf_helper.py", line 1246, in body
E                                           
E                                           required broadcastable shapes
E                                           	 [[{{node while/add_2}}]] [Op:__inference_shift_and_sum_50612]

../../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53: InvalidArgumentError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (20, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (20,)
objectGuess shape: (128, 128)
xcoords shape: (20,)
ycoords shape: (20,)
xcoords_start shape: (20,)
ycoords_start shape: (20,)
----------------------------- Captured stdout call -----------------------------
diff3d shape: (20, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (20,)
objectGuess shape: (128, 128)
xcoords shape: (20,)
ycoords shape: (20,)
xcoords_start shape: (20,)
ycoords_start shape: (20,)
DEBUG: nsamples: 10, gridsize: 2 (using efficient random sample-then-group strategy)
INFO: 'Y' array not found. Generating ground truth patches from 'objectGuess' as a fallback.
neighbor-sampled diffraction shape (10, 64, 64, 4)
----------------------------- Captured stderr call -----------------------------
2026-01-26 21:39:38.243932: W tensorflow/core/framework/op_kernel.cc:1844] INVALID_ARGUMENT: required broadcastable shapes
2026-01-26 21:39:38.243980: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: required broadcastable shapes
	 [[{{function_node while_body_50124}}{{node while/add_2}}]]
_ TestReassembleCdiImageTorchGreen.test_reassemble_cdi_image_torch_flip_transpose_contract[True-False-False] _

self = <test_workflows_components.TestReassembleCdiImageTorchGreen object at 0x7d53fea4fe90>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'silu', 'architecture': 'cnn', 'backend': 'tensorflow', ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=1, model_type='pinn', architecture='cnn', amp_activ...test-184/test_reassemble_cdi_image_torc2'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x7d53cbbbac90>
stitch_train_results = {'history': {'train_loss': [0.1, 0.05]}, 'models': {'autoencoder': {'_sentinel': 'autoencoder'}, 'diffraction_to_obj': MockLightningModule()}}
flip_x = True, flip_y = False, transpose = False

    @pytest.mark.parametrize("flip_x,flip_y,transpose", [
        (False, False, False),  # No transforms
        (True, False, False),   # Flip X only
        (False, True, False),   # Flip Y only
        (False, False, True),   # Transpose only
        (True, True, True),     # All transforms
    ])
    def test_reassemble_cdi_image_torch_flip_transpose_contract(
        self,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data,
        stitch_train_results,
        flip_x,
        flip_y,
        transpose
    ):
        """
        GREEN TEST: _reassemble_cdi_image_torch honors flip/transpose parameters.
    
        Requirement: TensorFlow parity per specs/ptychodus_api_spec.md §4.5.
        TensorFlow baseline (ptycho/workflows/components.py:582-666) applies coordinate
        transforms via flip_x, flip_y, transpose parameters before reassembly.
    
        Expected behavior (all parameter combinations):
        - Function accepts flip_x, flip_y, transpose parameters
        - Returns (recon_amp, recon_phase, results) tuple
        - Amplitude/phase are 2D numpy arrays with finite values
        - Output shape invariant under flip/transpose (same canvas size)
        - global_offsets in results dict reflect coordinate transforms
    
        Test mechanism:
        - Parametrize over 5 representative flag combinations
        - Supply train_results with mock Lightning module
        - Assert successful execution (no exceptions)
        - Validate output structure and finiteness
    
        Rationale for parametrization:
        - Documents TF parity requirement explicitly in test corpus
        - Ensures implementation handles all transform combinations
        - Provides clear failure message surfacing which transform broke
        """
        from ptycho_torch.workflows import components as torch_components
        from ptycho.config.config import update_legacy_dict
        from ptycho import params
    
        # Bridge config (CONFIG-001)
        update_legacy_dict(params.cfg, minimal_training_config)
    
        # GREEN PHASE VALIDATION: expect successful stitching with all transform combos
>       recon_amp, recon_phase, results = torch_components._reassemble_cdi_image_torch(
            test_data=dummy_raw_data,
            config=minimal_training_config,
            flip_x=flip_x,
            flip_y=flip_y,
            transpose=transpose,
            M=128,
            train_results=stitch_train_results  # CRITICAL: provide trained model
        )

tests/torch/test_workflows_components.py:1808: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ptycho_torch/workflows/components.py:1228: in _reassemble_cdi_image_torch
    obj_image = hh.reassemble_position(obj_tensor_np, global_offsets_np, M=M)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ptycho/tf_helper.py:1324: in reassemble_position
    return shift_and_sum(obj_tensor, global_offsets, M = M) /\
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

op_name = '__inference_shift_and_sum_50612', num_outputs = 1
inputs = [<tf.Tensor: shape=(10, 64, 64, 1), dtype=complex64, numpy=
array([[[[1.+0.5j],
         [1.+0.5j],
         [1.+0.5j]...      [[[-12.5],
         [ 25. ]]],


       [[[-10. ],
         [ 10. ]]],


       [[[-35. ],
         [ 17.5]]]])>]
attrs = ('executor_type', '', 'config_proto', b'\n\x07\n\x03CPU\x10\x01\n\x07\n\x03GPU\x10\x012\x07 \x01*\x010J\x008\x01\x82\x01\x00\x92\x01\x02J\x00')
ctx = <tensorflow.python.eager.context.Context object at 0x7d542d257110>
name = None

    def quick_execute(op_name, num_outputs, inputs, attrs, ctx, name=None):
      """Execute a TensorFlow operation.
    
      Args:
        op_name: Name of the TensorFlow operation (see REGISTER_OP in C++ code) to
          execute.
        num_outputs: The number of outputs of the operation to fetch. (Explicitly
          provided instead of being inferred for performance reasons).
        inputs: A list of inputs to the operation. Each entry should be a Tensor, or
          a value which can be passed to the Tensor constructor to create one.
        attrs: A tuple with alternating string attr names and attr values for this
          operation.
        ctx: The value of context.context().
        name: Customized name for the operation.
    
      Returns:
        List of output Tensor objects. The list is empty if there are no outputs
    
      Raises:
        An exception on error.
      """
      device_name = ctx.device_name
      # pylint: disable=protected-access
      try:
        ctx.ensure_initialized()
>       tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
                                            inputs, attrs, num_outputs)
E                                           tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:
E                                           
E                                           Detected at node while/add_2 defined at (most recent call last):
E                                             File "<frozen runpy>", line 198, in _run_module_as_main
E                                           
E                                             File "<frozen runpy>", line 88, in _run_code
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pytest/__main__.py", line 9, in <module>
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/config/__init__.py", line 201, in console_main
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/config/__init__.py", line 175, in main
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_hooks.py", line 512, in __call__
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_manager.py", line 120, in _hookexec
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_callers.py", line 121, in _multicall
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/main.py", line 336, in pytest_cmdline_main
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/main.py", line 289, in wrap_session
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/main.py", line 343, in _main
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_hooks.py", line 512, in __call__
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_manager.py", line 120, in _hookexec
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_callers.py", line 121, in _multicall
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/main.py", line 367, in pytest_runtestloop
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_hooks.py", line 512, in __call__
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_manager.py", line 120, in _hookexec
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_callers.py", line 121, in _multicall
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 117, in pytest_runtest_protocol
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 136, in runtestprotocol
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 245, in call_and_report
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 344, in from_call
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 246, in <lambda>
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_hooks.py", line 512, in __call__
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_manager.py", line 120, in _hookexec
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_callers.py", line 121, in _multicall
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 178, in pytest_runtest_call
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/python.py", line 1671, in runtest
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_hooks.py", line 512, in __call__
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_manager.py", line 120, in _hookexec
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_callers.py", line 121, in _multicall
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/python.py", line 157, in pytest_pyfunc_call
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/tests/torch/test_workflows_components.py", line 1808, in test_reassemble_cdi_image_torch_flip_transpose_contract
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/ptycho_torch/workflows/components.py", line 1228, in _reassemble_cdi_image_torch
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/ptycho/tf_helper.py", line 1324, in reassemble_position
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/ptycho/tf_helper.py", line 1252, in shift_and_sum
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/ptycho/tf_helper.py", line 1248, in _streaming
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/ptycho/tf_helper.py", line 1246, in body
E                                           
E                                           required broadcastable shapes
E                                           	 [[{{node while/add_2}}]] [Op:__inference_shift_and_sum_50612]

../../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53: InvalidArgumentError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (20, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (20,)
objectGuess shape: (128, 128)
xcoords shape: (20,)
ycoords shape: (20,)
xcoords_start shape: (20,)
ycoords_start shape: (20,)
----------------------------- Captured stdout call -----------------------------
diff3d shape: (20, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (20,)
objectGuess shape: (128, 128)
xcoords shape: (20,)
ycoords shape: (20,)
xcoords_start shape: (20,)
ycoords_start shape: (20,)
DEBUG: nsamples: 10, gridsize: 2 (using efficient random sample-then-group strategy)
INFO: 'Y' array not found. Generating ground truth patches from 'objectGuess' as a fallback.
neighbor-sampled diffraction shape (10, 64, 64, 4)
----------------------------- Captured stderr call -----------------------------
2026-01-26 21:39:38.929970: W tensorflow/core/framework/op_kernel.cc:1844] INVALID_ARGUMENT: required broadcastable shapes
2026-01-26 21:39:38.930020: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: required broadcastable shapes
	 [[{{function_node while_body_50124}}{{node while/add_2}}]]
_ TestReassembleCdiImageTorchGreen.test_reassemble_cdi_image_torch_flip_transpose_contract[False-True-False] _

self = <test_workflows_components.TestReassembleCdiImageTorchGreen object at 0x7d53fea4ffd0>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'silu', 'architecture': 'cnn', 'backend': 'tensorflow', ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=1, model_type='pinn', architecture='cnn', amp_activ...test-184/test_reassemble_cdi_image_torc3'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x7d53c8169dd0>
stitch_train_results = {'history': {'train_loss': [0.1, 0.05]}, 'models': {'autoencoder': {'_sentinel': 'autoencoder'}, 'diffraction_to_obj': MockLightningModule()}}
flip_x = False, flip_y = True, transpose = False

    @pytest.mark.parametrize("flip_x,flip_y,transpose", [
        (False, False, False),  # No transforms
        (True, False, False),   # Flip X only
        (False, True, False),   # Flip Y only
        (False, False, True),   # Transpose only
        (True, True, True),     # All transforms
    ])
    def test_reassemble_cdi_image_torch_flip_transpose_contract(
        self,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data,
        stitch_train_results,
        flip_x,
        flip_y,
        transpose
    ):
        """
        GREEN TEST: _reassemble_cdi_image_torch honors flip/transpose parameters.
    
        Requirement: TensorFlow parity per specs/ptychodus_api_spec.md §4.5.
        TensorFlow baseline (ptycho/workflows/components.py:582-666) applies coordinate
        transforms via flip_x, flip_y, transpose parameters before reassembly.
    
        Expected behavior (all parameter combinations):
        - Function accepts flip_x, flip_y, transpose parameters
        - Returns (recon_amp, recon_phase, results) tuple
        - Amplitude/phase are 2D numpy arrays with finite values
        - Output shape invariant under flip/transpose (same canvas size)
        - global_offsets in results dict reflect coordinate transforms
    
        Test mechanism:
        - Parametrize over 5 representative flag combinations
        - Supply train_results with mock Lightning module
        - Assert successful execution (no exceptions)
        - Validate output structure and finiteness
    
        Rationale for parametrization:
        - Documents TF parity requirement explicitly in test corpus
        - Ensures implementation handles all transform combinations
        - Provides clear failure message surfacing which transform broke
        """
        from ptycho_torch.workflows import components as torch_components
        from ptycho.config.config import update_legacy_dict
        from ptycho import params
    
        # Bridge config (CONFIG-001)
        update_legacy_dict(params.cfg, minimal_training_config)
    
        # GREEN PHASE VALIDATION: expect successful stitching with all transform combos
>       recon_amp, recon_phase, results = torch_components._reassemble_cdi_image_torch(
            test_data=dummy_raw_data,
            config=minimal_training_config,
            flip_x=flip_x,
            flip_y=flip_y,
            transpose=transpose,
            M=128,
            train_results=stitch_train_results  # CRITICAL: provide trained model
        )

tests/torch/test_workflows_components.py:1808: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ptycho_torch/workflows/components.py:1228: in _reassemble_cdi_image_torch
    obj_image = hh.reassemble_position(obj_tensor_np, global_offsets_np, M=M)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ptycho/tf_helper.py:1324: in reassemble_position
    return shift_and_sum(obj_tensor, global_offsets, M = M) /\
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

op_name = '__inference_shift_and_sum_50612', num_outputs = 1
inputs = [<tf.Tensor: shape=(10, 64, 64, 1), dtype=complex64, numpy=
array([[[[1.+0.5j],
         [1.+0.5j],
         [1.+0.5j]...      [[[ 12.5],
         [-25. ]]],


       [[[ 10. ],
         [-10. ]]],


       [[[ 35. ],
         [-17.5]]]])>]
attrs = ('executor_type', '', 'config_proto', b'\n\x07\n\x03CPU\x10\x01\n\x07\n\x03GPU\x10\x012\x07 \x01*\x010J\x008\x01\x82\x01\x00\x92\x01\x02J\x00')
ctx = <tensorflow.python.eager.context.Context object at 0x7d542d257110>
name = None

    def quick_execute(op_name, num_outputs, inputs, attrs, ctx, name=None):
      """Execute a TensorFlow operation.
    
      Args:
        op_name: Name of the TensorFlow operation (see REGISTER_OP in C++ code) to
          execute.
        num_outputs: The number of outputs of the operation to fetch. (Explicitly
          provided instead of being inferred for performance reasons).
        inputs: A list of inputs to the operation. Each entry should be a Tensor, or
          a value which can be passed to the Tensor constructor to create one.
        attrs: A tuple with alternating string attr names and attr values for this
          operation.
        ctx: The value of context.context().
        name: Customized name for the operation.
    
      Returns:
        List of output Tensor objects. The list is empty if there are no outputs
    
      Raises:
        An exception on error.
      """
      device_name = ctx.device_name
      # pylint: disable=protected-access
      try:
        ctx.ensure_initialized()
>       tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
                                            inputs, attrs, num_outputs)
E                                           tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:
E                                           
E                                           Detected at node while/add_2 defined at (most recent call last):
E                                             File "<frozen runpy>", line 198, in _run_module_as_main
E                                           
E                                             File "<frozen runpy>", line 88, in _run_code
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pytest/__main__.py", line 9, in <module>
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/config/__init__.py", line 201, in console_main
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/config/__init__.py", line 175, in main
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_hooks.py", line 512, in __call__
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_manager.py", line 120, in _hookexec
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_callers.py", line 121, in _multicall
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/main.py", line 336, in pytest_cmdline_main
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/main.py", line 289, in wrap_session
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/main.py", line 343, in _main
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_hooks.py", line 512, in __call__
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_manager.py", line 120, in _hookexec
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_callers.py", line 121, in _multicall
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/main.py", line 367, in pytest_runtestloop
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_hooks.py", line 512, in __call__
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_manager.py", line 120, in _hookexec
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_callers.py", line 121, in _multicall
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 117, in pytest_runtest_protocol
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 136, in runtestprotocol
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 245, in call_and_report
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 344, in from_call
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 246, in <lambda>
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_hooks.py", line 512, in __call__
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_manager.py", line 120, in _hookexec
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_callers.py", line 121, in _multicall
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 178, in pytest_runtest_call
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/python.py", line 1671, in runtest
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_hooks.py", line 512, in __call__
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_manager.py", line 120, in _hookexec
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_callers.py", line 121, in _multicall
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/python.py", line 157, in pytest_pyfunc_call
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/tests/torch/test_workflows_components.py", line 1808, in test_reassemble_cdi_image_torch_flip_transpose_contract
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/ptycho_torch/workflows/components.py", line 1228, in _reassemble_cdi_image_torch
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/ptycho/tf_helper.py", line 1324, in reassemble_position
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/ptycho/tf_helper.py", line 1252, in shift_and_sum
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/ptycho/tf_helper.py", line 1248, in _streaming
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/ptycho/tf_helper.py", line 1246, in body
E                                           
E                                           required broadcastable shapes
E                                           	 [[{{node while/add_2}}]] [Op:__inference_shift_and_sum_50612]

../../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53: InvalidArgumentError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (20, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (20,)
objectGuess shape: (128, 128)
xcoords shape: (20,)
ycoords shape: (20,)
xcoords_start shape: (20,)
ycoords_start shape: (20,)
----------------------------- Captured stdout call -----------------------------
diff3d shape: (20, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (20,)
objectGuess shape: (128, 128)
xcoords shape: (20,)
ycoords shape: (20,)
xcoords_start shape: (20,)
ycoords_start shape: (20,)
DEBUG: nsamples: 10, gridsize: 2 (using efficient random sample-then-group strategy)
INFO: 'Y' array not found. Generating ground truth patches from 'objectGuess' as a fallback.
neighbor-sampled diffraction shape (10, 64, 64, 4)
----------------------------- Captured stderr call -----------------------------
2026-01-26 21:39:39.257256: W tensorflow/core/framework/op_kernel.cc:1844] INVALID_ARGUMENT: required broadcastable shapes
_ TestReassembleCdiImageTorchGreen.test_reassemble_cdi_image_torch_flip_transpose_contract[False-False-True] _

self = <test_workflows_components.TestReassembleCdiImageTorchGreen object at 0x7d53fea58150>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'silu', 'architecture': 'cnn', 'backend': 'tensorflow', ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=1, model_type='pinn', architecture='cnn', amp_activ...test-184/test_reassemble_cdi_image_torc4'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x7d53fe9e12d0>
stitch_train_results = {'history': {'train_loss': [0.1, 0.05]}, 'models': {'autoencoder': {'_sentinel': 'autoencoder'}, 'diffraction_to_obj': MockLightningModule()}}
flip_x = False, flip_y = False, transpose = True

    @pytest.mark.parametrize("flip_x,flip_y,transpose", [
        (False, False, False),  # No transforms
        (True, False, False),   # Flip X only
        (False, True, False),   # Flip Y only
        (False, False, True),   # Transpose only
        (True, True, True),     # All transforms
    ])
    def test_reassemble_cdi_image_torch_flip_transpose_contract(
        self,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data,
        stitch_train_results,
        flip_x,
        flip_y,
        transpose
    ):
        """
        GREEN TEST: _reassemble_cdi_image_torch honors flip/transpose parameters.
    
        Requirement: TensorFlow parity per specs/ptychodus_api_spec.md §4.5.
        TensorFlow baseline (ptycho/workflows/components.py:582-666) applies coordinate
        transforms via flip_x, flip_y, transpose parameters before reassembly.
    
        Expected behavior (all parameter combinations):
        - Function accepts flip_x, flip_y, transpose parameters
        - Returns (recon_amp, recon_phase, results) tuple
        - Amplitude/phase are 2D numpy arrays with finite values
        - Output shape invariant under flip/transpose (same canvas size)
        - global_offsets in results dict reflect coordinate transforms
    
        Test mechanism:
        - Parametrize over 5 representative flag combinations
        - Supply train_results with mock Lightning module
        - Assert successful execution (no exceptions)
        - Validate output structure and finiteness
    
        Rationale for parametrization:
        - Documents TF parity requirement explicitly in test corpus
        - Ensures implementation handles all transform combinations
        - Provides clear failure message surfacing which transform broke
        """
        from ptycho_torch.workflows import components as torch_components
        from ptycho.config.config import update_legacy_dict
        from ptycho import params
    
        # Bridge config (CONFIG-001)
        update_legacy_dict(params.cfg, minimal_training_config)
    
        # GREEN PHASE VALIDATION: expect successful stitching with all transform combos
>       recon_amp, recon_phase, results = torch_components._reassemble_cdi_image_torch(
            test_data=dummy_raw_data,
            config=minimal_training_config,
            flip_x=flip_x,
            flip_y=flip_y,
            transpose=transpose,
            M=128,
            train_results=stitch_train_results  # CRITICAL: provide trained model
        )

tests/torch/test_workflows_components.py:1808: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ptycho_torch/workflows/components.py:1228: in _reassemble_cdi_image_torch
    obj_image = hh.reassemble_position(obj_tensor_np, global_offsets_np, M=M)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ptycho/tf_helper.py:1324: in reassemble_position
    return shift_and_sum(obj_tensor, global_offsets, M = M) /\
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

op_name = '__inference_shift_and_sum_50612', num_outputs = 1
inputs = [<tf.Tensor: shape=(10, 64, 64, 1), dtype=complex64, numpy=
array([[[[1.+0.5j],
         [1.+0.5j],
         [1.+0.5j]...],


       [[[12.5],
         [25. ]]],


       [[[10. ],
         [10. ]]],


       [[[35. ],
         [17.5]]]])>]
attrs = ('executor_type', '', 'config_proto', b'\n\x07\n\x03CPU\x10\x01\n\x07\n\x03GPU\x10\x012\x07 \x01*\x010J\x008\x01\x82\x01\x00\x92\x01\x02J\x00')
ctx = <tensorflow.python.eager.context.Context object at 0x7d542d257110>
name = None

    def quick_execute(op_name, num_outputs, inputs, attrs, ctx, name=None):
      """Execute a TensorFlow operation.
    
      Args:
        op_name: Name of the TensorFlow operation (see REGISTER_OP in C++ code) to
          execute.
        num_outputs: The number of outputs of the operation to fetch. (Explicitly
          provided instead of being inferred for performance reasons).
        inputs: A list of inputs to the operation. Each entry should be a Tensor, or
          a value which can be passed to the Tensor constructor to create one.
        attrs: A tuple with alternating string attr names and attr values for this
          operation.
        ctx: The value of context.context().
        name: Customized name for the operation.
    
      Returns:
        List of output Tensor objects. The list is empty if there are no outputs
    
      Raises:
        An exception on error.
      """
      device_name = ctx.device_name
      # pylint: disable=protected-access
      try:
        ctx.ensure_initialized()
>       tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
                                            inputs, attrs, num_outputs)
E                                           tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:
E                                           
E                                           Detected at node while/add_2 defined at (most recent call last):
E                                             File "<frozen runpy>", line 198, in _run_module_as_main
E                                           
E                                             File "<frozen runpy>", line 88, in _run_code
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pytest/__main__.py", line 9, in <module>
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/config/__init__.py", line 201, in console_main
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/config/__init__.py", line 175, in main
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_hooks.py", line 512, in __call__
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_manager.py", line 120, in _hookexec
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_callers.py", line 121, in _multicall
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/main.py", line 336, in pytest_cmdline_main
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/main.py", line 289, in wrap_session
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/main.py", line 343, in _main
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_hooks.py", line 512, in __call__
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_manager.py", line 120, in _hookexec
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_callers.py", line 121, in _multicall
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/main.py", line 367, in pytest_runtestloop
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_hooks.py", line 512, in __call__
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_manager.py", line 120, in _hookexec
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_callers.py", line 121, in _multicall
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 117, in pytest_runtest_protocol
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 136, in runtestprotocol
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 245, in call_and_report
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 344, in from_call
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 246, in <lambda>
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_hooks.py", line 512, in __call__
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_manager.py", line 120, in _hookexec
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_callers.py", line 121, in _multicall
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 178, in pytest_runtest_call
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/python.py", line 1671, in runtest
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_hooks.py", line 512, in __call__
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_manager.py", line 120, in _hookexec
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_callers.py", line 121, in _multicall
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/python.py", line 157, in pytest_pyfunc_call
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/tests/torch/test_workflows_components.py", line 1808, in test_reassemble_cdi_image_torch_flip_transpose_contract
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/ptycho_torch/workflows/components.py", line 1228, in _reassemble_cdi_image_torch
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/ptycho/tf_helper.py", line 1324, in reassemble_position
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/ptycho/tf_helper.py", line 1252, in shift_and_sum
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/ptycho/tf_helper.py", line 1248, in _streaming
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/ptycho/tf_helper.py", line 1246, in body
E                                           
E                                           required broadcastable shapes
E                                           	 [[{{node while/add_2}}]] [Op:__inference_shift_and_sum_50612]

../../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53: InvalidArgumentError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (20, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (20,)
objectGuess shape: (128, 128)
xcoords shape: (20,)
ycoords shape: (20,)
xcoords_start shape: (20,)
ycoords_start shape: (20,)
----------------------------- Captured stdout call -----------------------------
diff3d shape: (20, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (20,)
objectGuess shape: (128, 128)
xcoords shape: (20,)
ycoords shape: (20,)
xcoords_start shape: (20,)
ycoords_start shape: (20,)
DEBUG: nsamples: 10, gridsize: 2 (using efficient random sample-then-group strategy)
INFO: 'Y' array not found. Generating ground truth patches from 'objectGuess' as a fallback.
neighbor-sampled diffraction shape (10, 64, 64, 4)
----------------------------- Captured stderr call -----------------------------
2026-01-26 21:39:39.584369: W tensorflow/core/framework/op_kernel.cc:1844] INVALID_ARGUMENT: required broadcastable shapes
2026-01-26 21:39:39.584428: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: required broadcastable shapes
	 [[{{function_node while_body_50124}}{{node while/add_2}}]]
_ TestReassembleCdiImageTorchGreen.test_reassemble_cdi_image_torch_flip_transpose_contract[True-True-True] _

self = <test_workflows_components.TestReassembleCdiImageTorchGreen object at 0x7d53fea58290>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'silu', 'architecture': 'cnn', 'backend': 'tensorflow', ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=1, model_type='pinn', architecture='cnn', amp_activ...test-184/test_reassemble_cdi_image_torc5'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x7d52b04b2d10>
stitch_train_results = {'history': {'train_loss': [0.1, 0.05]}, 'models': {'autoencoder': {'_sentinel': 'autoencoder'}, 'diffraction_to_obj': MockLightningModule()}}
flip_x = True, flip_y = True, transpose = True

    @pytest.mark.parametrize("flip_x,flip_y,transpose", [
        (False, False, False),  # No transforms
        (True, False, False),   # Flip X only
        (False, True, False),   # Flip Y only
        (False, False, True),   # Transpose only
        (True, True, True),     # All transforms
    ])
    def test_reassemble_cdi_image_torch_flip_transpose_contract(
        self,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data,
        stitch_train_results,
        flip_x,
        flip_y,
        transpose
    ):
        """
        GREEN TEST: _reassemble_cdi_image_torch honors flip/transpose parameters.
    
        Requirement: TensorFlow parity per specs/ptychodus_api_spec.md §4.5.
        TensorFlow baseline (ptycho/workflows/components.py:582-666) applies coordinate
        transforms via flip_x, flip_y, transpose parameters before reassembly.
    
        Expected behavior (all parameter combinations):
        - Function accepts flip_x, flip_y, transpose parameters
        - Returns (recon_amp, recon_phase, results) tuple
        - Amplitude/phase are 2D numpy arrays with finite values
        - Output shape invariant under flip/transpose (same canvas size)
        - global_offsets in results dict reflect coordinate transforms
    
        Test mechanism:
        - Parametrize over 5 representative flag combinations
        - Supply train_results with mock Lightning module
        - Assert successful execution (no exceptions)
        - Validate output structure and finiteness
    
        Rationale for parametrization:
        - Documents TF parity requirement explicitly in test corpus
        - Ensures implementation handles all transform combinations
        - Provides clear failure message surfacing which transform broke
        """
        from ptycho_torch.workflows import components as torch_components
        from ptycho.config.config import update_legacy_dict
        from ptycho import params
    
        # Bridge config (CONFIG-001)
        update_legacy_dict(params.cfg, minimal_training_config)
    
        # GREEN PHASE VALIDATION: expect successful stitching with all transform combos
>       recon_amp, recon_phase, results = torch_components._reassemble_cdi_image_torch(
            test_data=dummy_raw_data,
            config=minimal_training_config,
            flip_x=flip_x,
            flip_y=flip_y,
            transpose=transpose,
            M=128,
            train_results=stitch_train_results  # CRITICAL: provide trained model
        )

tests/torch/test_workflows_components.py:1808: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ptycho_torch/workflows/components.py:1228: in _reassemble_cdi_image_torch
    obj_image = hh.reassemble_position(obj_tensor_np, global_offsets_np, M=M)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ptycho/tf_helper.py:1324: in reassemble_position
    return shift_and_sum(obj_tensor, global_offsets, M = M) /\
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

op_name = '__inference_shift_and_sum_50612', num_outputs = 1
inputs = [<tf.Tensor: shape=(10, 64, 64, 1), dtype=complex64, numpy=
array([[[[1.+0.5j],
         [1.+0.5j],
         [1.+0.5j]...      [[[-12.5],
         [-25. ]]],


       [[[-10. ],
         [-10. ]]],


       [[[-35. ],
         [-17.5]]]])>]
attrs = ('executor_type', '', 'config_proto', b'\n\x07\n\x03CPU\x10\x01\n\x07\n\x03GPU\x10\x012\x07 \x01*\x010J\x008\x01\x82\x01\x00\x92\x01\x02J\x00')
ctx = <tensorflow.python.eager.context.Context object at 0x7d542d257110>
name = None

    def quick_execute(op_name, num_outputs, inputs, attrs, ctx, name=None):
      """Execute a TensorFlow operation.
    
      Args:
        op_name: Name of the TensorFlow operation (see REGISTER_OP in C++ code) to
          execute.
        num_outputs: The number of outputs of the operation to fetch. (Explicitly
          provided instead of being inferred for performance reasons).
        inputs: A list of inputs to the operation. Each entry should be a Tensor, or
          a value which can be passed to the Tensor constructor to create one.
        attrs: A tuple with alternating string attr names and attr values for this
          operation.
        ctx: The value of context.context().
        name: Customized name for the operation.
    
      Returns:
        List of output Tensor objects. The list is empty if there are no outputs
    
      Raises:
        An exception on error.
      """
      device_name = ctx.device_name
      # pylint: disable=protected-access
      try:
        ctx.ensure_initialized()
>       tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
                                            inputs, attrs, num_outputs)
E                                           tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:
E                                           
E                                           Detected at node while/add_2 defined at (most recent call last):
E                                             File "<frozen runpy>", line 198, in _run_module_as_main
E                                           
E                                             File "<frozen runpy>", line 88, in _run_code
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pytest/__main__.py", line 9, in <module>
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/config/__init__.py", line 201, in console_main
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/config/__init__.py", line 175, in main
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_hooks.py", line 512, in __call__
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_manager.py", line 120, in _hookexec
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_callers.py", line 121, in _multicall
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/main.py", line 336, in pytest_cmdline_main
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/main.py", line 289, in wrap_session
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/main.py", line 343, in _main
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_hooks.py", line 512, in __call__
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_manager.py", line 120, in _hookexec
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_callers.py", line 121, in _multicall
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/main.py", line 367, in pytest_runtestloop
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_hooks.py", line 512, in __call__
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_manager.py", line 120, in _hookexec
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_callers.py", line 121, in _multicall
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 117, in pytest_runtest_protocol
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 136, in runtestprotocol
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 245, in call_and_report
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 344, in from_call
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 246, in <lambda>
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_hooks.py", line 512, in __call__
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_manager.py", line 120, in _hookexec
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_callers.py", line 121, in _multicall
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 178, in pytest_runtest_call
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/python.py", line 1671, in runtest
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_hooks.py", line 512, in __call__
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_manager.py", line 120, in _hookexec
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_callers.py", line 121, in _multicall
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/python.py", line 157, in pytest_pyfunc_call
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/tests/torch/test_workflows_components.py", line 1808, in test_reassemble_cdi_image_torch_flip_transpose_contract
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/ptycho_torch/workflows/components.py", line 1228, in _reassemble_cdi_image_torch
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/ptycho/tf_helper.py", line 1324, in reassemble_position
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/ptycho/tf_helper.py", line 1252, in shift_and_sum
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/ptycho/tf_helper.py", line 1248, in _streaming
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/ptycho/tf_helper.py", line 1246, in body
E                                           
E                                           required broadcastable shapes
E                                           	 [[{{node while/add_2}}]] [Op:__inference_shift_and_sum_50612]

../../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53: InvalidArgumentError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (20, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (20,)
objectGuess shape: (128, 128)
xcoords shape: (20,)
ycoords shape: (20,)
xcoords_start shape: (20,)
ycoords_start shape: (20,)
----------------------------- Captured stdout call -----------------------------
diff3d shape: (20, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (20,)
objectGuess shape: (128, 128)
xcoords shape: (20,)
ycoords shape: (20,)
xcoords_start shape: (20,)
ycoords_start shape: (20,)
DEBUG: nsamples: 10, gridsize: 2 (using efficient random sample-then-group strategy)
INFO: 'Y' array not found. Generating ground truth patches from 'objectGuess' as a fallback.
neighbor-sampled diffraction shape (10, 64, 64, 4)
----------------------------- Captured stderr call -----------------------------
2026-01-26 21:39:39.904383: W tensorflow/core/framework/op_kernel.cc:1844] INVALID_ARGUMENT: required broadcastable shapes
_ TestReassembleCdiImageTorchGreen.test_run_cdi_example_torch_do_stitching_delegates_to_reassemble _

self = <test_workflows_components.TestReassembleCdiImageTorchGreen object at 0x7d53fea58510>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'silu', 'architecture': 'cnn', 'backend': 'tensorflow', ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=1, model_type='pinn', architecture='cnn', amp_activ...test-184/test_run_cdi_example_torch_do_0'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x7d54e727ffd0>
stitch_train_results = {'history': {'train_loss': [0.1, 0.05]}, 'models': {'autoencoder': {'_sentinel': 'autoencoder'}, 'diffraction_to_obj': MockLightningModule()}}
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7d54e72efa10>

    def test_run_cdi_example_torch_do_stitching_delegates_to_reassemble(
        self,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data,
        stitch_train_results,
        monkeypatch
    ):
        """
        GREEN TEST: run_cdi_example_torch(do_stitching=True) delegates to stitching path.
    
        Requirement: Phase D2.C workflow integration — ensure orchestration calls reassembly.
    
        TensorFlow baseline (ptycho/workflows/components.py:676-732):
        - run_cdi_example(..., do_stitching=True) invokes reassemble_cdi_image
        - Stitching runs AFTER training completes
        - Returns (recon_amp, recon_phase, results) when stitching enabled
        - Returns (None, None, results) when do_stitching=False
    
        Expected behavior:
        - Runs training (mocked), then calls _reassemble_cdi_image_torch with test_data
        - Stitching results populate amplitude/phase return values
        - Returns (recon_amp, recon_phase, results) when do_stitching=True
    
        Test mechanism:
        - Monkeypatch training path to return stitch_train_results fixture (avoid GPU)
        - Call run_cdi_example_torch with do_stitching=True
        - Assert amplitude/phase are returned (not None)
        - Validate outputs are numpy arrays
    
        Validation coverage:
        - Confirms orchestration wiring exists
        - Ensures stitching path is reachable from public API
        - Documents return value contract for downstream consumers (e.g., ptychodus)
        """
        from ptycho_torch.workflows import components as torch_components
        from ptycho.config.config import update_legacy_dict
        from ptycho import params
    
        # Bridge config (CONFIG-001)
        update_legacy_dict(params.cfg, minimal_training_config)
    
        # Monkeypatch _train_with_lightning to return mock results with Lightning module
        def mock_train_with_lightning(train_container, test_container, config):
            """Stub that returns train_results with mock Lightning module."""
            # Return the stitch_train_results fixture enriched with containers
            results = stitch_train_results.copy()
            results["containers"] = {"train": train_container, "test": test_container}
            return results
    
        monkeypatch.setattr(
            torch_components,
            "_train_with_lightning",
            mock_train_with_lightning
        )
    
        # GREEN PHASE VALIDATION: expect successful stitching
>       recon_amp, recon_phase, results = torch_components.run_cdi_example_torch(
            train_data=dummy_raw_data,
            test_data=dummy_raw_data,  # Use same data for test (deterministic)
            config=minimal_training_config,
            flip_x=False,
            flip_y=False,
            transpose=False,
            M=128,
            do_stitching=True,  # CRITICAL: enable stitching path
        )

tests/torch/test_workflows_components.py:1897: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ptycho_torch/workflows/components.py:163: in run_cdi_example_torch
    train_results = train_cdi_model_torch(train_data, test_data, config, execution_config=execution_config)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

train_data = <ptycho.raw_data.RawData object at 0x7d54e727ffd0>
test_data = <ptycho.raw_data.RawData object at 0x7d54e727ffd0>
config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=1, model_type='pinn', architecture='cnn', amp_activ...test-184/test_run_cdi_example_torch_do_0'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
execution_config = None

    def train_cdi_model_torch(
        train_data: Union[RawData, 'RawDataTorch', 'PtychoDataContainerTorch', 'PtychoDataset'],
        test_data: Optional[Union[RawData, 'RawDataTorch', 'PtychoDataContainerTorch']],
        config: TrainingConfig,
        execution_config: Optional[Any] = None
    ) -> Dict[str, Any]:
        """
        Train the CDI model using PyTorch Lightning backend.
    
        This function provides API parity with ptycho.workflows.components.train_cdi_model,
        orchestrating data preparation, probe initialization, and Lightning trainer execution.
    
        Args:
            train_data: Training data (RawData, RawDataTorch, or PtychoDataContainerTorch)
            test_data: Optional test data for validation
            config: TrainingConfig instance (TensorFlow dataclass)
            execution_config: Optional PyTorchExecutionConfig for runtime control
    
        Returns:
            Dict[str, Any]: Results dictionary containing:
            - 'history': Training history (losses, metrics)
            - 'train_container': PtychoDataContainerTorch for training data
            - 'test_container': Optional PtychoDataContainerTorch for test data
            - Additional outputs from Lightning trainer
    
        Raises:
            ImportError: If Phase C adapters not available
            TypeError: If input data types are invalid
    
        Phase D2.B Status:
            - Entry signature: ✅ COMPLETE (matches TensorFlow)
            - _ensure_container helper: ✅ COMPLETE (normalizes inputs via Phase C adapters)
            - Lightning orchestration: 🔶 STUB (returns minimal dict, full impl pending)
            - Torch-optional: ✅ COMPLETE (importable without torch)
    
        Example:
            >>> config = TrainingConfig(model=ModelConfig(N=64), nepochs=10, ...)
            >>> results = train_cdi_model_torch(train_data, test_data, config)
            >>> print(results['history']['train_loss'][-1])
        """
        # Step 1: Normalize train_data to PtychoDataContainerTorch
        logger.info("Normalizing training data via _ensure_container")
        train_container = _ensure_container(train_data, config)
    
        # Step 2: Normalize test_data if provided
        test_container = None
        if test_data is not None:
            logger.info("Normalizing test data via _ensure_container")
            test_container = _ensure_container(test_data, config)
    
        # Step 3: Initialize probe (TODO: implement probe handling for PyTorch)
        # TensorFlow baseline: probe.set_probe_guess(None, train_container.probe)
        # For Phase D2.B stub, skip probe initialization
        logger.debug("Probe initialization deferred to full Lightning implementation")
    
        # Step 4: Delegate to Lightning trainer
        logger.info("Delegating to Lightning trainer via _train_with_lightning")
>       results = _train_with_lightning(train_container, test_container, config, execution_config=execution_config)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: TestReassembleCdiImageTorchGreen.test_run_cdi_example_torch_do_stitching_delegates_to_reassemble.<locals>.mock_train_with_lightning() got an unexpected keyword argument 'execution_config'

ptycho_torch/workflows/components.py:1309: TypeError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (20, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (20,)
objectGuess shape: (128, 128)
xcoords shape: (20,)
ycoords shape: (20,)
xcoords_start shape: (20,)
ycoords_start shape: (20,)
----------------------------- Captured stdout call -----------------------------
diff3d shape: (20, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (20,)
objectGuess shape: (128, 128)
xcoords shape: (20,)
ycoords shape: (20,)
xcoords_start shape: (20,)
ycoords_start shape: (20,)
DEBUG: nsamples: 10, gridsize: 2 (using efficient random sample-then-group strategy)
INFO: 'Y' array not found. Generating ground truth patches from 'objectGuess' as a fallback.
neighbor-sampled diffraction shape (10, 64, 64, 4)
diff3d shape: (20, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (20,)
objectGuess shape: (128, 128)
xcoords shape: (20,)
ycoords shape: (20,)
xcoords_start shape: (20,)
ycoords_start shape: (20,)
DEBUG: nsamples: 10, gridsize: 2 (using efficient random sample-then-group strategy)
INFO: 'Y' array not found. Generating ground truth patches from 'objectGuess' as a fallback.
neighbor-sampled diffraction shape (10, 64, 64, 4)
_ TestReassembleCdiImageTorchGreen.test_reassemble_cdi_image_torch_return_contract _

self = <test_workflows_components.TestReassembleCdiImageTorchGreen object at 0x7d53fea58910>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'silu', 'architecture': 'cnn', 'backend': 'tensorflow', ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=1, model_type='pinn', architecture='cnn', amp_activ...test-184/test_reassemble_cdi_image_torc6'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x7d53c024d690>
stitch_train_results = {'history': {'train_loss': [0.1, 0.05]}, 'models': {'autoencoder': {'_sentinel': 'autoencoder'}, 'diffraction_to_obj': MockLightningModule()}}

    def test_reassemble_cdi_image_torch_return_contract(
        self,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data,
        stitch_train_results
    ):
        """
        GREEN TEST: _reassemble_cdi_image_torch returns (recon_amp, recon_phase, results).
    
        Requirement: API parity with TensorFlow reassemble_cdi_image per spec §4.5.
    
        TensorFlow baseline signature (ptycho/workflows/components.py:582):
        ```python
        def reassemble_cdi_image(test_data, config, flip_x=False, flip_y=False,
                                  transpose=False, coord_scale=1, M=None):
            ...
            return recon_amp, recon_phase, results
        ```
    
        Implemented PyTorch signature (ptycho_torch/workflows/components.py:606-614):
        ```python
        def _reassemble_cdi_image_torch(test_data, config, flip_x=False, flip_y=False,
                                         transpose=False, M=None, train_results=None):
            ...
            return recon_amp, recon_phase, results
        ```
    
        Return value contract:
        - recon_amp: 2D numpy array (float), stitched amplitude image
        - recon_phase: 2D numpy array (float), stitched phase image
        - results: dict containing {"obj_tensor_full", "global_offsets", "containers"}
    
        Test mechanism:
        - Supply train_results with mock Lightning module
        - Validate return tuple structure and types
        - Ensure all required keys present in results dict
    
        Rationale:
        - Ensures signature compatibility with TensorFlow baseline
        - Prevents silent breaking changes to return format
        - Codifies downstream consumer expectations (ptychodus integration)
        """
        from ptycho_torch.workflows import components as torch_components
        from ptycho.config.config import update_legacy_dict
        from ptycho import params
    
        # Bridge config (CONFIG-001)
        update_legacy_dict(params.cfg, minimal_training_config)
    
        # GREEN PHASE VALIDATION: validate return contract
>       recon_amp, recon_phase, results = torch_components._reassemble_cdi_image_torch(
            test_data=dummy_raw_data,
            config=minimal_training_config,
            flip_x=False,
            flip_y=False,
            transpose=False,
            M=128,
            train_results=stitch_train_results  # CRITICAL: provide trained model
        )

tests/torch/test_workflows_components.py:1967: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ptycho_torch/workflows/components.py:1228: in _reassemble_cdi_image_torch
    obj_image = hh.reassemble_position(obj_tensor_np, global_offsets_np, M=M)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ptycho/tf_helper.py:1324: in reassemble_position
    return shift_and_sum(obj_tensor, global_offsets, M = M) /\
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

op_name = '__inference_shift_and_sum_50612', num_outputs = 1
inputs = [<tf.Tensor: shape=(10, 64, 64, 1), dtype=complex64, numpy=
array([[[[1.+0.5j],
         [1.+0.5j],
         [1.+0.5j]...],


       [[[12.5],
         [25. ]]],


       [[[10. ],
         [10. ]]],


       [[[35. ],
         [17.5]]]])>]
attrs = ('executor_type', '', 'config_proto', b'\n\x07\n\x03CPU\x10\x01\n\x07\n\x03GPU\x10\x012\x07 \x01*\x010J\x008\x01\x82\x01\x00\x92\x01\x02J\x00')
ctx = <tensorflow.python.eager.context.Context object at 0x7d542d257110>
name = None

    def quick_execute(op_name, num_outputs, inputs, attrs, ctx, name=None):
      """Execute a TensorFlow operation.
    
      Args:
        op_name: Name of the TensorFlow operation (see REGISTER_OP in C++ code) to
          execute.
        num_outputs: The number of outputs of the operation to fetch. (Explicitly
          provided instead of being inferred for performance reasons).
        inputs: A list of inputs to the operation. Each entry should be a Tensor, or
          a value which can be passed to the Tensor constructor to create one.
        attrs: A tuple with alternating string attr names and attr values for this
          operation.
        ctx: The value of context.context().
        name: Customized name for the operation.
    
      Returns:
        List of output Tensor objects. The list is empty if there are no outputs
    
      Raises:
        An exception on error.
      """
      device_name = ctx.device_name
      # pylint: disable=protected-access
      try:
        ctx.ensure_initialized()
>       tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
                                            inputs, attrs, num_outputs)
E                                           tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:
E                                           
E                                           Detected at node while/add_2 defined at (most recent call last):
E                                             File "<frozen runpy>", line 198, in _run_module_as_main
E                                           
E                                             File "<frozen runpy>", line 88, in _run_code
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pytest/__main__.py", line 9, in <module>
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/config/__init__.py", line 201, in console_main
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/config/__init__.py", line 175, in main
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_hooks.py", line 512, in __call__
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_manager.py", line 120, in _hookexec
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_callers.py", line 121, in _multicall
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/main.py", line 336, in pytest_cmdline_main
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/main.py", line 289, in wrap_session
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/main.py", line 343, in _main
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_hooks.py", line 512, in __call__
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_manager.py", line 120, in _hookexec
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_callers.py", line 121, in _multicall
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/main.py", line 367, in pytest_runtestloop
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_hooks.py", line 512, in __call__
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_manager.py", line 120, in _hookexec
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_callers.py", line 121, in _multicall
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 117, in pytest_runtest_protocol
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 136, in runtestprotocol
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 245, in call_and_report
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 344, in from_call
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 246, in <lambda>
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_hooks.py", line 512, in __call__
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_manager.py", line 120, in _hookexec
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_callers.py", line 121, in _multicall
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/runner.py", line 178, in pytest_runtest_call
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/python.py", line 1671, in runtest
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_hooks.py", line 512, in __call__
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_manager.py", line 120, in _hookexec
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/pluggy/_callers.py", line 121, in _multicall
E                                           
E                                             File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/_pytest/python.py", line 157, in pytest_pyfunc_call
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/tests/torch/test_workflows_components.py", line 1808, in test_reassemble_cdi_image_torch_flip_transpose_contract
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/ptycho_torch/workflows/components.py", line 1228, in _reassemble_cdi_image_torch
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/ptycho/tf_helper.py", line 1324, in reassemble_position
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/ptycho/tf_helper.py", line 1252, in shift_and_sum
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/ptycho/tf_helper.py", line 1248, in _streaming
E                                           
E                                             File "/home/ollie/Documents/tmp/PtychoPINN/ptycho/tf_helper.py", line 1246, in body
E                                           
E                                           required broadcastable shapes
E                                           	 [[{{node while/add_2}}]] [Op:__inference_shift_and_sum_50612]

../../../miniconda3/envs/ptycho311/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53: InvalidArgumentError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (20, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (20,)
objectGuess shape: (128, 128)
xcoords shape: (20,)
ycoords shape: (20,)
xcoords_start shape: (20,)
ycoords_start shape: (20,)
----------------------------- Captured stdout call -----------------------------
diff3d shape: (20, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (20,)
objectGuess shape: (128, 128)
xcoords shape: (20,)
ycoords shape: (20,)
xcoords_start shape: (20,)
ycoords_start shape: (20,)
DEBUG: nsamples: 10, gridsize: 2 (using efficient random sample-then-group strategy)
INFO: 'Y' array not found. Generating ground truth patches from 'objectGuess' as a fallback.
neighbor-sampled diffraction shape (10, 64, 64, 4)
----------------------------- Captured stderr call -----------------------------
2026-01-26 21:39:40.816324: W tensorflow/core/framework/op_kernel.cc:1844] INVALID_ARGUMENT: required broadcastable shapes
__________ TestDecoderLastShapeParity.test_probe_big_shape_alignment ___________

self = <test_workflows_components.TestDecoderLastShapeParity object at 0x7d53fea5a310>

    def test_probe_big_shape_alignment(self):
        """
        Test that Decoder_last.forward() returns spatially consistent outputs when probe_big=True.
    
        Expected Behavior (TensorFlow parity):
            When probe_big=True, the decoder combines two paths (x1 and x2) via element-wise addition.
            Both paths MUST produce tensors with identical spatial dimensions (height, width) to enable
            addition without RuntimeError.
    
        Implementation Fix (Phase D1e.B2):
            PyTorch decoder center-crops x2 to match x1 spatial dimensions before addition,
            mirroring TensorFlow's trim_and_pad_output logic. This resolves the shape mismatch
            where x1 padding (540 → 572) produced smaller dims than x2 upsampling (540 → 1080).
    
        Test Strategy:
            Construct representative decoder input with realistic gridsize=1, N=64 dimensions,
            instantiate Decoder_last with probe_big=True, execute forward(), and assert:
            (1) No RuntimeError (successful addition after spatial alignment)
            (2) Output spatial dimensions match x1 path dimensions (height, width padded by N/4)
        """
        import torch
        from ptycho_torch.config_params import ModelConfig, DataConfig
        from ptycho_torch.model import Decoder_last
    
        # --- 1. Configuration (realistic gridsize=1, N=64 case) ---
        model_config = ModelConfig(
            mode='Unsupervised',
            probe_big=True,  # Enable x2 branch (requires spatial alignment fix)
            n_filters_scale=2,
            decoder_last_c_outer_fraction=0.25
        )
        data_config = DataConfig(
            N=64,
            grid_size=(1, 1)  # gridsize=1
        )
    
        # --- 2. Construct representative decoder input ---
        # Based on shape trace evidence:
        # Input to Decoder_last: (batch=8, in_channels=64, height=32, width=540)
        # After encoder stack, spatial dims are typically H=N//2, W varies (e.g., 540 for probe_big cases)
        batch_size = 8
        in_channels = 64
        out_channels = 1
        height = 32  # Typical encoder output height (N=64 → 32 after pooling)
        width = 540  # Width observed in shape trace (varies based on probe handling)
    
        x_input = torch.randn(batch_size, in_channels, height, width)
    
        # --- 3. Instantiate Decoder_last ---
        decoder = Decoder_last(
            model_config=model_config,
            data_config=data_config,
            in_channels=in_channels,
            out_channels=out_channels,
            activation=torch.sigmoid,
            name='decoder_shape_parity_test',
            batch_norm=False
        )
        decoder.eval()  # Inference mode (no dropout)
    
        # --- 4. Execute forward and assert successful completion (GREEN phase) ---
>       output = decoder.forward(x_input)
                 ^^^^^^^^^^^^^^^^^^^^^^^^

tests/torch/test_workflows_components.py:2303: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Decoder_last(
  (conv1): Conv2d(48, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_up_block): ConvUpBlo..., 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (padding): ConstantPad2d(padding=(16, 16, 16, 16), value=0)
)
x = tensor([[[[-8.4612e-01, -6.3749e-01, -4.5281e-01,  ..., -4.7332e-01,
            1.0706e-01, -7.9826e-01],
          [...488e+00],
          [ 3.3694e-01,  1.7551e-01,  8.2066e-01,  ..., -9.8365e-01,
            2.8380e+00, -2.3816e-01]]]])

    def forward(self,x):
        # Path 1
        x1_in = x[:, :-self.c_outer, :, :]
        # x1_in = self.upsample(x1_in) #(N//2, N//2) -> N,N
        x1 = self.conv1(x1_in)
        if self.batch_norm and self.bn1:
             x1 = self.bn1(x1) # Apply BN before activation
        x1 = self.activation(x1) #05-20-2025 for now
        x1 = self.padding(x1)
    
        if not self.model_config.probe_big:
            return x1
    
        # Path 2
        x2 = self.conv_up_block(x[:, -self.c_outer:, :, :])
        x2 = self.conv2(x2)
        if self.batch_norm and self.bn2:
             x2 = self.bn2(x2) # Apply BN before silu
    
        x2 = F.silu(x2) #05-20-2025 for now
    
>       outputs = x1 + x2
                  ^^^^^^^
E       RuntimeError: The size of tensor a (572) must match the size of tensor b (1080) at non-singleton dimension 3

ptycho_torch/model.py:368: RuntimeError
_____ TestTrainWithLightningGreen.test_execution_config_overrides_trainer ______

self = <test_workflows_components.TestTrainWithLightningGreen object at 0x7d53fea5b3d0>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7d53cb85b290>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'sigmoid', 'architecture': 'cnn', 'backend': 'pytorch', ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', architecture='cnn', amp_activ..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x7d53cb859910>

    def test_execution_config_overrides_trainer(
        self,
        monkeypatch,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data
    ):
        """
        RED TEST: _train_with_lightning MUST pass execution config knobs to Trainer.
    
        Requirement: ADR-003 Phase C3.A3 — thread trainer kwargs from execution config.
    
        Expected behavior (after wiring):
        - When execution_config supplied, Trainer receives accelerator/deterministic/gradient_clip_val
        - Values override defaults (e.g., accelerator='gpu', deterministic=False, gradient_clip_val=1.0)
        - When execution_config=None, Trainer uses CPU-safe defaults
    
        Test mechanism:
        - Spy on Trainer.__init__ to capture kwargs
        - Supply non-default PyTorchExecutionConfig (accelerator='gpu', deterministic=False)
        - Assert Trainer received those exact values
        - Expect FAILURE because _train_with_lightning currently ignores execution_config
        """
        from ptycho_torch.workflows import components as torch_components
        from ptycho.config.config import PyTorchExecutionConfig
    
        # Spy to track Trainer.__init__ kwargs
        trainer_init_kwargs = {"called": False, "kwargs": None}
    
        class MockTrainer:
            """Stub Trainer that records __init__ kwargs."""
            def __init__(self, **kwargs):
                trainer_init_kwargs["called"] = True
                trainer_init_kwargs["kwargs"] = kwargs
    
            def fit(self, module, train_dataloaders=None, val_dataloaders=None, **kwargs):
                pass
    
        # Monkeypatch Lightning Trainer
        monkeypatch.setattr(
            "lightning.pytorch.Trainer",
            MockTrainer
        )
    
        # Monkeypatch Lightning module to prevent errors
        class StubLightningModule:
            def save_hyperparameters(self):
                pass
    
        monkeypatch.setattr(
            "ptycho_torch.model.PtychoPINN_Lightning",
            lambda *args, **kwargs: StubLightningModule()
        )
    
        # Create execution config with non-default values
        exec_config = PyTorchExecutionConfig(
            accelerator='gpu',  # Override default 'cpu'
            deterministic=False,  # Override default True
            gradient_clip_val=1.0,  # Override default None
            num_workers=4,  # Override default 0
        )
    
        # Create minimal containers
        train_container = {"X": np.ones((10, 64, 64))}
    
        # Call _train_with_lightning with execution_config
>       results = torch_components._train_with_lightning(
            train_container=train_container,
            test_container=None,
            config=minimal_training_config,
            execution_config=exec_config  # CRITICAL: new parameter
        )

tests/torch/test_workflows_components.py:2535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

train_container = {'X': array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., ... ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.]]], shape=(10, 64, 64))}
test_container = None
config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', architecture='cnn', amp_activ..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
execution_config = PyTorchExecutionConfig(accelerator='gpu', strategy='auto', deterministic=False, gradient_clip_val=1.0, accum_steps=1, ...nt_mode='min', early_stop_patience=100, logger_backend='csv', inference_batch_size=None, middle_trim=0, pad_eval=False)
overrides = None

    def _train_with_lightning(
        train_container: Union['PtychoDataContainerTorch','PtychoDataset'],
        test_container: Optional['PtychoDataContainerTorch'],
        config: TrainingConfig,
        execution_config: Optional['PyTorchExecutionConfig'] = None,
        overrides: Optional[dict] = None
    ) -> Dict[str, Any]:
        """
        Orchestrate Lightning trainer execution for PyTorch model training.
    
        This function implements the Lightning training workflow per Phase D2.B blueprint:
        1. Derives PyTorch config objects from TensorFlow TrainingConfig
        2. Instantiates PtychoPINN_Lightning module with all four config dependencies
        3. Builds train/val dataloaders via _build_lightning_dataloaders helper
        4. Configures Lightning Trainer with checkpoint/logging settings (ADR-003 Phase C3)
        5. Executes training via trainer.fit()
        6. Returns structured results dict with history, containers, and module handle
    
        Args:
            train_container: Normalized training data container
            test_container: Optional normalized test data container
            config: TrainingConfig with training hyperparameters
            execution_config: Optional PyTorchExecutionConfig with runtime knobs (Phase C3.A2)
    
        Returns:
            Dict[str, Any]: Training results including:
                - history: Dict with train_loss and optional val_loss trajectories
                - train_container: Original training container
                - test_container: Original test container
                - models: Dict with 'diffraction_to_obj' (Lightning module) and 'autoencoder' (sentinel)
                          for dual-model bundle persistence per spec §4.6
    
        Raises:
            RuntimeError: If torch or lightning packages are not installed (POLICY-001)
    
        References:
            - Blueprint: plans/active/INTEGRATE-PYTORCH-001/reports/2025-10-18T020940Z/phase_d2_completion/phase_b2_implementation.md
            - Spec: specs/ptychodus_api_spec.md:187 (reconstructor lifecycle contract)
            - Findings: POLICY-001 (PyTorch mandatory), CONFIG-001 (params.cfg already populated by caller)
            - ADR-003 Phase C3: execution_config controls Trainer kwargs (accelerator, deterministic, gradient_clip_val)
        """
        # B2.2: torch-optional imports with POLICY-001 compliant error messaging
        try:
            import torch
            import lightning.pytorch as L
            from ptycho_torch.model import PtychoPINN_Lightning
            from ptycho_torch.config_params import (
                DataConfig as PTDataConfig,
                ModelConfig as PTModelConfig,
                TrainingConfig as PTTrainingConfig,
                InferenceConfig as PTInferenceConfig
            )
            from ptycho_torch.train_utils import PrebuiltPtychoDataModule
        except ImportError as e:
            raise RuntimeError(
                "PyTorch backend requires torch>=2.2 and lightning. "
                "Install with: pip install -e .[torch]\n"
                "See docs/findings.md#policy-001 for PyTorch requirement policy."
            ) from e
    
        logger.info("_train_with_lightning orchestrating Lightning training")
        logger.info(f"Training config: nepochs={config.nepochs}, n_groups={config.n_groups}")
    
        # B2.1: Use config_factory to derive PyTorch configs with correct channel propagation
        # CRITICAL (Phase C4.D B2): Factory ensures C = gridsize**2 is propagated to
        # pt_model_config.C_model and pt_model_config.C_forward, preventing channel mismatch
        # when gridsize > 1 (see docs/findings.md#BUG-TF-001).
        from ptycho_torch.config_factory import create_training_payload
    
        # Build factory overrides from TrainingConfig fields
        # Factory requires n_groups in overrides dict; train_data_file and output_dir as positional
        # Note: Factory expects model_type in PyTorch naming ('Unsupervised'/'Supervised')
        #       but TrainingConfig uses TensorFlow naming ('pinn'/'supervised')
        mode_map = {'pinn': 'Unsupervised', 'supervised': 'Supervised'}
        factory_overrides = {
            'n_groups': config.n_groups,  # Required by factory validation
            'gridsize': config.model.gridsize,
            'model_type': mode_map.get(config.model.model_type, 'Unsupervised'),
            'amp_activation': config.model.amp_activation,
            'n_filters_scale': config.model.n_filters_scale,
            'nphotons': config.nphotons,
            'neighbor_count': config.neighbor_count,
            'max_epochs': config.nepochs,
            'batch_size': getattr(config, 'batch_size', 16),
            'subsample_seed': getattr(config, 'subsample_seed', None),
            'torch_loss_mode': getattr(config, 'torch_loss_mode', 'poisson'),
        }
    
        # Create payload with factory-derived PyTorch configs
        payload = create_training_payload(
            train_data_file=Path(config.train_data_file),
            output_dir=Path(getattr(config, 'output_dir', './outputs')),
            execution_config=execution_config,  # Pass through from caller
            overrides=factory_overrides
        )
    
        # Extract PyTorch configs from payload (gridsize → C propagation already applied)
        pt_data_config = payload.pt_data_config
        pt_model_config = payload.pt_model_config
        pt_training_config = payload.pt_training_config
    
        # CRITICAL: Supervised mode REQUIRES a compatible loss function (MAE)
        # The Lightning module expects loss_name to be defined, which only happens when:
        #   1. mode='Unsupervised' AND loss_function='Poisson' → sets loss_name='poisson_train'
        #   2. mode='Unsupervised' AND loss_function='MAE' → sets loss_name='mae_train'
        #   3. mode='Supervised' AND loss_function='MAE' → sets loss_name='mae_train'
        # Without this override, supervised mode with default loss_function='Poisson' causes
        # AttributeError: 'PtychoPINN_Lightning' object has no attribute 'loss_name'
        # See: ptycho_torch/model.py:1052-1066
        if pt_model_config.mode == 'Supervised' and pt_model_config.loss_function != 'MAE':
            logger.info(
                f"Backend override: supervised mode requires MAE loss "
                f"(was {pt_model_config.loss_function}), forcing loss_function='MAE'"
            )
            # Create new ModelConfig with corrected loss_function
            from dataclasses import replace
            pt_model_config = replace(pt_model_config, loss_function='MAE')
    
        # Create minimal InferenceConfig for Lightning module (training payload doesn't include it)
        pt_inference_config = PTInferenceConfig()
    
        # B2.4: Instantiate PtychoPINN_Lightning model with factory-derived config objects
        model = PtychoPINN_Lightning(
            model_config=pt_model_config,
            data_config=pt_data_config,
            training_config=pt_training_config,
            inference_config=pt_inference_config
        )
    
        # Save hyperparameters so checkpoint can reconstruct module without external state
        model.save_hyperparameters()
    
        # B2.3: Build dataloaders via helper
        data_product = _build_lightning_dataloaders(
            train_container, test_container, config, payload = payload
        )
    
        # Data product is a lightning datamodule if ddp strategy selected, otherwise
        # it is a regular train/val loader tuple
        # Note: Use execution_config.strategy for runtime check, not pt_training_config.strategy
        effective_strategy = execution_config.strategy if execution_config else 'auto'
        if effective_strategy != 'ddp':
            train_loader, val_loader = data_product
        else:
            train_loader, val_loader = None, None  # Set to None when using datamodule
    
        # DATA-SUP-001: Supervised mode requires labeled data
        # Check if supervised mode is requested but training data lacks required labels
        if pt_model_config.mode == 'Supervised':
            # Inspect first batch to verify label keys exist
            try:
                first_batch = next(iter(train_loader))
                batch_dict = first_batch[0]  # Extract tensor dict from batch tuple
                if 'label_amp' not in batch_dict or 'label_phase' not in batch_dict:
                    raise RuntimeError(
                        f"Supervised mode (model_type='supervised') requires labeled datasets with "
                        f"'label_amp' and 'label_phase' keys, but training data lacks these fields. "
                        f"Either: (1) Use a labeled NPZ dataset (see ptycho_torch/notebooks/create_supervised_datasets.ipynb), "
                        f"or (2) Switch to PINN mode (--model_type pinn) for self-supervised physics-based training. "
                        f"See DATA-SUP-001 in docs/findings.md for details."
                    )
            except StopIteration:
                raise RuntimeError(
                    f"Training dataloader is empty. Check dataset path and n_groups configuration."
                )
    
        # B2.5: Configure Trainer with settings from config
        # C3.A3: Thread execution config values to Trainer kwargs
        output_dir = Path(getattr(config, 'output_dir', './outputs'))
        debug_mode = getattr(config, 'debug', False)
    
        # Import execution config defaults if not provided
        if execution_config is None:
            from ptycho.config.config import PyTorchExecutionConfig
            execution_config = PyTorchExecutionConfig()
            logger.info(f"PyTorchExecutionConfig auto-instantiated for Lightning training (accelerator resolved to '{execution_config.accelerator}')")
    
        # Custom callback to track loss history across epochs
        class _LossHistoryCallback(L.Callback):
            """Callback to collect train/val loss per epoch for history dict.
    
            The model logs metrics with dynamic names like 'poisson_train_Amp_loss'
            based on model configuration. This callback searches for any metric
            containing 'train' and 'loss' (or 'val' and 'loss') to capture the loss.
            """
    
            def __init__(self):
                self.train_loss = []
                self.val_loss = []
    
            def _find_loss_metric(self, metrics, prefix):
                """Find loss metric by prefix ('train' or 'val')."""
                for key in metrics:
                    if prefix in key and 'loss' in key:
                        return float(metrics[key])
                return None
    
            def on_train_epoch_end(self, trainer, pl_module):
                metrics = trainer.callback_metrics
                loss_val = self._find_loss_metric(metrics, 'train')
                if loss_val is not None:
                    self.train_loss.append(loss_val)
    
            def on_validation_epoch_end(self, trainer, pl_module):
                metrics = trainer.callback_metrics
                loss_val = self._find_loss_metric(metrics, 'val')
                if loss_val is not None:
                    self.val_loss.append(loss_val)
    
        loss_history_cb = _LossHistoryCallback()
    
        # EB1.D: Configure checkpoint/early-stop callbacks (ADR-003 Phase EB1)
        callbacks: list = [loss_history_cb]
        if execution_config.enable_checkpointing:
            from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping
    
            # Determine if we have validation data to use val metrics
            # Ptycho Datamodule automatically creates a validation dataset on instantiation (see train_utils.py)
            # so this means validation set exists if data product is a datamodule.
            has_validation = test_container is not None or isinstance(data_product, PrebuiltPtychoDataModule)
    
            # EB2.B: Derive monitor metric from model.val_loss_name (ADR-003 Phase EB2)
            # The model's val_loss_name is dynamically constructed based on model_type and loss configuration
            # (e.g., 'poisson_val_Amp_loss' for PINN with amplitude loss, 'mae_val_Phase_loss' for supervised)
            # This ensures checkpoint/early-stop callbacks watch the correct logged metric
            if has_validation and hasattr(model, 'val_loss_name'):
                # Use the model's dynamic validation loss name
                monitor_metric = model.val_loss_name
            else:
                # Fall back to execution config default or train loss
                monitor_metric = execution_config.checkpoint_monitor_metric
                if 'val_' in monitor_metric and not has_validation:
                    # Fall back to train_loss if val metric requested but no validation data
                    monitor_metric = monitor_metric.replace('val_', 'train_')
    
            # Build checkpoint filename template using dynamic metric name
            # Format: epoch={epoch:02d}-<metric_short_name>={<full_metric_name>:.4f}
            if has_validation:
                # Extract short name for filename (remove '_loss' suffix if present)
                metric_short_name = monitor_metric.replace('_loss', '')
                filename_template = f'epoch={{epoch:02d}}-{metric_short_name}={{{monitor_metric}:.4f}}'
            else:
                filename_template = 'epoch={epoch:02d}'
    
            checkpoint_callback = ModelCheckpoint(
                dirpath=str(output_dir / "checkpoints"),
                filename=filename_template,
                monitor=monitor_metric,
                mode=execution_config.checkpoint_mode,
                save_top_k=execution_config.checkpoint_save_top_k,
                save_last=True,  # Always keep last checkpoint for recovery
                verbose=False,
            )
            callbacks.append(checkpoint_callback)
    
            # EarlyStopping callback (ADR-003 Phase EB1.D)
            # Only add early stopping if validation data is available (otherwise no metric to monitor)
            if has_validation:
                early_stop_callback = EarlyStopping(
                    monitor=monitor_metric,
                    mode=execution_config.checkpoint_mode,
                    patience=execution_config.early_stop_patience,
                    verbose=False,
                )
                callbacks.append(early_stop_callback)
    
        # Instantiate logger based on execution config (Phase EB3.B - ADR-003)
        lightning_logger = False  # Default: no logger
        if execution_config.logger_backend is not None:
            try:
                if execution_config.logger_backend == 'csv':
                    from lightning.pytorch.loggers import CSVLogger
                    lightning_logger = CSVLogger(
                        save_dir=str(output_dir),
                        name='lightning_logs',
                    )
                    logger.info(f"Enabled CSVLogger: metrics saved to {output_dir}/lightning_logs/")
                elif execution_config.logger_backend == 'tensorboard':
                    from lightning.pytorch.loggers import TensorBoardLogger
                    lightning_logger = TensorBoardLogger(
                        save_dir=str(output_dir),
                        name='lightning_logs',
                    )
                    logger.info(f"Enabled TensorBoardLogger: run `tensorboard --logdir={output_dir}/lightning_logs/`")
                elif execution_config.logger_backend == 'mlflow':
                    from lightning.pytorch.loggers import MLFlowLogger
                    lightning_logger = MLFlowLogger(
                        experiment_name=getattr(config, 'experiment_name', 'PtychoPINN'),
                        tracking_uri=str(output_dir / 'mlruns'),
                    )
                    logger.info(f"Enabled MLFlowLogger: tracking URI={output_dir}/mlruns")
                else:
                    logger.warning(
                        f"Unknown logger_backend '{execution_config.logger_backend}'. "
                        f"Falling back to logger=False. Supported: 'csv', 'tensorboard', 'mlflow'."
                    )
            except ImportError as e:
                logger.warning(
                    f"Failed to import Lightning logger '{execution_config.logger_backend}': {e}. "
                    f"Metrics logging disabled. Install the required package to enable logging."
                )
                lightning_logger = False
        else:
            logger.info("Logger disabled (logger_backend=None). Loss metrics will not be saved to disk.")
    
        # EXEC-ACCUM-001: Guard against manual optimization + gradient accumulation
        # Lightning's manual optimization (automatic_optimization=False) is incompatible with
        # Trainer(accumulate_grad_batches>1). The PtychoPINN_Lightning module uses manual optimization
        # for custom physics loss integration, so gradient accumulation must be disabled.
>       if not model.automatic_optimization and execution_config.accum_steps > 1:
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'StubLightningModule' object has no attribute 'automatic_optimization'

ptycho_torch/workflows/components.py:979: AttributeError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (10, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (10,)
xcoords shape: (10,)
ycoords shape: (10,)
xcoords_start shape: (10,)
ycoords_start shape: (10,)
----------------------------- Captured stderr call -----------------------------
INFO: Seed set to 42
------------------------------ Captured log call -------------------------------
INFO     lightning.fabric.utilities.seed:seed.py:57 Seed set to 42
____ TestTrainWithLightningGreen.test_execution_config_controls_determinism ____

self = <test_workflows_components.TestTrainWithLightningGreen object at 0x7d53fea5bad0>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7d53fea72a90>
params_cfg_snapshot = {'N': 64, 'amp_activation': 'sigmoid', 'architecture': 'cnn', 'backend': 'pytorch', ...}
minimal_training_config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', architecture='cnn', amp_activ..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
dummy_raw_data = <ptycho.raw_data.RawData object at 0x7d53fea70d50>

    def test_execution_config_controls_determinism(
        self,
        monkeypatch,
        params_cfg_snapshot,
        minimal_training_config,
        dummy_raw_data
    ):
        """
        RED TEST: execution_config.deterministic MUST trigger Lightning deterministic mode.
    
        Requirement: ADR-003 Phase C3.C2 — validate deterministic behaviour.
    
        Expected behavior:
        - When deterministic=True (default), Trainer receives deterministic=True
        - This triggers torch.use_deterministic_algorithms(True) and seeds
        - When deterministic=False, Trainer allows non-deterministic ops
    
        Test mechanism:
        - Supply execution_config with deterministic=True
        - Assert Trainer.__init__ received deterministic=True kwarg
        - Expect FAILURE because current stub doesn't wire deterministic flag
        """
        from ptycho_torch.workflows import components as torch_components
        from ptycho.config.config import PyTorchExecutionConfig
    
        # Spy to track Trainer.__init__ kwargs
        trainer_init_kwargs = {"called": False, "kwargs": None}
    
        class MockTrainer:
            def __init__(self, **kwargs):
                trainer_init_kwargs["called"] = True
                trainer_init_kwargs["kwargs"] = kwargs
    
            def fit(self, module, train_dataloaders=None, val_dataloaders=None, **kwargs):
                pass
    
        monkeypatch.setattr(
            "lightning.pytorch.Trainer",
            MockTrainer
        )
    
        # Monkeypatch Lightning module
        class StubLightningModule:
            def save_hyperparameters(self):
                pass
    
        monkeypatch.setattr(
            "ptycho_torch.model.PtychoPINN_Lightning",
            lambda *args, **kwargs: StubLightningModule()
        )
    
        # Create execution config with deterministic=True (default)
        exec_config = PyTorchExecutionConfig(
            deterministic=True,
            accelerator='cpu'
        )
    
        # Create minimal containers
        train_container = {"X": np.ones((10, 64, 64))}
    
        # Call _train_with_lightning
>       results = torch_components._train_with_lightning(
            train_container=train_container,
            test_container=None,
            config=minimal_training_config,
            execution_config=exec_config
        )

tests/torch/test_workflows_components.py:2629: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

train_container = {'X': array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., ... ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.]]], shape=(10, 64, 64))}
test_container = None
config = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', architecture='cnn', amp_activ..., output_dir=PosixPath('training_outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
execution_config = PyTorchExecutionConfig(accelerator='cpu', strategy='auto', deterministic=True, gradient_clip_val=None, accum_steps=1, ...nt_mode='min', early_stop_patience=100, logger_backend='csv', inference_batch_size=None, middle_trim=0, pad_eval=False)
overrides = None

    def _train_with_lightning(
        train_container: Union['PtychoDataContainerTorch','PtychoDataset'],
        test_container: Optional['PtychoDataContainerTorch'],
        config: TrainingConfig,
        execution_config: Optional['PyTorchExecutionConfig'] = None,
        overrides: Optional[dict] = None
    ) -> Dict[str, Any]:
        """
        Orchestrate Lightning trainer execution for PyTorch model training.
    
        This function implements the Lightning training workflow per Phase D2.B blueprint:
        1. Derives PyTorch config objects from TensorFlow TrainingConfig
        2. Instantiates PtychoPINN_Lightning module with all four config dependencies
        3. Builds train/val dataloaders via _build_lightning_dataloaders helper
        4. Configures Lightning Trainer with checkpoint/logging settings (ADR-003 Phase C3)
        5. Executes training via trainer.fit()
        6. Returns structured results dict with history, containers, and module handle
    
        Args:
            train_container: Normalized training data container
            test_container: Optional normalized test data container
            config: TrainingConfig with training hyperparameters
            execution_config: Optional PyTorchExecutionConfig with runtime knobs (Phase C3.A2)
    
        Returns:
            Dict[str, Any]: Training results including:
                - history: Dict with train_loss and optional val_loss trajectories
                - train_container: Original training container
                - test_container: Original test container
                - models: Dict with 'diffraction_to_obj' (Lightning module) and 'autoencoder' (sentinel)
                          for dual-model bundle persistence per spec §4.6
    
        Raises:
            RuntimeError: If torch or lightning packages are not installed (POLICY-001)
    
        References:
            - Blueprint: plans/active/INTEGRATE-PYTORCH-001/reports/2025-10-18T020940Z/phase_d2_completion/phase_b2_implementation.md
            - Spec: specs/ptychodus_api_spec.md:187 (reconstructor lifecycle contract)
            - Findings: POLICY-001 (PyTorch mandatory), CONFIG-001 (params.cfg already populated by caller)
            - ADR-003 Phase C3: execution_config controls Trainer kwargs (accelerator, deterministic, gradient_clip_val)
        """
        # B2.2: torch-optional imports with POLICY-001 compliant error messaging
        try:
            import torch
            import lightning.pytorch as L
            from ptycho_torch.model import PtychoPINN_Lightning
            from ptycho_torch.config_params import (
                DataConfig as PTDataConfig,
                ModelConfig as PTModelConfig,
                TrainingConfig as PTTrainingConfig,
                InferenceConfig as PTInferenceConfig
            )
            from ptycho_torch.train_utils import PrebuiltPtychoDataModule
        except ImportError as e:
            raise RuntimeError(
                "PyTorch backend requires torch>=2.2 and lightning. "
                "Install with: pip install -e .[torch]\n"
                "See docs/findings.md#policy-001 for PyTorch requirement policy."
            ) from e
    
        logger.info("_train_with_lightning orchestrating Lightning training")
        logger.info(f"Training config: nepochs={config.nepochs}, n_groups={config.n_groups}")
    
        # B2.1: Use config_factory to derive PyTorch configs with correct channel propagation
        # CRITICAL (Phase C4.D B2): Factory ensures C = gridsize**2 is propagated to
        # pt_model_config.C_model and pt_model_config.C_forward, preventing channel mismatch
        # when gridsize > 1 (see docs/findings.md#BUG-TF-001).
        from ptycho_torch.config_factory import create_training_payload
    
        # Build factory overrides from TrainingConfig fields
        # Factory requires n_groups in overrides dict; train_data_file and output_dir as positional
        # Note: Factory expects model_type in PyTorch naming ('Unsupervised'/'Supervised')
        #       but TrainingConfig uses TensorFlow naming ('pinn'/'supervised')
        mode_map = {'pinn': 'Unsupervised', 'supervised': 'Supervised'}
        factory_overrides = {
            'n_groups': config.n_groups,  # Required by factory validation
            'gridsize': config.model.gridsize,
            'model_type': mode_map.get(config.model.model_type, 'Unsupervised'),
            'amp_activation': config.model.amp_activation,
            'n_filters_scale': config.model.n_filters_scale,
            'nphotons': config.nphotons,
            'neighbor_count': config.neighbor_count,
            'max_epochs': config.nepochs,
            'batch_size': getattr(config, 'batch_size', 16),
            'subsample_seed': getattr(config, 'subsample_seed', None),
            'torch_loss_mode': getattr(config, 'torch_loss_mode', 'poisson'),
        }
    
        # Create payload with factory-derived PyTorch configs
        payload = create_training_payload(
            train_data_file=Path(config.train_data_file),
            output_dir=Path(getattr(config, 'output_dir', './outputs')),
            execution_config=execution_config,  # Pass through from caller
            overrides=factory_overrides
        )
    
        # Extract PyTorch configs from payload (gridsize → C propagation already applied)
        pt_data_config = payload.pt_data_config
        pt_model_config = payload.pt_model_config
        pt_training_config = payload.pt_training_config
    
        # CRITICAL: Supervised mode REQUIRES a compatible loss function (MAE)
        # The Lightning module expects loss_name to be defined, which only happens when:
        #   1. mode='Unsupervised' AND loss_function='Poisson' → sets loss_name='poisson_train'
        #   2. mode='Unsupervised' AND loss_function='MAE' → sets loss_name='mae_train'
        #   3. mode='Supervised' AND loss_function='MAE' → sets loss_name='mae_train'
        # Without this override, supervised mode with default loss_function='Poisson' causes
        # AttributeError: 'PtychoPINN_Lightning' object has no attribute 'loss_name'
        # See: ptycho_torch/model.py:1052-1066
        if pt_model_config.mode == 'Supervised' and pt_model_config.loss_function != 'MAE':
            logger.info(
                f"Backend override: supervised mode requires MAE loss "
                f"(was {pt_model_config.loss_function}), forcing loss_function='MAE'"
            )
            # Create new ModelConfig with corrected loss_function
            from dataclasses import replace
            pt_model_config = replace(pt_model_config, loss_function='MAE')
    
        # Create minimal InferenceConfig for Lightning module (training payload doesn't include it)
        pt_inference_config = PTInferenceConfig()
    
        # B2.4: Instantiate PtychoPINN_Lightning model with factory-derived config objects
        model = PtychoPINN_Lightning(
            model_config=pt_model_config,
            data_config=pt_data_config,
            training_config=pt_training_config,
            inference_config=pt_inference_config
        )
    
        # Save hyperparameters so checkpoint can reconstruct module without external state
        model.save_hyperparameters()
    
        # B2.3: Build dataloaders via helper
        data_product = _build_lightning_dataloaders(
            train_container, test_container, config, payload = payload
        )
    
        # Data product is a lightning datamodule if ddp strategy selected, otherwise
        # it is a regular train/val loader tuple
        # Note: Use execution_config.strategy for runtime check, not pt_training_config.strategy
        effective_strategy = execution_config.strategy if execution_config else 'auto'
        if effective_strategy != 'ddp':
            train_loader, val_loader = data_product
        else:
            train_loader, val_loader = None, None  # Set to None when using datamodule
    
        # DATA-SUP-001: Supervised mode requires labeled data
        # Check if supervised mode is requested but training data lacks required labels
        if pt_model_config.mode == 'Supervised':
            # Inspect first batch to verify label keys exist
            try:
                first_batch = next(iter(train_loader))
                batch_dict = first_batch[0]  # Extract tensor dict from batch tuple
                if 'label_amp' not in batch_dict or 'label_phase' not in batch_dict:
                    raise RuntimeError(
                        f"Supervised mode (model_type='supervised') requires labeled datasets with "
                        f"'label_amp' and 'label_phase' keys, but training data lacks these fields. "
                        f"Either: (1) Use a labeled NPZ dataset (see ptycho_torch/notebooks/create_supervised_datasets.ipynb), "
                        f"or (2) Switch to PINN mode (--model_type pinn) for self-supervised physics-based training. "
                        f"See DATA-SUP-001 in docs/findings.md for details."
                    )
            except StopIteration:
                raise RuntimeError(
                    f"Training dataloader is empty. Check dataset path and n_groups configuration."
                )
    
        # B2.5: Configure Trainer with settings from config
        # C3.A3: Thread execution config values to Trainer kwargs
        output_dir = Path(getattr(config, 'output_dir', './outputs'))
        debug_mode = getattr(config, 'debug', False)
    
        # Import execution config defaults if not provided
        if execution_config is None:
            from ptycho.config.config import PyTorchExecutionConfig
            execution_config = PyTorchExecutionConfig()
            logger.info(f"PyTorchExecutionConfig auto-instantiated for Lightning training (accelerator resolved to '{execution_config.accelerator}')")
    
        # Custom callback to track loss history across epochs
        class _LossHistoryCallback(L.Callback):
            """Callback to collect train/val loss per epoch for history dict.
    
            The model logs metrics with dynamic names like 'poisson_train_Amp_loss'
            based on model configuration. This callback searches for any metric
            containing 'train' and 'loss' (or 'val' and 'loss') to capture the loss.
            """
    
            def __init__(self):
                self.train_loss = []
                self.val_loss = []
    
            def _find_loss_metric(self, metrics, prefix):
                """Find loss metric by prefix ('train' or 'val')."""
                for key in metrics:
                    if prefix in key and 'loss' in key:
                        return float(metrics[key])
                return None
    
            def on_train_epoch_end(self, trainer, pl_module):
                metrics = trainer.callback_metrics
                loss_val = self._find_loss_metric(metrics, 'train')
                if loss_val is not None:
                    self.train_loss.append(loss_val)
    
            def on_validation_epoch_end(self, trainer, pl_module):
                metrics = trainer.callback_metrics
                loss_val = self._find_loss_metric(metrics, 'val')
                if loss_val is not None:
                    self.val_loss.append(loss_val)
    
        loss_history_cb = _LossHistoryCallback()
    
        # EB1.D: Configure checkpoint/early-stop callbacks (ADR-003 Phase EB1)
        callbacks: list = [loss_history_cb]
        if execution_config.enable_checkpointing:
            from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping
    
            # Determine if we have validation data to use val metrics
            # Ptycho Datamodule automatically creates a validation dataset on instantiation (see train_utils.py)
            # so this means validation set exists if data product is a datamodule.
            has_validation = test_container is not None or isinstance(data_product, PrebuiltPtychoDataModule)
    
            # EB2.B: Derive monitor metric from model.val_loss_name (ADR-003 Phase EB2)
            # The model's val_loss_name is dynamically constructed based on model_type and loss configuration
            # (e.g., 'poisson_val_Amp_loss' for PINN with amplitude loss, 'mae_val_Phase_loss' for supervised)
            # This ensures checkpoint/early-stop callbacks watch the correct logged metric
            if has_validation and hasattr(model, 'val_loss_name'):
                # Use the model's dynamic validation loss name
                monitor_metric = model.val_loss_name
            else:
                # Fall back to execution config default or train loss
                monitor_metric = execution_config.checkpoint_monitor_metric
                if 'val_' in monitor_metric and not has_validation:
                    # Fall back to train_loss if val metric requested but no validation data
                    monitor_metric = monitor_metric.replace('val_', 'train_')
    
            # Build checkpoint filename template using dynamic metric name
            # Format: epoch={epoch:02d}-<metric_short_name>={<full_metric_name>:.4f}
            if has_validation:
                # Extract short name for filename (remove '_loss' suffix if present)
                metric_short_name = monitor_metric.replace('_loss', '')
                filename_template = f'epoch={{epoch:02d}}-{metric_short_name}={{{monitor_metric}:.4f}}'
            else:
                filename_template = 'epoch={epoch:02d}'
    
            checkpoint_callback = ModelCheckpoint(
                dirpath=str(output_dir / "checkpoints"),
                filename=filename_template,
                monitor=monitor_metric,
                mode=execution_config.checkpoint_mode,
                save_top_k=execution_config.checkpoint_save_top_k,
                save_last=True,  # Always keep last checkpoint for recovery
                verbose=False,
            )
            callbacks.append(checkpoint_callback)
    
            # EarlyStopping callback (ADR-003 Phase EB1.D)
            # Only add early stopping if validation data is available (otherwise no metric to monitor)
            if has_validation:
                early_stop_callback = EarlyStopping(
                    monitor=monitor_metric,
                    mode=execution_config.checkpoint_mode,
                    patience=execution_config.early_stop_patience,
                    verbose=False,
                )
                callbacks.append(early_stop_callback)
    
        # Instantiate logger based on execution config (Phase EB3.B - ADR-003)
        lightning_logger = False  # Default: no logger
        if execution_config.logger_backend is not None:
            try:
                if execution_config.logger_backend == 'csv':
                    from lightning.pytorch.loggers import CSVLogger
                    lightning_logger = CSVLogger(
                        save_dir=str(output_dir),
                        name='lightning_logs',
                    )
                    logger.info(f"Enabled CSVLogger: metrics saved to {output_dir}/lightning_logs/")
                elif execution_config.logger_backend == 'tensorboard':
                    from lightning.pytorch.loggers import TensorBoardLogger
                    lightning_logger = TensorBoardLogger(
                        save_dir=str(output_dir),
                        name='lightning_logs',
                    )
                    logger.info(f"Enabled TensorBoardLogger: run `tensorboard --logdir={output_dir}/lightning_logs/`")
                elif execution_config.logger_backend == 'mlflow':
                    from lightning.pytorch.loggers import MLFlowLogger
                    lightning_logger = MLFlowLogger(
                        experiment_name=getattr(config, 'experiment_name', 'PtychoPINN'),
                        tracking_uri=str(output_dir / 'mlruns'),
                    )
                    logger.info(f"Enabled MLFlowLogger: tracking URI={output_dir}/mlruns")
                else:
                    logger.warning(
                        f"Unknown logger_backend '{execution_config.logger_backend}'. "
                        f"Falling back to logger=False. Supported: 'csv', 'tensorboard', 'mlflow'."
                    )
            except ImportError as e:
                logger.warning(
                    f"Failed to import Lightning logger '{execution_config.logger_backend}': {e}. "
                    f"Metrics logging disabled. Install the required package to enable logging."
                )
                lightning_logger = False
        else:
            logger.info("Logger disabled (logger_backend=None). Loss metrics will not be saved to disk.")
    
        # EXEC-ACCUM-001: Guard against manual optimization + gradient accumulation
        # Lightning's manual optimization (automatic_optimization=False) is incompatible with
        # Trainer(accumulate_grad_batches>1). The PtychoPINN_Lightning module uses manual optimization
        # for custom physics loss integration, so gradient accumulation must be disabled.
>       if not model.automatic_optimization and execution_config.accum_steps > 1:
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'StubLightningModule' object has no attribute 'automatic_optimization'

ptycho_torch/workflows/components.py:979: AttributeError
---------------------------- Captured stdout setup -----------------------------
diff3d shape: (10, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (10,)
xcoords shape: (10,)
ycoords shape: (10,)
xcoords_start shape: (10,)
ycoords_start shape: (10,)
----------------------------- Captured stderr call -----------------------------
INFO: Seed set to 42
------------------------------ Captured log call -------------------------------
INFO     lightning.fabric.utilities.seed:seed.py:57 Seed set to 42
_______ TestLightningExecutionConfig.test_trainer_receives_accumulation ________

self = <test_workflows_components.TestLightningExecutionConfig object at 0x7d53fea62590>
minimal_training_config_with_val = TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', architecture='cnn', amp_activ.../test_trainer_receives_accumula0/outputs'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7d52b04ebf90>

    def test_trainer_receives_accumulation(self, minimal_training_config_with_val, monkeypatch):
        """
        RED Test: Verify Lightning Trainer receives accumulate_grad_batches from execution config.
    
        Expected RED Failure:
        - Trainer not receiving accum_steps from execution config
        OR
        - accumulate_grad_batches not passed to Trainer kwargs
    
        Resolution (GREEN):
        - _train_with_lightning should pass execution_config.accum_steps to Trainer(accumulate_grad_batches=...)
        """
        from ptycho.config.config import PyTorchExecutionConfig
        from unittest.mock import patch, MagicMock
    
        try:
            import lightning.pytorch as L
        except ImportError:
            pytest.skip("Lightning not available")
    
        # Create execution config with custom accumulation
        exec_config = PyTorchExecutionConfig(
            accum_steps=4,  # Override default (1)
            accelerator='cpu',
            deterministic=True,
            num_workers=0,
            enable_checkpointing=False,  # Disable callbacks for simpler mocking
        )
    
        # Mock Trainer to spy on kwargs
        mock_trainer_cls = MagicMock(spec=L.Trainer)
        mock_trainer_instance = MagicMock()
        mock_trainer_cls.return_value = mock_trainer_instance
    
        # Mock data containers
        mock_train_container = MagicMock()
        mock_test_container = MagicMock()
    
        from ptycho_torch.workflows.components import _train_with_lightning
    
        # Patch Trainer at import site
        with patch('lightning.pytorch.Trainer', mock_trainer_cls):
            try:
                _train_with_lightning(
                    train_container=mock_train_container,
                    test_container=mock_test_container,
                    config=minimal_training_config_with_val,
                    execution_config=exec_config,
                )
            except Exception:
                pass  # May fail during training; we only care about Trainer instantiation
    
        # GREEN Phase Assertion:
        # Trainer should receive accumulate_grad_batches from execution_config.accum_steps
>       assert mock_trainer_cls.called, "Trainer not instantiated"
E       AssertionError: Trainer not instantiated
E       assert False
E        +  where False = <MagicMock spec='Trainer' id='137798658496272'>.called

tests/torch/test_workflows_components.py:3137: AssertionError
----------------------------- Captured stdout call -----------------------------
Decoder Block 1: No attention module added.
Decoder Block 2: No attention module added.
Decoder Block 1: No attention module added.
Decoder Block 2: No attention module added.
----------------------------- Captured stderr call -----------------------------
INFO: Seed set to 42
------------------------------ Captured log call -------------------------------
INFO     lightning.fabric.utilities.seed:seed.py:57 Seed set to 42
=============================== warnings summary ===============================
tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_num_workers_flag_roundtrip
  /home/ollie/Documents/tmp/PtychoPINN/ptycho_torch/train.py:801: UserWarning: Deterministic mode with num_workers=4 may cause performance degradation. Consider setting --num-workers 0 for reproducibility.
    execution_config = build_execution_config_from_args(args, mode='training')

tests/torch/test_cli_train_torch.py::TestExecutionConfigCLI::test_bundle_persistence
  /home/ollie/Documents/tmp/PtychoPINN/ptycho_torch/config_factory.py:553: UserWarning: Error reading probeGuess from /tmp/pytest-of-ollie/pytest-184/test_bundle_persistence0/train.npz: No data left in file. Using fallback N=64.
    warnings.warn(

tests/torch/test_cli_train_torch.py: 1 warning
tests/torch/test_config_factory.py: 22 warnings
tests/torch/test_fno_lightning_integration.py: 1 warning
tests/torch/test_patch_stats_cli.py: 2 warnings
tests/torch/test_workflows_components.py: 12 warnings
  /home/ollie/Documents/tmp/PtychoPINN/ptycho_torch/config_factory.py:255: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    tf_training_config = to_training_config(

tests/torch/test_cli_train_torch.py: 1 warning
tests/torch/test_config_factory.py: 25 warnings
tests/torch/test_fno_lightning_integration.py: 1 warning
tests/torch/test_patch_stats_cli.py: 2 warnings
tests/torch/test_workflows_components.py: 12 warnings
  /home/ollie/Documents/tmp/PtychoPINN/ptycho_torch/config_factory.py:614: UserWarning: params.cfg already populated. Set force=True to overwrite existing values.
    warnings.warn(

tests/torch/test_config_bridge.py::TestConfigBridgeMVP::test_mvp_config_bridge_populates_params_cfg
  /home/ollie/Documents/tmp/PtychoPINN/tests/torch/test_config_bridge.py:100: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    spec_train = config_bridge.to_training_config(

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_direct_fields[batch_size-direct]
  /home/ollie/Documents/tmp/PtychoPINN/tests/torch/test_config_bridge.py:235: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    tf_train = config_bridge.to_training_config(

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_transform_fields[nepochs-rename]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_transform_fields[nll_weight-true]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_transform_fields[nll_weight-false]
  /home/ollie/Documents/tmp/PtychoPINN/tests/torch/test_config_bridge.py:298: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    tf_train = config_bridge.to_training_config(

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[mae_weight-default]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[mae_weight-override]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[realspace_mae_weight-default]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[realspace_weight-default]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[positions_provided-default]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[probe_trainable-default]
tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_override_fields[sequential_sampling-default]
  /home/ollie/Documents/tmp/PtychoPINN/tests/torch/test_config_bridge.py:367: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    tf_train = config_bridge.to_training_config(

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_default_divergence_detection[nphotons-divergence]
  /home/ollie/Documents/tmp/PtychoPINN/tests/torch/test_config_bridge.py:494: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    tf_train = config_bridge.to_training_config(

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_train_data_file_required_error
  /home/ollie/Documents/tmp/PtychoPINN/tests/torch/test_config_bridge.py:576: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    config_bridge.to_training_config(

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_nphotons_override_passes_validation
  /home/ollie/Documents/tmp/PtychoPINN/tests/torch/test_config_bridge.py:667: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    tf_train = config_bridge.to_training_config(

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_n_subsample_missing_override_uses_none
  /home/ollie/Documents/tmp/PtychoPINN/tests/torch/test_config_bridge.py:708: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    tf_train = config_bridge.to_training_config(

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_n_subsample_explicit_override
  /home/ollie/Documents/tmp/PtychoPINN/tests/torch/test_config_bridge.py:743: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    tf_train = config_bridge.to_training_config(

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_subsample_seed_from_dataconfig
  /home/ollie/Documents/tmp/PtychoPINN/tests/torch/test_config_bridge.py:854: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    tf_train = config_bridge.to_training_config(

tests/torch/test_config_bridge.py::TestConfigBridgeParity::test_training_config_subsample_seed_override
  /home/ollie/Documents/tmp/PtychoPINN/tests/torch/test_config_bridge.py:883: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
    tf_train = config_bridge.to_training_config(

tests/torch/test_config_factory.py::TestProbeSizeInference::test_infer_probe_size_missing_file_fallback
  /home/ollie/Documents/tmp/PtychoPINN/ptycho_torch/config_factory.py:547: UserWarning: Data file /nonexistent/data.npz not found. Using fallback N=64.
    warnings.warn(

tests/torch/test_fno_lightning_integration.py::test_train_history_collects_epochs
  /home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.

tests/torch/test_fno_lightning_integration.py::test_train_history_collects_epochs
tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_lightning_training_respects_gridsize
tests/torch/test_workflows_components.py::TestTrainWithLightningRed::test_train_with_lightning_instantiates_module
  /home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.

tests/torch/test_fno_lightning_integration.py::test_train_history_collects_epochs
  /home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/core/module.py:520: You called `self.log('poisson_train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`

tests/torch/test_lightning_checkpoint.py::TestLightningCheckpointSerialization::test_checkpoint_contains_hyperparameters
tests/torch/test_lightning_checkpoint.py::TestLightningCheckpointSerialization::test_load_from_checkpoint_without_kwargs
tests/torch/test_lightning_checkpoint.py::TestLightningCheckpointSerialization::test_checkpoint_configs_are_serializable
  /home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.

tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_lightning_training_respects_gridsize
tests/torch/test_workflows_components.py::TestTrainWithLightningRed::test_train_with_lightning_instantiates_module
  /home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:751: Checkpoint directory /home/ollie/Documents/tmp/PtychoPINN/training_outputs/checkpoints exists and is not empty.

tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_lightning_training_respects_gridsize
  /home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:467: `ModelCheckpoint(monitor='train_loss')` could not find the monitored key in the returned metrics: ['epoch', 'step']. HINT: Did you call `log('train_loss', value)` in the `LightningModule`?

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
SKIPPED [1] tests/torch/test_execution_config_defaults.py:162: Test requires CPU-only host to verify fallback warning
SKIPPED [1] tests/torch/test_fixture_pytorch_integration.py:73: Fixture not generated yet: /home/ollie/Documents/tmp/PtychoPINN/tests/fixtures/pytorch_integration/minimal_dataset_v1.npz. Expected during TDD RED phase (B2.B). Run scripts/tools/make_pytorch_integration_fixture.py to generate.
SKIPPED [1] tests/torch/test_fixture_pytorch_integration.py:95: Fixture not generated: /home/ollie/Documents/tmp/PtychoPINN/tests/fixtures/pytorch_integration/minimal_dataset_v1.npz
SKIPPED [1] tests/torch/test_fixture_pytorch_integration.py:157: Fixture not generated: /home/ollie/Documents/tmp/PtychoPINN/tests/fixtures/pytorch_integration/minimal_dataset_v1.npz
SKIPPED [1] tests/torch/test_fixture_pytorch_integration.py:180: Fixture or metadata missing
SKIPPED [1] tests/torch/test_fixture_pytorch_integration.py:224: Fixture not generated: /home/ollie/Documents/tmp/PtychoPINN/tests/fixtures/pytorch_integration/minimal_dataset_v1.npz
SKIPPED [1] tests/torch/test_fixture_pytorch_integration.py:267: Fixture not generated: /home/ollie/Documents/tmp/PtychoPINN/tests/fixtures/pytorch_integration/minimal_dataset_v1.npz
SKIPPED [1] tests/torch/test_fixture_pytorch_integration.py:303: Fixture not generated: /home/ollie/Documents/tmp/PtychoPINN/tests/fixtures/pytorch_integration/minimal_dataset_v1.npz
SKIPPED [1] tests/torch/test_fno_integration.py:97: neuraloperator not installed
SKIPPED [2] tests/torch/test_integration_workflow_torch.py: Migrated to pytest-native test_run_pytorch_train_save_load_infer
SKIPPED [1] tests/torch/test_tf_helper.py:64: torch tf_helper module not available
SKIPPED [1] tests/torch/test_tf_helper.py:57: torch tf_helper module not available
SKIPPED [1] tests/torch/test_tf_helper.py:74: torch tf_helper module not available - tests would fail
FAILED tests/torch/test_cli_inference_torch.py::TestInferenceCLI::test_accelerator_flag_roundtrip
FAILED tests/torch/test_cli_train_torch.py::TestPatchStatsCLI::test_patch_stats_dump
FAILED tests/torch/test_config_factory.py::TestTrainingPayloadStructure::test_gridsize_sets_channel_count
FAILED tests/torch/test_config_factory.py::TestConfigBridgeTranslation::test_epochs_to_nepochs_conversion
FAILED tests/torch/test_config_factory.py::TestConfigBridgeTranslation::test_k_to_neighbor_count_conversion
FAILED tests/torch/test_dataloader.py::TestDataloaderCanonicalKeySupport::test_backward_compat_legacy_diff3d
FAILED tests/torch/test_dataloader.py::TestDataloaderCanonicalKeySupport::test_loads_canonical_diffraction
FAILED tests/torch/test_dataloader.py::TestDataloaderFormatAutoTranspose::test_npz_headers_also_transposes_shape
FAILED tests/torch/test_integration_workflow_torch.py::test_bundle_loader_returns_modules
FAILED tests/torch/test_integration_workflow_torch.py::test_run_pytorch_train_save_load_infer
FAILED tests/torch/test_loss_modes.py::test_poisson_loss_mode_logs_poisson_metrics
FAILED tests/torch/test_loss_modes.py::test_mae_loss_mode_logs_mae_metrics_only
FAILED tests/torch/test_patch_stats_cli.py::TestPatchStatsCLI::test_patch_stats_flags_accepted
FAILED tests/torch/test_patch_stats_cli.py::TestPatchStatsCLI::test_patch_stats_default_disabled
FAILED tests/torch/test_patch_stats_cli.py::TestPatchStatsCLI::test_factory_creates_inference_config_with_patch_stats
FAILED tests/torch/test_patch_stats_cli.py::TestPatchStatsCLI::test_factory_inference_config_defaults
FAILED tests/torch/test_train_probe_size.py::TestNPZProbeSizeExtraction::test_infer_probe_size_128
FAILED tests/torch/test_train_probe_size.py::TestNPZProbeSizeExtraction::test_infer_probe_size_from_npz
FAILED tests/torch/test_train_probe_size.py::TestNPZProbeSizeExtraction::test_infer_probe_size_real_dataset
FAILED tests/torch/test_train_probe_size.py::TestNPZProbeSizeExtraction::test_infer_probe_size_rectangular
FAILED tests/torch/test_workflows_components.py::TestWorkflowsComponentsScaffold::test_run_cdi_example_calls_update_legacy_dict
FAILED tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_train_cdi_model_torch_invokes_lightning
FAILED tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_lightning_dataloader_tensor_dict_structure
FAILED tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_lightning_poisson_count_contract
FAILED tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_lightning_training_respects_gridsize
FAILED tests/torch/test_workflows_components.py::TestWorkflowsComponentsTraining::test_coords_relative_layout
FAILED tests/torch/test_workflows_components.py::TestWorkflowsComponentsRun::test_run_cdi_example_invokes_training
FAILED tests/torch/test_workflows_components.py::TestWorkflowsComponentsRun::test_run_cdi_example_persists_models
FAILED tests/torch/test_workflows_components.py::TestTrainWithLightningRed::test_train_with_lightning_runs_trainer_fit
FAILED tests/torch/test_workflows_components.py::TestTrainWithLightningRed::test_train_with_lightning_returns_models_dict
FAILED tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_reassemble_cdi_image_torch_flip_transpose_contract[False-False-False]
FAILED tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_reassemble_cdi_image_torch_flip_transpose_contract[True-False-False]
FAILED tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_reassemble_cdi_image_torch_flip_transpose_contract[False-True-False]
FAILED tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_reassemble_cdi_image_torch_flip_transpose_contract[False-False-True]
FAILED tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_reassemble_cdi_image_torch_flip_transpose_contract[True-True-True]
FAILED tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_run_cdi_example_torch_do_stitching_delegates_to_reassemble
FAILED tests/torch/test_workflows_components.py::TestReassembleCdiImageTorchGreen::test_reassemble_cdi_image_torch_return_contract
FAILED tests/torch/test_workflows_components.py::TestDecoderLastShapeParity::test_probe_big_shape_alignment
FAILED tests/torch/test_workflows_components.py::TestTrainWithLightningGreen::test_execution_config_overrides_trainer
FAILED tests/torch/test_workflows_components.py::TestTrainWithLightningGreen::test_execution_config_controls_determinism
FAILED tests/torch/test_workflows_components.py::TestLightningExecutionConfig::test_trainer_receives_accumulation
========== 41 failed, 223 passed, 14 skipped, 112 warnings in 48.96s ===========
