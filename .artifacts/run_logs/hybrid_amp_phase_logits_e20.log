INFO:__main__:Starting Torch grid-lines runner: arch=hybrid
INFO:__main__:Loading train data from outputs/grid_lines_gs1_n64_outmode/datasets/N64/gs1/train.npz
INFO:__main__:Loading test data from outputs/grid_lines_gs1_n64_outmode/datasets/N64/gs1/test.npz
INFO:__main__:Training hybrid model...
2026-01-27 15:13:47.088546: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1769555627.100310  931795 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1769555627.104215  931795 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1769555627.115388  931795 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1769555627.115402  931795 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1769555627.115404  931795 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1769555627.115406  931795 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2026-01-27 15:13:47.118279: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO:ptycho_torch.workflows.components:_train_with_lightning orchestrating Lightning training
INFO:ptycho_torch.workflows.components:Training config: nepochs=20, n_groups=512
/home/ollie/Documents/tmp/PtychoPINN/ptycho_torch/config_factory.py:259: UserWarning: test_data_file not provided in TrainingConfig overrides. Evaluation workflows require test_data_file to be set during inference update. Consider providing: overrides=dict(..., test_data_file=Path('test.npz'))
  tf_training_config = to_training_config(
/home/ollie/Documents/tmp/PtychoPINN/ptycho_torch/config_factory.py:618: UserWarning: params.cfg already populated. Set force=True to overwrite existing values.
  warnings.warn(
INFO:ptycho_torch.model:Overriding model_config.loss_function=Poisson to MAE to match torch_loss_mode=mae
INFO: Seed set to 42
INFO:lightning.fabric.utilities.seed:Seed set to 42
INFO:ptycho_torch.workflows.components:Enabled CSVLogger: metrics saved to training_outputs/lightning_logs/
INFO:ptycho_torch.workflows.components:Manual optimization enabled; disabling Lightning Trainer gradient_clip_val and relying on model-level gradient clipping.
INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True
INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs
INFO:ptycho_torch.workflows.components:Starting Lightning training: 20 epochs
INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:751: Checkpoint directory /home/ollie/Documents/tmp/PtychoPINN/training_outputs/checkpoints exists and is not empty.
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name  | Type       | Params | Mode 
---------------------------------------------
0 | model | PtychoPINN | 17.2 M | train
1 | Loss  | MAELoss    | 0      | train
---------------------------------------------
17.2 M    Trainable params
0         Non-trainable params
17.2 M    Total params
68.810    Total estimated model params size (MB)
60        Modules in train mode
0         Modules in eval mode
INFO:lightning.pytorch.callbacks.model_summary:
  | Name  | Type       | Params | Mode 
---------------------------------------------
0 | model | PtychoPINN | 17.2 M | train
1 | Loss  | MAELoss    | 0      | train
---------------------------------------------
17.2 M    Trainable params
0         Non-trainable params
17.2 M    Total params
68.810    Total estimated model params size (MB)
60        Modules in train mode
0         Modules in eval mode
/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.
/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.
INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=20` reached.
INFO:ptycho_torch.workflows.components:Lightning training complete
INFO:__main__:Running inference...
INFO:__main__:Computing metrics...
I0000 00:00:1769555845.580722  931795 gpu_process_state.cc:208] Using CUDA malloc Async allocator for GPU: 0
I0000 00:00:1769555845.581970  931795 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21709 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:04:00.0, compute capability: 8.6
/home/ollie/Documents/tmp/PtychoPINN/ptycho/FRC/fourier_ring_corr.py:38: RuntimeWarning: divide by zero encountered in divide
  FSC = abs(C)/np.sqrt(abs(np.multiply(C1,C2)))
INFO:__main__:Saved artifacts to outputs/grid_lines_gs1_n64_hybrid_amp_phase_logits_e20/runs/pinn_hybrid
INFO:__main__:Torch runner complete. Artifacts in outputs/grid_lines_gs1_n64_hybrid_amp_phase_logits_e20/runs/pinn_hybrid
DEBUG: Setting nimgs_test to 1 in params
DEBUG: Setting outer_offset_test to 20 in params
Amplitude normalization scale factor: 17.892750
mean scale adjustment: 1
mean scale adjustment: 1
DEBUG eval_reconstruction [pinn_hybrid]: amp_target stats: mean=2.889508, std=0.641223, shape=(326, 326, 1)
DEBUG eval_reconstruction [pinn_hybrid]: amp_pred stats: mean=0.161490, std=0.000000, shape=(326, 326, 1)
DEBUG eval_reconstruction [pinn_hybrid]: phi_target stats: mean=0.000000, std=0.000000, shape=(326, 326)
DEBUG eval_reconstruction [pinn_hybrid]: phi_pred stats: mean=-0.000000, std=0.000000, shape=(326, 326)
performed by index method
performed by index method
performed by index method
mean scale adjustment: 1
mean scale adjustment: 1
Phase preprocessing: plane-fitted range [0.000, 0.000] -> scaled range [0.500, 0.500]
{
  "architecture": "hybrid",
  "run_dir": "outputs/grid_lines_gs1_n64_hybrid_amp_phase_logits_e20/runs/pinn_hybrid",
  "metrics": {
    "mae": [
      "0.513058",
      1.6052561315158214e-15
    ],
    "mse": [
      "0.4111674",
      3.8609548535456835e-30
    ],
    "psnr": [
      51.99061718520853,
      341.333923806335
    ],
    "ssim": [
      0.2771137893998457,
      0.9999999999998624
    ],
    "ms_ssim": [
      0.10228907417169902,
      0.12324482476084282
    ],
    "frc50": [
      "2",
      NaN
    ],
    "frc": [
      "[1.         0.76383413 0.15907912 0.11700857 0.05955865 0.21308327\n 0.15496264 0.0714763  0.17464462 0.02918579 0.08453561 0.07299492\n 0.06845303 0.13935958 0.22317889 0.25448648 0.04292403 0.09680618\n 0.11771453 0.02979295 0.02094102 0.10060801 0.05367453 0.14611383\n 0.08338318 0.05975544 0.02273086 0.09857491 0.20162302 0.01948149\n 0.04913201 0.07729001 0.08296487 0.02318801 0.08995226 0.06788732\n 0.06151864 0.07227563 0.0478633  0.10371469 0.03946756 0.1842616\n 0.0815722  0.07395495 0.11824731 0.02804799 0.02874845 0.11764796\n 0.03340498 0.01533404 0.03240616 0.07656464 0.03716444 0.10223691\n 0.01372626 0.07077825 0.05044708 0.02318251 0.01480213 0.01454651\n 0.03216074 0.093161   0.04774577 0.07321516 0.11033801 0.01185915\n 0.09470427 0.1028076  0.07252825 0.02683727 0.00590803 0.04297549\n 0.10080148 0.0624296  0.06113964 0.13353531 0.11749529 0.09959696\n 0.0614716  0.02883733 0.00636467 0.03099465 0.08330889 0.06459715\n 0.08884953 0.11167525 0.16872307 0.0209713  0.07636647 0.08410422\n 0.04378147 0.01731977 0.07891111 0.0890705  0.09782207 0.17454652\n 0.10118539 0.0504926  0.0606041  0.02217999 0.03518279 0.03938607\n 0.06209293 0.05025231 0.15568548 0.22740469 0.03416547 0.1249931\n 0.06612772 0.09262289 0.04591487 0.03491311 0.12867487 0.15125736\n 0.14993038 0.13244374 0.12478961 0.0748474  0.07224788 0.10700077\n 0.0500864  0.13724991 0.01297996 0.1523055  0.15156939 0.20323776\n 0.02141419 0.21926019 0.17572337 0.17291908 0.17722386 0.02373667\n 0.03720212 0.04898156 0.18174122 0.1577418  0.13237386 0.19100942\n 0.19222822 0.14902636 0.11605326 0.14114891 0.15122368 0.16166577\n 0.0299884  0.20969772 0.10109426 0.14439376 0.11023767 0.22710054\n 0.20441326 0.14541823 0.07840537 0.20193699 0.15205404 0.22302783\n 0.12695204 0.23175841 0.12864593 0.21271633 0.14563325 0.21815069\n 0.19293462 0.16055247        inf        inf        inf        inf\n        inf        inf        inf        inf        inf        inf\n        inf        inf        inf        inf        inf        inf\n        inf        inf        inf        inf        inf        inf\n        inf        inf        inf        inf        inf        inf\n        inf        inf        inf        inf        inf        inf\n        inf        inf        inf        inf        inf        inf\n        inf        inf        inf        inf        inf        inf\n        inf        inf        inf        inf        inf        inf\n        inf        inf        inf        inf        inf        inf\n        inf        inf        inf        inf        inf        inf\n        inf        inf        inf        inf]",
      null
    ]
  },
  "history": {
    "train_loss": [
      0.2212618887424469,
      0.10133476555347443,
      0.08424456417560577,
      0.07785554975271225,
      0.06905189156532288,
      0.06576785445213318,
      0.056377097964286804,
      0.05987078696489334,
      0.0505753718316555,
      0.04687215015292168,
      0.042572587728500366,
      0.04251815378665924,
      0.027988366782665253,
      0.026113202795386314,
      0.024975795298814774,
      45.728553771972656,
      0.21062692999839783,
      0.20970377326011658,
      0.2082645446062088,
      0.20837487280368805
    ],
    "val_loss": [
      4.619941711425781,
      0.146124005317688,
      0.11794284731149673,
      0.10087194293737411,
      0.07893133908510208,
      0.108210988342762,
      0.07363852113485336,
      0.05644821748137474,
      0.06080726161599159,
      0.03560672700405121,
      0.07602638751268387,
      0.0428505465388298,
      0.03780674934387207,
      0.03269030526280403,
      0.02710990607738495,
      0.020934956148266792,
      0.23103347420692444,
      0.18141654133796692,
      0.18710720539093018,
      0.1881089061498642,
      0.17798279225826263
    ]
  },
  "recon_path": "outputs/grid_lines_gs1_n64_hybrid_amp_phase_logits_e20/recons/pinn_hybrid/recon.npz",
  "model_params": 17202404,
  "inference_time_s": 4.4049897328950465
}
