[2026-01-11 02:31:10] === Starting Complete Generalization Study ===
[2026-01-11 02:31:10] Training group sizes: 64
[2026-01-11 02:31:10] Training subsample sizes: 64
[2026-01-11 02:31:10] Test groups: 606
[2026-01-11 02:31:10] Test subsample: 2424
[2026-01-11 02:31:10] Number of trials per size: 1
[2026-01-11 02:31:10] Output directory: gs2_full_subsample_study_20260111_0230
[2026-01-11 02:31:10] Total training runs planned: 2
[2026-01-11 02:31:10] Validating environment...
PtychoPINN module found
[2026-01-11 02:31:10] Environment validation passed
[2026-01-11 02:31:10] Configuration saved to: gs2_full_subsample_study_20260111_0230/study_config.txt
[2026-01-11 02:31:10] Skipping dataset preparation (--skip-data-prep)
[2026-01-11 02:31:10] === STEP 2: Model Training ===
[2026-01-11 02:31:10] Training models sequentially with 1 trials per training size
[2026-01-11 02:31:10] Starting training with train_groups=64, train_subsample=64 (1 trials)
[2026-01-11 02:31:10] Training models for train_subsample=64, train_groups=64 (Trial 1/1)
[2026-01-11 02:31:10] EXECUTING: PtychoPINN training (subsample=64, groups=64, trial=1)
[2026-01-11 02:31:10] COMMAND: python scripts/training/train.py --config 'configs/comparison_config_gs2_quick.yaml' \
            --train_data_file 'prepare_1e4_photons_5k/dataset/train.npz' \
            --test_data_file 'prepare_1e4_photons_5k/dataset/test.npz' \
            --n_groups 64 --n_subsample 64 \
            --neighbor_count 7 \
            --output_dir 'gs2_full_subsample_study_20260111_0230/train_subsample_64_groups_64/trial_1/pinn_run' --gridsize 2
2026-01-11 02:31:11.121117: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1768127471.132375  916136 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1768127471.136062  916136 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1768127471.146007  916136 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1768127471.146016  916136 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1768127471.146018  916136 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1768127471.146019  916136 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2026-01-11 02:31:11.148768: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2026-01-11 02:31:13,991 - INFO - Configuration setup complete
2026-01-11 02:31:13,991 - INFO - Final configuration: TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', object_big=True, probe_big=True, probe_mask=False, pad_object=True, probe_scale=4.0, gaussian_smoothing_sigma=0.0), train_data_file=PosixPath('prepare_1e4_photons_5k/dataset/train.npz'), test_data_file=PosixPath('prepare_1e4_photons_5k/dataset/test.npz'), batch_size=16, nepochs=50, mae_weight=0.0, nll_weight=1.0, realspace_mae_weight=0.0, realspace_weight=0.0, nphotons=1000000000.0, n_groups=64, n_images=512, n_subsample=64, subsample_seed=None, neighbor_count=7, enable_oversampling=False, neighbor_pool_size=None, positions_provided=True, probe_trainable=False, intensity_scale_trainable=True, output_dir=PosixPath('gs2_full_subsample_study_20260111_0230/train_subsample_64_groups_64/trial_1/pinn_run'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
2026-01-11 02:31:13,991 - INFO - Independent sampling control: subsampling 64 images, creating 64 groups (approx 256 patterns from groups)
2026-01-11 02:31:13,991 - WARNING - n_subsample (64) may be too small to create 64 groups of size 2Â². Consider increasing n_subsample to at least 256
2026-01-11 02:31:13,992 - INFO - Starting training with n_subsample=64, n_groups=64, stitching=disabled
2026-01-11 02:31:13,992 - INFO - Loading data from prepare_1e4_photons_5k/dataset/train.npz with n_images=64, n_subsample=64
2026-01-11 02:31:14,307 - INFO - Independent sampling: subsampling 64 images from 2576 total
2026-01-11 02:31:14,307 - INFO - Randomly subsampled 64 images
diff3d shape: (64, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (64,)
objectGuess shape: (232, 232)
xcoords shape: (64,)
ycoords shape: (64,)
xcoords_start shape: (64,)
ycoords_start shape: (64,)
2026-01-11 02:31:14,623 - INFO - Overriding nphotons from config (1.0e+09) with value from dataset metadata: 1.0e+04
2026-01-11 02:31:14,623 - INFO - Loading data from prepare_1e4_photons_5k/dataset/test.npz with n_images=None, n_subsample=None
2026-01-11 02:31:14,918 - INFO - Using full dataset of 2424 images
diff3d shape: (2424, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (2424,)
objectGuess shape: (232, 232)
xcoords shape: (2424,)
ycoords shape: (2424,)
xcoords_start shape: (2424,)
ycoords_start shape: (2424,)
2026-01-11 02:31:14,934 - INFO - Loaded test data from prepare_1e4_photons_5k/dataset/test.npz
2026-01-11 02:31:14,934 - INFO - Backend dispatcher: params.cfg synchronized with TrainingConfig (backend=tensorflow)
2026-01-11 02:31:14,934 - INFO - Backend dispatcher: routing to TensorFlow workflow (ptycho.workflows.components)
2026-01-11 02:31:14,935 - INFO - [OVERSAMPLING DEBUG] generate_grouped_data called with: nsamples=64, n_points=64, C=4, K=7
2026-01-11 02:31:14,935 - INFO - [OVERSAMPLING DEBUG] Parameters: gridsize=2, N=64, sequential_sampling=False
2026-01-11 02:31:14,935 - INFO - [OVERSAMPLING DEBUG] Oversampling flags: enable_oversampling=False, neighbor_pool_size=None, effective_K=7
2026-01-11 02:31:14,935 - INFO - [OVERSAMPLING DEBUG] Oversampling check: nsamples > n_points = 64 > 64 = False
2026-01-11 02:31:14,935 - INFO - [OVERSAMPLING DEBUG] Oversampling check: C > 1 = 4 > 1 = True
2026-01-11 02:31:14,935 - INFO - [OVERSAMPLING DEBUG] needs_oversampling = False
DEBUG: nsamples: 64, gridsize: 2 (using efficient random sample-then-group strategy)
2026-01-11 02:31:14,935 - INFO - Using efficient random sampling strategy for gridsize=2
2026-01-11 02:31:14,935 - INFO - [OVERSAMPLING DEBUG] Taking efficient branch: standard sample-then-group
2026-01-11 02:31:14,935 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently called with: nsamples=64, K=7, C=4
2026-01-11 02:31:14,935 - INFO - Generating 64 groups efficiently from 64 points (K=7, C=4)
2026-01-11 02:31:14,935 - INFO - [OVERSAMPLING DEBUG] Standard case: using 64 groups from 64 points
2026-01-11 02:31:14,935 - INFO - [OVERSAMPLING DEBUG] Using all 64 points as seeds (no sampling needed)
2026-01-11 02:31:14,935 - INFO - Using all 64 points as seeds
2026-01-11 02:31:14,936 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently completed: generated 64 groups
2026-01-11 02:31:14,936 - INFO - Successfully generated 64 groups with shape (64, 4)
2026-01-11 02:31:14,936 - INFO - [OVERSAMPLING DEBUG] Generated 64 groups in total
2026-01-11 02:31:14,936 - INFO - Generated 64 groups efficiently
I0000 00:00:1768127475.065920  916136 gpu_process_state.cc:208] Using CUDA malloc Async allocator for GPU: 0
I0000 00:00:1768127475.067165  916136 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22259 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:04:00.0, compute capability: 8.6
INFO: Using pre-computed 'Y' array from the input file.
neighbor-sampled diffraction shape (64, 64, 64, 4)
loader: using provided ground truth patches.
INFO: None
<PtychoDataContainer X=(64, 64, 64, 4) mean=0.406 Y_I=(64, 64, 64, 4) mean=0.564 Y_phi=(64, 64, 64, 4) mean=-0.051 coords_nominal=(64, 1, 2, 4) mean=0.000 coords_true=(64, 1, 2, 4) mean=0.000 probe=(64, 64) mean_amplitude=0.006 norm_Y_I=<scalar> nn_indices=(64, 4) global_offsets=(64, 1, 2, 1) local_offsets=(64, 1, 2, 4)>
2026-01-11 02:31:15,384 - INFO - [OVERSAMPLING DEBUG] generate_grouped_data called with: nsamples=64, n_points=2424, C=4, K=7
2026-01-11 02:31:15,384 - INFO - [OVERSAMPLING DEBUG] Parameters: gridsize=2, N=64, sequential_sampling=False
2026-01-11 02:31:15,384 - INFO - [OVERSAMPLING DEBUG] Oversampling flags: enable_oversampling=False, neighbor_pool_size=None, effective_K=7
2026-01-11 02:31:15,384 - INFO - [OVERSAMPLING DEBUG] Oversampling check: nsamples > n_points = 64 > 2424 = False
2026-01-11 02:31:15,384 - INFO - [OVERSAMPLING DEBUG] Oversampling check: C > 1 = 4 > 1 = True
2026-01-11 02:31:15,384 - INFO - [OVERSAMPLING DEBUG] needs_oversampling = False
DEBUG: nsamples: 64, gridsize: 2 (using efficient random sample-then-group strategy)
2026-01-11 02:31:15,384 - INFO - Using efficient random sampling strategy for gridsize=2
2026-01-11 02:31:15,384 - INFO - [OVERSAMPLING DEBUG] Taking efficient branch: standard sample-then-group
2026-01-11 02:31:15,384 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently called with: nsamples=64, K=7, C=4
2026-01-11 02:31:15,384 - INFO - Generating 64 groups efficiently from 2424 points (K=7, C=4)
2026-01-11 02:31:15,384 - INFO - [OVERSAMPLING DEBUG] Standard case: using 64 groups from 2424 points
2026-01-11 02:31:15,384 - INFO - [OVERSAMPLING DEBUG] Randomly sampled 64 seed points
2026-01-11 02:31:15,384 - INFO - Sampled 64 seed points from 2424 total points
2026-01-11 02:31:15,385 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently completed: generated 64 groups
2026-01-11 02:31:15,385 - INFO - Successfully generated 64 groups with shape (64, 4)
2026-01-11 02:31:15,385 - INFO - [OVERSAMPLING DEBUG] Generated 64 groups in total
2026-01-11 02:31:15,385 - INFO - Generated 64 groups efficiently
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1768127476.810631  916136 service.cc:152] XLA service 0x14b1e2a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1768127476.810651  916136 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6
2026-01-11 02:31:16.825668: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
I0000 00:00:1768127476.842325  916136 cuda_dnn.cc:529] Loaded cuDNN version 91002
I0000 00:00:1768127477.059874  916136 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
INFO: Using pre-computed 'Y' array from the input file.
neighbor-sampled diffraction shape (64, 64, 64, 4)
loader: using provided ground truth patches.
INFO: None
<PtychoDataContainer X=(64, 64, 64, 4) mean=0.407 Y_I=(64, 64, 64, 4) mean=0.563 Y_phi=(64, 64, 64, 4) mean=-0.035 coords_nominal=(64, 1, 2, 4) mean=-0.000 coords_true=(64, 1, 2, 4) mean=-0.000 probe=(64, 64) mean_amplitude=0.006 norm_Y_I=<scalar> nn_indices=(64, 4) global_offsets=(64, 1, 2, 1) local_offsets=(64, 1, 2, 4)>
DEBUG: Setting probe to tf.Tensor(
[[[ 0.13510734-0.08366051j]
  [-0.08103182-0.0294797j ]
  [ 0.0637618 +0.02550233j]
  ...
  [-0.05742923+0.10465012j]
  [ 0.061039  +0.02731375j]
  [-0.07001488-0.0336627j ]]

 [[-0.08103182-0.0294797j ]
  [ 0.11364837-0.04388378j]
  [-0.08643168+0.03117809j]
  ...
  [ 0.02872955+0.03712314j]
  [-0.08476108+0.03176096j]
  [ 0.10533146-0.04452544j]]

 [[ 0.0637618 +0.02550233j]
  [-0.08643168+0.03117809j]
  [ 0.06238977-0.04183936j]
  ...
  [-0.01890127-0.04941669j]
  [ 0.06107059-0.04259919j]
  [-0.07948506+0.03188787j]]

 ...

 [[-0.05742923+0.10465012j]
  [ 0.02872955+0.03712314j]
  [-0.01890127-0.04941669j]
  ...
  [-0.00577373-0.08714116j]
  [-0.01781569-0.0517069j ]
  [ 0.02285527+0.04448525j]]

 [[ 0.061039  +0.02731375j]
  [-0.08476108+0.03176096j]
  [ 0.06107059-0.04259919j]
  ...
  [-0.01781569-0.0517069j ]
  [ 0.05980607-0.0434007j ]
  [-0.07803912+0.03257083j]]

 [[-0.07001488-0.0336627j ]
  [ 0.10533146-0.04452544j]
  [-0.07948506+0.03188787j]
  ...
  [ 0.02285527+0.04448525j]
  [-0.07803912+0.03257083j]
  [ 0.09786177-0.04529788j]]], shape=(64, 64, 1), dtype=complex64) in params
DEBUG: Setting intensity_scale to 3.125 in params
DEBUG: Setting probe to tf.Tensor(
[[[ 0.13510734-0.08366051j]
  [-0.08103182-0.0294797j ]
  [ 0.0637618 +0.02550233j]
  ...
  [-0.05742923+0.10465012j]
  [ 0.061039  +0.02731375j]
  [-0.07001488-0.0336627j ]]

 [[-0.08103182-0.0294797j ]
  [ 0.11364837-0.04388378j]
  [-0.08643168+0.03117809j]
  ...
  [ 0.02872955+0.03712314j]
  [-0.08476108+0.03176096j]
  [ 0.10533146-0.04452544j]]

 [[ 0.0637618 +0.02550233j]
  [-0.08643168+0.03117809j]
  [ 0.06238977-0.04183936j]
  ...
  [-0.01890127-0.04941669j]
  [ 0.06107059-0.04259919j]
  [-0.07948506+0.03188787j]]

 ...

 [[-0.05742923+0.10465012j]
  [ 0.02872955+0.03712314j]
  [-0.01890127-0.04941669j]
  ...
  [-0.00577373-0.08714116j]
  [-0.01781569-0.0517069j ]
  [ 0.02285527+0.04448525j]]

 [[ 0.061039  +0.02731375j]
  [-0.08476108+0.03176096j]
  [ 0.06107059-0.04259919j]
  ...
  [-0.01781569-0.0517069j ]
  [ 0.05980607-0.0434007j ]
  [-0.07803912+0.03257083j]]

 [[-0.07001488-0.0336627j ]
  [ 0.10533146-0.04452544j]
  [-0.07948506+0.03188787j]
  ...
  [ 0.02285527+0.04448525j]
  [-0.07803912+0.03257083j]
  [ 0.09786177-0.04529788j]]], shape=(64, 64, 1), dtype=complex64) in params
Current Parameters:
--------------------
N: 64
amp_activation: sigmoid
backend: tensorflow
batch_size: 16
bigN: 68
big_gridsize: 10
data_source: generic
debug: True
default_probe_scale: 0.7
enable_oversampling: False
gaussian_smoothing_sigma: 0.0
gridsize: 2
h5_path: wts.h5
intensity_scale: 3.125
intensity_scale.trainable: True
label: 
mae_weight: 0.0
max_position_jitter: 10
model_type: pinn
n_filters_scale: 2
n_groups: 64
n_images: 512
n_subsample: 64
neighbor_count: 7
nepochs: 50
nimgs_test: 3
nimgs_train: 9
nll_weight: 1.0
nphotons: 10000.0
npseed: 42
object.big: True
offset: 4
outer_offset_test: None
outer_offset_train: None
output_prefix: gs2_full_subsample_study_20260111_0230/train_subsample_64_groups_64/trial_1/pinn_run
pad_object: True
positions.provided: True
probe:
  shape: (64, 64, 1)
  mean: -0.000-0.000j
  std: 0.627
  min: -2.156-0.387j
  max: 2.182-0.308j
probe.big: True
probe.mask: False
probe.trainable: False
probe_scale: 4.0
realspace_mae_weight: 0.0
realspace_weight: 0.0
sequential_sampling: False
set_phi: False
sim_jitter_scale: 0.0
size: 392
test_data_file_path: prepare_1e4_photons_5k/dataset/test.npz
torch_loss_mode: poisson
train_data_file_path: prepare_1e4_photons_5k/dataset/train.npz
tv_weight: 0.0
use_xla_translate: True
DEBUG _flat_to_channel: gridsize from global params: 2
DEBUG _flat_to_channel: N from global params: 64
DEBUG _flat_to_channel: input shape=(256, 64, 64, 1), reshaping to (-1, 4, 64, 64)
Epoch 1/50
DEBUG _flat_to_channel: gridsize from parameter: 2
DEBUG _flat_to_channel: N from parameter: 78
DEBUG _flat_to_channel: input shape=(None, 78, 78, 1), reshaping to (-1, 4, 78, 78)
DEBUG _flat_to_channel: gridsize from parameter: 2
DEBUG _flat_to_channel: N from parameter: 78
DEBUG _flat_to_channel: input shape=(None, 78, 78, 1), reshaping to (-1, 4, 78, 78)
DEBUG _flat_to_channel: gridsize from parameter: 2
DEBUG _flat_to_channel: N from parameter: 78
DEBUG _flat_to_channel: input shape=(None, 78, 78, 1), reshaping to (-1, 4, 78, 78)
DEBUG _flat_to_channel: gridsize from parameter: 2
DEBUG _flat_to_channel: N from parameter: 78
DEBUG _flat_to_channel: input shape=(None, 78, 78, 1), reshaping to (-1, 4, 78, 78)
input shape (None, 64, 64, 1)
DEBUG _flat_to_channel: gridsize from parameter: 2
DEBUG _flat_to_channel: N from parameter: 64
DEBUG _flat_to_channel: input shape=(None, 64, 64, 1), reshaping to (-1, 4, 64, 64)
2026-01-11 02:31:17,846 - WARNING - You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.
DEBUG _flat_to_channel: gridsize from parameter: 2
DEBUG _flat_to_channel: N from parameter: 78
DEBUG _flat_to_channel: input shape=(None, 78, 78, 1), reshaping to (-1, 4, 78, 78)
DEBUG _flat_to_channel: gridsize from parameter: 2
DEBUG _flat_to_channel: N from parameter: 78
DEBUG _flat_to_channel: input shape=(None, 78, 78, 1), reshaping to (-1, 4, 78, 78)
DEBUG _flat_to_channel: gridsize from parameter: 2
DEBUG _flat_to_channel: N from parameter: 78
DEBUG _flat_to_channel: input shape=(None, 78, 78, 1), reshaping to (-1, 4, 78, 78)
DEBUG _flat_to_channel: gridsize from parameter: 2
DEBUG _flat_to_channel: N from parameter: 78
DEBUG _flat_to_channel: input shape=(None, 78, 78, 1), reshaping to (-1, 4, 78, 78)
input shape (None, 64, 64, 1)
DEBUG _flat_to_channel: gridsize from parameter: 2
DEBUG _flat_to_channel: N from parameter: 64
DEBUG _flat_to_channel: input shape=(None, 64, 64, 1), reshaping to (-1, 4, 64, 64)
2026-01-11 02:31:21,484 - WARNING - You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.
2026-01-11 02:31:23.604982: W tensorflow/core/common_runtime/type_inference.cc:340] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:
type_id: TFT_OPTIONAL
args {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_TENSOR
    args {
      type_id: TFT_INT32
    }
  }
}
 is neither a subtype nor a supertype of the combined inputs preceding it:
type_id: TFT_OPTIONAL
args {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_TENSOR
    args {
      type_id: TFT_FLOAT
    }
  }
}

	for Tuple type infernce function 0
	while inferring type of node 'StatefulPartitionedCall/functional_1/padded_obj_2_1/cond/output/_167'
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m35s[0m 12s/step - intensity_scaler_inv_loss: 0.6419 - loss: 0.5825 - pred_intensity_loss: 0.5825 - trimmed_obj_loss: 0.0000e+00[1m3/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 25ms/step - intensity_scaler_inv_loss: 2.0795 - loss: 22.1405 - pred_intensity_loss: 22.1405 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 381ms/step - intensity_scaler_inv_loss: 2.0472 - loss: 21.9302 - pred_intensity_loss: 21.6081 - trimmed_obj_loss: 0.0000e+00DEBUG _flat_to_channel: gridsize from parameter: 2
DEBUG _flat_to_channel: N from parameter: 78
DEBUG _flat_to_channel: input shape=(None, 78, 78, 1), reshaping to (-1, 4, 78, 78)
DEBUG _flat_to_channel: gridsize from parameter: 2
DEBUG _flat_to_channel: N from parameter: 78
DEBUG _flat_to_channel: input shape=(None, 78, 78, 1), reshaping to (-1, 4, 78, 78)
DEBUG _flat_to_channel: gridsize from parameter: 2
DEBUG _flat_to_channel: N from parameter: 78
DEBUG _flat_to_channel: input shape=(None, 78, 78, 1), reshaping to (-1, 4, 78, 78)
DEBUG _flat_to_channel: gridsize from parameter: 2
DEBUG _flat_to_channel: N from parameter: 78
DEBUG _flat_to_channel: input shape=(None, 78, 78, 1), reshaping to (-1, 4, 78, 78)
input shape (None, 64, 64, 1)
DEBUG _flat_to_channel: gridsize from parameter: 2
DEBUG _flat_to_channel: N from parameter: 64
DEBUG _flat_to_channel: input shape=(None, 64, 64, 1), reshaping to (-1, 4, 64, 64)
2026-01-11 02:31:30,712 - WARNING - You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.
[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m14s[0m 899ms/step - intensity_scaler_inv_loss: 1.9503 - loss: 21.2994 - pred_intensity_loss: 20.0111 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.7100 - val_loss: 0.7174 - val_pred_intensity_loss: 0.7174 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 2/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 40ms/step - intensity_scaler_inv_loss: 0.7026 - loss: 0.7158 - pred_intensity_loss: 0.7158 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 21ms/step - intensity_scaler_inv_loss: 0.7193 - loss: 0.7370 - pred_intensity_loss: 0.7322 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step - intensity_scaler_inv_loss: 0.7145 - loss: 0.6841 - pred_intensity_loss: 0.6648 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.7812 - val_loss: 0.4433 - val_pred_intensity_loss: 0.4433 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 3/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 39ms/step - intensity_scaler_inv_loss: 0.8100 - loss: 0.5356 - pred_intensity_loss: 0.5356 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 20ms/step - intensity_scaler_inv_loss: 0.6930 - loss: 0.2827 - pred_intensity_loss: 0.2830 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step - intensity_scaler_inv_loss: 0.6386 - loss: 0.1930 - pred_intensity_loss: 0.1943 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5794 - val_loss: 0.1228 - val_pred_intensity_loss: 0.1228 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 4/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 39ms/step - intensity_scaler_inv_loss: 0.5811 - loss: 0.1249 - pred_intensity_loss: 0.1249 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 20ms/step - intensity_scaler_inv_loss: 0.5655 - loss: 0.0516 - pred_intensity_loss: 0.0499 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step - intensity_scaler_inv_loss: 0.5598 - loss: -0.0120 - pred_intensity_loss: -0.0188 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5984 - val_loss: -0.0796 - val_pred_intensity_loss: -0.0796 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 5/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 40ms/step - intensity_scaler_inv_loss: 0.6129 - loss: -0.0259 - pred_intensity_loss: -0.0259 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 20ms/step - intensity_scaler_inv_loss: 0.5779 - loss: -0.0981 - pred_intensity_loss: -0.0986 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step - intensity_scaler_inv_loss: 0.5531 - loss: -0.1357 - pred_intensity_loss: -0.1375 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5150 - val_loss: -0.1676 - val_pred_intensity_loss: -0.1676 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 6/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 39ms/step - intensity_scaler_inv_loss: 0.5288 - loss: -0.1285 - pred_intensity_loss: -0.1285 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 20ms/step - intensity_scaler_inv_loss: 0.5229 - loss: -0.1527 - pred_intensity_loss: -0.1531 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step - intensity_scaler_inv_loss: 0.5200 - loss: -0.1726 - pred_intensity_loss: -0.1746 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5180 - val_loss: -0.2576 - val_pred_intensity_loss: -0.2576 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 7/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 38ms/step - intensity_scaler_inv_loss: 0.5291 - loss: -0.2163 - pred_intensity_loss: -0.2163 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 20ms/step - intensity_scaler_inv_loss: 0.5380 - loss: -0.2005 - pred_intensity_loss: -0.2010 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step - intensity_scaler_inv_loss: 0.5403 - loss: -0.1994 - pred_intensity_loss: -0.2016 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5094 - val_loss: -0.2643 - val_pred_intensity_loss: -0.2643 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 8/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 39ms/step - intensity_scaler_inv_loss: 0.5203 - loss: -0.2310 - pred_intensity_loss: -0.2310 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 20ms/step - intensity_scaler_inv_loss: 0.5173 - loss: -0.2255 - pred_intensity_loss: -0.2253 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step - intensity_scaler_inv_loss: 0.5134 - loss: -0.2214 - pred_intensity_loss: -0.2209 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5024 - val_loss: -0.2496 - val_pred_intensity_loss: -0.2496 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 9/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 39ms/step - intensity_scaler_inv_loss: 0.5081 - loss: -0.2224 - pred_intensity_loss: -0.2224 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 20ms/step - intensity_scaler_inv_loss: 0.5091 - loss: -0.2287 - pred_intensity_loss: -0.2286 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step - intensity_scaler_inv_loss: 0.5148 - loss: -0.2258 - pred_intensity_loss: -0.2254 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5107 - val_loss: -0.2694 - val_pred_intensity_loss: -0.2694 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 10/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 39ms/step - intensity_scaler_inv_loss: 0.5229 - loss: -0.2327 - pred_intensity_loss: -0.2327 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 20ms/step - intensity_scaler_inv_loss: 0.5217 - loss: -0.2358 - pred_intensity_loss: -0.2351 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step - intensity_scaler_inv_loss: 0.5253 - loss: -0.2287 - pred_intensity_loss: -0.2262 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5045 - val_loss: -0.2753 - val_pred_intensity_loss: -0.2753 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 11/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 39ms/step - intensity_scaler_inv_loss: 0.5113 - loss: -0.2499 - pred_intensity_loss: -0.2499 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 20ms/step - intensity_scaler_inv_loss: 0.5135 - loss: -0.2386 - pred_intensity_loss: -0.2386 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step - intensity_scaler_inv_loss: 0.5121 - loss: -0.2356 - pred_intensity_loss: -0.2360 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4979 - val_loss: -0.2713 - val_pred_intensity_loss: -0.2713 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 12/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 38ms/step - intensity_scaler_inv_loss: 0.5121 - loss: -0.2332 - pred_intensity_loss: -0.2332 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 20ms/step - intensity_scaler_inv_loss: 0.5114 - loss: -0.2354 - pred_intensity_loss: -0.2351 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step - intensity_scaler_inv_loss: 0.5118 - loss: -0.2358 - pred_intensity_loss: -0.2349 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5028 - val_loss: -0.2782 - val_pred_intensity_loss: -0.2782 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 13/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 38ms/step - intensity_scaler_inv_loss: 0.5178 - loss: -0.2265 - pred_intensity_loss: -0.2265 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 20ms/step - intensity_scaler_inv_loss: 0.5184 - loss: -0.2290 - pred_intensity_loss: -0.2294 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step - intensity_scaler_inv_loss: 0.5164 - loss: -0.2370 - pred_intensity_loss: -0.2383 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5001 - val_loss: -0.2796 - val_pred_intensity_loss: -0.2796 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 14/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 38ms/step - intensity_scaler_inv_loss: 0.5102 - loss: -0.2417 - pred_intensity_loss: -0.2417 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 20ms/step - intensity_scaler_inv_loss: 0.5100 - loss: -0.2415 - pred_intensity_loss: -0.2415 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step - intensity_scaler_inv_loss: 0.5115 - loss: -0.2396 - pred_intensity_loss: -0.2395 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4978 - val_loss: -0.2794 - val_pred_intensity_loss: -0.2794 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 15/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 39ms/step - intensity_scaler_inv_loss: 0.5116 - loss: -0.2426 - pred_intensity_loss: -0.2426 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 20ms/step - intensity_scaler_inv_loss: 0.5096 - loss: -0.2449 - pred_intensity_loss: -0.2445 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step - intensity_scaler_inv_loss: 0.5123 - loss: -0.2405 - pred_intensity_loss: -0.2391 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.5014 - val_loss: -0.2810 - val_pred_intensity_loss: -0.2810 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 16/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 39ms/step - intensity_scaler_inv_loss: 0.5294 - loss: -0.2054 - pred_intensity_loss: -0.2054 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 20ms/step - intensity_scaler_inv_loss: 0.5170 - loss: -0.2327 - pred_intensity_loss: -0.2327 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 37ms/step - intensity_scaler_inv_loss: 0.5124 - loss: -0.2410 - pred_intensity_loss: -0.2413 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4998 - val_loss: -0.2808 - val_pred_intensity_loss: -0.2808 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 17/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 39ms/step - intensity_scaler_inv_loss: 0.5080 - loss: -0.2500 - pred_intensity_loss: -0.2500 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 20ms/step - intensity_scaler_inv_loss: 0.5121 - loss: -0.2395 - pred_intensity_loss: -0.2401 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step - intensity_scaler_inv_loss: 0.5097 - loss: -0.2422 - pred_intensity_loss: -0.2447 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4993 - val_loss: -0.2831 - val_pred_intensity_loss: -0.2831 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 18/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 39ms/step - intensity_scaler_inv_loss: 0.5120 - loss: -0.2355 - pred_intensity_loss: -0.2355 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 20ms/step - intensity_scaler_inv_loss: 0.5132 - loss: -0.2374 - pred_intensity_loss: -0.2380 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step - intensity_scaler_inv_loss: 0.5103 - loss: -0.2430 - pred_intensity_loss: -0.2455 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4968 - val_loss: -0.2844 - val_pred_intensity_loss: -0.2844 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 19/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 40ms/step - intensity_scaler_inv_loss: 0.5103 - loss: -0.2471 - pred_intensity_loss: -0.2471 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 20ms/step - intensity_scaler_inv_loss: 0.5089 - loss: -0.2492 - pred_intensity_loss: -0.2486 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step - intensity_scaler_inv_loss: 0.5110 - loss: -0.2445 - pred_intensity_loss: -0.2420 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4959 - val_loss: -0.2849 - val_pred_intensity_loss: -0.2849 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 20/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 38ms/step - intensity_scaler_inv_loss: 0.5177 - loss: -0.2145 - pred_intensity_loss: -0.2145 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 20ms/step - intensity_scaler_inv_loss: 0.5116 - loss: -0.2359 - pred_intensity_loss: -0.2358 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step - intensity_scaler_inv_loss: 0.5090 - loss: -0.2454 - pred_intensity_loss: -0.2452 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4983 - val_loss: -0.2876 - val_pred_intensity_loss: -0.2876 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 21/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 39ms/step - intensity_scaler_inv_loss: 0.5184 - loss: -0.2305 - pred_intensity_loss: -0.2305 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 21ms/step - intensity_scaler_inv_loss: 0.5115 - loss: -0.2454 - pred_intensity_loss: -0.2454 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 39ms/step - intensity_scaler_inv_loss: 0.5093 - loss: -0.2466 - pred_intensity_loss: -0.2468 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4959 - val_loss: -0.2876 - val_pred_intensity_loss: -0.2876 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 22/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 39ms/step - intensity_scaler_inv_loss: 0.4975 - loss: -0.2670 - pred_intensity_loss: -0.2670 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 20ms/step - intensity_scaler_inv_loss: 0.5058 - loss: -0.2520 - pred_intensity_loss: -0.2522 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step - intensity_scaler_inv_loss: 0.5091 - loss: -0.2482 - pred_intensity_loss: -0.2490 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4973 - val_loss: -0.2893 - val_pred_intensity_loss: -0.2893 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 23/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 40ms/step - intensity_scaler_inv_loss: 0.5173 - loss: -0.2331 - pred_intensity_loss: -0.2331 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 20ms/step - intensity_scaler_inv_loss: 0.5098 - loss: -0.2477 - pred_intensity_loss: -0.2475 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step - intensity_scaler_inv_loss: 0.5088 - loss: -0.2493 - pred_intensity_loss: -0.2486 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4974 - val_loss: -0.2901 - val_pred_intensity_loss: -0.2901 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 24/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 39ms/step - intensity_scaler_inv_loss: 0.5120 - loss: -0.2472 - pred_intensity_loss: -0.2472 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 21ms/step - intensity_scaler_inv_loss: 0.5086 - loss: -0.2497 - pred_intensity_loss: -0.2496 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step - intensity_scaler_inv_loss: 0.5089 - loss: -0.2506 - pred_intensity_loss: -0.2501 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4980 - val_loss: -0.2911 - val_pred_intensity_loss: -0.2911 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 25/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 39ms/step - intensity_scaler_inv_loss: 0.5098 - loss: -0.2529 - pred_intensity_loss: -0.2529 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 20ms/step - intensity_scaler_inv_loss: 0.5111 - loss: -0.2464 - pred_intensity_loss: -0.2464 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step - intensity_scaler_inv_loss: 0.5096 - loss: -0.2494 - pred_intensity_loss: -0.2495 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4987 - val_loss: -0.2846 - val_pred_intensity_loss: -0.2846 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 26/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 39ms/step - intensity_scaler_inv_loss: 0.5100 - loss: -0.2490 - pred_intensity_loss: -0.2490 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 21ms/step - intensity_scaler_inv_loss: 0.5108 - loss: -0.2474 - pred_intensity_loss: -0.2478 - trimmed_obj_loss: 0.0000e+00
Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step - intensity_scaler_inv_loss: 0.5096 - loss: -0.2489 - pred_intensity_loss: -0.2506 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4997 - val_loss: -0.2860 - val_pred_intensity_loss: -0.2860 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 0.0010
Epoch 27/50
[1m1/4[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 39ms/step - intensity_scaler_inv_loss: 0.5039 - loss: -0.2650 - pred_intensity_loss: -0.2650 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 21ms/step - intensity_scaler_inv_loss: 0.5085 - loss: -0.2564 - pred_intensity_loss: -0.2561 - trimmed_obj_loss: 0.0000e+00[1m4/4[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step - intensity_scaler_inv_loss: 0.5119 - loss: -0.2492 - pred_intensity_loss: -0.2480 - trimmed_obj_loss: 0.0000e+00 - val_intensity_scaler_inv_loss: 0.4924 - val_loss: -0.2928 - val_pred_intensity_loss: -0.2928 - val_trimmed_obj_loss: 0.0000e+00 - learning_rate: 5.0000e-04
W0000 00:00:1768127496.391889  916136 loop_optimizer.cc:934] Skipping loop optimization for Merge node with control input: functional_1/padded_obj_2_1/cond_3/branch_executed/_47
DEBUG: Setting probe to tf.Tensor(
[[[ 0.13510734-0.08366051j]
  [-0.08103182-0.0294797j ]
  [ 0.0637618 +0.02550233j]
  ...
  [-0.05742923+0.10465012j]
  [ 0.061039  +0.02731375j]
  [-0.07001488-0.0336627j ]]

 [[-0.08103182-0.0294797j ]
  [ 0.11364837-0.04388378j]
  [-0.08643168+0.03117809j]
  ...
  [ 0.02872955+0.03712314j]
  [-0.08476108+0.03176096j]
  [ 0.10533146-0.04452544j]]

 [[ 0.0637618 +0.02550233j]
  [-0.08643168+0.03117809j]
  [ 0.06238977-0.04183936j]
  ...
  [-0.01890127-0.04941669j]
  [ 0.06107059-0.04259919j]
  [-0.07948506+0.03188787j]]

 ...

 [[-0.05742923+0.10465012j]
  [ 0.02872955+0.03712314j]
  [-0.01890127-0.04941669j]
  ...
  [-0.00577373-0.08714116j]
  [-0.01781569-0.0517069j ]
  [ 0.02285527+0.04448525j]]

 [[ 0.061039  +0.02731375j]
  [-0.08476108+0.03176096j]
  [ 0.06107059-0.04259919j]
  ...
  [-0.01781569-0.0517069j ]
  [ 0.05980607-0.0434007j ]
  [-0.07803912+0.03257083j]]

 [[-0.07001488-0.0336627j ]
  [ 0.10533146-0.04452544j]
  [-0.07948506+0.03188787j]
  ...
  [ 0.02285527+0.04448525j]
  [-0.07803912+0.03257083j]
  [ 0.09786177-0.04529788j]]], shape=(64, 64, 1), dtype=complex64) in params
INFO: setting probe from test data container. It MUST be consistent with the training probe
DEBUG _flat_to_channel: gridsize from parameter: 2
DEBUG _flat_to_channel: N from parameter: 78
DEBUG _flat_to_channel: input shape=(128, 78, 78, 1), reshaping to (-1, 4, 78, 78)
DEBUG _flat_to_channel: gridsize from parameter: 2
DEBUG _flat_to_channel: N from parameter: 78
DEBUG _flat_to_channel: input shape=(128, 78, 78, 1), reshaping to (-1, 4, 78, 78)
DEBUG _flat_to_channel: gridsize from parameter: 2
DEBUG _flat_to_channel: N from parameter: 78
DEBUG _flat_to_channel: input shape=(128, 78, 78, 1), reshaping to (-1, 4, 78, 78)
DEBUG _flat_to_channel: gridsize from parameter: 2
DEBUG _flat_to_channel: N from parameter: 78
DEBUG _flat_to_channel: input shape=(128, 78, 78, 1), reshaping to (-1, 4, 78, 78)
input shape (None, 64, 64, 1)
DEBUG _flat_to_channel: gridsize from parameter: 2
DEBUG _flat_to_channel: N from parameter: 64
DEBUG _flat_to_channel: input shape=(None, 64, 64, 1), reshaping to (-1, 4, 64, 64)
[1m1/2[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1s/step[1m2/2[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 23ms/step
Object stitching failed: unsupported operand type(s) for /: 'NoneType' and 'int'
cannot reshape array of size 0 into shape (0,0,1)
2026-01-11 02:31:37,041 - INFO - Skipping image stitching (disabled or no test data available)
2026-01-11 02:31:37,041 - INFO - Backend dispatcher: workflow complete (backend=tensorflow)
2026-01-11 02:31:38,478 - INFO - Outputs saved to gs2_full_subsample_study_20260111_0230/train_subsample_64_groups_64/trial_1/pinn_run
2026-01-11 02:31:38,478 - INFO - TensorFlow artifacts saved via model_manager and save_outputs
[2026-01-11 02:31:40] SUCCESS: PtychoPINN training (subsample=64, groups=64, trial=1)
[2026-01-11 02:31:40] EXECUTING: Baseline training (subsample=64, groups=64, trial=1)
[2026-01-11 02:31:40] COMMAND: python scripts/run_baseline.py --config 'configs/comparison_config_gs2_quick.yaml' \
            --train_data_file 'prepare_1e4_photons_5k/dataset/train.npz' \
            --test_data 'prepare_1e4_photons_5k/dataset/test.npz' \
            --n_groups 64 --n_subsample 64 \
            --neighbor_count 7 \
            --output_dir 'gs2_full_subsample_study_20260111_0230/train_subsample_64_groups_64/trial_1/baseline_run' --gridsize 2
2026-01-11 02:31:40.489729: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1768127500.501066  920602 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1768127500.504945  920602 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1768127500.514731  920602 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1768127500.514741  920602 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1768127500.514742  920602 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1768127500.514743  920602 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2026-01-11 02:31:40.517742: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2026-01-11 02:31:43,043 - INFO - Configuration setup complete
2026-01-11 02:31:43,043 - INFO - Final configuration: TrainingConfig(model=ModelConfig(N=64, gridsize=2, n_filters_scale=2, model_type='pinn', amp_activation='sigmoid', object_big=True, probe_big=True, probe_mask=False, pad_object=True, probe_scale=4.0, gaussian_smoothing_sigma=0.0), train_data_file=PosixPath('prepare_1e4_photons_5k/dataset/train.npz'), test_data_file=PosixPath('prepare_1e4_photons_5k/dataset/test.npz'), batch_size=16, nepochs=50, mae_weight=0.0, nll_weight=1.0, realspace_mae_weight=0.0, realspace_weight=0.0, nphotons=1000000000.0, n_groups=64, n_images=512, n_subsample=64, subsample_seed=None, neighbor_count=7, enable_oversampling=False, neighbor_pool_size=None, positions_provided=True, probe_trainable=False, intensity_scale_trainable=True, output_dir=PosixPath('gs2_full_subsample_study_20260111_0230/train_subsample_64_groups_64/trial_1/baseline_run'), sequential_sampling=False, backend='tensorflow', torch_loss_mode='poisson')
2026-01-11 02:31:43,043 - INFO - âœ… Validated model_type = 'supervised' for baseline training
2026-01-11 02:31:43,043 - INFO - --- Starting Supervised Baseline Run ---
2026-01-11 02:31:43,043 - INFO - Results will be saved to: gs2_full_subsample_study_20260111_0230/train_subsample_64_groups_64/trial_1/baseline_run/01-11-2026-02.31.43_baseline_gs2/
2026-01-11 02:31:43,043 - INFO - 
[1/6] Initializing probe...
I0000 00:00:1768127503.157954  920602 gpu_process_state.cc:208] Using CUDA malloc Async allocator for GPU: 0
I0000 00:00:1768127503.159270  920602 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22259 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:04:00.0, compute capability: 8.6
2026-01-11 02:31:43,193 - INFO - 
[2/6] Loading data...
2026-01-11 02:31:43,193 - INFO - Loading from .npz files: prepare_1e4_photons_5k/dataset/train.npz
2026-01-11 02:31:43,193 - INFO - Loading data from prepare_1e4_photons_5k/dataset/train.npz with n_images=512, n_subsample=None
2026-01-11 02:31:43,509 - INFO - Legacy sampling: using 512 images from 2576 total
2026-01-11 02:31:43,509 - INFO - Randomly subsampled 512 images
2026-01-11 02:31:43,515 - INFO - Loading data from prepare_1e4_photons_5k/dataset/test.npz with n_images=None, n_subsample=None
2026-01-11 02:31:43,814 - INFO - Using full dataset of 2424 images
2026-01-11 02:31:43,830 - INFO - [OVERSAMPLING DEBUG] generate_grouped_data called with: nsamples=64, n_points=512, C=4, K=7
2026-01-11 02:31:43,830 - INFO - [OVERSAMPLING DEBUG] Parameters: gridsize=2, N=64, sequential_sampling=False
2026-01-11 02:31:43,830 - INFO - [OVERSAMPLING DEBUG] Oversampling flags: enable_oversampling=False, neighbor_pool_size=None, effective_K=7
2026-01-11 02:31:43,830 - INFO - [OVERSAMPLING DEBUG] Oversampling check: nsamples > n_points = 64 > 512 = False
2026-01-11 02:31:43,830 - INFO - [OVERSAMPLING DEBUG] Oversampling check: C > 1 = 4 > 1 = True
2026-01-11 02:31:43,830 - INFO - [OVERSAMPLING DEBUG] needs_oversampling = False
2026-01-11 02:31:43,830 - INFO - Using efficient random sampling strategy for gridsize=2
2026-01-11 02:31:43,830 - INFO - [OVERSAMPLING DEBUG] Taking efficient branch: standard sample-then-group
2026-01-11 02:31:43,830 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently called with: nsamples=64, K=7, C=4
2026-01-11 02:31:43,830 - INFO - Generating 64 groups efficiently from 512 points (K=7, C=4)
2026-01-11 02:31:43,830 - INFO - [OVERSAMPLING DEBUG] Standard case: using 64 groups from 512 points
2026-01-11 02:31:43,830 - INFO - [OVERSAMPLING DEBUG] Randomly sampled 64 seed points
2026-01-11 02:31:43,830 - INFO - Sampled 64 seed points from 512 total points
2026-01-11 02:31:43,831 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently completed: generated 64 groups
2026-01-11 02:31:43,831 - INFO - Successfully generated 64 groups with shape (64, 4)
2026-01-11 02:31:43,831 - INFO - [OVERSAMPLING DEBUG] Generated 64 groups in total
2026-01-11 02:31:43,831 - INFO - Generated 64 groups efficiently
2026-01-11 02:31:44,125 - INFO - [OVERSAMPLING DEBUG] generate_grouped_data called with: nsamples=64, n_points=2424, C=4, K=7
2026-01-11 02:31:44,125 - INFO - [OVERSAMPLING DEBUG] Parameters: gridsize=2, N=64, sequential_sampling=False
2026-01-11 02:31:44,126 - INFO - [OVERSAMPLING DEBUG] Oversampling flags: enable_oversampling=False, neighbor_pool_size=None, effective_K=7
2026-01-11 02:31:44,126 - INFO - [OVERSAMPLING DEBUG] Oversampling check: nsamples > n_points = 64 > 2424 = False
2026-01-11 02:31:44,126 - INFO - [OVERSAMPLING DEBUG] Oversampling check: C > 1 = 4 > 1 = True
2026-01-11 02:31:44,126 - INFO - [OVERSAMPLING DEBUG] needs_oversampling = False
2026-01-11 02:31:44,126 - INFO - Using efficient random sampling strategy for gridsize=2
2026-01-11 02:31:44,126 - INFO - [OVERSAMPLING DEBUG] Taking efficient branch: standard sample-then-group
2026-01-11 02:31:44,126 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently called with: nsamples=64, K=7, C=4
2026-01-11 02:31:44,126 - INFO - Generating 64 groups efficiently from 2424 points (K=7, C=4)
2026-01-11 02:31:44,126 - INFO - [OVERSAMPLING DEBUG] Standard case: using 64 groups from 2424 points
2026-01-11 02:31:44,126 - INFO - [OVERSAMPLING DEBUG] Randomly sampled 64 seed points
2026-01-11 02:31:44,126 - INFO - Sampled 64 seed points from 2424 total points
2026-01-11 02:31:44,127 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently completed: generated 64 groups
2026-01-11 02:31:44,127 - INFO - Successfully generated 64 groups with shape (64, 4)
2026-01-11 02:31:44,127 - INFO - [OVERSAMPLING DEBUG] Generated 64 groups in total
2026-01-11 02:31:44,127 - INFO - Generated 64 groups efficiently
2026-01-11 02:31:44,153 - INFO - Globally set intensity_scale to: 988.2117919921875
2026-01-11 02:31:44,153 - INFO - 
[3/6] Shaping data for the baseline model...
2026-01-11 02:31:44,165 - INFO - Flattening 4 channels to independent samples for baseline model
2026-01-11 02:31:44,175 - INFO - DEBUG: global_offsets original shape: [64  1  2  1]
2026-01-11 02:31:44,179 - INFO - Final training input shape: (256, 64, 64, 1)
2026-01-11 02:31:44,180 - INFO - 
[4/6] Training the baseline model for 50 epochs with batch size 16...
2026-01-11 02:31:44,180 - INFO - Training with 256 images
DEBUG: Setting timestamp to 01/11/2026, 02:31:43 in params
Current Parameters:
--------------------
N: 64
amp_activation: sigmoid
backend: tensorflow
batch_size: 16
bigN: 68
big_gridsize: 10
data_source: generic
debug: True
default_probe_scale: 0.7
enable_oversampling: False
gaussian_smoothing_sigma: 0.0
gridsize: 2
h5_path: wts.h5
intensity_scale.trainable: True
label: baseline_gs2
mae_weight: 0.0
max_position_jitter: 10
model_type: supervised
n_filters_scale: 2
n_groups: 64
n_images: 512
n_subsample: 64
neighbor_count: 7
nepochs: 50
nimgs_test: 3
nimgs_train: 9
nll_weight: 1.0
nphotons: 1000000000.0
npseed: 42
object.big: True
offset: 4
outer_offset_test: None
outer_offset_train: None
output_prefix: gs2_full_subsample_study_20260111_0230/train_subsample_64_groups_64/trial_1/baseline_run/01-11-2026-02.31.43_baseline_gs2/
pad_object: True
positions.provided: True
probe.big: True
probe.mask: False
probe.trainable: False
probe_scale: 4.0
realspace_mae_weight: 0.0
realspace_weight: 0.0
sequential_sampling: False
set_phi: False
sim_jitter_scale: 0.0
size: 392
test_data_file_path: prepare_1e4_photons_5k/dataset/test.npz
timestamp: 01/11/2026, 02:31:43
torch_loss_mode: poisson
train_data_file_path: prepare_1e4_photons_5k/dataset/train.npz
tv_weight: 0.0
use_xla_translate: True
DEBUG: Setting probe to tf.Tensor(
[[[2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  ...
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]]

 [[2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  ...
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]]

 [[2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  ...
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]]

 ...

 [[2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  ...
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]]

 [[2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  ...
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]]

 [[2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  ...
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]
  [2.7234042e-09+0.j]]], shape=(64, 64, 1), dtype=complex64) in params
diff3d shape: (512, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (512,)
objectGuess shape: (232, 232)
xcoords shape: (512,)
ycoords shape: (512,)
xcoords_start shape: (512,)
ycoords_start shape: (512,)
diff3d shape: (2424, 64, 64)
probeGuess shape: (64, 64)
scan_index shape: (2424,)
objectGuess shape: (232, 232)
xcoords shape: (2424,)
ycoords shape: (2424,)
xcoords_start shape: (2424,)
ycoords_start shape: (2424,)
DEBUG: nsamples: 64, gridsize: 2 (using efficient random sample-then-group strategy)
INFO: Using pre-computed 'Y' array from the input file.
neighbor-sampled diffraction shape (64, 64, 64, 4)
loader: using provided ground truth patches.
INFO: None
<PtychoDataContainer X=(64, 64, 64, 4) mean=0.407 Y_I=(64, 64, 64, 4) mean=0.564 Y_phi=(64, 64, 64, 4) mean=-0.052 coords_nominal=(64, 1, 2, 4) mean=0.000 coords_true=(64, 1, 2, 4) mean=0.000 probe=(64, 64) mean_amplitude=0.006 norm_Y_I=<scalar> nn_indices=(64, 4) global_offsets=(64, 1, 2, 1) local_offsets=(64, 1, 2, 4)>
DEBUG: nsamples: 64, gridsize: 2 (using efficient random sample-then-group strategy)
INFO: Using pre-computed 'Y' array from the input file.
neighbor-sampled diffraction shape (64, 64, 64, 4)
loader: using provided ground truth patches.
INFO: None
<PtychoDataContainer X=(64, 64, 64, 4) mean=0.407 Y_I=(64, 64, 64, 4) mean=0.563 Y_phi=(64, 64, 64, 4) mean=-0.028 coords_nominal=(64, 1, 2, 4) mean=0.000 coords_true=(64, 1, 2, 4) mean=0.000 probe=(64, 64) mean_amplitude=0.006 norm_Y_I=<scalar> nn_indices=(64, 4) global_offsets=(64, 1, 2, 1) local_offsets=(64, 1, 2, 4)>
DEBUG: Setting intensity_scale to tf.Tensor(988.2118, shape=(), dtype=float32) in params
Model: "functional"
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Layer (type)        â”ƒ Output Shape      â”ƒ    Param # â”ƒ Connected to      â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ input_layer         â”‚ (None, 64, 64, 1) â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d (Conv2D)     â”‚ (None, 64, 64,    â”‚        640 â”‚ input_layer[0][0] â”‚
â”‚                     â”‚ 64)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_1 (Conv2D)   â”‚ (None, 64, 64,    â”‚     36,928 â”‚ conv2d[0][0]      â”‚
â”‚                     â”‚ 64)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ max_pooling2d       â”‚ (None, 32, 32,    â”‚          0 â”‚ conv2d_1[0][0]    â”‚
â”‚ (MaxPooling2D)      â”‚ 64)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_2 (Conv2D)   â”‚ (None, 32, 32,    â”‚     73,856 â”‚ max_pooling2d[0]â€¦ â”‚
â”‚                     â”‚ 128)              â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_3 (Conv2D)   â”‚ (None, 32, 32,    â”‚    147,584 â”‚ conv2d_2[0][0]    â”‚
â”‚                     â”‚ 128)              â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ max_pooling2d_1     â”‚ (None, 16, 16,    â”‚          0 â”‚ conv2d_3[0][0]    â”‚
â”‚ (MaxPooling2D)      â”‚ 128)              â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_4 (Conv2D)   â”‚ (None, 16, 16,    â”‚    295,168 â”‚ max_pooling2d_1[â€¦ â”‚
â”‚                     â”‚ 256)              â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_5 (Conv2D)   â”‚ (None, 16, 16,    â”‚    590,080 â”‚ conv2d_4[0][0]    â”‚
â”‚                     â”‚ 256)              â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ max_pooling2d_2     â”‚ (None, 8, 8, 256) â”‚          0 â”‚ conv2d_5[0][0]    â”‚
â”‚ (MaxPooling2D)      â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_6 (Conv2D)   â”‚ (None, 8, 8, 256) â”‚    590,080 â”‚ max_pooling2d_2[â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_13 (Conv2D)  â”‚ (None, 8, 8, 256) â”‚    590,080 â”‚ max_pooling2d_2[â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_7 (Conv2D)   â”‚ (None, 8, 8, 256) â”‚    590,080 â”‚ conv2d_6[0][0]    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_14 (Conv2D)  â”‚ (None, 8, 8, 256) â”‚    590,080 â”‚ conv2d_13[0][0]   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling2d       â”‚ (None, 16, 16,    â”‚          0 â”‚ conv2d_7[0][0]    â”‚
â”‚ (UpSampling2D)      â”‚ 256)              â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling2d_3     â”‚ (None, 16, 16,    â”‚          0 â”‚ conv2d_14[0][0]   â”‚
â”‚ (UpSampling2D)      â”‚ 256)              â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_8 (Conv2D)   â”‚ (None, 16, 16,    â”‚    295,040 â”‚ up_sampling2d[0]â€¦ â”‚
â”‚                     â”‚ 128)              â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_15 (Conv2D)  â”‚ (None, 16, 16,    â”‚    295,040 â”‚ up_sampling2d_3[â€¦ â”‚
â”‚                     â”‚ 128)              â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_9 (Conv2D)   â”‚ (None, 16, 16,    â”‚    147,584 â”‚ conv2d_8[0][0]    â”‚
â”‚                     â”‚ 128)              â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_16 (Conv2D)  â”‚ (None, 16, 16,    â”‚    147,584 â”‚ conv2d_15[0][0]   â”‚
â”‚                     â”‚ 128)              â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling2d_1     â”‚ (None, 32, 32,    â”‚          0 â”‚ conv2d_9[0][0]    â”‚
â”‚ (UpSampling2D)      â”‚ 128)              â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling2d_4     â”‚ (None, 32, 32,    â”‚          0 â”‚ conv2d_16[0][0]   â”‚
â”‚ (UpSampling2D)      â”‚ 128)              â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_10 (Conv2D)  â”‚ (None, 32, 32,    â”‚     73,792 â”‚ up_sampling2d_1[â€¦ â”‚
â”‚                     â”‚ 64)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_17 (Conv2D)  â”‚ (None, 32, 32,    â”‚     73,792 â”‚ up_sampling2d_4[â€¦ â”‚
â”‚                     â”‚ 64)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_11 (Conv2D)  â”‚ (None, 32, 32,    â”‚     36,928 â”‚ conv2d_10[0][0]   â”‚
â”‚                     â”‚ 64)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_18 (Conv2D)  â”‚ (None, 32, 32,    â”‚     36,928 â”‚ conv2d_17[0][0]   â”‚
â”‚                     â”‚ 64)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling2d_2     â”‚ (None, 64, 64,    â”‚          0 â”‚ conv2d_11[0][0]   â”‚
â”‚ (UpSampling2D)      â”‚ 64)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling2d_5     â”‚ (None, 64, 64,    â”‚          0 â”‚ conv2d_18[0][0]   â”‚
â”‚ (UpSampling2D)      â”‚ 64)               â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_12 (Conv2D)  â”‚ (None, 64, 64, 1) â”‚        577 â”‚ up_sampling2d_2[â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_19 (Conv2D)  â”‚ (None, 64, 64, 1) â”‚        577 â”‚ up_sampling2d_5[â€¦ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 4,612,418 (17.59 MB)
 Trainable params: 4,612,418 (17.59 MB)
 Non-trainable params: 0 (0.00 B)
None
Training with 50 epochs and batch size 16
Epoch 1/50
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1768127506.981534  920741 service.cc:152] XLA service 0x71d278002620 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1768127506.981553  920741 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6
2026-01-11 02:31:47.048017: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
I0000 00:00:1768127507.458494  920741 cuda_dnn.cc:529] Loaded cuDNN version 91002
I0000 00:00:1768127510.362928  920741 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[1m 1/16[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1:23[0m 6s/step - conv2d_12_loss: 0.5643 - conv2d_19_loss: 0.2084 - loss: 0.7727[1m 6/16[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - conv2d_12_loss: 0.4417 - conv2d_19_loss: 0.2247 - loss: 0.6664[1m11/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - conv2d_12_loss: 0.4116 - conv2d_19_loss: 0.2238 - loss: 0.6354[1m16/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 190ms/step - conv2d_12_loss: 0.3846 - conv2d_19_loss: 0.2227 - loss: 0.6081[1m16/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m9s[0m 246ms/step - conv2d_12_loss: 0.2988 - conv2d_19_loss: 0.2185 - loss: 0.5300 - val_conv2d_12_loss: 0.1225 - val_conv2d_19_loss: 0.2016 - val_loss: 0.3241 - learning_rate: 0.0010
Epoch 2/50
[1m 1/16[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 27ms/step - conv2d_12_loss: 0.1222 - conv2d_19_loss: 0.2196 - loss: 0.3418[1m 6/16[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.1116 - conv2d_19_loss: 0.2184 - loss: 0.3300[1m11/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.1057 - conv2d_19_loss: 0.2172 - loss: 0.3228[1m16/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - conv2d_12_loss: 0.0897 - conv2d_19_loss: 0.2138 - loss: 0.3057 - val_conv2d_12_loss: 0.0707 - val_conv2d_19_loss: 0.2014 - val_loss: 0.2721 - learning_rate: 0.0010
Epoch 3/50
[1m 1/16[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0719 - conv2d_19_loss: 0.2205 - loss: 0.2924[1m 6/16[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0710 - conv2d_19_loss: 0.2165 - loss: 0.2875[1m11/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0698 - conv2d_19_loss: 0.2161 - loss: 0.2859[1m16/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - conv2d_12_loss: 0.0657 - conv2d_19_loss: 0.2153 - loss: 0.2809 - val_conv2d_12_loss: 0.0582 - val_conv2d_19_loss: 0.2014 - val_loss: 0.2596 - learning_rate: 0.0010
Epoch 4/50
[1m 1/16[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 27ms/step - conv2d_12_loss: 0.0615 - conv2d_19_loss: 0.2233 - loss: 0.2848[1m 6/16[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0602 - conv2d_19_loss: 0.2170 - loss: 0.2772[1m11/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0596 - conv2d_19_loss: 0.2171 - loss: 0.2766[1m16/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - conv2d_12_loss: 0.0568 - conv2d_19_loss: 0.2142 - loss: 0.2721 - val_conv2d_12_loss: 0.0519 - val_conv2d_19_loss: 0.2013 - val_loss: 0.2532 - learning_rate: 0.0010
Epoch 5/50
[1m 1/16[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0537 - conv2d_19_loss: 0.2160 - loss: 0.2697[1m 6/16[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0531 - conv2d_19_loss: 0.2144 - loss: 0.2675[1m11/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0529 - conv2d_19_loss: 0.2156 - loss: 0.2684[1m16/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - conv2d_12_loss: 0.0511 - conv2d_19_loss: 0.2151 - loss: 0.2662 - val_conv2d_12_loss: 0.0466 - val_conv2d_19_loss: 0.2013 - val_loss: 0.2479 - learning_rate: 0.0010
Epoch 6/50
[1m 1/16[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0476 - conv2d_19_loss: 0.2076 - loss: 0.2552[1m 6/16[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0475 - conv2d_19_loss: 0.2116 - loss: 0.2592[1m11/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0473 - conv2d_19_loss: 0.2128 - loss: 0.2601[1m16/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - conv2d_12_loss: 0.0466 - conv2d_19_loss: 0.2161 - loss: 0.2614 - val_conv2d_12_loss: 0.0417 - val_conv2d_19_loss: 0.2013 - val_loss: 0.2430 - learning_rate: 0.0010
Epoch 7/50
[1m 1/16[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0432 - conv2d_19_loss: 0.2116 - loss: 0.2549[1m 6/16[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0436 - conv2d_19_loss: 0.2168 - loss: 0.2605[1m11/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0433 - conv2d_19_loss: 0.2169 - loss: 0.2602[1m16/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - conv2d_12_loss: 0.0421 - conv2d_19_loss: 0.2149 - loss: 0.2570 - val_conv2d_12_loss: 0.0393 - val_conv2d_19_loss: 0.2012 - val_loss: 0.2405 - learning_rate: 0.0010
Epoch 8/50
[1m 1/16[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 28ms/step - conv2d_12_loss: 0.0385 - conv2d_19_loss: 0.2070 - loss: 0.2455[1m 6/16[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0398 - conv2d_19_loss: 0.2123 - loss: 0.2521[1m11/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0401 - conv2d_19_loss: 0.2138 - loss: 0.2539[1m16/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - conv2d_12_loss: 0.0403 - conv2d_19_loss: 0.2156 - loss: 0.2551 - val_conv2d_12_loss: 0.0385 - val_conv2d_19_loss: 0.2012 - val_loss: 0.2397 - learning_rate: 0.0010
Epoch 9/50
[1m 1/16[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0405 - conv2d_19_loss: 0.2179 - loss: 0.2584[1m 6/16[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0403 - conv2d_19_loss: 0.2175 - loss: 0.2578[1m11/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0401 - conv2d_19_loss: 0.2161 - loss: 0.2562[1m16/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - conv2d_12_loss: 0.0399 - conv2d_19_loss: 0.2154 - loss: 0.2548 - val_conv2d_12_loss: 0.0390 - val_conv2d_19_loss: 0.2012 - val_loss: 0.2402 - learning_rate: 0.0010
Epoch 10/50
[1m 1/16[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0412 - conv2d_19_loss: 0.2150 - loss: 0.2562[1m 6/16[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0407 - conv2d_19_loss: 0.2158 - loss: 0.2565[1m11/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0406 - conv2d_19_loss: 0.2158 - loss: 0.2565[1m16/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - conv2d_12_loss: 0.0402 - conv2d_19_loss: 0.2155 - loss: 0.2550 - val_conv2d_12_loss: 0.0379 - val_conv2d_19_loss: 0.2012 - val_loss: 0.2391 - learning_rate: 0.0010
Epoch 11/50
[1m 1/16[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0411 - conv2d_19_loss: 0.2229 - loss: 0.2640[1m 6/16[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0399 - conv2d_19_loss: 0.2154 - loss: 0.2553[1m11/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0398 - conv2d_19_loss: 0.2146 - loss: 0.2544[1m16/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - conv2d_12_loss: 0.0397 - conv2d_19_loss: 0.2145 - loss: 0.2546 - val_conv2d_12_loss: 0.0373 - val_conv2d_19_loss: 0.2012 - val_loss: 0.2385 - learning_rate: 0.0010
Epoch 12/50
[1m 1/16[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0406 - conv2d_19_loss: 0.2213 - loss: 0.2618[1m 6/16[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0395 - conv2d_19_loss: 0.2169 - loss: 0.2564[1m11/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0393 - conv2d_19_loss: 0.2157 - loss: 0.2550[1m16/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - conv2d_12_loss: 0.0392 - conv2d_19_loss: 0.2154 - loss: 0.2541 - val_conv2d_12_loss: 0.0373 - val_conv2d_19_loss: 0.2011 - val_loss: 0.2384 - learning_rate: 0.0010
Epoch 13/50
[1m 1/16[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0400 - conv2d_19_loss: 0.2185 - loss: 0.2585[1m 6/16[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0396 - conv2d_19_loss: 0.2168 - loss: 0.2563[1m11/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0393 - conv2d_19_loss: 0.2159 - loss: 0.2553
Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
[1m16/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - conv2d_12_loss: 0.0394 - conv2d_19_loss: 0.2154 - loss: 0.2541 - val_conv2d_12_loss: 0.0378 - val_conv2d_19_loss: 0.2011 - val_loss: 0.2389 - learning_rate: 0.0010
Epoch 14/50
[1m 1/16[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0375 - conv2d_19_loss: 0.2055 - loss: 0.2430[1m 6/16[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0385 - conv2d_19_loss: 0.2115 - loss: 0.2500[1m11/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0387 - conv2d_19_loss: 0.2122 - loss: 0.2509[1m16/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - conv2d_12_loss: 0.0393 - conv2d_19_loss: 0.2145 - loss: 0.2542 - val_conv2d_12_loss: 0.0374 - val_conv2d_19_loss: 0.2011 - val_loss: 0.2385 - learning_rate: 5.0000e-04
Epoch 15/50
[1m 1/16[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 25ms/step - conv2d_12_loss: 0.0391 - conv2d_19_loss: 0.2137 - loss: 0.2528[1m 6/16[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0390 - conv2d_19_loss: 0.2123 - loss: 0.2513[1m11/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0390 - conv2d_19_loss: 0.2128 - loss: 0.2518[1m16/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - conv2d_12_loss: 0.0393 - conv2d_19_loss: 0.2162 - loss: 0.2540 - val_conv2d_12_loss: 0.0371 - val_conv2d_19_loss: 0.2012 - val_loss: 0.2383 - learning_rate: 5.0000e-04
Epoch 16/50
[1m 1/16[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0383 - conv2d_19_loss: 0.2107 - loss: 0.2490[1m 6/16[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0394 - conv2d_19_loss: 0.2146 - loss: 0.2540[1m11/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0391 - conv2d_19_loss: 0.2140 - loss: 0.2531[1m16/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - conv2d_12_loss: 0.0392 - conv2d_19_loss: 0.2153 - loss: 0.2539 - val_conv2d_12_loss: 0.0372 - val_conv2d_19_loss: 0.2011 - val_loss: 0.2383 - learning_rate: 5.0000e-04
Epoch 17/50
[1m 1/16[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0399 - conv2d_19_loss: 0.2157 - loss: 0.2556[1m 6/16[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0394 - conv2d_19_loss: 0.2151 - loss: 0.2546[1m11/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0392 - conv2d_19_loss: 0.2141 - loss: 0.2533
Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
[1m16/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - conv2d_12_loss: 0.0387 - conv2d_19_loss: 0.2126 - loss: 0.2539 - val_conv2d_12_loss: 0.0373 - val_conv2d_19_loss: 0.2012 - val_loss: 0.2384 - learning_rate: 5.0000e-04
Epoch 18/50
[1m 1/16[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0392 - conv2d_19_loss: 0.2154 - loss: 0.2546[1m 6/16[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0388 - conv2d_19_loss: 0.2125 - loss: 0.2513[1m11/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0388 - conv2d_19_loss: 0.2129 - loss: 0.2517[1m16/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - conv2d_12_loss: 0.0389 - conv2d_19_loss: 0.2136 - loss: 0.2539 - val_conv2d_12_loss: 0.0373 - val_conv2d_19_loss: 0.2012 - val_loss: 0.2384 - learning_rate: 2.5000e-04
Epoch 19/50
[1m 1/16[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 28ms/step - conv2d_12_loss: 0.0387 - conv2d_19_loss: 0.2127 - loss: 0.2514[1m 6/16[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0387 - conv2d_19_loss: 0.2129 - loss: 0.2516[1m11/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0389 - conv2d_19_loss: 0.2138 - loss: 0.2526
Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
[1m16/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - conv2d_12_loss: 0.0389 - conv2d_19_loss: 0.2134 - loss: 0.2538 - val_conv2d_12_loss: 0.0371 - val_conv2d_19_loss: 0.2012 - val_loss: 0.2383 - learning_rate: 2.5000e-04
Epoch 20/50
[1m 1/16[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0384 - conv2d_19_loss: 0.2123 - loss: 0.2507[1m 6/16[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0384 - conv2d_19_loss: 0.2122 - loss: 0.2505[1m11/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0386 - conv2d_19_loss: 0.2131 - loss: 0.2517[1m16/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - conv2d_12_loss: 0.0390 - conv2d_19_loss: 0.2155 - loss: 0.2537 - val_conv2d_12_loss: 0.0371 - val_conv2d_19_loss: 0.2012 - val_loss: 0.2383 - learning_rate: 1.2500e-04
Epoch 21/50
[1m 1/16[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0406 - conv2d_19_loss: 0.2216 - loss: 0.2623[1m 6/16[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0401 - conv2d_19_loss: 0.2209 - loss: 0.2610[1m11/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0397 - conv2d_19_loss: 0.2185 - loss: 0.2582
Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0001.
[1m16/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - conv2d_12_loss: 0.0388 - conv2d_19_loss: 0.2146 - loss: 0.2537 - val_conv2d_12_loss: 0.0371 - val_conv2d_19_loss: 0.2012 - val_loss: 0.2383 - learning_rate: 1.2500e-04
Epoch 22/50
[1m 1/16[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0395 - conv2d_19_loss: 0.2125 - loss: 0.2519[1m 6/16[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0386 - conv2d_19_loss: 0.2111 - loss: 0.2498[1m11/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0385 - conv2d_19_loss: 0.2116 - loss: 0.2501[1m16/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - conv2d_12_loss: 0.0391 - conv2d_19_loss: 0.2158 - loss: 0.2537 - val_conv2d_12_loss: 0.0371 - val_conv2d_19_loss: 0.2012 - val_loss: 0.2383 - learning_rate: 1.0000e-04
Epoch 23/50
[1m 1/16[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 28ms/step - conv2d_12_loss: 0.0416 - conv2d_19_loss: 0.2274 - loss: 0.2690[1m 6/16[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0394 - conv2d_19_loss: 0.2174 - loss: 0.2568[1m11/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0391 - conv2d_19_loss: 0.2159 - loss: 0.2551[1m16/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - conv2d_12_loss: 0.0390 - conv2d_19_loss: 0.2147 - loss: 0.2537 - val_conv2d_12_loss: 0.0371 - val_conv2d_19_loss: 0.2013 - val_loss: 0.2384 - learning_rate: 1.0000e-04
Epoch 24/50
[1m 1/16[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 26ms/step - conv2d_12_loss: 0.0392 - conv2d_19_loss: 0.2198 - loss: 0.2590[1m 6/16[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0392 - conv2d_19_loss: 0.2160 - loss: 0.2552[1m11/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - conv2d_12_loss: 0.0392 - conv2d_19_loss: 0.2162 - loss: 0.2554[1m16/16[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - conv2d_12_loss: 0.0387 - conv2d_19_loss: 0.2135 - loss: 0.2537 - val_conv2d_12_loss: 0.0371 - val_conv2d_19_loss: 0.2013 - val_loss: 0.2383 - learning_rate: 1.0000e-04
2026-01-11 02:31:59,243 - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
2026-01-11 02:31:59,357 - INFO - Trained model saved to gs2_full_subsample_study_20260111_0230/train_subsample_64_groups_64/trial_1/baseline_run/01-11-2026-02.31.43_baseline_gs2/baseline_model.h5
2026-01-11 02:31:59,357 - INFO - 
[5/6] Performing inference and stitching...
[1m1/8[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m5s[0m 795ms/step[1m8/8[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 5ms/step  
2026-01-11 02:32:02,103 - INFO - Stitched object shape: (1, 228, 228, 1, 1)
2026-01-11 02:32:02,103 - INFO - 
[6/6] Evaluating reconstruction and saving results...
2026-01-11 02:32:02,103 - INFO - Aligning ground truth to match reconstruction bounds...
2026-01-11 02:32:02,103 - INFO - --- Aligning reconstruction with ground truth for evaluation ---
2026-01-11 02:32:02,103 - INFO - Calculated ground truth crop region: rows [107:207], cols [23:205]
2026-01-11 02:32:02,103 - INFO - Initial shapes: Recon=(228, 228), Cropped GT=(100, 182)
2026-01-11 02:32:02,104 - INFO - Center-cropping from (228, 228) to (100, 182)
2026-01-11 02:32:02,104 - INFO - Final aligned shape: (100, 182)
2026-01-11 02:32:02,104 - INFO - --- Alignment complete ---
2026-01-11 02:32:02,104 - INFO - Final evaluation shapes: Reconstruction=(1, 100, 182, 1), Ground Truth=(100, 182, 1)
2026-01-11 02:32:02,121 - INFO - Evaluation Metrics (Amplitude, Phase):
2026-01-11 02:32:02,122 - INFO -   MAE:  (np.float32(0.8778081), np.float64(0.243223555676886))
2026-01-11 02:32:02,122 - INFO -   PSNR: (47.65073433241679, 59.36416376157225)
2026-01-11 02:32:02,155 - INFO - Metrics and reconstruction images saved.
2026-01-11 02:32:02,155 - INFO - 
--- Baseline script finished successfully. ---
Amplitude normalization scale factor: 4.407824
mean scale adjustment: 1
mean scale adjustment: 1
DEBUG eval_reconstruction []: amp_target stats: mean=0.563449, std=0.045413, shape=(96, 178, 1)
DEBUG eval_reconstruction []: amp_pred stats: mean=0.127829, std=0.239201, shape=(96, 178, 1)
DEBUG eval_reconstruction []: phi_target stats: mean=0.000000, std=0.272163, shape=(96, 178)
DEBUG eval_reconstruction []: phi_pred stats: mean=-0.000000, std=0.031594, shape=(96, 178)
performed by index method
performed by index method
performed by index method
mean scale adjustment: 1
mean scale adjustment: 1
Phase preprocessing: plane-fitted range [-0.678, 0.413] -> scaled range [0.392, 0.566]
performed by index method
performed by index method
performed by index method
Amplitude normalization scale factor: 4.407824
mean scale adjustment: 1
mean scale adjustment: 1
DEBUG eval_reconstruction [baseline_gs2]: amp_target stats: mean=0.563449, std=0.045413, shape=(96, 178, 1)
DEBUG eval_reconstruction [baseline_gs2]: amp_pred stats: mean=0.127829, std=0.239201, shape=(96, 178, 1)
DEBUG eval_reconstruction [baseline_gs2]: phi_target stats: mean=0.000000, std=0.272163, shape=(96, 178)
DEBUG eval_reconstruction [baseline_gs2]: phi_pred stats: mean=-0.000000, std=0.031594, shape=(96, 178)
performed by index method
performed by index method
performed by index method
mean scale adjustment: 1
mean scale adjustment: 1
Phase preprocessing: plane-fitted range [-0.678, 0.413] -> scaled range [0.392, 0.566]
performed by index method
performed by index method
performed by index method
[2026-01-11 02:32:03] SUCCESS: Baseline training (subsample=64, groups=64, trial=1)
[2026-01-11 02:32:03] Completed training for train_groups=64 (Trial 1/1)
[2026-01-11 02:32:03] Completed all trials for train_groups=64
[2026-01-11 02:32:03] Model training phase completed
[2026-01-11 02:32:03] === STEP 3: Model Comparison ===
[2026-01-11 02:32:03] Running comparisons for train_subsample=64, train_groups=64 (1 trials)
[2026-01-11 02:32:03] EXECUTING: Model comparison (train_subsample=64, train_groups=64, trial=1)
[2026-01-11 02:32:03] COMMAND: USE_XLA_TRANSLATE=0 USE_XLA_COMPILE=0 bash scripts/run_comparison.sh \
                'prepare_1e4_photons_5k/dataset/train.npz' \
                'prepare_1e4_photons_5k/dataset/test.npz' \
                'gs2_full_subsample_study_20260111_0230/train_subsample_64_groups_64/trial_1' \
                --skip-training \
                --pinn-model 'gs2_full_subsample_study_20260111_0230/train_subsample_64_groups_64/trial_1/pinn_run' \
                --baseline-model 'gs2_full_subsample_study_20260111_0230/train_subsample_64_groups_64/trial_1/baseline_run/01-11-2026-02.31.43_baseline_gs2' \
                --n-test-subsample 2424 \
                --n-test-groups 606 \
                --neighbor-count 7
Creating output directories...
==========================================
Starting PtychoPINN vs Baseline Comparison
==========================================
Train data: prepare_1e4_photons_5k/dataset/train.npz
Test data: prepare_1e4_photons_5k/dataset/test.npz
Output directory: gs2_full_subsample_study_20260111_0230/train_subsample_64_groups_64/trial_1
Config file: configs/comparison_config.yaml
Training groups: 512 (from config)
Test subsample: 2424 images
Test groups: 606

Skipping training, using existing models:
  PtychoPINN: gs2_full_subsample_study_20260111_0230/train_subsample_64_groups_64/trial_1/pinn_run
  Baseline: gs2_full_subsample_study_20260111_0230/train_subsample_64_groups_64/trial_1/baseline_run/01-11-2026-02.31.43_baseline_gs2

Step 3/3: Running comparison analysis (subsample=2424, groups=606)...
---------------------------------------
2026-01-11 02:32:03.972373: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1768127523.983626  924517 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1768127523.987459  924517 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1768127523.997427  924517 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1768127523.997437  924517 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1768127523.997438  924517 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1768127523.997439  924517 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2026-01-11 02:32:04.000219: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2026-01-11 02:32:06,290 - INFO - Configuration: phase_align_method='plane', frc_sigma=0.0
2026-01-11 02:32:06,290 - INFO - Registration: enabled
2026-01-11 02:32:06,290 - INFO - NPZ output: raw=enabled, aligned=enabled
2026-01-11 02:32:06,290 - INFO - Loading PtychoPINN inference model (diffraction_to_obj) to restore configuration...
2026-01-11 02:32:06,290 - INFO - Loading model from: gs2_full_subsample_study_20260111_0230/train_subsample_64_groups_64/trial_1/pinn_run
I0000 00:00:1768127526.534980  924517 gpu_process_state.cc:208] Using CUDA malloc Async allocator for GPU: 0
I0000 00:00:1768127526.536310  924517 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22259 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:04:00.0, compute capability: 8.6
/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'center_mask_layer', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.
  warnings.warn(
2026-01-11 02:32:07,249 - ERROR - /home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'center_mask_layer', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.
  warnings.warn(
/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'center_mask_layer_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.
  warnings.warn(
2026-01-11 02:32:07,250 - ERROR - /home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'center_mask_layer_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.
  warnings.warn(
2026-01-11 02:32:07,904 - INFO - Successfully loaded model from gs2_full_subsample_study_20260111_0230/train_subsample_64_groups_64/trial_1/pinn_run
2026-01-11 02:32:07,906 - INFO - Restored configuration from model: gridsize=2, N=64
2026-01-11 02:32:07,906 - INFO - Loading test data from prepare_1e4_photons_5k/dataset/test.npz with restored gridsize=2...
2026-01-11 02:32:07,906 - INFO - Using independent sampling control: n_subsample=2424, n_groups=606
2026-01-11 02:32:07,906 - WARNING - n_test_subsample (2424) > n_test_groups (606), may use more data than expected
2026-01-11 02:32:07,906 - INFO - Loading data from prepare_1e4_photons_5k/dataset/test.npz with n_images=606, n_subsample=2424
2026-01-11 02:32:08,205 - INFO - Independent sampling: subsampling 2424 images from 2424 total
diff3d shape: (2424, 64, 64)2026-01-11 02:32:08,221 - INFO - diff3d shape: (2424, 64, 64)

probeGuess shape: (64, 64)2026-01-11 02:32:08,221 - INFO - probeGuess shape: (64, 64)

scan_index shape: (2424,)2026-01-11 02:32:08,221 - INFO - scan_index shape: (2424,)

objectGuess shape: (232, 232)2026-01-11 02:32:08,221 - INFO - objectGuess shape: (232, 232)

xcoords shape: (2424,)2026-01-11 02:32:08,221 - INFO - xcoords shape: (2424,)

ycoords shape: (2424,)2026-01-11 02:32:08,221 - INFO - ycoords shape: (2424,)

xcoords_start shape: (2424,)2026-01-11 02:32:08,221 - INFO - xcoords_start shape: (2424,)

ycoords_start shape: (2424,)2026-01-11 02:32:08,221 - INFO - ycoords_start shape: (2424,)

2026-01-11 02:32:08,222 - INFO - Final configuration: gridsize=2, N=64, n_images=2424
2026-01-11 02:32:08,222 - INFO - Using stitch_crop_size M=20 (N=64)
2026-01-11 02:32:08,222 - INFO - Loading Baseline model from gs2_full_subsample_study_20260111_0230/train_subsample_64_groups_64/trial_1/baseline_run/01-11-2026-02.31.43_baseline_gs2...
2026-01-11 02:32:08,222 - INFO - Found baseline model at: gs2_full_subsample_study_20260111_0230/train_subsample_64_groups_64/trial_1/baseline_run/01-11-2026-02.31.43_baseline_gs2/baseline_model.h5
2026-01-11 02:32:08,315 - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
2026-01-11 02:32:08,320 - INFO - Running two-way comparison (PtychoPINN vs. Baseline)
2026-01-11 02:32:08,320 - INFO - Running inference with PtychoPINN (diffraction_to_obj)...
2026-01-11 02:32:08,320 - INFO - Using single-shot PINN inference: 2424 groups (batch_size=32)
2026-01-11 02:32:08,320 - INFO - [OVERSAMPLING DEBUG] generate_grouped_data called with: nsamples=606, n_points=2424, C=4, K=7
2026-01-11 02:32:08,320 - INFO - [OVERSAMPLING DEBUG] Parameters: gridsize=2, N=64, sequential_sampling=False
2026-01-11 02:32:08,320 - INFO - [OVERSAMPLING DEBUG] Oversampling flags: enable_oversampling=False, neighbor_pool_size=None, effective_K=7
2026-01-11 02:32:08,320 - INFO - [OVERSAMPLING DEBUG] Oversampling check: nsamples > n_points = 606 > 2424 = False
2026-01-11 02:32:08,321 - INFO - [OVERSAMPLING DEBUG] Oversampling check: C > 1 = 4 > 1 = True
2026-01-11 02:32:08,321 - INFO - [OVERSAMPLING DEBUG] needs_oversampling = False
DEBUG:2026-01-11 02:32:08,321 - INFO - DEBUG:
 nsamples: 606, gridsize: 2 (using efficient random sample-then-group strategy)2026-01-11 02:32:08,321 - INFO - nsamples: 606, gridsize: 2 (using efficient random sample-then-group strategy)

2026-01-11 02:32:08,321 - INFO - Using efficient random sampling strategy for gridsize=2
2026-01-11 02:32:08,321 - INFO - [OVERSAMPLING DEBUG] Taking efficient branch: standard sample-then-group
2026-01-11 02:32:08,321 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently called with: nsamples=606, K=7, C=4
2026-01-11 02:32:08,321 - INFO - Generating 606 groups efficiently from 2424 points (K=7, C=4)
2026-01-11 02:32:08,321 - INFO - [OVERSAMPLING DEBUG] Standard case: using 606 groups from 2424 points
2026-01-11 02:32:08,321 - INFO - [OVERSAMPLING DEBUG] Randomly sampled 606 seed points
2026-01-11 02:32:08,321 - INFO - Sampled 606 seed points from 2424 total points
2026-01-11 02:32:08,325 - INFO - [OVERSAMPLING DEBUG] _generate_groups_efficiently completed: generated 606 groups
2026-01-11 02:32:08,325 - INFO - Successfully generated 606 groups with shape (606, 4)
2026-01-11 02:32:08,325 - INFO - [OVERSAMPLING DEBUG] Generated 606 groups in total
2026-01-11 02:32:08,325 - INFO - Generated 606 groups efficiently
INFO: Using pre-computed 'Y' array from the input file.2026-01-11 02:32:08,331 - INFO - INFO: Using pre-computed 'Y' array from the input file.

neighbor-sampled diffraction shape2026-01-11 02:32:08,413 - INFO - neighbor-sampled diffraction shape
 (606, 64, 64, 4)2026-01-11 02:32:08,413 - INFO - (606, 64, 64, 4)

loader: using provided ground truth patches.2026-01-11 02:32:08,514 - INFO - loader: using provided ground truth patches.

INFO:2026-01-11 02:32:08,834 - INFO - INFO:
 None2026-01-11 02:32:08,835 - INFO - None

<PtychoDataContainer X=(606, 64, 64, 4) mean=0.406 Y_I=(606, 64, 64, 4) mean=0.563 Y_phi=(606, 64, 64, 4) mean=-0.032 coords_nominal=(606, 1, 2, 4) mean=-0.000 coords_true=(606, 1, 2, 4) mean=-0.000 probe=(64, 64) mean_amplitude=0.006 norm_Y_I=<scalar> nn_indices=(606, 4) global_offsets=(606, 1, 2, 1) local_offsets=(606, 1, 2, 4)>2026-01-11 02:32:08,840 - INFO - <PtychoDataContainer X=(606, 64, 64, 4) mean=0.406 Y_I=(606, 64, 64, 4) mean=0.563 Y_phi=(606, 64, 64, 4) mean=-0.032 coords_nominal=(606, 1, 2, 4) mean=-0.000 coords_true=(606, 1, 2, 4) mean=-0.000 probe=(64, 64) mean_amplitude=0.006 norm_Y_I=<scalar> nn_indices=(606, 4) global_offsets=(606, 1, 2, 1) local_offsets=(606, 1, 2, 4)>

2026-01-11 02:32:08,902 - INFO - Gridsize validated: params.cfg['gridsize']=2 matches 4 channels
DEBUG _flat_to_channel: gridsize from parameter: 22026-01-11 02:32:09,196 - INFO - DEBUG _flat_to_channel: gridsize from parameter: 2

DEBUG _flat_to_channel: N from parameter: 782026-01-11 02:32:09,196 - INFO - DEBUG _flat_to_channel: N from parameter: 78

DEBUG _flat_to_channel: input shape=(128, 78, 78, 1), reshaping to (-1, 4, 78, 78)2026-01-11 02:32:09,196 - INFO - DEBUG _flat_to_channel: input shape=(128, 78, 78, 1), reshaping to (-1, 4, 78, 78)

DEBUG _flat_to_channel: gridsize from parameter: 22026-01-11 02:32:09,363 - INFO - DEBUG _flat_to_channel: gridsize from parameter: 2

DEBUG _flat_to_channel: N from parameter: 782026-01-11 02:32:09,363 - INFO - DEBUG _flat_to_channel: N from parameter: 78

DEBUG _flat_to_channel: input shape=(128, 78, 78, 1), reshaping to (-1, 4, 78, 78)2026-01-11 02:32:09,363 - INFO - DEBUG _flat_to_channel: input shape=(128, 78, 78, 1), reshaping to (-1, 4, 78, 78)

DEBUG _flat_to_channel: gridsize from parameter: 22026-01-11 02:32:09,515 - INFO - DEBUG _flat_to_channel: gridsize from parameter: 2

DEBUG _flat_to_channel: N from parameter: 782026-01-11 02:32:09,516 - INFO - DEBUG _flat_to_channel: N from parameter: 78

DEBUG _flat_to_channel: input shape=(128, 78, 78, 1), reshaping to (-1, 4, 78, 78)2026-01-11 02:32:09,516 - INFO - DEBUG _flat_to_channel: input shape=(128, 78, 78, 1), reshaping to (-1, 4, 78, 78)

DEBUG _flat_to_channel: gridsize from parameter: 22026-01-11 02:32:09,680 - INFO - DEBUG _flat_to_channel: gridsize from parameter: 2

DEBUG _flat_to_channel: N from parameter: 782026-01-11 02:32:09,680 - INFO - DEBUG _flat_to_channel: N from parameter: 78

DEBUG _flat_to_channel: input shape=(128, 78, 78, 1), reshaping to (-1, 4, 78, 78)2026-01-11 02:32:09,680 - INFO - DEBUG _flat_to_channel: input shape=(128, 78, 78, 1), reshaping to (-1, 4, 78, 78)

WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1768127530.133091  924651 service.cc:152] XLA service 0x72585000cfc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1768127530.133119  924651 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6
2026-01-11 02:32:10.193202: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2026-01-11 02:32:10.304490: W tensorflow/core/framework/op_kernel.cc:1857] OP_REQUIRES failed at sequence_ops.cc:98 : INVALID_ARGUMENT: Input 0 to node `functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_0` with op Range must be a compile-time constant.

XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

2026-01-11 02:32:10.304589: W tensorflow/core/framework/op_kernel.cc:1857] OP_REQUIRES failed at if_op.cc:285 : INVALID_ARGUMENT: Input 0 to node `functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_0` with op Range must be a compile-time constant.

XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[{{node functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_0}}]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_while_cond_true_4774_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_while_cond_tfg_inlined_functional_1_1_padded_obj_2_1_cond_while_tfg_inlined_functional_1_1_padded_obj_2_1_cond_1_1_0_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

2026-01-11 02:32:10.304633: W tensorflow/core/framework/op_kernel.cc:1857] OP_REQUIRES failed at while_op.cc:403 : INVALID_ARGUMENT: Input 0 to node `functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_0` with op Range must be a compile-time constant.

XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[{{node functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_0}}]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_while_cond_true_4774_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_while_cond_tfg_inlined_functional_1_1_padded_obj_2_1_cond_while_tfg_inlined_functional_1_1_padded_obj_2_1_cond_1_1_0_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_tfg_inlined_functional_1_1/padded_obj_2_1/cond_1_1]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_while_body_4740_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_while_tfg_inlined_functional_1_1_padded_obj_2_1_cond_1_1_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

2026-01-11 02:32:10.304694: W tensorflow/core/framework/op_kernel.cc:1857] OP_REQUIRES failed at if_op.cc:291 : INVALID_ARGUMENT: Input 0 to node `functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_0` with op Range must be a compile-time constant.

XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[{{node functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_0}}]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_while_cond_true_4774_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_while_cond_tfg_inlined_functional_1_1_padded_obj_2_1_cond_while_tfg_inlined_functional_1_1_padded_obj_2_1_cond_1_1_0_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_tfg_inlined_functional_1_1/padded_obj_2_1/cond_1_1]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_while_body_4740_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_while_tfg_inlined_functional_1_1_padded_obj_2_1_cond_1_1_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[functional_1_1/padded_obj_2_1/cond/while_tfg_inlined_functional_1_1/padded_obj_2_1/cond_1]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_false_4478_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_1_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
File "Documents/PtychoPINN/scripts/compare_models.py", line 1774, in <module>
File "Documents/PtychoPINN/scripts/compare_models.py", line 1166, in main
File "Documents/PtychoPINN/ptycho/workflows/components.py", line 163, in predict
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 566, in predict
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 260, in one_step_on_data_distributed
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 250, in one_step_on_data
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 105, in predict_step
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py", line 941, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/operation.py", line 59, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 156, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/models/functional.py", line 183, in call
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/function.py", line 206, in _run_through_graph
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/models/functional.py", line 644, in call
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py", line 941, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/operation.py", line 59, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 156, in error_handler
File "Documents/PtychoPINN/ptycho/custom_layers.py", line 172, in call
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1100, in reassemble_patches
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 692, in newf
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1350, in reassemble_patches_position_batched_real
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1027, in _reassemble_position_batched

2026-01-11 02:32:10.308763: W tensorflow/core/framework/op_kernel.cc:1857] OP_REQUIRES failed at xla_ops.cc:591 : INVALID_ARGUMENT: Input 0 to node `functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_0` with op Range must be a compile-time constant.

XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[{{node functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_0}}]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_while_cond_true_4774_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_while_cond_tfg_inlined_functional_1_1_padded_obj_2_1_cond_while_tfg_inlined_functional_1_1_padded_obj_2_1_cond_1_1_0_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_tfg_inlined_functional_1_1/padded_obj_2_1/cond_1_1]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_while_body_4740_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_while_tfg_inlined_functional_1_1_padded_obj_2_1_cond_1_1_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[functional_1_1/padded_obj_2_1/cond/while_tfg_inlined_functional_1_1/padded_obj_2_1/cond_1]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_false_4478_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_1_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
File "Documents/PtychoPINN/scripts/compare_models.py", line 1774, in <module>
File "Documents/PtychoPINN/scripts/compare_models.py", line 1166, in main
File "Documents/PtychoPINN/ptycho/workflows/components.py", line 163, in predict
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 566, in predict
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 260, in one_step_on_data_distributed
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 250, in one_step_on_data
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 105, in predict_step
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py", line 941, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/operation.py", line 59, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 156, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/models/functional.py", line 183, in call
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/function.py", line 206, in _run_through_graph
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/models/functional.py", line 644, in call
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py", line 941, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/operation.py", line 59, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 156, in error_handler
File "Documents/PtychoPINN/ptycho/custom_layers.py", line 172, in call
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1100, in reassemble_patches
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 692, in newf
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1350, in reassemble_patches_position_batched_real
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1027, in _reassemble_position_batched

	 [[functional_1_1/padded_obj_2_1/cond]]
	tf2xla conversion failed while converting __inference_one_step_on_data_7352[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.
2026-01-11 02:32:10.308799: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Input 0 to node `functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_0` with op Range must be a compile-time constant.

XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[{{node functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_0}}]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_while_cond_true_4774_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_while_cond_tfg_inlined_functional_1_1_padded_obj_2_1_cond_while_tfg_inlined_functional_1_1_padded_obj_2_1_cond_1_1_0_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_tfg_inlined_functional_1_1/padded_obj_2_1/cond_1_1]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_while_body_4740_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_while_tfg_inlined_functional_1_1_padded_obj_2_1_cond_1_1_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[functional_1_1/padded_obj_2_1/cond/while_tfg_inlined_functional_1_1/padded_obj_2_1/cond_1]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_false_4478_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_1_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
File "Documents/PtychoPINN/scripts/compare_models.py", line 1774, in <module>
File "Documents/PtychoPINN/scripts/compare_models.py", line 1166, in main
File "Documents/PtychoPINN/ptycho/workflows/components.py", line 163, in predict
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 566, in predict
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 260, in one_step_on_data_distributed
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 250, in one_step_on_data
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 105, in predict_step
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py", line 941, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/operation.py", line 59, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 156, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/models/functional.py", line 183, in call
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/function.py", line 206, in _run_through_graph
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/models/functional.py", line 644, in call
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py", line 941, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/operation.py", line 59, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 156, in error_handler
File "Documents/PtychoPINN/ptycho/custom_layers.py", line 172, in call
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1100, in reassemble_patches
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 692, in newf
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1350, in reassemble_patches_position_batched_real
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1027, in _reassemble_position_batched

	 [[functional_1_1/padded_obj_2_1/cond]]
	tf2xla conversion failed while converting __inference_one_step_on_data_7352[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.
	 [[StatefulPartitionedCall]]
2026-01-11 02:32:10,385 - CRITICAL - Uncaught exception
Traceback (most recent call last):
  File "/home/ollie/Documents/PtychoPINN/scripts/compare_models.py", line 1774, in <module>
    main()
  File "/home/ollie/Documents/PtychoPINN/scripts/compare_models.py", line 1166, in main
    pinn_patches = pinn_model.predict(
                   ^^^^^^^^^^^^^^^^^^^
  File "/home/ollie/Documents/PtychoPINN/ptycho/workflows/components.py", line 163, in predict
    return self._model.predict(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/tensorflow/python/eager/execute.py", line 59, in quick_execute
    except TypeError as e:
tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:

Detected at node functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_0 defined at (most recent call last):
<stack traces unavailable>
Input 0 to node `functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_0` with op Range must be a compile-time constant.

XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[{{node functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_0}}]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_while_cond_true_4774_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_while_cond_tfg_inlined_functional_1_1_padded_obj_2_1_cond_while_tfg_inlined_functional_1_1_padded_obj_2_1_cond_1_1_0_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_tfg_inlined_functional_1_1/padded_obj_2_1/cond_1_1]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_while_body_4740_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_while_tfg_inlined_functional_1_1_padded_obj_2_1_cond_1_1_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[functional_1_1/padded_obj_2_1/cond/while_tfg_inlined_functional_1_1/padded_obj_2_1/cond_1]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_false_4478_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_1_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
File "Documents/PtychoPINN/scripts/compare_models.py", line 1774, in <module>
File "Documents/PtychoPINN/scripts/compare_models.py", line 1166, in main
File "Documents/PtychoPINN/ptycho/workflows/components.py", line 163, in predict
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 566, in predict
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 260, in one_step_on_data_distributed
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 250, in one_step_on_data
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 105, in predict_step
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py", line 941, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/operation.py", line 59, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 156, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/models/functional.py", line 183, in call
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/function.py", line 206, in _run_through_graph
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/models/functional.py", line 644, in call
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py", line 941, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/operation.py", line 59, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 156, in error_handler
File "Documents/PtychoPINN/ptycho/custom_layers.py", line 172, in call
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1100, in reassemble_patches
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 692, in newf
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1350, in reassemble_patches_position_batched_real
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1027, in _reassemble_position_batched

	 [[functional_1_1/padded_obj_2_1/cond]]
	tf2xla conversion failed while converting __inference_one_step_on_data_7352[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.
	 [[StatefulPartitionedCall]] [Op:__inference_one_step_on_data_distributed_7445]
Traceback (most recent call last):
2026-01-11 02:32:10,386 - ERROR - Traceback (most recent call last):
  File "/home/ollie/Documents/PtychoPINN/scripts/compare_models.py", line 1774, in <module>
2026-01-11 02:32:10,386 - ERROR - File "/home/ollie/Documents/PtychoPINN/scripts/compare_models.py", line 1774, in <module>
    main()2026-01-11 02:32:10,386 - ERROR - main()

  File "/home/ollie/Documents/PtychoPINN/scripts/compare_models.py", line 1166, in main
2026-01-11 02:32:10,386 - ERROR - File "/home/ollie/Documents/PtychoPINN/scripts/compare_models.py", line 1166, in main
    pinn_patches = pinn_model.predict(2026-01-11 02:32:10,387 - ERROR - pinn_patches = pinn_model.predict(

                   ^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^

  File "/home/ollie/Documents/PtychoPINN/ptycho/workflows/components.py", line 163, in predict
2026-01-11 02:32:10,387 - ERROR - File "/home/ollie/Documents/PtychoPINN/ptycho/workflows/components.py", line 163, in predict
    return self._model.predict(*args, **kwargs)2026-01-11 02:32:10,387 - ERROR - return self._model.predict(*args, **kwargs)

           ^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,387 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^

  File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
2026-01-11 02:32:10,388 - ERROR - File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None2026-01-11 02:32:10,388 - ERROR - raise e.with_traceback(filtered_tb) from None

  File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
2026-01-11 02:32:10,388 - ERROR - File "/home/ollie/miniconda3/envs/ptycho311/lib/python3.11/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,2026-01-11 02:32:10,388 - ERROR - tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,

  ^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,388 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^
^2026-01-11 02:32:10,389 - ERROR - ^

tensorflow.python.framework.errors_impl2026-01-11 02:32:10,389 - ERROR - tensorflow.python.framework.errors_impl
.2026-01-11 02:32:10,389 - ERROR - .
InvalidArgumentError2026-01-11 02:32:10,389 - ERROR - InvalidArgumentError
: 2026-01-11 02:32:10,389 - ERROR - :
Graph execution error:

Detected at node functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_0 defined at (most recent call last):
<stack traces unavailable>
Input 0 to node `functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_0` with op Range must be a compile-time constant.

XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[{{node functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_0}}]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_while_cond_true_4774_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_while_cond_tfg_inlined_functional_1_1_padded_obj_2_1_cond_while_tfg_inlined_functional_1_1_padded_obj_2_1_cond_1_1_0_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_tfg_inlined_functional_1_1/padded_obj_2_1/cond_1_1]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_while_body_4740_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_while_tfg_inlined_functional_1_1_padded_obj_2_1_cond_1_1_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[functional_1_1/padded_obj_2_1/cond/while_tfg_inlined_functional_1_1/padded_obj_2_1/cond_1]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_false_4478_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_1_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
File "Documents/PtychoPINN/scripts/compare_models.py", line 1774, in <module>
File "Documents/PtychoPINN/scripts/compare_models.py", line 1166, in main
File "Documents/PtychoPINN/ptycho/workflows/components.py", line 163, in predict
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 566, in predict
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 260, in one_step_on_data_distributed
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 250, in one_step_on_data
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 105, in predict_step
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py", line 941, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/operation.py", line 59, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 156, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/models/functional.py", line 183, in call
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/function.py", line 206, in _run_through_graph
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/models/functional.py", line 644, in call
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py", line 941, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/operation.py", line 59, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 156, in error_handler
File "Documents/PtychoPINN/ptycho/custom_layers.py", line 172, in call
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1100, in reassemble_patches
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 692, in newf
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1350, in reassemble_patches_position_batched_real
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1027, in _reassemble_position_batched

	 [[functional_1_1/padded_obj_2_1/cond]]
	tf2xla conversion failed while converting __inference_one_step_on_data_7352[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.
	 [[StatefulPartitionedCall]] [Op:__inference_one_step_on_data_distributed_7445]2026-01-11 02:32:10,389 - ERROR - Graph execution error:

Detected at node functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_0 defined at (most recent call last):
<stack traces unavailable>
Input 0 to node `functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_0` with op Range must be a compile-time constant.

XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[{{node functional_1_1/padded_obj_2_1/cond/while/cond/range_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while/cond_0}}]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_while_cond_true_4774_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_while_cond_tfg_inlined_functional_1_1_padded_obj_2_1_cond_while_tfg_inlined_functional_1_1_padded_obj_2_1_cond_1_1_0_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[functional_1_1/padded_obj_2_1/cond/while/cond_tfg_inlined_functional_1_1/padded_obj_2_1/cond/while_tfg_inlined_functional_1_1/padded_obj_2_1/cond_1_1]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_while_body_4740_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_while_tfg_inlined_functional_1_1_padded_obj_2_1_cond_1_1_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
dummy_file_name:10:dummy_function_name

	 [[functional_1_1/padded_obj_2_1/cond/while_tfg_inlined_functional_1_1/padded_obj_2_1/cond_1]]
	tf2xla conversion failed while converting functional_1_1_padded_obj_2_1_cond_false_4478_tfg_region_specialized_functional_1_1_padded_obj_2_1_cond_1_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
File "Documents/PtychoPINN/scripts/compare_models.py", line 1774, in <module>
File "Documents/PtychoPINN/scripts/compare_models.py", line 1166, in main
File "Documents/PtychoPINN/ptycho/workflows/components.py", line 163, in predict
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 566, in predict
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 260, in one_step_on_data_distributed
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 250, in one_step_on_data
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py", line 105, in predict_step
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py", line 941, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/operation.py", line 59, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 156, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/models/functional.py", line 183, in call
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/function.py", line 206, in _run_through_graph
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/models/functional.py", line 644, in call
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/layers/layer.py", line 941, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/ops/operation.py", line 59, in __call__
File "miniconda3/envs/ptycho311/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 156, in error_handler
File "Documents/PtychoPINN/ptycho/custom_layers.py", line 172, in call
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1100, in reassemble_patches
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 692, in newf
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1350, in reassemble_patches_position_batched_real
File "Documents/PtychoPINN/ptycho/tf_helper.py", line 1027, in _reassemble_position_batched

	 [[functional_1_1/padded_obj_2_1/cond]]
	tf2xla conversion failed while converting __inference_one_step_on_data_7352[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.
	 [[StatefulPartitionedCall]] [Op:__inference_one_step_on_data_distributed_7445]

[2026-01-11 02:32:11] SUCCESS: Model comparison (train_subsample=64, train_groups=64, trial=1)
[2026-01-11 02:32:11] Completed comparisons for train_groups=64
[2026-01-11 02:32:11] Model comparison phase completed
[2026-01-11 02:32:11] === STEP 4: Results Aggregation ===
[2026-01-11 02:32:11] EXECUTING: PSNR phase generalization plot
[2026-01-11 02:32:11] COMMAND: python scripts/studies/aggregate_and_plot_results.py \
        'gs2_full_subsample_study_20260111_0230' \
        --metric psnr \
        --part phase \
        --output psnr_phase_generalization.png
02:32:11 - INFO - Processing study directory: gs2_full_subsample_study_20260111_0230
02:32:11 - INFO - Analyzing psnr_phase
02:32:11 - ERROR - Failed to process results: No valid comparison files found in gs2_full_subsample_study_20260111_0230
[2026-01-11 02:32:11] SUCCESS: PSNR phase generalization plot
[2026-01-11 02:32:11] EXECUTING: FRC amplitude generalization plot
[2026-01-11 02:32:11] COMMAND: python scripts/studies/aggregate_and_plot_results.py \
        'gs2_full_subsample_study_20260111_0230' \
        --metric frc50 \
        --part amp \
        --output frc50_amp_generalization.png
02:32:12 - INFO - Processing study directory: gs2_full_subsample_study_20260111_0230
02:32:12 - INFO - Analyzing frc50_amp
02:32:12 - ERROR - Failed to process results: No valid comparison files found in gs2_full_subsample_study_20260111_0230
[2026-01-11 02:32:12] SUCCESS: FRC amplitude generalization plot
[2026-01-11 02:32:12] EXECUTING: MAE amplitude generalization plot
[2026-01-11 02:32:12] COMMAND: python scripts/studies/aggregate_and_plot_results.py \
        'gs2_full_subsample_study_20260111_0230' \
        --metric mae \
        --part amp \
        --output mae_amp_generalization.png
02:32:12 - INFO - Processing study directory: gs2_full_subsample_study_20260111_0230
02:32:12 - INFO - Analyzing mae_amp
02:32:12 - ERROR - Failed to process results: No valid comparison files found in gs2_full_subsample_study_20260111_0230
[2026-01-11 02:32:12] SUCCESS: MAE amplitude generalization plot
[2026-01-11 02:32:12] EXECUTING: SSIM amplitude generalization plot
[2026-01-11 02:32:12] COMMAND: python scripts/studies/aggregate_and_plot_results.py \
        'gs2_full_subsample_study_20260111_0230' \
        --metric ssim \
        --part amp \
        --output ssim_amp_generalization.png
02:32:13 - INFO - Processing study directory: gs2_full_subsample_study_20260111_0230
02:32:13 - INFO - Analyzing ssim_amp
02:32:13 - ERROR - Failed to process results: No valid comparison files found in gs2_full_subsample_study_20260111_0230
[2026-01-11 02:32:13] SUCCESS: SSIM amplitude generalization plot
[2026-01-11 02:32:13] EXECUTING: SSIM phase generalization plot
[2026-01-11 02:32:13] COMMAND: python scripts/studies/aggregate_and_plot_results.py \
        'gs2_full_subsample_study_20260111_0230' \
        --metric ssim \
        --part phase \
        --output ssim_phase_generalization.png
02:32:13 - INFO - Processing study directory: gs2_full_subsample_study_20260111_0230
02:32:13 - INFO - Analyzing ssim_phase
02:32:13 - ERROR - Failed to process results: No valid comparison files found in gs2_full_subsample_study_20260111_0230
[2026-01-11 02:32:13] SUCCESS: SSIM phase generalization plot
[2026-01-11 02:32:13] EXECUTING: MS-SSIM amplitude generalization plot
[2026-01-11 02:32:13] COMMAND: python scripts/studies/aggregate_and_plot_results.py \
        'gs2_full_subsample_study_20260111_0230' \
        --metric ms_ssim \
        --part amp \
        --output ms_ssim_amp_generalization.png
02:32:14 - INFO - Processing study directory: gs2_full_subsample_study_20260111_0230
02:32:14 - INFO - Analyzing ms_ssim_amp
02:32:14 - ERROR - Failed to process results: No valid comparison files found in gs2_full_subsample_study_20260111_0230
[2026-01-11 02:32:14] SUCCESS: MS-SSIM amplitude generalization plot
[2026-01-11 02:32:14] EXECUTING: MS-SSIM phase generalization plot
[2026-01-11 02:32:14] COMMAND: python scripts/studies/aggregate_and_plot_results.py \
        'gs2_full_subsample_study_20260111_0230' \
        --metric ms_ssim \
        --part phase \
        --output ms_ssim_phase_generalization.png
02:32:14 - INFO - Processing study directory: gs2_full_subsample_study_20260111_0230
02:32:14 - INFO - Analyzing ms_ssim_phase
02:32:14 - ERROR - Failed to process results: No valid comparison files found in gs2_full_subsample_study_20260111_0230
[2026-01-11 02:32:14] SUCCESS: MS-SSIM phase generalization plot
[2026-01-11 02:32:14] Results aggregation completed
[2026-01-11 02:32:14] === Generating Summary Report ===
[2026-01-11 02:32:14] Summary report generated: gs2_full_subsample_study_20260111_0230/STUDY_SUMMARY.md
[2026-01-11 02:32:14] === Study Completed Successfully ===
[2026-01-11 02:32:14] Training sizes tested: 1
[2026-01-11 02:32:14] Trials per size: 1
[2026-01-11 02:32:14] Total trials completed: 1
[2026-01-11 02:32:14] Total runtime: 00:01:04
[2026-01-11 02:32:14] Results directory: gs2_full_subsample_study_20260111_0230
[2026-01-11 02:32:14] Summary report: gs2_full_subsample_study_20260111_0230/STUDY_SUMMARY.md

ğŸ‰ Complete generalization study finished!
ğŸ“ Results: gs2_full_subsample_study_20260111_0230
ğŸ“Š Key plots:
   - gs2_full_subsample_study_20260111_0230/psnr_phase_generalization.png
   - gs2_full_subsample_study_20260111_0230/frc50_amp_generalization.png
   - gs2_full_subsample_study_20260111_0230/mae_amp_generalization.png
   - gs2_full_subsample_study_20260111_0230/ssim_amp_generalization.png
   - gs2_full_subsample_study_20260111_0230/ssim_phase_generalization.png
   - gs2_full_subsample_study_20260111_0230/ms_ssim_amp_generalization.png
   - gs2_full_subsample_study_20260111_0230/ms_ssim_phase_generalization.png
ğŸ“‹ Summary: gs2_full_subsample_study_20260111_0230/STUDY_SUMMARY.md
