Files_Requiring_Updates:
  - path: ./ptycho/loader.py
    reason: Core data container class needs to be extended
    changes_needed:
      - Create new MultiPtychoDataContainer class that can hold multiple probes and datasets 
      - Add probe_indices attribute to track which probe is used for each sample
      - Modify data loading/merging logic to handle multiple probes
      - Add method to shuffle/interleave samples for training while maintaining probe associations
    dependencies:
      - data_preprocessing.py
      - train_pinn.py
      - components.py

  - path: ./ptycho/data_preprocessing.py  
    reason: Dataset creation and merging logic needs updates
    changes_needed:
      - Modify create_ptycho_dataset to handle multiple probes
      - Add logic to interleave datasets during training data preparation
      - Update shuffle_data to maintain probe index associations
    dependencies:
      - loader.py
      - train_pinn.py

  - path: ./ptycho/train_pinn.py
    reason: Training pipeline needs to handle per-sample probes 
    changes_needed:
      - Update prepare_inputs to include probe selection based on indices
      - Modify train() to handle MultiPtychoDataContainer
      - Update eval() to use correct probe per sample
    dependencies:
      - loader.py
      - model.py

  - path: ./ptycho/model.py
    reason: Model needs to accept probe input per sample
    changes_needed:
      - Modify ProbeIllumination layer to accept probe tensor input
      - Update model architecture to take probe tensor as input
      - Remove global probe variable
    dependencies:
      - train_pinn.py
      - tf_helper.py

  - path: ./ptycho/components.py
    reason: Helper functions need updates for multiple probes
    changes_needed:
      - Update create_ptycho_data_container to handle probe lists
      - Modify data loading functions for multiple probes
      - Add validation for probe shapes/dtypes
    dependencies:
      - loader.py

Architectural_Impact_Assessment:
  - Changes introduce a new data container class that maintains backward compatibility
  - Training pipeline needs significant updates to handle per-sample probes
  - Global probe state is removed in favor of per-sample inputs
  - Dataset merging needs careful handling to maintain probe associations
  - Testing flow remains largely unchanged except for probe selection

Questions_For_Clarification:
  - Should probe indices be passed through the entire model pipeline or just used for data loading?
  - Do we need to validate that all probes in a list have compatible shapes/dtypes?
  - Should there be a validation step when merging datasets to check probe compatibility?
  - Do we need methods to extract subsets of data by probe index?
  - How should probe selection errors be handled during training/inference?