# Change Analysis Document

files_requiring_updates:
  - path: "ptychography/data/ptycho_data_container.py"
    reason: "Primary data structure needs significant modifications"
    changes_needed:
      - "Create new class extending PtychoDataContainer for multi-dataset support"  
      - "Add probe_list attribute to store multiple probes as tf.Tensor"
      - "Add probe_indices attribute matching X dimension as int64"
      - "Modify data loading to support probe index assignment"
    dependencies:
      - "Training pipeline expecting single probe"
      - "Data loading utilities"
      - "Model forward pass implementation"

  - path: "ptychography/data/data_loader.py"
    reason: "Must support loading multiple datasets with probe mapping"
    changes_needed:
      - "Add functionality to load multiple datasets"
      - "Implement probe index assignment during loading"
      - "Add shuffling logic for training data"
    dependencies:
      - "Dataset preprocessing pipeline"
      - "Training data iteration"

  - path: "ptychography/models/ptycho_model.py"
    reason: "Model needs to handle per-sample probe selection"
    changes_needed:
      - "Modify forward pass to use probe indices"  
      - "Update probe handling to select from probe list"
      - "Ensure test-time behavior maintains dataset grouping"
    dependencies:
      - "Loss calculation"
      - "Inference pipeline"

  - path: "ptychography/training/trainer.py"
    reason: "Training loop must handle multi-dataset batching"
    changes_needed:
      - "Update batch creation to handle probe indices"
      - "Implement dataset shuffling for training"
      - "Maintain unshuffled test data handling"
    dependencies:
      - "Validation pipeline"
      - "Checkpoint handling"

architectural_impact:
  - "Data Flow Changes:"
    - "Probe selection moves from container level to sample level"
    - "New multi-dataset container introduces dataset boundary tracking"
    - "Training pipeline needs to handle interleaved datasets"
  
  - "Memory Considerations:"
    - "Multiple probes stored simultaneously"
    - "Additional memory for probe indices"
    - "Potential impact on batch size limits"
  
  - "Performance Implications:"
    - "Additional probe lookup operation per sample"
    - "Shuffling overhead for training data"
    - "Increased memory bandwidth usage"

questions_for_clarification:
  - "Should there be a maximum limit on number of probes/datasets that can be merged?"
  - "Are there any special requirements for validation data handling?"
  - "Should probe indices be one-hot encoded or integer indices?"
  - "Do we need to support dynamic addition of new probes during training?"
  - "Should we implement any probe-specific data augmentation?"
  - "Are there any special requirements for checkpoint/saving format to handle multiple probes?"